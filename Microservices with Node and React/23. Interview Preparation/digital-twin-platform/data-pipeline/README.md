# Digital Twin Data Pipeline - Day 2 Implementation

## ğŸ—ï¸ **Architecture Overview**

The Day 2 data pipeline provides enterprise-grade data processing capabilities with the following components:

```mermaid
graph TB
    subgraph "IoT Edge Layer"
        A1[IoT Sensors] --> A2[MQTT Client]
    end
    
    subgraph "Message Streaming"
        B1[Mosquitto MQTT] --> B2[MQTT-Kafka Bridge] --> B3[Kafka Cluster]
    end
    
    subgraph "Stream Processing"
        C1[Stream Processor] --> C2[Data Aggregation]
        C3[Event Store] --> C4[Event Sourcing]
    end
    
    subgraph "Data Storage"
        D1[PostgreSQL] --> D2[Time-series Tables]
        D3[Event Store Tables]
    end
    
    A2 --> B1
    B3 --> C1
    B3 --> C3
    C1 --> D1
    C3 --> D1
```

## ğŸ“ **Directory Structure**

```
data-pipeline/
â”œâ”€â”€ mqtt/
â”‚   â””â”€â”€ config/          # MQTT broker configuration
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init/            # PostgreSQL initialization scripts
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bridge/          # MQTT-Kafka bridge
â”‚   â”œâ”€â”€ processor/       # Stream processing engine
â”‚   â”œâ”€â”€ eventstore/      # Event sourcing implementation
â”‚   â”œâ”€â”€ simulator/       # IoT device simulator
â”‚   â””â”€â”€ shared/          # Common utilities and models
â”œâ”€â”€ Dockerfile.bridge    # MQTT-Kafka bridge container
â”œâ”€â”€ Dockerfile.processor # Stream processor container
â”œâ”€â”€ Dockerfile.eventstore # Event store container
â”œâ”€â”€ Dockerfile.simulator # IoT simulator container
â””â”€â”€ requirements.txt     # Python dependencies
```

## ğŸš€ **Key Features**

### **Message Streaming Pipeline**
- **MQTT**: IoT device connectivity with QoS guarantees
- **Kafka**: Reliable event streaming with exactly-once semantics
- **Bridge**: Automatic MQTT to Kafka message forwarding

### **Stream Processing**
- **Real-time Aggregation**: 10-second windowing for metrics
- **Exactly-Once Processing**: Idempotent consumers with database transactions
- **Dead Letter Queues**: Error handling and message replay

### **Event Sourcing**
- **Complete Audit Trail**: Every sensor reading preserved
- **Temporal Queries**: Time-travel capabilities
- **State Reconstruction**: Rebuild machine state from events

### **Data Storage**
- **Time-series Optimization**: Partitioned tables by time
- **Efficient Indexing**: Optimized for time-range queries
- **Scalable Architecture**: Horizontal partitioning ready

## ğŸ”§ **Technology Stack**

- **MQTT**: Eclipse Mosquitto 2.0
- **Event Streaming**: Apache Kafka with Confluent Platform
- **Database**: PostgreSQL 16 with time-series extensions
- **Processing**: Python asyncio with aiokafka
- **Monitoring**: Kafka UI, pgAdmin

## ğŸ“Š **Performance Targets**

- **Throughput**: 10K+ messages/second
- **Latency**: <5s end-to-end pipeline
- **Durability**: Zero message loss
- **Availability**: 99.9% uptime
