# Interview Preparation Cheat Sheet

## Quick Reference Guide for Digital Twin Manufacturing Platform

---

# Technical Deep-Dives

## Redis Fundamentals & Patterns

*Since you're new to Redis - comprehensive coverage*

### Redis Basics

```python
# Connection and basic operations
import redis
import asyncio
import aioredis

# Async Redis client
redis_client = aioredis.from_url("redis://localhost:6379")

# Basic operations
await redis_client.set("sensor:CNC-001:temp", "85.4", ex=300)  # 5 min expiry
temp = await redis_client.get("sensor:CNC-001:temp")
await redis_client.delete("sensor:CNC-001:temp")
```

### Redis Data Types & Use Cases

```python
# Strings - Simple key-value caching
await redis_client.set("user:123:session", "active")

# Hashes - Object storage
await redis_client.hset("machine:CNC-001", mapping={
    "status": "running",
    "temperature": "85.4",
    "last_update": "2024-01-15T10:30:00Z"
})

# Lists - Message queues, recent items
await redis_client.lpush("recent_alerts", "High temperature on CNC-001")
recent = await redis_client.lrange("recent_alerts", 0, 9)  # Last 10

# Sets - Unique collections
await redis_client.sadd("active_machines", "CNC-001", "CNC-002")
active = await redis_client.smembers("active_machines")

# Sorted Sets - Leaderboards, time-series
await redis_client.zadd("machine_scores", {"CNC-001": 95.5, "CNC-002": 87.2})
top_machines = await redis_client.zrevrange("machine_scores", 0, 4)
```

### Redis Pub/Sub for Real-time Communication

```python
# Publisher (in FastAPI endpoint)
async def broadcast_sensor_update(machine_id: str, data: dict):
    await redis_client.publish(f"sensor_updates:{machine_id}", json.dumps(data))
    await redis_client.publish("sensor_updates:all", json.dumps(data))

# Subscriber (in WebSocket manager)
async def subscribe_to_updates():
    pubsub = redis_client.pubsub()
    await pubsub.subscribe("sensor_updates:all")
    
    async for message in pubsub.listen():
        if message["type"] == "message":
            data = json.loads(message["data"])
            await websocket_manager.broadcast(data)
```

### Redis Caching Patterns

```python
# Cache-aside pattern
async def get_machine_data(machine_id: str):
    # Try cache first
    cached = await redis_client.get(f"machine:{machine_id}")
    if cached:
        return json.loads(cached)
    
    # Cache miss - fetch from database
    data = await database.fetch_machine(machine_id)
    
    # Store in cache with expiry
    await redis_client.setex(
        f"machine:{machine_id}", 
        300,  # 5 minutes
        json.dumps(data)
    )
    return data

# Write-through pattern
async def update_machine_data(machine_id: str, data: dict):
    # Update database
    await database.update_machine(machine_id, data)
    
    # Update cache
    await redis_client.setex(
        f"machine:{machine_id}",
        300,
        json.dumps(data)
    )
```

### Redis Performance Optimization

```python
# Pipeline for batch operations
pipe = redis_client.pipeline()
for i in range(1000):
    pipe.set(f"sensor:{i}", f"value_{i}")
await pipe.execute()

# Connection pooling
redis_pool = aioredis.ConnectionPool.from_url(
    "redis://localhost:6379",
    max_connections=20
)
redis_client = aioredis.Redis(connection_pool=redis_pool)
```

**Interview Talking Points:**

- Redis is single-threaded but uses event loop for I/O
- Memory usage: 1M keys â‰ˆ 85MB RAM
- Persistence: RDB snapshots vs AOF logs
- Clustering: Automatic sharding across nodes
- Use cases: Caching, session storage, real-time messaging, rate limiting

---

## FastAPI Advanced Patterns

*Building on your basic knowledge*

### Dependency Injection System

```python
from fastapi import Depends, HTTPException
from typing import AsyncGenerator

# Database dependency
async def get_db() -> AsyncGenerator:
    async with AsyncSession() as session:
        try:
            yield session
        finally:
            await session.close()

# Redis dependency
async def get_redis():
    return redis_client

# Authentication dependency
async def get_current_user(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username = payload.get("sub")
        if username is None:
            raise HTTPException(401, "Invalid token")
        return username
    except JWTError:
        raise HTTPException(401, "Invalid token")

# Using dependencies in endpoints
@app.post("/sensors/")
async def create_sensor_reading(
    reading: SensorReading,
    db: AsyncSession = Depends(get_db),
    redis: Redis = Depends(get_redis),
    user: str = Depends(get_current_user)
):
    # Store in database
    await db.execute(insert(sensor_readings).values(**reading.dict()))
    await db.commit()
    
    # Cache latest reading
    await redis.setex(f"latest:{reading.machine_id}", 300, reading.json())
    
    return {"status": "created"}
```

### Background Tasks and Async Processing

```python
from fastapi import BackgroundTasks
import asyncio

# Background task function
async def process_sensor_data(machine_id: str, data: dict):
    # Simulate heavy processing
    await asyncio.sleep(5)
    
    # Calculate aggregations
    avg_temp = sum(data["temperatures"]) / len(data["temperatures"])
    
    # Store results
    await store_aggregated_data(machine_id, {"avg_temp": avg_temp})

# Endpoint with background task
@app.post("/sensors/batch")
async def process_batch(
    batch_data: BatchSensorData,
    background_tasks: BackgroundTasks
):
    # Immediate response
    background_tasks.add_task(process_sensor_data, batch_data.machine_id, batch_data.data)
    return {"status": "processing"}

# Advanced: Task queue with Celery-like pattern
from asyncio import Queue

task_queue = Queue()

async def worker():
    while True:
        task = await task_queue.get()
        try:
            await task["func"](*task["args"])
        except Exception as e:
            logger.error(f"Task failed: {e}")
        finally:
            task_queue.task_done()

# Start workers
for _ in range(5):
    asyncio.create_task(worker())
```

### Middleware and Request Processing

```python
from fastapi import Request, Response
import time

# Custom timing middleware
@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response

# Rate limiting middleware
from collections import defaultdict
from datetime import datetime, timedelta

rate_limit_cache = defaultdict(list)

@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    client_ip = request.client.host
    now = datetime.now()
    
    # Clean old requests
    rate_limit_cache[client_ip] = [
        req_time for req_time in rate_limit_cache[client_ip]
        if now - req_time < timedelta(minutes=1)
    ]
    
    # Check rate limit (100 requests per minute)
    if len(rate_limit_cache[client_ip]) >= 100:
        return Response("Rate limit exceeded", status_code=429)
    
    rate_limit_cache[client_ip].append(now)
    return await call_next(request)
```

### Advanced Pydantic Models

```python
from pydantic import BaseModel, validator, Field
from typing import Optional, List
from datetime import datetime
from enum import Enum

class SensorType(str, Enum):
    temperature = "temperature"
    pressure = "pressure"
    vibration = "vibration"

class SensorReading(BaseModel):
    machine_id: str = Field(..., regex=r'^[A-Z]{3}-\d{3}$')
    sensor_type: SensorType
    value: float = Field(..., ge=0, le=1000)
    timestamp: datetime
    quality: Optional[str] = "good"
    
    @validator('value')
    def validate_sensor_value(cls, v, values):
        sensor_type = values.get('sensor_type')
        if sensor_type == SensorType.temperature and (v < -50 or v > 200):
            raise ValueError('Temperature out of valid range')
        return v
    
    class Config:
        # Example for API docs
        schema_extra = {
            "example": {
                "machine_id": "CNC-001",
                "sensor_type": "temperature",
                "value": 85.4,
                "timestamp": "2024-01-15T10:30:00Z",
                "quality": "good"
            }
        }
```

**Interview Talking Points:**

- FastAPI auto-generates OpenAPI/Swagger docs
- Dependency injection enables clean separation of concerns
- Async endpoints handle I/O-bound operations efficiently
- Pydantic provides runtime validation and serialization
- ASGI vs WSGI: FastAPI runs on uvicorn (ASGI) for async support

---

## WebSocket Scaling & Connection Management

*Advancing your basic WebSocket knowledge*

### Connection Manager Pattern

```python
from fastapi import WebSocket, WebSocketDisconnect
from typing import Dict, List
import json

class ConnectionManager:
    def __init__(self):
        # Active connections by machine_id
        self.machine_connections: Dict[str, List[WebSocket]] = {}
        # All connections
        self.active_connections: List[WebSocket] = []
        # User-specific connections
        self.user_connections: Dict[str, List[WebSocket]] = {}
    
    async def connect(self, websocket: WebSocket, machine_id: str = None, user_id: str = None):
        await websocket.accept()
        self.active_connections.append(websocket)
        
        if machine_id:
            if machine_id not in self.machine_connections:
                self.machine_connections[machine_id] = []
            self.machine_connections[machine_id].append(websocket)
        
        if user_id:
            if user_id not in self.user_connections:
                self.user_connections[user_id] = []
            self.user_connections[user_id].append(websocket)
    
    def disconnect(self, websocket: WebSocket, machine_id: str = None, user_id: str = None):
        self.active_connections.remove(websocket)
        
        if machine_id and machine_id in self.machine_connections:
            self.machine_connections[machine_id].remove(websocket)
            if not self.machine_connections[machine_id]:
                del self.machine_connections[machine_id]
        
        if user_id and user_id in self.user_connections:
            self.user_connections[user_id].remove(websocket)
            if not self.user_connections[user_id]:
                del self.user_connections[user_id]
    
    async def send_personal_message(self, message: str, websocket: WebSocket):
        try:
            await websocket.send_text(message)
        except:
            self.disconnect(websocket)
    
    async def broadcast_to_machine(self, machine_id: str, data: dict):
        if machine_id in self.machine_connections:
            message = json.dumps(data)
            disconnected = []
            
            for connection in self.machine_connections[machine_id]:
                try:
                    await connection.send_text(message)
                except:
                    disconnected.append(connection)
            
            # Clean up disconnected connections
            for conn in disconnected:
                self.disconnect(conn, machine_id)
    
    async def broadcast_to_all(self, data: dict):
        message = json.dumps(data)
        disconnected = []
        
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except:
                disconnected.append(connection)
        
        # Clean up disconnected connections
        for conn in disconnected:
            self.disconnect(conn)

manager = ConnectionManager()
```

### WebSocket Endpoints with Authentication

```python
@app.websocket("/ws/{machine_id}")
async def websocket_endpoint(websocket: WebSocket, machine_id: str, token: str = None):
    # Authenticate WebSocket connection
    if token:
        try:
            user = await verify_token(token)
        except:
            await websocket.close(code=status.WS_1008_POLICY_VIOLATION)
            return
    else:
        await websocket.close(code=status.WS_1008_POLICY_VIOLATION)
        return
    
    await manager.connect(websocket, machine_id, user.id)
    
    try:
        while True:
            # Keep connection alive and handle incoming messages
            data = await websocket.receive_text()
            message = json.loads(data)
            
            # Handle different message types
            if message["type"] == "subscribe":
                # Subscribe to specific sensor types
                pass
            elif message["type"] == "unsubscribe":
                # Unsubscribe from updates
                pass
                
    except WebSocketDisconnect:
        manager.disconnect(websocket, machine_id, user.id)
        print(f"Connection closed for machine {machine_id}")
```

### Horizontal Scaling with Redis Pub/Sub

```python
# WebSocket service with Redis pub/sub for scaling
import asyncio
import aioredis

class ScalableConnectionManager:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.local_connections = ConnectionManager()
        self.pubsub = None
    
    async def start_subscriber(self):
        self.pubsub = self.redis.pubsub()
        await self.pubsub.subscribe("websocket_broadcasts")
        
        # Background task to handle Redis messages
        asyncio.create_task(self._handle_redis_messages())
    
    async def _handle_redis_messages(self):
        async for message in self.pubsub.listen():
            if message["type"] == "message":
                data = json.loads(message["data"])
                
                # Broadcast to local connections only
                if data["target"] == "all":
                    await self.local_connections.broadcast_to_all(data["payload"])
                elif data["target"] == "machine":
                    await self.local_connections.broadcast_to_machine(
                        data["machine_id"], data["payload"]
                    )
    
    async def broadcast_globally(self, target: str, payload: dict, machine_id: str = None):
        # Publish to Redis for all instances
        message = {
            "target": target,
            "payload": payload,
            "machine_id": machine_id
        }
        await self.redis.publish("websocket_broadcasts", json.dumps(message))
```

### Connection Health & Heartbeat

```python
@app.websocket("/ws/{machine_id}")
async def websocket_endpoint_with_heartbeat(websocket: WebSocket, machine_id: str):
    await manager.connect(websocket, machine_id)
    
    # Heartbeat task
    async def heartbeat():
        while True:
            try:
                await websocket.send_text(json.dumps({"type": "ping", "timestamp": time.time()}))
                await asyncio.sleep(30)  # Ping every 30 seconds
            except:
                break
    
    heartbeat_task = asyncio.create_task(heartbeat())
    
    try:
        while True:
            data = await asyncio.wait_for(websocket.receive_text(), timeout=60)
            message = json.loads(data)
            
            if message["type"] == "pong":
                # Client is alive
                continue
            else:
                # Handle other messages
                await handle_websocket_message(message, machine_id)
                
    except (WebSocketDisconnect, asyncio.TimeoutError):
        heartbeat_task.cancel()
        manager.disconnect(websocket, machine_id)
```

**Interview Talking Points:**

- WebSocket vs HTTP: Persistent connection, full-duplex communication
- Scaling: Redis pub/sub for multi-instance coordination
- Memory usage: 1K concurrent connections â‰ˆ 2MB RAM
- Load balancing: Sticky sessions or Redis-based state sharing
- Graceful handling of disconnections and reconnections

---

## Async Patterns and Concurrency Models

### AsyncIO Fundamentals

```python
import asyncio
import aiohttp
from asyncio import Semaphore
import time

# Basic async patterns you're familiar with
async def fetch_data(url: str):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.json()

# Gather pattern - concurrent execution
async def fetch_multiple_sensors():
    urls = [f"http://api/sensors/{i}" for i in range(10)]
    results = await asyncio.gather(*[fetch_data(url) for url in urls])
    return results

# Advanced: Semaphore for rate limiting
async def fetch_with_limit(urls: list, limit: int = 5):
    semaphore = Semaphore(limit)
    
    async def fetch_limited(url):
        async with semaphore:
            return await fetch_data(url)
    
    return await asyncio.gather(*[fetch_limited(url) for url in urls])

# Task cancellation and timeout
async def fetch_with_timeout():
    try:
        result = await asyncio.wait_for(fetch_data("http://slow-api"), timeout=5.0)
        return result
    except asyncio.TimeoutError:
        print("Request timed out")
        return None

# Background task management
class TaskManager:
    def __init__(self):
        self.tasks = set()
    
    def create_task(self, coro):
        task = asyncio.create_task(coro)
        self.tasks.add(task)
        task.add_done_callback(self.tasks.discard)
        return task
    
    async def shutdown(self):
        # Cancel all running tasks
        for task in self.tasks:
            task.cancel()
        
        # Wait for cancellation to complete
        await asyncio.gather(*self.tasks, return_exceptions=True)
```

### Producer-Consumer Patterns

```python
from asyncio import Queue
import random

# Producer-consumer with queue
async def sensor_producer(queue: Queue):
    """Simulates sensor data production"""
    while True:
        sensor_data = {
            "machine_id": f"CNC-{random.randint(1, 10):03d}",
            "temperature": random.uniform(70, 90),
            "timestamp": time.time()
        }
        await queue.put(sensor_data)
        await asyncio.sleep(0.1)  # 10 readings per second

async def data_processor(queue: Queue, worker_id: int):
    """Processes sensor data"""
    while True:
        try:
            data = await asyncio.wait_for(queue.get(), timeout=1.0)
            
            # Simulate processing
            await asyncio.sleep(0.05)
            print(f"Worker {worker_id} processed {data['machine_id']}")
            
            queue.task_done()
        except asyncio.TimeoutError:
            continue

# Start producer-consumer system
async def start_processing_system():
    queue = Queue(maxsize=100)
    
    # Start producer
    producer_task = asyncio.create_task(sensor_producer(queue))
    
    # Start multiple consumers
    workers = [
        asyncio.create_task(data_processor(queue, i))
        for i in range(3)
    ]
    
    # Run for 10 seconds
    await asyncio.sleep(10)
    
    # Cleanup
    producer_task.cancel()
    for worker in workers:
        worker.cancel()
```

### Error Handling and Retries

```python
import random
from functools import wraps

def async_retry(max_attempts=3, delay=1, backoff=2):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            attempt = 1
            current_delay = delay
            
            while attempt <= max_attempts:
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts:
                        raise e
                    
                    print(f"Attempt {attempt} failed: {e}. Retrying in {current_delay}s...")
                    await asyncio.sleep(current_delay)
                    current_delay *= backoff
                    attempt += 1
        
        return wrapper
    return decorator

@async_retry(max_attempts=3, delay=1)
async def unreliable_api_call():
    # Simulate 50% failure rate
    if random.random() < 0.5:
        raise Exception("API temporarily unavailable")
    return {"status": "success", "data": "sensor_reading"}

# Circuit breaker pattern
class CircuitBreaker:
    def __init__(self, failure_threshold=5, recovery_timeout=60):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "closed"  # closed, open, half-open
    
    async def call(self, func, *args, **kwargs):
        if self.state == "open":
            if time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = "half-open"
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = await func(*args, **kwargs)
            
            # Success - reset failure count
            if self.state == "half-open":
                self.state = "closed"
                self.failure_count = 0
            
            return result
            
        except Exception as e:
            self.failure_count += 1
            self.last_failure_time = time.time()
            
            if self.failure_count >= self.failure_threshold:
                self.state = "open"
            
            raise e
```

**Interview Talking Points:**

- AsyncIO event loop is single-threaded but handles I/O concurrency
- `await` points are where context switching happens
- Use `asyncio.gather()` for concurrent execution, `asyncio.create_task()` for fire-and-forget
- Semaphores control concurrency, preventing resource exhaustion
- Circuit breakers prevent cascade failures in distributed systems

---

## Event Streaming Patterns (Kafka)

### Producer Patterns

```python
from aiokafka import AIOKafkaProducer
import json
import asyncio

class SensorDataProducer:
    def __init__(self, bootstrap_servers="localhost:9092"):
        self.producer = AIOKafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            # Exactly-once semantics
            enable_idempotence=True,
            acks='all',
            retries=3,
            # Batching for performance
            batch_size=16384,
            linger_ms=100
        )
    
    async def start(self):
        await self.producer.start()
    
    async def stop(self):
        await self.producer.stop()
    
    async def send_sensor_reading(self, reading: dict):
        # Partition by machine_id for ordering
        await self.producer.send_and_wait(
            "sensor.readings",
            value=reading,
            key=reading["machine_id"].encode('utf-8')
        )
    
    async def send_batch(self, readings: list):
        # Send multiple messages efficiently
        tasks = []
        for reading in readings:
            task = self.producer.send(
                "sensor.readings",
                value=reading,
                key=reading["machine_id"].encode('utf-8')
            )
            tasks.append(task)
        
        # Wait for all to complete
        await asyncio.gather(*tasks)
```

### Consumer Patterns

```python
from aiokafka import AIOKafkaConsumer
import json

class SensorDataConsumer:
    def __init__(self, group_id="sensor-processors"):
        self.consumer = AIOKafkaConsumer(
            "sensor.readings",
            bootstrap_servers="localhost:9092",
            group_id=group_id,
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            # Exactly-once processing
            enable_auto_commit=False,
            isolation_level="read_committed"
        )
        self.db = None  # Database connection
    
    async def start(self):
        await self.consumer.start()
    
    async def stop(self):
        await self.consumer.stop()
    
    async def process_messages(self):
        try:
            async for msg in self.consumer:
                await self.process_single_message(msg)
                
                # Manual commit for exactly-once
                await self.consumer.commit()
                
        except Exception as e:
            print(f"Consumer error: {e}")
            raise
    
    async def process_single_message(self, msg):
        reading = msg.value
        
        try:
            # Process the message
            await self.store_reading(reading)
            await self.update_aggregates(reading)
            await self.check_alerts(reading)
            
        except Exception as e:
            # Handle poison messages
            await self.send_to_dead_letter_queue(msg, e)
            raise
    
    async def store_reading(self, reading):
        # Idempotent database operation
        query = """
        INSERT INTO sensor_readings (machine_id, sensor_type, value, timestamp)
        VALUES (%(machine_id)s, %(sensor_type)s, %(value)s, %(timestamp)s)
        ON CONFLICT (machine_id, timestamp) DO NOTHING
        """
        await self.db.execute(query, reading)
```

### Exactly-Once Processing

```python
class ExactlyOnceProcessor:
    def __init__(self):
        self.processed_messages = set()  # In production: use database
    
    async def process_with_idempotency(self, message):
        # Create unique message ID
        message_id = f"{message.partition}:{message.offset}"
        
        if message_id in self.processed_messages:
            print(f"Message {message_id} already processed, skipping")
            return
        
        try:
            # Begin transaction
            async with self.db.transaction():
                # Process message
                await self.process_business_logic(message.value)
                
                # Record that we processed this message
                await self.db.execute(
                    "INSERT INTO processed_messages (message_id) VALUES (%s)",
                    [message_id]
                )
                
                # Commit transaction
                await self.consumer.commit()
                
            self.processed_messages.add(message_id)
            
        except Exception as e:
            # Transaction automatically rolled back
            print(f"Failed to process message {message_id}: {e}")
            raise

# Event sourcing pattern
class EventStore:
    def __init__(self):
        self.events = []  # In production: use database
    
    async def append_event(self, stream_id: str, event: dict):
        event_data = {
            "stream_id": stream_id,
            "event_type": event["type"],
            "event_data": event,
            "version": await self.get_next_version(stream_id),
            "timestamp": time.time()
        }
        
        # Append to event log
        await self.db.execute("""
            INSERT INTO events (stream_id, event_type, event_data, version, timestamp)
            VALUES (%(stream_id)s, %(event_type)s, %(event_data)s, %(version)s, %(timestamp)s)
        """, event_data)
    
    async def replay_events(self, stream_id: str):
        events = await self.db.fetch_all(
            "SELECT * FROM events WHERE stream_id = %s ORDER BY version",
            [stream_id]
        )
        
        # Rebuild current state
        state = {}
        for event in events:
            state = await self.apply_event(state, event)
        
        return state
```

**Interview Talking Points:**

- Kafka partitioning ensures ordering within partition
- Consumer groups enable horizontal scaling
- Exactly-once = idempotent producer + transactional consumer
- Event sourcing provides audit trail and temporal queries
- Dead letter queues handle poison messages

---

## Time-Series Data Modeling

### PostgreSQL Time-Series Schema

```sql
-- Hypertable with automatic partitioning
CREATE TABLE sensor_readings (
    id BIGSERIAL,
    machine_id VARCHAR(50) NOT NULL,
    sensor_type VARCHAR(50) NOT NULL,
    timestamp TIMESTAMPTZ NOT NULL,
    value DECIMAL(10,4) NOT NULL,
    unit VARCHAR(20) NOT NULL,
    quality VARCHAR(20) DEFAULT 'good',
    metadata JSONB
);

-- Convert to hypertable (TimescaleDB)
SELECT create_hypertable('sensor_readings', 'timestamp', chunk_time_interval => INTERVAL '1 day');

-- Indexes for time-series queries
CREATE INDEX idx_sensor_machine_time ON sensor_readings (machine_id, timestamp DESC);
CREATE INDEX idx_sensor_type_time ON sensor_readings (sensor_type, timestamp DESC);
CREATE INDEX idx_sensor_quality ON sensor_readings (quality) WHERE quality != 'good';

-- Composite index for common query patterns
CREATE INDEX idx_sensor_composite ON sensor_readings (machine_id, sensor_type, timestamp DESC);

-- Partial index for recent data (hot queries)
CREATE INDEX idx_sensor_recent ON sensor_readings (machine_id, timestamp DESC) 
WHERE timestamp > NOW() - INTERVAL '1 hour';
```

### Aggregation Tables

```sql
-- Pre-computed aggregations for dashboard performance
CREATE TABLE sensor_aggregates_hourly (
    machine_id VARCHAR(50),
    sensor_type VARCHAR(50),
    hour_bucket TIMESTAMPTZ,
    avg_value DECIMAL(10,4),
    min_value DECIMAL(10,4),
    max_value DECIMAL(10,4),
    count_readings INTEGER,
    stddev_value DECIMAL(10,4),
    PRIMARY KEY (machine_id, sensor_type, hour_bucket)
);

-- Continuous aggregates (TimescaleDB)
CREATE MATERIALIZED VIEW sensor_hourly_avg
WITH (timescaledb.continuous) AS
SELECT
    machine_id,
    sensor_type,
    time_bucket('1 hour', timestamp) AS hour_bucket,
    AVG(value) as avg_value,
    MIN(value) as min_value,
    MAX(value) as max_value,
    COUNT(*) as count_readings,
    STDDEV(value) as stddev_value
FROM sensor_readings
GROUP BY machine_id, sensor_type, hour_bucket;

-- Refresh policy for continuous aggregates
SELECT add_continuous_aggregate_policy('sensor_hourly_avg',
    start_offset => INTERVAL '1 month',
    end_offset => INTERVAL '1 hour',
    schedule_interval => INTERVAL '30 minutes');
```

### Time-Series Query Optimization

```python
# Efficient time-series queries
class SensorDataRepository:
    def __init__(self, db_pool):
        self.db = db_pool
    
    async def get_recent_readings(self, machine_id: str, hours: int = 1):
        """Get recent readings with index optimization"""
        query = """
        SELECT timestamp, value, sensor_type
        FROM sensor_readings
        WHERE machine_id = %s 
        AND timestamp > NOW() - INTERVAL '%s hours'
        ORDER BY timestamp DESC
        LIMIT 1000
        """
        return await self.db.fetch_all(query, [machine_id, hours])
    
    async def get_aggregated_data(self, machine_id: str, start_time: datetime, end_time: datetime):
        """Use pre-computed aggregates for performance"""
        query = """
        SELECT hour_bucket, avg_value, min_value, max_value
        FROM sensor_aggregates_hourly
        WHERE machine_id = %s
        AND hour_bucket BETWEEN %s AND %s
        ORDER BY hour_bucket
        """
        return await self.db.fetch_all(query, [machine_id, start_time, end_time])
    
    async def get_anomalies(self, machine_id: str, threshold: float = 2.0):
        """Find anomalies using statistical functions"""
        query = """
        WITH stats AS (
            SELECT 
                AVG(value) as mean_val,
                STDDEV(value) as std_val
            FROM sensor_readings
            WHERE machine_id = %s
            AND timestamp > NOW() - INTERVAL '1 day'
        )
        SELECT sr.timestamp, sr.value, sr.sensor_type
        FROM sensor_readings sr, stats s
        WHERE sr.machine_id = %s
        AND ABS(sr.value - s.mean_val) > %s * s.std_val
        AND sr.timestamp > NOW() - INTERVAL '1 hour'
        ORDER BY sr.timestamp DESC
        """
        return await self.db.fetch_all(query, [machine_id, machine_id, threshold])
```

**Interview Talking Points:**

- TimescaleDB hypertables automatically partition by time
- Continuous aggregates provide real-time OLAP capabilities
- Partial indexes reduce storage for filtered queries
- Window functions enable complex time-series analysis
- Data retention policies manage storage costs

---

## Security and Authentication Flow

### JWT Authentication System

```python
from datetime import datetime, timedelta
from jose import JWTError, jwt
from passlib.context import CryptContext
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

# Security configuration
SECRET_KEY = "your-secret-key"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
security = HTTPBearer()

class AuthService:
    @staticmethod
    def verify_password(plain_password: str, hashed_password: str) -> bool:
        return pwd_context.verify(plain_password, hashed_password)
    
    @staticmethod
    def get_password_hash(password: str) -> str:
        return pwd_context.hash(password)
    
    @staticmethod
    def create_access_token(data: dict, expires_delta: timedelta = None):
        to_encode = data.copy()
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(minutes=15)
        
        to_encode.update({"exp": expire})
        encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
        return encoded_jwt
    
    @staticmethod
    async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
        try:
            payload = jwt.decode(credentials.credentials, SECRET_KEY, algorithms=[ALGORITHM])
            username: str = payload.get("sub")
            if username is None:
                raise HTTPException(401, "Invalid authentication credentials")
            return username
        except JWTError:
            raise HTTPException(401, "Invalid authentication credentials")

# Role-based access control
class RoleChecker:
    def __init__(self, allowed_roles: List[str]):
        self.allowed_roles = allowed_roles
    
    def __call__(self, user: dict = Depends(AuthService.verify_token)):
        if user.get("role") not in self.allowed_roles:
            raise HTTPException(403, "Operation not permitted")
        return user

# Usage in endpoints
admin_required = RoleChecker(["admin"])
operator_required = RoleChecker(["admin", "operator"])

@app.post("/machines/{machine_id}/shutdown")
async def shutdown_machine(
    machine_id: str,
    user: dict = Depends(admin_required)
):
    # Only admins can shutdown machines
    await machine_service.shutdown(machine_id)
    return {"status": "shutdown_initiated"}
```

### Rate Limiting and API Security

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

# Rate limiter setup
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# Different limits for different endpoints
@app.post("/sensors/")
@limiter.limit("1000/minute")  # High throughput for sensor data
async def create_sensor_reading(request: Request, reading: SensorReading):
    return await sensor_service.create_reading(reading)

@app.post("/auth/login")
@limiter.limit("5/minute")  # Strict limit for authentication
async def login(request: Request, user_credentials: UserLogin):
    return await auth_service.authenticate(user_credentials)

# API key authentication for machine-to-machine
class APIKeyAuth:
    def __init__(self):
        self.api_keys = {
            "machine-api-key-123": {"client": "CNC-001", "permissions": ["write"]},
            "dashboard-key-456": {"client": "dashboard", "permissions": ["read"]}
        }
    
    async def verify_api_key(self, api_key: str = Header(None, alias="X-API-Key")):
        if not api_key or api_key not in self.api_keys:
            raise HTTPException(401, "Invalid API key")
        return self.api_keys[api_key]

api_key_auth = APIKeyAuth()

@app.post("/api/sensors/")
async def api_create_sensor_reading(
    reading: SensorReading,
    client: dict = Depends(api_key_auth.verify_api_key)
):
    if "write" not in client["permissions"]:
        raise HTTPException(403, "Write permission required")
    return await sensor_service.create_reading(reading)
```

### Input Validation and Sanitization

```python
from pydantic import validator, ValidationError
import re

class SecureSensorReading(BaseModel):
    machine_id: str = Field(..., regex=r'^[A-Z]{3}-\d{3}$')
    sensor_type: str = Field(..., regex=r'^[a-z_]+$')
    value: float = Field(..., ge=-1000, le=1000)
    timestamp: datetime
    metadata: Optional[dict] = None
    
    @validator('metadata')
    def validate_metadata(cls, v):
        if v is None:
            return v
        
        # Limit metadata size
        if len(str(v)) > 1000:
            raise ValueError("Metadata too large")
        
        # Sanitize keys and values
        sanitized = {}
        for key, value in v.items():
            # Only allow alphanumeric keys
            if not re.match(r'^[a-zA-Z0-9_]+$', key):
                raise ValueError(f"Invalid metadata key: {key}")
            
            # Sanitize string values
            if isinstance(value, str):
                # Remove potentially dangerous characters
                value = re.sub(r'[<>"\']', '', value)
                value = value[:100]  # Limit string length
            
            sanitized[key] = value
        
        return sanitized

# CORS configuration
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://dashboard.company.com"],  # Specific origins only
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["Authorization", "Content-Type"],
)
```

**Interview Talking Points:**

- JWT tokens are stateless but require secret key management
- Rate limiting prevents abuse and DoS attacks
- Role-based access control (RBAC) provides fine-grained permissions
- Input validation prevents injection attacks
- CORS protects against cross-origin attacks

---

## Monitoring and Observability Setup

### OpenTelemetry Instrumentation

```python
from opentelemetry import trace, metrics
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.asyncpg import AsyncPGInstrumentor
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.jaeger.thrift import JaegerExporter

# Tracing setup
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831,
)

span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# Auto-instrument FastAPI
FastAPIInstrumentor.instrument_app(app)
RequestsInstrumentor().instrument()
AsyncPGInstrumentor().instrument()

# Custom instrumentation
@app.post("/sensors/")
async def create_sensor_reading(reading: SensorReading):
    with tracer.start_as_current_span("process_sensor_reading") as span:
        span.set_attribute("machine_id", reading.machine_id)
        span.set_attribute("sensor_type", reading.sensor_type)
        
        # Store in database
        with tracer.start_as_current_span("store_reading"):
            result = await sensor_service.store(reading)
        
        # Update cache
        with tracer.start_as_current_span("update_cache"):
            await cache_service.update(reading.machine_id, reading)
        
        # Broadcast to WebSocket
        with tracer.start_as_current_span("websocket_broadcast"):
            await websocket_manager.broadcast_to_machine(reading.machine_id, reading.dict())
        
        span.set_attribute("processing_status", "success")
        return result
```

### Metrics Collection

```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# Prometheus metrics
REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint'])
REQUEST_DURATION = Histogram('http_request_duration_seconds', 'HTTP request duration')
ACTIVE_CONNECTIONS = Gauge('websocket_connections_active', 'Active WebSocket connections')
SENSOR_READINGS_TOTAL = Counter('sensor_readings_total', 'Total sensor readings', ['machine_id', 'sensor_type'])
PROCESSING_ERRORS = Counter('processing_errors_total', 'Processing errors', ['error_type'])

# Middleware for metrics collection
@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    # Record metrics
    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.url.path
    ).inc()
    
    REQUEST_DURATION.observe(time.time() - start_time)
    
    return response

# Business metrics
class MetricsService:
    @staticmethod
    def record_sensor_reading(machine_id: str, sensor_type: str):
        SENSOR_READINGS_TOTAL.labels(
            machine_id=machine_id,
            sensor_type=sensor_type
        ).inc()
    
    @staticmethod
    def record_processing_error(error_type: str):
        PROCESSING_ERRORS.labels(error_type=error_type).inc()
    
    @staticmethod
    def update_active_connections(count: int):
        ACTIVE_CONNECTIONS.set(count)

# Start Prometheus metrics server
start_http_server(8000)
```

### Structured Logging

```python
import structlog
import logging
from pythonjsonlogger import jsonlogger

# Configure structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# Usage in application
@app.post("/sensors/")
async def create_sensor_reading(reading: SensorReading):
    logger.info(
        "sensor_reading_received",
        machine_id=reading.machine_id,
        sensor_type=reading.sensor_type,
        value=reading.value,
        user_id=user.id if user else None
    )
    
    try:
        result = await sensor_service.store(reading)
        
        logger.info(
            "sensor_reading_stored",
            machine_id=reading.machine_id,
            processing_time_ms=(time.time() - start_time) * 1000
        )
        
        return result
        
    except Exception as e:
        logger.error(
            "sensor_reading_failed",
            machine_id=reading.machine_id,
            error_type=type(e).__name__,
            error_message=str(e)
        )
        raise
```

### Health Checks and Readiness Probes

```python
from fastapi import status

class HealthService:
    def __init__(self, db_pool, redis_client, kafka_producer):
        self.db = db_pool
        self.redis = redis_client
        self.kafka = kafka_producer
    
    async def check_database(self) -> bool:
        try:
            await self.db.execute("SELECT 1")
            return True
        except Exception:
            return False
    
    async def check_redis(self) -> bool:
        try:
            await self.redis.ping()
            return True
        except Exception:
            return False
    
    async def check_kafka(self) -> bool:
        try:
            # Try to get metadata
            metadata = await self.kafka.client.check_version()
            return True
        except Exception:
            return False

health_service = HealthService(db_pool, redis_client, kafka_producer)

@app.get("/health")
async def health_check():
    """Basic health check for load balancer"""
    return {"status": "healthy", "timestamp": time.time()}

@app.get("/health/ready")
async def readiness_check():
    """Detailed readiness check for Kubernetes"""
    checks = {
        "database": await health_service.check_database(),
        "redis": await health_service.check_redis(),
        "kafka": await health_service.check_kafka()
    }
    
    all_healthy = all(checks.values())
    
    return {
        "status": "ready" if all_healthy else "not_ready",
        "checks": checks,
        "timestamp": time.time()
    }, status.HTTP_200_OK if all_healthy else status.HTTP_503_SERVICE_UNAVAILABLE

@app.get("/health/live")
async def liveness_check():
    """Liveness check - application is running"""
    return {
        "status": "alive",
        "uptime": time.time() - app_start_time,
        "timestamp": time.time()
    }
```

**Interview Talking Points:**

- Distributed tracing shows request flow across services
- Prometheus metrics provide quantitative system insights
- Structured logging enables efficient log analysis
- Health checks ensure reliable deployments
- Three pillars of observability: metrics, logs, traces

---

# Interview Preparation Integration

## System Design Talking Points

### Scalability Strategies

- **Horizontal scaling**: Multiple FastAPI instances behind load balancer
- **Database scaling**: Read replicas, connection pooling, query optimization
- **Kafka partitioning**: Partition by machine_id for ordering guarantees
- **WebSocket scaling**: Redis pub/sub for multi-instance coordination
- **Caching strategy**: Redis for hot data, CDN for static assets

### Performance Benchmarks

- **Latency targets**:
  - API response time: < 100ms P95
  - End-to-end sensor data: < 3s (ingestion â†’ dashboard)
  - WebSocket message delivery: < 50ms
- **Throughput targets**:
  - 50K sensor readings/second
  - 10K concurrent WebSocket connections
  - 1M database queries/minute

### Reliability Patterns

- **Circuit breakers**: Prevent cascade failures
- **Retries with exponential backoff**: Handle transient failures
- **Dead letter queues**: Handle poison messages
- **Graceful degradation**: Continue with reduced functionality
- **Health checks**: Enable automatic recovery

## Component-to-Interview-Question Mapping

### FastAPI Questions

**Q**: "How does FastAPI handle async operations differently from Flask?"
**A**: FastAPI is built on ASGI (uvicorn) enabling true async/await support. Uses event loop for concurrent I/O operations. Dependency injection system enables clean async patterns.

**Q**: "Explain FastAPI's dependency injection and its benefits"
**A**: Dependencies are resolved automatically using Python's type hints. Enables separation of concerns, testability, and clean async resource management (database connections, etc.).

### WebSocket Questions

**Q**: "How do you scale WebSocket connections horizontally?"
**A**: Use Redis pub/sub to coordinate messages between instances. Implement sticky sessions or stateless connection management. Monitor connection health with heartbeats.

**Q**: "Handle WebSocket connection failures gracefully"
**A**: Implement connection manager with automatic cleanup, reconnection logic on client, heartbeat/ping-pong for health monitoring.

### Redis Questions

**Q**: "What caching patterns would you use for real-time dashboard data?"
**A**: Cache-aside for machine metadata, write-through for critical updates, pub/sub for real-time notifications. Use appropriate TTLs based on data freshness requirements.

**Q**: "How does Redis pub/sub differ from Kafka?"
**A**: Redis pub/sub is in-memory, fire-and-forget messaging. Kafka persists messages, provides ordering guarantees, supports consumer groups for scaling.

### Database Questions

**Q**: "Design time-series data schema for optimal query performance"
**A**: Use time partitioning (daily/weekly chunks), compound indexes on (machine_id, timestamp), partial indexes for recent data, pre-computed aggregates.

**Q**: "Handle high write throughput to PostgreSQL"
**A**: Connection pooling, batch inserts, async operations, proper indexing strategy, consider write-ahead logging tuning.

### Kafka Questions

**Q**: "Ensure exactly-once processing in event streaming"
**A**: Idempotent producers + transactional consumers. Use message deduplication, database transactions, and proper offset management.

**Q**: "Design topic partitioning strategy"
**A**: Partition by machine_id to maintain ordering per machine. Number of partitions = max expected parallelism. Consider rebalancing costs.

### Kubernetes Questions

**Q**: "Configure health checks for zero-downtime deployments"
**A**: Readiness probes check dependencies (DB, Redis), liveness probes check application health. Graceful shutdown handles in-flight requests.

**Q**: "Scale services based on business metrics"
**A**: Use custom metrics (queue length, connection count) with HPA. Implement circuit breakers and bulkhead patterns for resilience.

## Common Gotchas and Solutions

### Async Programming Pitfalls

**Gotcha**: Blocking operations in async functions
**Solution**: Always use async libraries (aioredis, asyncpg), never use sync operations in async context

**Gotcha**: Memory leaks from unclosed connections
**Solution**: Use context managers, proper connection pooling, cleanup in finally blocks

### WebSocket Management Issues

**Gotcha**: Stale connections consuming resources
**Solution**: Implement heartbeat mechanism, connection timeouts, proper cleanup on disconnect

**Gotcha**: Message ordering issues in scaled deployment
**Solution**: Use Redis pub/sub with consistent routing, consider sticky sessions for ordering-critical features

### Database Performance Problems

**Gotcha**: N+1 queries in time-series data
**Solution**: Use batch queries, proper joins, pre-computed aggregations

**Gotcha**: Index bloat on high-write tables
**Solution**: Partial indexes, regular maintenance, proper partitioning strategy

### Security Oversights

**Gotcha**: JWT tokens without proper validation
**Solution**: Verify signature, check expiration, validate claims, use proper secret management

**Gotcha**: Unvalidated input leading to injection
**Solution**: Use Pydantic models, parameterized queries, input sanitization

## STAR Stories for Behavioral Questions

### Leadership Story: "Architected Streaming Pipeline"

**Situation**: Team needed real-time analytics for 1M+ IoT devices
**Task**: Design scalable event streaming architecture
**Action**: Led technical design sessions, chose Kafka for reliability, implemented exactly-once processing, mentored team on event-driven patterns
**Result**: System handled 10x traffic growth, reduced data latency from 30s to <3s, became template for other teams

### Problem-Solving Story: "Handled Out-of-Order Events"

**Situation**: Manufacturing sensors sending data with network delays causing incorrect aggregations
**Task**: Ensure data consistency despite message ordering issues
**Action**: Implemented event sourcing with timestamp-based ordering, added late-arrival handling with watermarks, created reconciliation process
**Result**: Achieved 99.9% data accuracy, automated recovery from network issues

### Quality Story: "Comprehensive Testing Strategy"

**Situation**: Critical production issues due to insufficient testing
**Task**: Build robust testing framework for microservices
**Action**: Implemented unit tests (90%+ coverage), integration tests with TestContainers, end-to-end tests, performance benchmarking
**Result**: Reduced production incidents by 80%, improved deployment confidence, shortened release cycles

---

# Quick Reference Cards

## Performance Numbers to Remember

- **Redis**: 100K+ ops/sec, <1ms latency
- **PostgreSQL**: 10K+ TPS with proper tuning
- **Kafka**: 1M+ messages/sec per broker
- **WebSocket**: 1K connections â‰ˆ 2MB RAM
- **FastAPI**: 10K+ req/sec on single core

## Essential Commands

```bash
# Docker
docker-compose up -d postgres redis kafka
docker-compose logs -f api

# Kubernetes
kubectl apply -f k8s/
kubectl get pods -w
kubectl logs -f deployment/api

# Database
psql -h localhost -U postgres -d sensors
EXPLAIN ANALYZE SELECT * FROM sensor_readings WHERE machine_id = 'CNC-001';

# Redis
redis-cli
MONITOR  # Watch all commands
INFO memory  # Memory usage

# Kafka
kafka-topics.sh --list --bootstrap-server localhost:9092
kafka-console-consumer.sh --topic sensor.readings --bootstrap-server localhost:9092
```

## Code Templates

```python
# FastAPI endpoint template
@app.post("/resource/")
async def create_resource(
    resource: ResourceModel,
    db: AsyncSession = Depends(get_db),
    user: User = Depends(get_current_user)
):
    try:
        result = await service.create(resource)
        logger.info("resource_created", resource_id=result.id)
        return result
    except Exception as e:
        logger.error("resource_creation_failed", error=str(e))
        raise HTTPException(500, "Internal server error")

# Async error handling template
@retry(max_attempts=3, delay=1)
async def resilient_operation():
    async with circuit_breaker:
        return await external_service.call()
```

This cheat sheet provides comprehensive coverage of all technical areas needed for your senior full-stack engineer interview. Keep it handy during development and use it for quick reference during the interview process.
