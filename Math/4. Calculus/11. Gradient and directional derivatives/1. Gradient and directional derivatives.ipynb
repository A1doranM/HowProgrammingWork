{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c5cefe",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38964bc8",
   "metadata": {},
   "source": [
    "\n",
    "### The Core Idea: Slope on a Mountain\n",
    "\n",
    "Imagine you are standing on the side of a mountain. A friend asks, \"What's the slope here?\"\n",
    "\n",
    "You can't give a single answer. The slope depends on the **direction** you are facing.\n",
    "*   If you face directly uphill, the slope is steep and positive.\n",
    "*   If you face directly downhill, the slope is steep and negative.\n",
    "*   If you face sideways, along a path of constant elevation, the slope is zero.\n",
    "*   If you face any other direction, you get some other value for the slope.\n",
    "\n",
    "Multivariable calculus gives us the tools to answer this question precisely.\n",
    "*   The **Gradient (∇f)** tells you which way is \"directly uphill\" and how steep it is.\n",
    "*   The **Directional Derivative (Dᵤf)** tells you the slope in any specific direction you choose.\n",
    "\n",
    "---\n",
    "\n",
    "### The Gradient\n",
    "\n",
    "#### Theory\n",
    "The **gradient** of a multivariable function, `f(x, y)`, is a **vector** that packages the function's partial derivatives together. It is denoted by `∇f` (read \"del f\" or \"grad f\").\n",
    "\n",
    "This vector has a profound physical meaning: **it points in the direction of the greatest rate of increase (steepest ascent) of the function at a given point.**\n",
    "\n",
    "*   **Formula:** For a function `f(x, y)`, the gradient is:\n",
    "    **∇f(x, y) = < ∂f/∂x , ∂f/∂y >**\n",
    "    *   `∂f/∂x`: The partial derivative of `f` with respect to `x` (treat `y` as a constant).\n",
    "    *   `∂f/∂y`: The partial derivative of `f` with respect to `y` (treat `x` as a constant).\n",
    "\n",
    "The **magnitude** or length of the gradient vector, `||∇f||`, is also important: it tells you the *value* of the slope in that steepest direction.\n",
    "\n",
    "#### Calculation Example\n",
    "Let's find the gradient of the function `f(x, y) = x³y²` at the point `P(2, 1)`.\n",
    "\n",
    "1.  **Find the partial derivatives:**\n",
    "    *   `∂f/∂x`: Treat `y²` as a constant. The derivative of `x³` is `3x²`.\n",
    "        *   `∂f/∂x = 3x²y²`\n",
    "    *   `∂f/∂y`: Treat `x³` as a constant. The derivative of `y²` is `2y`.\n",
    "        *   `∂f/∂y = 2x³y`\n",
    "\n",
    "2.  **Assemble the gradient vector:**\n",
    "    *   `∇f(x, y) = < 3x²y² , 2x³y >`\n",
    "\n",
    "3.  **Evaluate the gradient at the point P(2, 1):**\n",
    "    *   `∇f(2, 1) = < 3(2)²(1)² , 2(2)³(1) > = < 3(4)(1) , 2(8)(1) > = <12, 16>`\n",
    "\n",
    "**Interpretation:**\n",
    "*   At the point (2, 1) on the surface defined by `f(x, y)`, the direction of steepest ascent is given by the vector `<12, 16>`.\n",
    "*   The slope in this steepest direction is the magnitude of the gradient: `||∇f(2,1)|| = √(12² + 16²) = √(144 + 256) = √400 = 20`.\n",
    "\n",
    "#### Real-Life Usage\n",
    "*   **Machine Learning (Gradient Descent):** This is the core algorithm for training most machine learning models. The \"loss\" is a function of the model's many parameters. The algorithm calculates the gradient of the loss function. To minimize the loss, it repeatedly takes a small step in the direction **opposite** to the gradient (the direction of steepest *descent*), thereby updating the model's parameters to make it more accurate.\n",
    "*   **Meteorology:** Weather maps show isobars (lines of constant pressure). The wind flows from high to low pressure, and the pressure gradient vector (`-∇P`) points in the direction of the fastest pressure drop, indicating the direction of the strongest winds.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient and Graphs / 3. Gradient and Contour Maps\n",
    "\n",
    "These two concepts are best understood together as they provide the geometric interpretation of the gradient.\n",
    "\n",
    "#### Gradient and Graphs (3D View)\n",
    "If you visualize the function `f(x, y)` as a 3D surface, the gradient vector `∇f(a,b)` at a point `(a,b)` can be thought of as a vector in the xy-plane that points in the direction you would have to walk to go straight uphill on the surface.\n",
    "\n",
    "#### Gradient and Contour Maps (2D View)\n",
    "This is the more powerful visualization. A **contour map** (or level set) shows lines where the function's value `f(x, y)` is constant. For our mountain analogy, these are lines of constant elevation.\n",
    "\n",
    "**Key Property:** The gradient vector `∇f` at any point is always **perpendicular (orthogonal)** to the contour line that passes through that point.\n",
    "\n",
    "**Why?** The contour line represents the direction of **zero change** in the function's value. The gradient represents the direction of **maximum change**. It makes intuitive sense that the direction of maximum change must be perpendicular to the direction of no change.\n",
    "\n",
    "\n",
    "*Image: The red arrows are gradient vectors. Notice how each one is perpendicular to the blue contour line it starts on.*\n",
    "\n",
    "---\n",
    "\n",
    "### Directional Derivative and Slope\n",
    "\n",
    "#### Theory\n",
    "The gradient tells us the slope in the steepest direction, but what about all the other directions? That's what the **directional derivative** is for.\n",
    "\n",
    "The directional derivative of `f` at a point `(x, y)` in the direction of a **unit vector** `u` is the slope of the surface at that point in that specific direction. It is denoted `Dᵤf`.\n",
    "\n",
    "**Crucial Point:** The direction must be specified by a **unit vector `u`** (a vector with a magnitude of 1). If you have a direction vector `v` that is not a unit vector, you must first normalize it: `u = v / ||v||`.\n",
    "\n",
    "The formal definition is based on a limit, but the practical calculation is much simpler and elegantly involves the gradient.\n",
    "\n",
    "*   **Formula:** **Dᵤf(x, y) = ∇f(x, y) ⋅ u**\n",
    "    *   The directional derivative is the **dot product** of the gradient and the direction unit vector.\n",
    "\n",
    "#### Calculation Example\n",
    "Let's continue with our function `f(x, y) = x³y²` at the point `P(2, 1)`, where we found `∇f(2, 1) = <12, 16>`.\n",
    "\n",
    "We want to find the slope of the surface at this point in the direction of the vector **v = <3, 4>**.\n",
    "\n",
    "1.  **Normalize the direction vector `v` to get the unit vector `u`:**\n",
    "    *   `||v|| = √(3² + 4²) = √(9 + 16) = √25 = 5`\n",
    "    *   `u = v / ||v|| = <3, 4> / 5 = <3/5, 4/5>`\n",
    "\n",
    "2.  **Calculate the dot product:**\n",
    "    *   `Dᵤf(2, 1) = ∇f(2, 1) ⋅ u`\n",
    "    *   `Dᵤf(2, 1) = <12, 16> ⋅ <3/5, 4/5>`\n",
    "    *   `Dᵤf(2, 1) = (12 * 3/5) + (16 * 4/5) = 36/5 + 64/5 = 100/5 = 20`\n",
    "\n",
    "**Interpretation:**\n",
    "*   The slope of the surface at the point (2, 1) in the direction of `<3, 4>` is exactly **20**.\n",
    "\n",
    "**Wait, this is the same as the magnitude of the gradient!** Why? Because the direction vector we chose, `<3, 4>`, happens to point in the exact same direction as our gradient vector, `<12, 16>` (since `<12, 16> = 4 * <3, 4>`). We calculated the slope in the direction of steepest ascent, which is, by definition, the magnitude of the gradient.\n",
    "\n",
    "**Let's try a different direction:** Find the slope in the direction of **v = <1, 0>** (the positive x-direction).\n",
    "1.  `v = <1, 0>` is already a unit vector, so `u = <1, 0>`.\n",
    "2.  `Dᵤf(2, 1) = <12, 16> ⋅ <1, 0> = (12*1) + (16*0) = 12`.\n",
    "    *   **Interpretation:** The slope in the pure x-direction is 12. Notice that this is exactly the value of the partial derivative `∂f/∂x` at that point! The directional derivative is a generalization of the partial derivative.\n",
    "\n",
    "***\n",
    "\n",
    "### Python Code Illustration\n",
    "\n",
    "This code will use `sympy` for symbolic math to find the gradient and `numpy` and `matplotlib` to calculate values and visualize the concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Gradient Calculation using SymPy ---\n",
    "# Define symbolic variables\n",
    "x, y = sp.symbols('x y')\n",
    "# Define a function\n",
    "f = x**2 * sp.sin(y)\n",
    "\n",
    "# Calculate partial derivatives\n",
    "df_dx = sp.diff(f, x)\n",
    "df_dy = sp.diff(f, y)\n",
    "\n",
    "# Create the gradient vector\n",
    "gradient = [df_dx, df_dy]\n",
    "\n",
    "# Lambdify the symbolic expressions into fast, numerical Python functions\n",
    "f_np = sp.lambdify((x, y), f, 'numpy')\n",
    "grad_np = [sp.lambdify((x, y), comp, 'numpy') for comp in gradient]\n",
    "\n",
    "# Point of interest\n",
    "point = (1, np.pi/2)\n",
    "\n",
    "# Evaluate the gradient at the point\n",
    "grad_at_point = [comp(*point) for comp in grad_np]\n",
    "grad_magnitude = np.linalg.norm(grad_at_point)\n",
    "\n",
    "print(\"--- Gradient Calculation ---\")\n",
    "print(f\"Function f(x, y) = {f}\")\n",
    "print(f\"Gradient ∇f = {gradient}\")\n",
    "print(f\"\\nAt point {point}:\")\n",
    "print(f\"  ∇f({point[0]}, {point[1]:.2f}) = <{grad_at_point[0]:.2f}, {grad_at_point[1]:.2f}>\")\n",
    "print(f\"  Magnitude ||∇f|| (steepest slope): {grad_magnitude:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 2. Directional Derivative Calculation ---\n",
    "# Find the slope in the direction of vector v = <-1, 2>\n",
    "v = np.array([-1, 2])\n",
    "# Normalize v to get the unit vector u\n",
    "u = v / np.linalg.norm(v)\n",
    "\n",
    "# Calculate the dot product: Dᵤf = ∇f ⋅ u\n",
    "directional_derivative = np.dot(grad_at_point, u)\n",
    "\n",
    "print(\"\\n--- Directional Derivative Calculation ---\")\n",
    "print(f\"Direction vector v = {v}\")\n",
    "print(f\"Unit vector u = <{u[0]:.4f}, {u[1]:.4f}>\")\n",
    "print(f\"Directional derivative Dᵤf = ∇f ⋅ u = {directional_derivative:.4f}\")\n",
    "print(f\"Interpretation: The slope of the surface at the point in this direction is {directional_derivative:.4f}.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 3. Visualization: Gradient and Contour Map ---\n",
    "print(\"\\n--- Visualization ---\")\n",
    "# Create a grid of points for plotting\n",
    "x_vals = np.linspace(-2, 2, 40)\n",
    "y_vals = np.linspace(0, np.pi, 40)\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "Z = f_np(X, Y)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 3D Surface Plot\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "ax1.set_title('3D Surface Plot of f(x, y)')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "\n",
    "# 2D Contour Plot\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "contours = ax2.contour(X, Y, Z, 20, cmap='viridis')\n",
    "ax2.clabel(contours, inline=True, fontsize=8)\n",
    "ax2.set_title('Contour Map with Gradient Vector')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "# Plot the gradient vector at our point\n",
    "ax2.quiver(point[0], point[1], grad_at_point[0], grad_at_point[1], \n",
    "           color='red', scale=15, label='Gradient ∇f')\n",
    "ax2.plot(point[0], point[1], 'ro') # Mark the point\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the red gradient vector is perpendicular to the contour line at the point.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
