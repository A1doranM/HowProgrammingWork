{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c5cefe",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38964bc8",
   "metadata": {},
   "source": [
    "\n",
    "### The Core Idea: Slope on a Mountain\n",
    "\n",
    "Imagine you are standing on the side of a mountain. A friend asks, \"What's the slope here?\"\n",
    "\n",
    "You can't give a single answer. The slope depends on the **direction** you are facing.\n",
    "*   If you face directly uphill, the slope is steep and positive.\n",
    "*   If you face directly downhill, the slope is steep and negative.\n",
    "*   If you face sideways, along a path of constant elevation, the slope is zero.\n",
    "*   If you face any other direction, you get some other value for the slope.\n",
    "\n",
    "Multivariable calculus gives us the tools to answer this question precisely.\n",
    "*   The **Gradient (∇f)** tells you which way is \"directly uphill\" and how steep it is.\n",
    "*   The **Directional Derivative (Dᵤf)** tells you the slope in any specific direction you choose.\n",
    "\n",
    "---\n",
    "\n",
    "### The Gradient\n",
    "\n",
    "#### Theory\n",
    "The **gradient** of a multivariable function, `f(x, y)`, is a **vector** that packages the function's partial derivatives together. It is denoted by `∇f` (read \"del f\" or \"grad f\").\n",
    "\n",
    "This vector has a profound physical meaning: **it points in the direction of the greatest rate of increase (steepest ascent) of the function at a given point.**\n",
    "\n",
    "*   **Formula:** For a function `f(x, y)`, the gradient is:\n",
    "    **∇f(x, y) = < ∂f/∂x , ∂f/∂y >**\n",
    "    *   `∂f/∂x`: The partial derivative of `f` with respect to `x` (treat `y` as a constant).\n",
    "    *   `∂f/∂y`: The partial derivative of `f` with respect to `y` (treat `x` as a constant).\n",
    "\n",
    "The **magnitude** or length of the gradient vector, `||∇f||`, is also important: it tells you the *value* of the slope in that steepest direction.\n",
    "\n",
    "#### Calculation Example\n",
    "Let's find the gradient of the function `f(x, y) = x³y²` at the point `P(2, 1)`.\n",
    "\n",
    "1.  **Find the partial derivatives:**\n",
    "    *   `∂f/∂x`: Treat `y²` as a constant. The derivative of `x³` is `3x²`.\n",
    "        *   `∂f/∂x = 3x²y²`\n",
    "    *   `∂f/∂y`: Treat `x³` as a constant. The derivative of `y²` is `2y`.\n",
    "        *   `∂f/∂y = 2x³y`\n",
    "\n",
    "2.  **Assemble the gradient vector:**\n",
    "    *   `∇f(x, y) = < 3x²y² , 2x³y >`\n",
    "\n",
    "3.  **Evaluate the gradient at the point P(2, 1):**\n",
    "    *   `∇f(2, 1) = < 3(2)²(1)² , 2(2)³(1) > = < 3(4)(1) , 2(8)(1) > = <12, 16>`\n",
    "\n",
    "**Interpretation:**\n",
    "*   At the point (2, 1) on the surface defined by `f(x, y)`, the direction of steepest ascent is given by the vector `<12, 16>`.\n",
    "*   The slope in this steepest direction is the magnitude of the gradient: `||∇f(2,1)|| = √(12² + 16²) = √(144 + 256) = √400 = 20`.\n",
    "\n",
    "#### Real-Life Usage\n",
    "*   **Machine Learning (Gradient Descent):** This is the core algorithm for training most machine learning models. The \"loss\" is a function of the model's many parameters. The algorithm calculates the gradient of the loss function. To minimize the loss, it repeatedly takes a small step in the direction **opposite** to the gradient (the direction of steepest *descent*), thereby updating the model's parameters to make it more accurate.\n",
    "*   **Meteorology:** Weather maps show isobars (lines of constant pressure). The wind flows from high to low pressure, and the pressure gradient vector (`-∇P`) points in the direction of the fastest pressure drop, indicating the direction of the strongest winds.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient and Graphs / 3. Gradient and Contour Maps\n",
    "\n",
    "These two concepts are best understood together as they provide the geometric interpretation of the gradient.\n",
    "\n",
    "#### Gradient and Graphs (3D View)\n",
    "If you visualize the function `f(x, y)` as a 3D surface, the gradient vector `∇f(a,b)` at a point `(a,b)` can be thought of as a vector in the xy-plane that points in the direction you would have to walk to go straight uphill on the surface.\n",
    "\n",
    "#### Gradient and Contour Maps (2D View)\n",
    "This is the more powerful visualization. A **contour map** (or level set) shows lines where the function's value `f(x, y)` is constant. For our mountain analogy, these are lines of constant elevation.\n",
    "\n",
    "**Key Property:** The gradient vector `∇f` at any point is always **perpendicular (orthogonal)** to the contour line that passes through that point.\n",
    "\n",
    "**Why?** The contour line represents the direction of **zero change** in the function's value. The gradient represents the direction of **maximum change**. It makes intuitive sense that the direction of maximum change must be perpendicular to the direction of no change.\n",
    "\n",
    "\n",
    "*Image: The red arrows are gradient vectors. Notice how each one is perpendicular to the blue contour line it starts on.*\n",
    "\n",
    "---\n",
    "\n",
    "### Directional Derivative and Slope\n",
    "\n",
    "#### Theory\n",
    "The gradient tells us the slope in the steepest direction, but what about all the other directions? That's what the **directional derivative** is for.\n",
    "\n",
    "The directional derivative of `f` at a point `(x, y)` in the direction of a **unit vector** `u` is the slope of the surface at that point in that specific direction. It is denoted `Dᵤf`.\n",
    "\n",
    "**Crucial Point:** The direction must be specified by a **unit vector `u`** (a vector with a magnitude of 1). If you have a direction vector `v` that is not a unit vector, you must first normalize it: `u = v / ||v||`.\n",
    "\n",
    "The formal definition is based on a limit, but the practical calculation is much simpler and elegantly involves the gradient.\n",
    "\n",
    "*   **Formula:** **Dᵤf(x, y) = ∇f(x, y) ⋅ u**\n",
    "    *   The directional derivative is the **dot product** of the gradient and the direction unit vector.\n",
    "\n",
    "#### Calculation Example\n",
    "Let's continue with our function `f(x, y) = x³y²` at the point `P(2, 1)`, where we found `∇f(2, 1) = <12, 16>`.\n",
    "\n",
    "We want to find the slope of the surface at this point in the direction of the vector **v = <3, 4>**.\n",
    "\n",
    "1.  **Normalize the direction vector `v` to get the unit vector `u`:**\n",
    "    *   `||v|| = √(3² + 4²) = √(9 + 16) = √25 = 5`\n",
    "    *   `u = v / ||v|| = <3, 4> / 5 = <3/5, 4/5>`\n",
    "\n",
    "2.  **Calculate the dot product:**\n",
    "    *   `Dᵤf(2, 1) = ∇f(2, 1) ⋅ u`\n",
    "    *   `Dᵤf(2, 1) = <12, 16> ⋅ <3/5, 4/5>`\n",
    "    *   `Dᵤf(2, 1) = (12 * 3/5) + (16 * 4/5) = 36/5 + 64/5 = 100/5 = 20`\n",
    "\n",
    "**Interpretation:**\n",
    "*   The slope of the surface at the point (2, 1) in the direction of `<3, 4>` is exactly **20**.\n",
    "\n",
    "**Wait, this is the same as the magnitude of the gradient!** Why? Because the direction vector we chose, `<3, 4>`, happens to point in the exact same direction as our gradient vector, `<12, 16>` (since `<12, 16> = 4 * <3, 4>`). We calculated the slope in the direction of steepest ascent, which is, by definition, the magnitude of the gradient.\n",
    "\n",
    "**Let's try a different direction:** Find the slope in the direction of **v = <1, 0>** (the positive x-direction).\n",
    "1.  `v = <1, 0>` is already a unit vector, so `u = <1, 0>`.\n",
    "2.  `Dᵤf(2, 1) = <12, 16> ⋅ <1, 0> = (12*1) + (16*0) = 12`.\n",
    "    *   **Interpretation:** The slope in the pure x-direction is 12. Notice that this is exactly the value of the partial derivative `∂f/∂x` at that point! The directional derivative is a generalization of the partial derivative.\n",
    "\n",
    "***\n",
    "\n",
    "### Python Code Illustration\n",
    "\n",
    "This code will use `sympy` for symbolic math to find the gradient and `numpy` and `matplotlib` to calculate values and visualize the concepts.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
