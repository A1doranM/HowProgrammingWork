Website with cool explanations: https://mlu-explain.github.io/

# Что такое дерево, и как оно обучается?

Обучение дерева происходит при помощи установки признаков и порогов для них:

1. На каждом шаге выбираем признак и порог формирующий бинарное распределение

2. В зависимости от ответа данные распределяются по двум ветвям

3. Повторяем этот процесс пока листья не станут достаточно однородными, или пока не будут выполненны заданные условия останова.

4. В результате в каждом листе оказывается группа объектов с похожими значениями признака. 

Обучение дерева это жадный алгоритм он всегда фокусируется на лучшем распределении в данный конкретный момент (для данного конкретного узла, тоесть ищет локальный минимум) и не учитывает глобальное влияние выбора.  

# Что такое критерий Джинни, и как он работает?

Критерий Джини это метрика которая измеряет степень неоднородности в нашем дереве, она показывает вероятность того что случайно выбранный объект будет не правильно классифицирован. Чем меньше тем лучше.

$$
Gini(D) = 1 - \sum_{i=1}^{C} \left(\frac{|C_i|}{|D|}\right)^2
$$

где:
- $D$ - датасет
- $C$ - количество классов
- $|C_i|$ - количество примеров класса $i$
- $|D|$ - общее количество примеров

# Как выбирается оптимальное значение для разбиения в дереве по конкретному признаку?

## __Алгоритм выбора оптимального split__

### __1. Для числовых признаков:__

## Процесс поиска оптимального порога

1. **Сортировка значений**: Отсортировать все уникальные значения признака
2. **Перебор кандидатов**: Для каждой пары соседних значений рассмотреть середину как потенциальный порог
3. **Вычисление критерия**: Для каждого порога вычислить информационный прирост (Information Gain)
4. **Выбор лучшего**: Выбрать порог с максимальным приростом

### Формула Information Gain:

$$
IG(D, feature, threshold) = Gini(D) - \sum_{child \in \{left, right\}} \frac{|D_{child}|}{|D|} \cdot Gini(D_{child})
$$

### Пример:

Для признака "возраст" со значениями: [23, 25, 30, 35, 40]

Кандидаты на split:
- threshold = 24 (между 23 и 25)
- threshold = 27.5 (между 25 и 30)
- threshold = 32.5 (между 30 и 35)
- threshold = 37.5 (между 35 и 40)

Для каждого threshold:
1. Разделяем: left = (X < threshold), right = (X ≥ threshold)
2. Вычисляем взвешенный Gini
3. Находим максимальный IG

### __2. Для категориальных признаков:__

## Два подхода:

### Бинарное разбиение (CART):
- Для каждой категории: {категория} vs {все остальные}
- Вычисляем IG для каждого варианта
- Выбираем лучший

### Множественное разбиение (ID3, C4.5):
- Создаём по одной ветви на каждую категорию
- Вычисляем общий прирост информации

$$
IG(D, feature) = Gini(D) - \sum_{v \in values(feature)} \frac{|D_v|}{|D|} \cdot Gini(D_v)
$$

# Что такое разброс и смещение?

Это ключевые концепции машинного обучения, описывающие различные типы ошибок модели.

## Bias (Смещение)

**Смещение** - это ошибка, возникающая из-за упрощающих предположений модели.

- **Высокий bias**: модель слишком простая, не может уловить сложные паттерны (underfitting)
- **Низкий bias**: модель достаточно гибкая для представления данных

### Математическое определение:

$$
Bias[\hat{f}(x)] = E[\hat{f}(x)] - f(x)
$$


где:
- $\hat{f}(x)$ - предсказание модели
- $f(x)$ - истинное значение
- $E[\hat{f}(x)]$ - ожидаемое предсказание по всем возможным обучающим выборкам

## Variance (Разброс)

**Разброс** - это чувствительность модели к изменениям в обучающих данных.

- **Высокий variance**: модель слишком сложная, "запоминает" шум в данных (overfitting)
- **Низкий variance**: модель стабильна, мало меняется при изменении данных

### Математическое определение:

$$
Variance[\hat{f}(x)] = E\left[(\hat{f}(x) - E[\hat{f}(x)])^2\right]
$$

Это средний квадрат отклонения предсказаний от среднего предсказания.

## Основной принцип:

$$
\text{При увеличении сложности модели:}
$$

$$
\begin{cases}
Bias \downarrow & \text{(уменьшается)} \\
Variance \uparrow & \text{(увеличивается)}
\end{cases}
$$

### Оптимальная модель:

$$
\text{optimal complexity} = \arg\min_{complexity} \left(Bias^2 + Variance + \sigma^2\right)
$$

где:
- $\sigma^2$ - неустранимая ошибка (шум в данных)

# Что такое бэггинг?

Bagging (Bootstrap Aggregating)

__Bagging__ - это метод ансамблирования, который уменьшает variance и предотвращает overfitting путём обучения нескольких моделей на различных подвыборках данных.

## __Основная идея__

## Bagging = Bootstrap + Aggregating

1. **Bootstrap**: Создать M различных обучающих выборок
2. **Train**: Обучить M моделей независимо
3. **Aggregate**: Объединить предсказания всех моделей

# Как устроен случайный лес?

## Random Forest = Bagging + Feature Randomness

### Три уровня рандомизации:

1. **Bootstrap Aggregating (Bagging)** - различные обучающие выборки
2. **Feature Sampling** - случайный набор признаков в каждом split
3. **Random Splits** (опционально) - случайные пороги для split


### Архитектура:

```javascript
                Training Data (N примеров, D признаков)
                          |
        ┌─────────────────┼─────────────────┐
        │                 │                 │
  Bootstrap 1       Bootstrap 2      Bootstrap M
   (N примеров)      (N примеров)    (N примеров)
        │                 │                 │
        ↓                 ↓                 ↓
   Tree 1            Tree 2            Tree M
(используют        (используют       (используют
 √D признаков       √D признаков      √D признаков
 в каждом split)    в каждом split)   в каждом split)
        │                 │                 │
        └─────────────────┼─────────────────┘
                          ↓
                Aggregation (Voting/Averaging)
                          ↓
                    Final Prediction
```

## **Feature Sampling (Ключевое отличие от Bagging)**

## Сколько признаков выбирать?

### Типичные значения max_features:

| Задача | max_features | Обоснование |
|--------|--------------|-------------|
| **Классификация** | $\sqrt{D}$ | Баланс между разнообразием и качеством |
| **Регрессия** | $\frac{D}{3}$ | Регрессия требует больше признаков |
| **Максимум** | $D$ | Превращается в обычный Bagging |
| **Минимум** | $1$ | Экстремально декоррелированные деревья |

где $D$ - общее количество признаков.

### Математическое обоснование:

Корреляция между деревьями:

$$
\rho(max\_features) = \frac{max\_features}{D}
$$

Variance ансамбля:

$$
Var[RF] = \rho \sigma^2 + \frac{(1-\rho) \sigma^2}{M}
$$

При уменьшении $max\_features$:
- ✅ $\rho \downarrow$ → variance ансамбля уменьшается
- ❌ Каждое дерево становится слабее (выше bias)

## Почему Random Forest работает?

### 1. Уменьшение Variance через декорреляцию

Variance ансамбля из $M$ моделей с корреляцией $\rho$:

$$
Var[\bar{Y}] = \rho \sigma^2 + \frac{(1-\rho)\sigma^2}{M}
$$

где:
- $\sigma^2$ - variance одного дерева
- $\rho$ - попарная корреляция между деревьями

#### Bagging (все признаки):
$$
\rho_{bagging} \approx 0.5\text{-}0.8 \quad \text{(высокая корреляция)}
$$

#### Random Forest (feature sampling):
$$
\rho_{RF} \approx 0.05\text{-}0.3 \quad \text{(низкая корреляция)}
$$

**Результат:** Значительно меньшая variance!

### 2. Decomposition ошибки

$$
Error_{RF} = Bias^2 + Variance + Noise
$$

$$
\begin{align}
Bias_{RF} &\approx Bias_{single\\_tree} \quad \text{(немного выше)} \\
Variance_{RF} &\ll Variance_{single\\_tree} \quad \text{(гораздо меньше)}
\end{align}
$$

### 3. Теорема о сильной сходимости

При $M \to \infty$:

$$
Error_{RF} \to \bar{\rho} \cdot \frac{\text{Var}[tree]}{M} + (1 - \bar{\rho}) \cdot \text{Var}[tree] + Bias^2
$$

**Вывод:** Даже при бесконечном количестве деревьев ошибка ограничена корреляцией!

## Преимущества Random Forest

✅ **Высокая точность**
- Один из лучших out-of-the-box алгоритмов
- Побеждает во многих Kaggle соревнованиях

✅ **Устойчивость к переобучению**
- Практически не переобучается при увеличении деревьев
- Хорошо работает с высокоразмерными данными

✅ **Автоматическая обработка признаков**
- Не требует нормализации
- Устойчив к выбросам
- Работает с пропущенными значениями (через surrogate splits)

✅ **Feature Importance**
- Автоматически оценивает важность признаков
- Можно использовать для feature selection

✅ **Универсальность**
- Работает для классификации и регрессии
- Мультиклассовая классификация
- Несбалансированные данные (через class_weight)

✅ **Параллелизация**
- Деревья обучаются независимо
- Легко распараллелить на множество ядер

✅ **OOB оценка**
- Бесплатная валидация без отдельной выборки

## Недостатки Random Forest

❌ **Интерпретируемость**
- Сложно интерпретировать ансамбль из сотен деревьев
- Нет простой формулы для предсказания

❌ **Размер модели**
- Большой размер в памяти (M деревьев × размер одного дерева)
- Медленное предсказание (нужно пройти через M деревьев)

❌ **Экстраполяция**
- Не может предсказывать за пределами обучающих данных
- Для регрессии: предсказания ограничены [min(y_train), max(y_train)]

❌ **Несбалансированные данные**
- Может быть смещение к мажоритарному классу
- Нужно использовать class_weight или balanced sampling

❌ **Категориальные признаки с большим числом категорий**
- Feature importance может быть смещена
- Лучше использовать one-hot encoding или target encoding

❌ **Вычислительная сложность**
- Обучение: $O(M \cdot N \cdot D \cdot \log N)$
- Предсказание: $O(M \cdot \text{depth})$