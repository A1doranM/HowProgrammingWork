# Что такое RNN?

Представьте, что вы читаете книгу. Вы не забываете предыдущие слова сразу после их прочтения - вы держите контекст в голове. RNN работает похожим образом: он обрабатывает текст __слово за словом__ и __запоминает предыдущий контекст__.

Обычная нейросеть видит все слова сразу и независимо. RNN видит слова __последовательно__ и __помнит__, что было раньше.

__Ключевая идея:__ На каждом шаге RNN берет:

1. __Текущее слово__ ($x_t$)

2. __Память о прошлом__ ($h_{t-1}$)
 (скрытое состояние с прошлого обработанного слова)
3. И создает __новую память__ ($h_t$)
, которая содержит информацию обо ВСЁМ, что было до этого момента (новое скрытое состояние)

Скрытое состояние - по сути это представление входящего ембединга (слова) с учетом скрытого состояния которое передалось на предыдущем шаге (тоесть предыдущего слова). Либо еще проще, это представление этого слова с учетом слов до него. Скрытое состояние для первого входящего слова обычно инициализируют либо нулями, либо случайно.  

### Формула 1: Обновление скрытого состояния (памяти)

$$h_t = \tanh(W_{hh} \cdot h_{t-1} + W_{xh} \cdot x_t + b_h)$$

Где: 

$$h_t = \tanh(\underbrace{W_{hh} \cdot h_{t-1}}_{\text{все старые слова}} + \underbrace{W_{xh} \cdot x_t}_{\text{новое слово}} + b_h)$$

```
Новая память = tanh(
    "как учесть старую память" × старая_память +
    "как учесть новое слово" × новое_слово +
    смещение
)
```

## Ключевые моменты

1. __Последовательная обработка__: RNN обрабатывает слова по очереди, а не все сразу
2. __Память контекста__: Вектор $h_t$ хранит информацию обо всех предыдущих словах
3. __Одни и те же веса__: Матрицы $W_{hh}$, $W_{xh}$, $W_{hy}$ - одинаковые для всех шагов
4. __Цепочка зависимостей__: $h_3$ зависит от $h_2$, который зависит от $h_1$, который зависит от $h_0$

# Какие есть плюс и минусы у RNN ?

## ✅ Плюсы

### 1. __Работа с последовательностями переменной длины__

- Может обрабатывать тексты любой длины (от 1 слова до целой книги)
- Один и тот же RNN работает с предложениями из 3 слов и из 100 слов

### 2. __Память о контексте__

- Запоминает предыдущие слова в векторе $h_t$
- Понимает, что "яблоко" в контексте "я ем яблоко" - это еда, а в "Apple выпустила iPhone" - это компания

### 3. __Небольшое количество параметров__

- Одни и те же веса $W_{hh}$, $W_{xh}$, $W_{hy}$ используются для всех слов
- Не растет с увеличением длины текста (в отличие от полносвязных сетей)

### 4. __Учитывает порядок слов__

- "Собака кусает человека" ≠ "Человек кусает собаку"
- Порядок имеет значение, и RNN это понимает

## ❌ Минусы

### __Проблема затухающих/взрывающихся градиентов__

__Самая критичная проблема!__

При обучении через backpropagation через время (BPTT), градиенты умножаются на каждом шаге:

$$\frac{\partial L}{\partial h_1} = \frac{\partial L}{\partial h_T} \cdot \prod_{t=2}^{T} \frac{\partial h_t}{\partial h_{t-1}}$$

__Проблема:__

- Если производная < 1: градиенты __затухают__ (vanishing gradients) → сеть не учит долгосрочные зависимости
- Если производная > 1: градиенты __взрываются__ (exploding gradients) → нестабильное обучение

__Пример:__

```javascript
Предложение: "Кот, который жил у бабушки в деревне прошлым летом, был рыжим"

RNN плохо запомнит связь "кот" → "был рыжим" из-за большого расстояния
```

### 2. __Медленное обучение (нельзя распараллелить)__

- Нужно обработать слово 1, чтобы получить $h_1$
- Потом слово 2, чтобы получить $h_2$
- И так далее последовательно ❌ не параллелится
- В отличие от Transformers, где все слова обрабатываются одновременно

### 3. __Короткая память__

- В теории $h_t$ хранит всю информацию
- На практике информация из начала длинного текста "забывается"
- Эффективная память: обычно 5-10 слов назад

# Какая сложность у RNN ?

## Вывод временной сложности (Forward Pass)

### Шаг 1: Анализ одного временного шага

Формула на одном шаге $t$:

$$h_t = \tanh(W_{hh} \cdot h_{t-1} + W_{xh} \cdot x_t + b_h)$$

__Операции умножения матриц:__

1. __$W_{hh} \cdot h_{t-1}$__

   - $W_{hh}$: размерность $[h \times h]$
   - $h_{t-1}$: размерность $[h \times 1]$
   - Сложность: $O(h^2)$ операций

2. __$W_{xh} \cdot x_t$__

   - $W_{xh}$: размерность $[h \times d]$
   - $x_t$: размерность $[d \times 1]$
   - Сложность: $O(h \cdot d)$ операций

3. __Сложение и tanh__

   - Сложность: $O(h)$ операций

__Итого для одного шага:__

$$\text{Complexity}_{\text{step}} = O(h^2 + h \cdot d + h) = O(h^2 + h \cdot d)$$

(Упрощаем, так как $h^2$ и $h \cdot d$ доминируют над $h$)

---

### Шаг 2: Для всей последовательности

У нас $T$ временных шагов, каждый с сложностью $O(h^2 + h \cdot d)$:

$$\boxed{\text{Forward Pass} = O(T \cdot (h^2 + h \cdot d))}$$

---

### Шаг 3: Вычисление выхода (опционально)

Если нужно вычислить выход на каждом шаге:

$$y_t = W_{hy} \cdot h_t + b_y$$

- $W_{hy}$: размерность $[V \times h]$
- $h_t$: размерность $[h \times 1]$
- Сложность: $O(V \cdot h)$ на каждый шаг

__С учетом выхода:__

$$\text{Forward Pass (full)} = O(T \cdot (h^2 + h \cdot d + V \cdot h))$$

## Вывод сложности Backward Pass (BPTT)

__Backpropagation Through Time (BPTT):__

При обучении градиенты вычисляются для каждого шага в обратном порядке:

$$\frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^{T} \frac{\partial L}{\partial h_t} \cdot \frac{\partial h_t}{\partial W_{hh}}$$

__Сложность:__

- Для каждого из $T$ шагов нужно вычислить градиенты
- Операции аналогичны forward pass (умножение матриц)

$$\boxed{\text{Backward Pass (BPTT)} = O(T \cdot (h^2 + h \cdot d))}$$

## Конкретный пример

Параметры:

- $T = 50$ (длина предложения)
- $d = 300$ (размер эмбеддинга)
- $h = 512$ (размер скрытого состояния)
- $V = 30,000$ (размер словаря)

### Forward Pass:

$$T \cdot (h^2 + h \cdot d) = 50 \cdot (512^2 + 512 \cdot 300)$$ $$= 50 \cdot (262,144 + 153,600) = 50 \cdot 415,744$$ $$= 20,787,200 \text{ операций} \approx 20.8M \text{ ops}$$

### Параметры:

$$h^2 + h \cdot d + V \cdot h = 512^2 + 512 \cdot 300 + 30,000 \cdot 512$$ $$= 262,144 + 153,600 + 15,360,000$$ $$= 15,775,744 \text{ параметров} \approx 15.8M \text{ params}$$

# Что такое transformers (трансформеры)? Чем они отличаются от RNN ?

# Почему у RNN и у transformers эмбеддинги называются контекстуальными ?

# Объяснение - что такое контекстуальные эмбеддинги ?

# Нужно ли в transformers использовать предобработку текста ?

# Что такое токенайзер ? Какие виды существуют ?

# Объяснение - Что такое токенайзер ? Какие виды существуют ?

# Можно ли расширять токенайзер ? И в каких случаях это стоит делать?

# Объяснение - Можно ли расширять токенайзер ? И в каких случаях это стоит использовать?

# Зачем нужна матрица эмбеддингов в transformers? И как её обучать с нуля ?

# Объяснение - Зачем нужна матрица эмбеддингов в transformers? 

# Чем обычные токены отличаются от специализированных ?

# На что влияет размер словаря в transformers ?

# Что такое self-attention в transformers ? 

# Объяснение - Что такое self-attention в transformers ? 

# Ещё больше вопросов с собесов тут - телеграм канал

# Какая архитектура у self-attention ? Как он устроен внутри ?

# Подробнейшее объяснение устройства self-attention

# Написание self-attention с нуля на torch
