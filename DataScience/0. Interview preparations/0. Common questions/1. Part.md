## Что такое линейная регрессия?

Линейная регрессия это метод машинного обучения для предсказания некоторого бесконечного набора чисел, он сводится к тому чтобы для выбранных признаков выставить определенные значения весов. 

$$y = \beta_0 + \beta_1x + \epsilon$$

Where:
- $y$ = dependent variable (target)
- $x$ = independent variable (feature)
- $\beta_0$ = intercept (bias term)
- $\beta_1$ = slope (coefficient)
- $\epsilon$ = error term


Основные подходы для обучения линейной регрессии это аналитический и градиентный спуск: 

### В аналитическом подходе мы решаем уравнение 

$$\mathbf{\hat{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$

Where:
- $\mathbf{\hat{\beta}}$ = vector of estimated coefficients $[\beta_0, \beta_1, ..., \beta_n]^T$
- $\mathbf{X}$ = design matrix (n × (p+1)) with intercept column
- $\mathbf{y}$ = vector of target values (n × 1)
- $\mathbf{X}^T$ = transpose of X
- $(\mathbf{X}^T\mathbf{X})^{-1}$ = inverse of $\mathbf{X}^T\mathbf{X}$

#### Advantages of Analytical Solution

✅ **Exact solution** - No approximation or iteration needed  
✅ **Fast for small datasets** - One-step calculation  
✅ **No hyperparameters** - No learning rate to tune  
✅ **Guaranteed convergence** - Always finds the optimal solution  

#### Disadvantages

❌ **Computational cost** - Expensive for large feature sets (matrix inversion is $O(p^3)$)  
❌ **Memory intensive** - Requires storing $\mathbf{X}^T\mathbf{X}$ matrix  
❌ **Numerical stability** - Can fail if $\mathbf{X}^T\mathbf{X}$ is singular or near-singular  
❌ **Not scalable** - Poor performance with millions of features  

#### When to Use

- **Use Analytical Solution when:**
  - Number of features $p < 10,000$
  - Need exact solution
  - Dataset fits in memory
  - $\mathbf{X}^T\mathbf{X}$ is well-conditioned

### Второй метод это использование градиентного спуска 

При этом методе мы решаем задачу минимизации некоторой функции потерь. Тоесть мы случайно инициализируем веса, делаем прогноз, считаем функцию ошибки, берем градиент от функции потерь и делаем шаг в сторону его уменьшения. 

#### Gradient Descent Update Rules

$$\beta_0 := \beta_0 - \alpha\frac{1}{n}\sum_{i=1}^{n}(\hat{y}_i - y_i)$$

$$\beta_1 := \beta_1 - \alpha\frac{1}{n}\sum_{i=1}^{n}(\hat{y}_i - y_i)x_i$$

Where $\alpha$ is the learning rate.

Самой базовой функцией счиатется MSE (Mean Square Error):

#### Standard Form
$$J(\beta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\beta}(x^{(i)}) - y^{(i)})^2$$

Or equivalently:
$$J(\beta_0, \beta_1, ..., \beta_n) = \frac{1}{2m}\sum_{i=1}^{m}(\hat{y}^{(i)} - y^{(i)})^2$$

Where:
- $J(\beta)$ = cost function
- $m$ = number of training examples
- $h_{\beta}(x^{(i)})$ or $\hat{y}^{(i)}$ = predicted value for example $i$
- $y^{(i)}$ = actual value for example $i$
- $\frac{1}{2}$ = constant for mathematical convenience (cancels during derivative)

## Что такое регуляризация?

Регуляризация используется для передотвращения переобучения в моделях базовые регуляризации это L1 и L2: 


## Что такое переобучение?

## Как можно обнаружить переобучение без использования метрик?

## Что такое асимметричные метрики?

## Что такое логистическая регрессия?

## Что такое отступ логистической регрессии?

## Какие бывают метрики классификации?

## Что такое ROC-AUC и PR-AUC? Какие плюсы и минусы?

## Когда важен Precision, а когда Recall? Практические примеры

## Какие методы существуют для многоклассовой классификации?

## Какое количество моделей при All-vs-All?

## Что такое макро-микро метрики?

## Считаем Precision, Recall, ROC-AUC ручками!

## Объясняю подсчёт ROC-AUC

## Проблемы построенной кривой ROC-AUC
