{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc63ead5",
   "metadata": {},
   "source": [
    "# Regularization Techniques: Complete Explanation\n",
    "## L2 (Ridge/Weight Decay), L1 (Lasso), and Dropout\n",
    "### (Detailed Step-by-Step with Cat vs Dog Classification)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— **Connection to Previous Topics**\n",
    "\n",
    "### **What We Know So Far:**\n",
    "\n",
    "**From Neural Networks:**\n",
    "```\n",
    "Training process:\n",
    "1. Forward pass: Make predictions\n",
    "2. Calculate loss: How wrong we are\n",
    "3. Backward pass: Compute gradients\n",
    "4. Update weights: w := w - Î±Â·âˆ‚L/âˆ‚w\n",
    "```\n",
    "\n",
    "**From CNNs:**\n",
    "```\n",
    "Cat vs Dog classifier:\n",
    "Input (64Ã—64Ã—3 image) â†’ Conv layers â†’ Dense layers â†’ Output [P(cat), P(dog)]\n",
    "\n",
    "Network might have:\n",
    "- 50,000 weights in conv layers\n",
    "- 100,000 weights in dense layers\n",
    "- Total: 150,000 parameters!\n",
    "```\n",
    "\n",
    "**The New Problem: OVERFITTING**\n",
    "\n",
    "---\n",
    "\n",
    "# Part 1: Understanding Overfitting\n",
    "\n",
    "## 1. What is Overfitting?\n",
    "\n",
    "### **Simple Analogy**\n",
    "\n",
    "Imagine a student preparing for an exam:\n",
    "\n",
    "**Good Student (Generalization):**\n",
    "```\n",
    "Studies: Understands concepts\n",
    "Exam: Solves new problems correctly\n",
    "âœ“ Can apply knowledge to unseen questions\n",
    "```\n",
    "\n",
    "**Overfit Student (Memorization):**\n",
    "```\n",
    "Studies: Memorizes every practice problem exactly\n",
    "Exam: Fails on slightly different questions\n",
    "âœ— Only knows exact problems, can't generalize\n",
    "```\n",
    "\n",
    "**Neural networks can make the same mistake!**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Overfitting in Cat vs Dog Classification\n",
    "\n",
    "### **Our Dataset:**\n",
    "\n",
    "```\n",
    "Training Set: 100 images\n",
    "- 50 cats\n",
    "- 50 dogs\n",
    "\n",
    "Test Set: 20 images (never seen before)\n",
    "- 10 cats  \n",
    "- 10 dogs\n",
    "```\n",
    "\n",
    "### **Scenario 1: Healthy Model (Good Generalization)**\n",
    "\n",
    "```\n",
    "Training Accuracy: 95%\n",
    "Test Accuracy: 93%\n",
    "\n",
    "The model learned general features:\n",
    "- \"Pointy ears\" â†’ Cat\n",
    "- \"Floppy ears\" â†’ Dog\n",
    "- \"Whiskers\" â†’ Cat\n",
    "- \"Long snout\" â†’ Dog\n",
    "\n",
    "âœ“ Performs well on new images!\n",
    "```\n",
    "\n",
    "### **Scenario 2: Overfit Model (Memorization)**\n",
    "\n",
    "```\n",
    "Training Accuracy: 100%\n",
    "Test Accuracy: 65%\n",
    "\n",
    "The model memorized specific images:\n",
    "- \"This exact pixel pattern at position (23,45)\" â†’ Cat\n",
    "- \"This specific noise pattern\" â†’ Dog\n",
    "- \"Training image #37's exact colors\" â†’ Cat\n",
    "\n",
    "âœ— Fails on new images! Too specific!\n",
    "```\n",
    "\n",
    "**Visualize the problem:**\n",
    "\n",
    "```\n",
    "         UNDERFITTING          GOOD FIT           OVERFITTING\n",
    "         \n",
    "Train:   â—  â—  â—‹  â—‹          â—  â—  â—‹  â—‹          â—  â—  â—‹  â—‹\n",
    "         â—  â—  â—‹  â—‹          â—  â—  â—‹  â—‹          â—  â—  â—‹  â—‹\n",
    "         \n",
    "Learned: â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â•±â•²â•±â•²â•±â•²â•±â•²â•±â•²\n",
    "         (too simple)       (just right)        (too complex)\n",
    "         \n",
    "Test:    â—  ?  â—‹  ?         â—  â—  â—‹  â—‹          â—  ?  â—‹  ?\n",
    "         Poor on both       Good on both!       Good train,\n",
    "         train & test       85-95%              bad test!\n",
    "         <70%                                   Train: 100%\n",
    "                                                Test: 60%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Real Numbers: Watching Overfitting Happen\n",
    "\n",
    "Let's train a Cat vs Dog classifier and watch it overfit!\n",
    "\n",
    "### **Network Architecture:**\n",
    "\n",
    "```\n",
    "Input: 64Ã—64Ã—3 = 12,288 pixels\n",
    "â†“\n",
    "Hidden Layer 1: 1000 neurons (12,288,000 weights!)\n",
    "â†“\n",
    "Hidden Layer 2: 500 neurons (500,000 weights)\n",
    "â†“\n",
    "Output Layer: 2 neurons (1,000 weights)\n",
    "\n",
    "Total: ~12.8 MILLION parameters\n",
    "Training samples: Only 100 images!\n",
    "\n",
    "Ratio: 128,000 parameters per training sample!\n",
    "This is a recipe for overfitting! ğŸš¨\n",
    "```\n",
    "\n",
    "### **Training Progress (Epoch by Epoch):**\n",
    "\n",
    "| Epoch | Training Loss | Training Acc | Test Loss | Test Acc | What's Happening |\n",
    "|-------|--------------|--------------|-----------|----------|------------------|\n",
    "| 1 | 0.693 | 50% | 0.695 | 48% | Random guessing |\n",
    "| 5 | 0.420 | 78% | 0.435 | 76% | Learning general features |\n",
    "| 10 | 0.210 | 92% | 0.235 | 89% | Good generalization! |\n",
    "| 20 | 0.085 | 98% | 0.315 | 85% | Starting to overfit... |\n",
    "| 30 | 0.025 | 100% | 0.520 | 78% | Overfitting badly! |\n",
    "| 50 | 0.005 | 100% | 0.890 | 65% | Memorized training set |\n",
    "| 100 | 0.001 | 100% | 1.450 | 62% | Complete overfitting |\n",
    "\n",
    "**The Warning Signs:**\n",
    "\n",
    "```\n",
    "    Loss\n",
    "     â†‘\n",
    "  1.5â”‚              â•±â”€ Test loss rising\n",
    "  1.0â”‚            â•±  (BAD SIGN!)\n",
    "  0.5â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±\n",
    "  0.0â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Training loss falling\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs\n",
    "         â†‘ \n",
    "    Best point! (Epoch 10)\n",
    "    Should stop here!\n",
    "```\n",
    "\n",
    "**What the weights look like:**\n",
    "\n",
    "```\n",
    "Epoch 10 (Good):\n",
    "Weights: Small, smooth\n",
    "w = [0.23, -0.15, 0.08, -0.31, 0.19, ...]\n",
    "Most weights: -1 to +1\n",
    "\n",
    "Epoch 100 (Overfit):\n",
    "Weights: Large, chaotic\n",
    "w = [15.3, -23.7, 8.9, -45.2, 31.8, ...]\n",
    "Many weights: -50 to +50!\n",
    "\n",
    "The network became too sensitive!\n",
    "Tiny changes in input â†’ huge changes in output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Why Does Overfitting Happen?\n",
    "\n",
    "### **Reason 1: Too Many Parameters**\n",
    "\n",
    "```\n",
    "100 training images\n",
    "12,800,000 parameters\n",
    "\n",
    "It's like trying to fit 100 data points with a \n",
    "12-million-degree polynomial! You can fit perfectly\n",
    "but it's meaningless.\n",
    "\n",
    "Example with simple data:\n",
    "\n",
    "3 points:  (1,2), (2,4), (3,5)\n",
    "\n",
    "Fit with line (2 parameters): y = 1.5x + 0.5\n",
    "  â†’ Smooth, generalizes well âœ“\n",
    "  \n",
    "Fit with degree-10 polynomial (11 parameters):\n",
    "  â†’ Wiggly, passes through exactly but crazy between points âœ—\n",
    "```\n",
    "\n",
    "### **Reason 2: Not Enough Data**\n",
    "\n",
    "```\n",
    "Network capacity: 12M parameters\n",
    "Training samples: 100\n",
    "\n",
    "The network has too much \"freedom\"\n",
    "Many different weight configurations\n",
    "can all perfectly fit 100 images!\n",
    "\n",
    "Like having 100 equations with 12 million unknowns\n",
    "â†’ Infinite solutions!\n",
    "```\n",
    "\n",
    "### **Reason 3: Training Too Long**\n",
    "\n",
    "```\n",
    "Early training: Learning general patterns\n",
    "Later training: Memorizing noise and specifics\n",
    "\n",
    "It's like studying:\n",
    "- First hour: Understand concepts (good!)\n",
    "- Hour 10: Start memorizing exact wording\n",
    "- Hour 100: Memorized every comma, can't adapt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part 2: L2 Regularization (Ridge / Weight Decay)\n",
    "\n",
    "## 1. Plain English Explanation\n",
    "\n",
    "### **The Core Idea**\n",
    "\n",
    "**L2 regularization says:** \"Keep weights small!\"\n",
    "\n",
    "**Why?** Small weights = simpler model = better generalization\n",
    "\n",
    "```\n",
    "Without L2:                    With L2:\n",
    "Weights can grow huge          Penalty for large weights\n",
    "w = [50, -80, 120, -200]      w = [0.5, -0.8, 1.2, -2.0]\n",
    "     â†‘                              â†‘\n",
    "Over-sensitive!                Reasonable!\n",
    "\n",
    "Small input change â†’ HUGE output  Small input change â†’ Small output\n",
    "Overfits to training noise        Generalizes to new data\n",
    "```\n",
    "\n",
    "### **The Intuition: Financial Penalty**\n",
    "\n",
    "Think of it like a tax on large weights:\n",
    "\n",
    "```\n",
    "Original loss: \"How wrong are predictions?\"\n",
    "L2 loss: \"How wrong are predictions? + Penalty for large weights\"\n",
    "\n",
    "Network must balance:\n",
    "- Fitting training data (low prediction error)\n",
    "- Keeping weights small (low weight penalty)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Mathematics\n",
    "\n",
    "### **Original Loss Function:**\n",
    "\n",
    "$$L = \\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Just measures prediction errors.\n",
    "\n",
    "### **L2 Regularized Loss:**\n",
    "\n",
    "$$L_{L2} = \\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n}w_j^2$$\n",
    "\n",
    "**Components:**\n",
    "\n",
    "| Part | Name | Meaning |\n",
    "|------|------|---------|\n",
    "| $\\frac{1}{m}\\sum(y_i - \\hat{y}_i)^2$ | Prediction loss | How wrong predictions are |\n",
    "| $\\frac{\\lambda}{2}\\sum w_j^2$ | L2 penalty | Sum of squared weights |\n",
    "| $\\lambda$ | Regularization strength | How much to penalize (0.001 to 0.1 typical) |\n",
    "| $w_j$ | Weight j | A specific parameter in the network |\n",
    "| $n$ | Number of weights | Total parameters |\n",
    "\n",
    "**Key insight:** Squaring weights means:\n",
    "- Large weights get HUGE penalty (10Â² = 100)\n",
    "- Small weights get tiny penalty (0.1Â² = 0.01)\n",
    "- Network prefers many small weights over few large ones\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Step-by-Step Numerical Example\n",
    "\n",
    "### **Scenario: Single Neuron**\n",
    "\n",
    "```\n",
    "Input: x = [2.0, 3.0]\n",
    "Weights: w = [wâ‚, wâ‚‚]\n",
    "Bias: b = 0.5\n",
    "True label: y = 1.0\n",
    "\n",
    "Forward pass:\n",
    "z = wâ‚Ã—2.0 + wâ‚‚Ã—3.0 + 0.5\n",
    "Å· = sigmoid(z)\n",
    "```\n",
    "\n",
    "### **Training Step 1: No Regularization (Î» = 0)**\n",
    "\n",
    "**Current weights:**\n",
    "```\n",
    "wâ‚ = 5.0\n",
    "wâ‚‚ = 8.0\n",
    "b = 0.5\n",
    "```\n",
    "\n",
    "**Forward pass:**\n",
    "```\n",
    "z = 5.0Ã—2.0 + 8.0Ã—3.0 + 0.5\n",
    "  = 10.0 + 24.0 + 0.5\n",
    "  = 34.5\n",
    "\n",
    "Å· = sigmoid(34.5) = 1/(1 + e^(-34.5)) â‰ˆ 0.99999999\n",
    "\n",
    "Prediction loss:\n",
    "L_pred = (1.0 - 0.99999999)Â² = 0.00000001\n",
    "\n",
    "Total loss (no regularization):\n",
    "L = 0.00000001 âœ“ (seems perfect!)\n",
    "```\n",
    "\n",
    "**Gradient (no regularization):**\n",
    "```\n",
    "âˆ‚L/âˆ‚wâ‚ = (Å· - y) Ã— xâ‚\n",
    "       = (0.99999999 - 1.0) Ã— 2.0\n",
    "       = -0.00000002\n",
    "\n",
    "âˆ‚L/âˆ‚wâ‚‚ = (Å· - y) Ã— xâ‚‚\n",
    "       = (0.99999999 - 1.0) Ã— 3.0\n",
    "       = -0.00000003\n",
    "```\n",
    "\n",
    "**Update (Î± = 0.1):**\n",
    "```\n",
    "wâ‚ := 5.0 - 0.1Ã—(-0.00000002) = 5.00000000002\n",
    "wâ‚‚ := 8.0 - 0.1Ã—(-0.00000003) = 8.00000000003\n",
    "\n",
    "Weights barely change! They're stuck at large values!\n",
    "Network is overconfident and overfit.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Step 2: With L2 Regularization (Î» = 0.01)**\n",
    "\n",
    "**Same starting weights:**\n",
    "```\n",
    "wâ‚ = 5.0\n",
    "wâ‚‚ = 8.0\n",
    "```\n",
    "\n",
    "**Forward pass (same as before):**\n",
    "```\n",
    "Å· = 0.99999999\n",
    "\n",
    "Prediction loss:\n",
    "L_pred = (1.0 - 0.99999999)Â² = 0.00000001\n",
    "```\n",
    "\n",
    "**L2 penalty:**\n",
    "```\n",
    "L2_penalty = (Î»/2) Ã— (wâ‚Â² + wâ‚‚Â²)\n",
    "           = (0.01/2) Ã— (5.0Â² + 8.0Â²)\n",
    "           = 0.005 Ã— (25 + 64)\n",
    "           = 0.005 Ã— 89\n",
    "           = 0.445\n",
    "\n",
    "Total loss:\n",
    "L = L_pred + L2_penalty\n",
    "  = 0.00000001 + 0.445\n",
    "  = 0.445\n",
    "\n",
    "Loss dominated by regularization!\n",
    "Network says: \"Your weights are too large!\"\n",
    "```\n",
    "\n",
    "**Gradient with L2:**\n",
    "```\n",
    "âˆ‚L/âˆ‚wâ‚ = âˆ‚L_pred/âˆ‚wâ‚ + âˆ‚L2_penalty/âˆ‚wâ‚\n",
    "       = -0.00000002 + Î»Ã—wâ‚\n",
    "       = -0.00000002 + 0.01Ã—5.0\n",
    "       = -0.00000002 + 0.05\n",
    "       = 0.05 (positive! wants to decrease wâ‚)\n",
    "\n",
    "âˆ‚L/âˆ‚wâ‚‚ = âˆ‚L_pred/âˆ‚wâ‚‚ + âˆ‚L2_penalty/âˆ‚wâ‚‚\n",
    "       = -0.00000003 + Î»Ã—wâ‚‚\n",
    "       = -0.00000003 + 0.01Ã—8.0\n",
    "       = -0.00000003 + 0.08\n",
    "       = 0.08 (positive! wants to decrease wâ‚‚)\n",
    "```\n",
    "\n",
    "**Update (Î± = 0.1):**\n",
    "```\n",
    "wâ‚ := 5.0 - 0.1Ã—0.05 = 5.0 - 0.005 = 4.995\n",
    "wâ‚‚ := 8.0 - 0.1Ã—0.08 = 8.0 - 0.008 = 7.992\n",
    "\n",
    "Weights are shrinking!\n",
    "This is called \"weight decay\"\n",
    "```\n",
    "\n",
    "**After 100 iterations:**\n",
    "```\n",
    "Without L2: w = [5.00, 8.00] (stuck, overfit)\n",
    "With L2:    w = [1.23, 1.98] (smaller, generalized)\n",
    "\n",
    "The L2 penalty drove weights down!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Complete Example: Cat vs Dog Network\n",
    "\n",
    "### **Network Architecture:**\n",
    "\n",
    "```\n",
    "Input: 12,288 features (64Ã—64Ã—3 image)\n",
    "Hidden: 100 neurons\n",
    "Output: 2 neurons (cat, dog)\n",
    "\n",
    "Total weights: 12,288Ã—100 + 100Ã—2 = 1,229,000 weights!\n",
    "```\n",
    "\n",
    "### **Training: 3 Different Î» Values**\n",
    "\n",
    "**Dataset:**\n",
    "- 100 training images (50 cats, 50 dogs)\n",
    "- 20 test images (10 cats, 10 dogs)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Case 1: No Regularization (Î» = 0)**\n",
    "\n",
    "**Epoch 1:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [0.02, -0.01, 0.03, -0.02, 0.01]\n",
    "\n",
    "Training loss: 0.693\n",
    "Test loss: 0.701\n",
    "\n",
    "Weight sum: Î£wÂ² = 0.0007 (small)\n",
    "```\n",
    "\n",
    "**Epoch 50:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [45.2, -67.3, 89.1, -123.5, 156.8]\n",
    "\n",
    "Training loss: 0.001 (perfect fit!)\n",
    "Test loss: 1.850 (terrible!)\n",
    "\n",
    "Weight sum: Î£wÂ² = 45,892 (HUGE!)\n",
    "\n",
    "Network memorized training set!\n",
    "```\n",
    "\n",
    "**Weight evolution:**\n",
    "```\n",
    "    |Weight|\n",
    "      â†‘\n",
    "  150â”‚                     â•±\n",
    "  100â”‚                  â•±\n",
    "   50â”‚              â•±\n",
    "    0â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs\n",
    "      \n",
    "Weights exploded!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Case 2: Moderate Regularization (Î» = 0.01)**\n",
    "\n",
    "**Epoch 1:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [0.02, -0.01, 0.03, -0.02, 0.01]\n",
    "\n",
    "L_pred = 0.693\n",
    "L2_penalty = 0.01/2 Ã— 0.0007 = 0.0000035\n",
    "L_total = 0.693\n",
    "\n",
    "Negligible penalty at start (weights small)\n",
    "```\n",
    "\n",
    "**Epoch 50:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [2.3, -1.8, 3.1, -2.7, 1.9]\n",
    "\n",
    "Training loss: 0.085 (good fit)\n",
    "Test loss: 0.095 (generalizes well!)\n",
    "\n",
    "Weight sum: Î£wÂ² = 85.2\n",
    "L2_penalty = 0.01/2 Ã— 85.2 = 0.426\n",
    "\n",
    "Network balanced fitting vs weight size!\n",
    "```\n",
    "\n",
    "**Weight evolution:**\n",
    "```\n",
    "    |Weight|\n",
    "      â†‘\n",
    "    5â”‚      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    0â”‚â”€â”€â”€â”€â•±\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs\n",
    "      \n",
    "Weights grew but stabilized!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Case 3: Too Strong Regularization (Î» = 1.0)**\n",
    "\n",
    "**Epoch 50:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [0.05, -0.03, 0.08, -0.06, 0.04]\n",
    "\n",
    "Training loss: 0.520 (underfit!)\n",
    "Test loss: 0.535 (consistent but poor)\n",
    "\n",
    "Weight sum: Î£wÂ² = 0.14\n",
    "L2_penalty = 1.0/2 Ã— 0.14 = 0.07\n",
    "\n",
    "L2 penalty dominated!\n",
    "Weights stayed too small.\n",
    "Network couldn't fit even training data!\n",
    "```\n",
    "\n",
    "**Weight evolution:**\n",
    "```\n",
    "    |Weight|\n",
    "      â†‘\n",
    " 0.10â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    " 0.05â”‚â•±\n",
    " 0.00â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs\n",
    "      \n",
    "Weights barely grew!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Table:**\n",
    "\n",
    "| Î» | Training Acc | Test Acc | Weight Magnitude | Status |\n",
    "|---|-------------|----------|------------------|--------|\n",
    "| **0** | 100% | 65% | Very large (45-150) | Overfit |\n",
    "| **0.001** | 98% | 88% | Large (5-15) | Slight overfit |\n",
    "| **0.01** | 94% | 92% | Medium (1-5) | **Good!** âœ“ |\n",
    "| **0.1** | 85% | 84% | Small (0.3-2) | Slight underfit |\n",
    "| **1.0** | 68% | 67% | Very small (0.01-0.1) | Underfit |\n",
    "\n",
    "**The sweet spot: Î» = 0.01**\n",
    "\n",
    "---\n",
    "\n",
    "## 5. How Gradient Descent Changes with L2\n",
    "\n",
    "### **Without L2:**\n",
    "\n",
    "```\n",
    "âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w\n",
    "\n",
    "Update:\n",
    "w := w - Î± Ã— âˆ‚L_pred/âˆ‚w\n",
    "\n",
    "Only cares about prediction error!\n",
    "```\n",
    "\n",
    "### **With L2:**\n",
    "\n",
    "```\n",
    "âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w + Î»w\n",
    "\n",
    "Update:\n",
    "w := w - Î± Ã— (âˆ‚L_pred/âˆ‚w + Î»w)\n",
    "  = w - Î±Ã—âˆ‚L_pred/âˆ‚w - Î±Ã—Î»w\n",
    "  = w(1 - Î±Î») - Î±Ã—âˆ‚L_pred/âˆ‚w\n",
    "      â†‘           â†‘\n",
    "   \"decay\"    usual gradient\n",
    "\n",
    "Weight decay factor: (1 - Î±Î»)\n",
    "```\n",
    "\n",
    "**Numerical example:**\n",
    "```\n",
    "w = 10.0\n",
    "Î± = 0.1\n",
    "Î» = 0.01\n",
    "âˆ‚L_pred/âˆ‚w = 2.0\n",
    "\n",
    "Without L2:\n",
    "w := 10.0 - 0.1Ã—2.0 = 10.0 - 0.2 = 9.8\n",
    "\n",
    "With L2:\n",
    "w := 10.0 - 0.1Ã—2.0 - 0.1Ã—0.01Ã—10.0\n",
    "  = 10.0 - 0.2 - 0.01\n",
    "  = 9.79\n",
    "\n",
    "Extra 0.01 from weight decay!\n",
    "```\n",
    "\n",
    "**Over many iterations:**\n",
    "```\n",
    "Iteration 0:   w = 10.00\n",
    "Iteration 1:   w = 9.79\n",
    "Iteration 10:  w = 8.52\n",
    "Iteration 100: w = 3.21\n",
    "Iteration 500: w = 1.15\n",
    "\n",
    "Weight gradually shrinks!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Visualizing L2's Effect\n",
    "\n",
    "### **Loss Landscape Without L2:**\n",
    "\n",
    "```\n",
    "         wâ‚‚\n",
    "          â†‘\n",
    "        5 â”‚     â•±â•²â•±â•²â•±â•²\n",
    "          â”‚   â•±        â•²\n",
    "        0 â”‚â”€â•±   (min)   â•²â”€â†’ wâ‚\n",
    "          â”‚              \n",
    "       -5 â”‚\n",
    "       \n",
    "Network can reach minimum\n",
    "with huge weights (overfit)\n",
    "Many equivalent solutions\n",
    "```\n",
    "\n",
    "### **Loss Landscape With L2:**\n",
    "\n",
    "```\n",
    "         wâ‚‚\n",
    "          â†‘\n",
    "        5 â”‚\n",
    "          â”‚     â•±â”€â•²\n",
    "        0 â”‚â”€â”€â”€â—â”€â”€â”€â”€â”€â†’ wâ‚\n",
    "          â”‚   min\n",
    "       -5 â”‚\n",
    "       \n",
    "L2 adds a \"bowl\" penalty\n",
    "centered at origin\n",
    "Network prefers solutions\n",
    "near (0,0) - small weights!\n",
    "```\n",
    "\n",
    "**Combined landscape:**\n",
    "\n",
    "```\n",
    "         Loss\n",
    "          â†‘\n",
    "          â”‚      â”Œâ”€ L2 penalty (bowl shape)\n",
    "          â”‚     â•±â”‚â•²\n",
    "          â”‚   â•±  â”‚  â•²   â† Combined loss\n",
    "          â”‚ â•±   â—â”‚    â•²  (shifted minimum)\n",
    "          â”‚â•±_____â”‚_____â•²\n",
    "          â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â†’ Weight magnitude\n",
    "                 â†‘\n",
    "            New minimum\n",
    "         (smaller weights)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Why L2 Prevents Overfitting\n",
    "\n",
    "### **Reason 1: Limits Model Complexity**\n",
    "\n",
    "```\n",
    "Without L2:\n",
    "Any weight configuration allowed\n",
    "â†’ Can memorize training data\n",
    "\n",
    "With L2:\n",
    "Large weights heavily penalized\n",
    "â†’ Forces simpler model\n",
    "â†’ Simpler model = better generalization\n",
    "```\n",
    "\n",
    "### **Reason 2: Reduces Sensitivity**\n",
    "\n",
    "```\n",
    "Output = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™\n",
    "\n",
    "Large weights:\n",
    "If wâ‚ = 100, small noise in xâ‚ (Â±0.01)\n",
    "causes huge output change (Â±1.0)\n",
    "â†’ Overly sensitive to noise!\n",
    "\n",
    "Small weights:\n",
    "If wâ‚ = 1.0, noise in xâ‚ (Â±0.01)\n",
    "causes small output change (Â±0.01)\n",
    "â†’ Robust to noise!\n",
    "```\n",
    "\n",
    "### **Reason 3: Spreads Information**\n",
    "\n",
    "```\n",
    "Without L2:\n",
    "Model might use: w = [100, 0, 0, 0, 0]\n",
    "Relies on single feature (brittle!)\n",
    "\n",
    "With L2:\n",
    "Model forced to use: w = [0.5, 0.4, 0.3, 0.6, 0.5]\n",
    "Uses many features (robust!)\n",
    "\n",
    "If one feature fails â†’ Others compensate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Practical Implementation\n",
    "\n",
    "### **PyTorch-style Code:**\n",
    "\n",
    "```python\n",
    "# Without L2\n",
    "loss = criterion(outputs, targets)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# With L2 (Method 1: Manual)\n",
    "lambda_l2 = 0.01\n",
    "l2_penalty = 0\n",
    "for param in model.parameters():\n",
    "    l2_penalty += torch.sum(param ** 2)\n",
    "\n",
    "loss = criterion(outputs, targets) + (lambda_l2 / 2) * l2_penalty\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# With L2 (Method 2: Built-in weight_decay)\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    weight_decay=0.01  # This is Î»!\n",
    ")\n",
    "\n",
    "loss = criterion(outputs, targets)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()  # Automatically applies weight decay\n",
    "```\n",
    "\n",
    "### **Step-by-step: One training batch**\n",
    "\n",
    "```python\n",
    "# Forward pass\n",
    "batch_x = [cat_image_1, dog_image_1, cat_image_2]  # 3 images\n",
    "batch_y = [1, 0, 1]  # 1=cat, 0=dog\n",
    "\n",
    "outputs = model(batch_x)\n",
    "# outputs = [[0.8, 0.2],   # 80% cat - correct!\n",
    "#            [0.3, 0.7],   # 70% dog - correct!\n",
    "#            [0.6, 0.4]]   # 60% cat - correct but uncertain\n",
    "\n",
    "# Prediction loss\n",
    "pred_loss = CrossEntropyLoss(outputs, batch_y)\n",
    "# pred_loss = 0.223 + 0.357 + 0.511 = 1.091 / 3 = 0.364\n",
    "\n",
    "# L2 penalty (Î» = 0.01)\n",
    "all_weights = model.parameters()  # 1.23 million weights\n",
    "weight_sum_sq = sum(w**2 for w in all_weights)  # = 45,200\n",
    "\n",
    "l2_penalty = (0.01 / 2) * 45,200 = 226\n",
    "\n",
    "# Total loss\n",
    "total_loss = 0.364 + 226 = 226.364\n",
    "\n",
    "# Backward and update\n",
    "total_loss.backward()  # Computes âˆ‚L/âˆ‚w for ALL weights\n",
    "optimizer.step()       # Updates: w := w - Î±Ã—(âˆ‚L_pred/âˆ‚w + Î»w)\n",
    "\n",
    "# After update\n",
    "weight_sum_sq = 44,870  # Decreased! Weight decay at work!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Choosing Î» (Hyperparameter Tuning)\n",
    "\n",
    "### **Rule of Thumb:**\n",
    "\n",
    "| Î» value | Effect | When to use |\n",
    "|---------|--------|-------------|\n",
    "| **0** | No regularization | Lots of data, simple model |\n",
    "| **0.0001-0.001** | Very weak | Slight overfitting |\n",
    "| **0.01** | Moderate | **Default starting point** |\n",
    "| **0.1** | Strong | Heavy overfitting |\n",
    "| **1.0+** | Very strong | Extreme overfitting |\n",
    "\n",
    "### **Grid Search Example:**\n",
    "\n",
    "```python\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1.0]\n",
    "best_lambda = None\n",
    "best_val_acc = 0\n",
    "\n",
    "for lam in lambdas:\n",
    "    model = train_model(lambda_l2=lam)\n",
    "    val_acc = evaluate(model, validation_set)\n",
    "    \n",
    "    print(f\"Î»={lam}: Val Acc = {val_acc:.2%}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_lambda = lam\n",
    "\n",
    "# Output:\n",
    "# Î»=0:     Val Acc = 68%  (overfit)\n",
    "# Î»=0.001: Val Acc = 89%\n",
    "# Î»=0.01:  Val Acc = 93%  â† Best!\n",
    "# Î»=0.1:   Val Acc = 86%\n",
    "# Î»=1.0:   Val Acc = 72%  (underfit)\n",
    "\n",
    "print(f\"Best Î»: {best_lambda}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Summary: L2 Regularization\n",
    "\n",
    "### **What L2 Does:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ L2 Regularization (Ridge / Weight Decay) â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ADDS: Penalty term Î»/2 Ã— Î£wÂ²\n",
    "\n",
    "EFFECT: Shrinks weights toward zero\n",
    "\n",
    "GRADIENT: âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w + Î»w\n",
    "\n",
    "UPDATE: w := w(1-Î±Î») - Î±Ã—âˆ‚L_pred/âˆ‚w\n",
    "             â†‘\n",
    "        Weight decay!\n",
    "\n",
    "RESULT: Smaller, more stable weights\n",
    "        â†’ Better generalization\n",
    "```\n",
    "\n",
    "### **Pros and Cons:**\n",
    "\n",
    "**Pros:**\n",
    "- âœ“ Simple to implement\n",
    "- âœ“ Computationally cheap\n",
    "- âœ“ Works well in practice\n",
    "- âœ“ Smooth, differentiable\n",
    "- âœ“ All weights shrink (distributes info)\n",
    "\n",
    "**Cons:**\n",
    "- âœ— Doesn't set weights to exactly zero\n",
    "- âœ— Adds hyperparameter (Î») to tune\n",
    "- âœ— Can underfit if Î» too large\n",
    "\n",
    "---\n",
    "\n",
    "**Next up: L1 Regularization (Lasso) - Similar idea but with a twist!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629ccc2",
   "metadata": {},
   "source": [
    "# Part 3: L1 Regularization (Lasso)\n",
    "\n",
    "## 1. Plain English Explanation\n",
    "\n",
    "### **The Core Difference from L2**\n",
    "\n",
    "**L2 (Ridge):** \"Keep weights small\"\n",
    "- Penalty = Î»/2 Ã— (wâ‚Â² + wâ‚‚Â² + wâ‚ƒÂ² + ...)\n",
    "- Shrinks all weights toward zero\n",
    "- Weights get small but rarely exactly zero\n",
    "\n",
    "**L1 (Lasso):** \"Keep weights small AND prefer sparsity\"\n",
    "- Penalty = Î» Ã— (|wâ‚| + |wâ‚‚| + |wâ‚ƒ| + ...)\n",
    "- Shrinks weights toward zero\n",
    "- Many weights become EXACTLY zero!\n",
    "\n",
    "```\n",
    "L2 Result:                    L1 Result:\n",
    "w = [0.3, 0.5, 0.2, 0.8,     w = [0, 0.7, 0, 1.2,\n",
    "     0.1, 0.4, 0.6, 0.3]          0, 0, 0.9, 0]\n",
    "     \n",
    "All weights small            Half the weights are ZERO!\n",
    "All features used            Only important features used!\n",
    "```\n",
    "\n",
    "### **Why Is This Useful?**\n",
    "\n",
    "**Feature Selection Automatically!**\n",
    "\n",
    "```\n",
    "Cat vs Dog Classifier:\n",
    "64Ã—64Ã—3 = 12,288 pixel features\n",
    "\n",
    "With L2: All 12,288 features have small weights\n",
    "         â†’ Uses all pixels (even noisy ones)\n",
    "\n",
    "With L1: Maybe only 500 features have non-zero weights\n",
    "         â†’ Uses only important pixels!\n",
    "         â†’ \"Cat has whiskers\" âœ“\n",
    "         â†’ \"Random noise in corner\" âœ— (weight = 0)\n",
    "```\n",
    "\n",
    "### **Real-World Analogy**\n",
    "\n",
    "**Hiring for a job:**\n",
    "\n",
    "**L2 approach:** \n",
    "```\n",
    "Give everyone small tasks\n",
    "- Expert: 20% time\n",
    "- Good person: 15% time  \n",
    "- Mediocre: 10% time\n",
    "- Bad person: 5% time\n",
    "\n",
    "Everyone works a little!\n",
    "```\n",
    "\n",
    "**L1 approach:**\n",
    "```\n",
    "Fire the bad performers!\n",
    "- Expert: 40% time\n",
    "- Good person: 35% time\n",
    "- Mediocre: 25% time\n",
    "- Bad person: 0% (FIRED!)\n",
    "\n",
    "Only keep the best!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Mathematics\n",
    "\n",
    "### **L1 Regularized Loss:**\n",
    "\n",
    "$$L_{L1} = \\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2 + \\lambda\\sum_{j=1}^{n}|w_j|$$\n",
    "\n",
    "**Compare with L2:**\n",
    "\n",
    "| Aspect | L2 | L1 |\n",
    "|--------|----|----|\n",
    "| **Formula** | $\\lambda/2 \\times \\sum w_j^2$ | $\\lambda \\times \\sum \\|w_j\\|$ |\n",
    "| **Penalty** | Squared weights | Absolute values |\n",
    "| **Gradient** | $\\lambda w$ (proportional to weight) | $\\lambda \\times \\text{sign}(w)$ (constant!) |\n",
    "| **Effect** | Weights â†’ small | Weights â†’ zero |\n",
    "| **Sparsity** | No | Yes |\n",
    "\n",
    "**Key difference in gradient:**\n",
    "\n",
    "```\n",
    "L2 gradient: âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w + Î»w\n",
    "             If w=10  â†’ adds +10Î»\n",
    "             If w=1   â†’ adds +1Î»\n",
    "             If w=0.1 â†’ adds +0.1Î»\n",
    "             (Gentle push, proportional to size)\n",
    "\n",
    "L1 gradient: âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w + Î»Ã—sign(w)\n",
    "             If w=10  â†’ adds +Î»\n",
    "             If w=1   â†’ adds +Î»  \n",
    "             If w=0.1 â†’ adds +Î»\n",
    "             (Constant push, same for any positive w!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why L1 Creates Sparsity (Intuition)\n",
    "\n",
    "### **The Gradient Behavior**\n",
    "\n",
    "```\n",
    "L2 Gradient vs Weight:        L1 Gradient vs Weight:\n",
    "\n",
    " âˆ‚L/âˆ‚w                         âˆ‚L/âˆ‚w\n",
    "   â†‘                             â†‘\n",
    "10 â”‚        â•±                  1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    " 5 â”‚      â•±                    0 â”‚\n",
    " 0 â”‚â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â†’ w              -1â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "-5 â”‚  â•±                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ w\n",
    "-10â”‚â•±                             \n",
    "   \n",
    "Gradient grows with w         Gradient is constant!\n",
    "Large w â†’ large push          Any w â†’ same push\n",
    "Small w â†’ tiny push           Keeps pushing until w=0!\n",
    "```\n",
    "\n",
    "**What this means:**\n",
    "\n",
    "**L2:**\n",
    "```\n",
    "w = 10  â†’ Push of -10Î» â†’ w becomes 9.9\n",
    "w = 1   â†’ Push of -1Î»  â†’ w becomes 0.99\n",
    "w = 0.1 â†’ Push of -0.1Î» â†’ w becomes 0.099\n",
    "w = 0.01 â†’ Push of -0.01Î» â†’ w becomes 0.0099\n",
    "...\n",
    "Never quite reaches zero!\n",
    "```\n",
    "\n",
    "**L1:**\n",
    "```\n",
    "w = 10  â†’ Push of -Î» â†’ w becomes 9.9\n",
    "w = 1   â†’ Push of -Î» â†’ w becomes 0.9\n",
    "w = 0.1 â†’ Push of -Î» â†’ w becomes 0.0\n",
    "        (if Î»=0.1 and learning rate makes this happen)\n",
    "        \n",
    "Can hit zero exactly!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Step-by-Step Numerical Example\n",
    "\n",
    "### **Scenario: Single Neuron (Comparing L1 vs L2)**\n",
    "\n",
    "```\n",
    "Input: x = [2.0, 1.5, 3.0]\n",
    "Weights: w = [wâ‚, wâ‚‚, wâ‚ƒ]\n",
    "Bias: b = 0.5\n",
    "True label: y = 1.0\n",
    "\n",
    "Forward: z = wâ‚Ã—2.0 + wâ‚‚Ã—1.5 + wâ‚ƒÃ—3.0 + 0.5\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Initial State:**\n",
    "\n",
    "```\n",
    "wâ‚ = 2.0\n",
    "wâ‚‚ = 0.3  (small weight)\n",
    "wâ‚ƒ = 5.0\n",
    "b = 0.5\n",
    "```\n",
    "\n",
    "**Forward pass:**\n",
    "```\n",
    "z = 2.0Ã—2.0 + 0.3Ã—1.5 + 5.0Ã—3.0 + 0.5\n",
    "  = 4.0 + 0.45 + 15.0 + 0.5\n",
    "  = 19.95\n",
    "\n",
    "Å· = sigmoid(19.95) â‰ˆ 0.9999999975\n",
    "\n",
    "Prediction loss:\n",
    "L_pred = (1.0 - 0.9999999975)Â² â‰ˆ 0.00000000006\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Case 1: L2 Regularization (Î» = 0.1)**\n",
    "\n",
    "**L2 penalty:**\n",
    "```\n",
    "L2 = (Î»/2) Ã— (wâ‚Â² + wâ‚‚Â² + wâ‚ƒÂ²)\n",
    "   = (0.1/2) Ã— (2.0Â² + 0.3Â² + 5.0Â²)\n",
    "   = 0.05 Ã— (4.0 + 0.09 + 25.0)\n",
    "   = 0.05 Ã— 29.09\n",
    "   = 1.4545\n",
    "\n",
    "Total loss:\n",
    "L = 0.00000000006 + 1.4545 â‰ˆ 1.4545\n",
    "```\n",
    "\n",
    "**Gradients:**\n",
    "```\n",
    "âˆ‚L_pred/âˆ‚wâ‚ â‰ˆ 0 (prediction almost perfect)\n",
    "âˆ‚L_pred/âˆ‚wâ‚‚ â‰ˆ 0\n",
    "âˆ‚L_pred/âˆ‚wâ‚ƒ â‰ˆ 0\n",
    "\n",
    "L2 gradients:\n",
    "âˆ‚L2/âˆ‚wâ‚ = Î»wâ‚ = 0.1 Ã— 2.0 = 0.2\n",
    "âˆ‚L2/âˆ‚wâ‚‚ = Î»wâ‚‚ = 0.1 Ã— 0.3 = 0.03\n",
    "âˆ‚L2/âˆ‚wâ‚ƒ = Î»wâ‚ƒ = 0.1 Ã— 5.0 = 0.5\n",
    "\n",
    "Total gradients:\n",
    "âˆ‚L/âˆ‚wâ‚ â‰ˆ 0 + 0.2 = 0.2\n",
    "âˆ‚L/âˆ‚wâ‚‚ â‰ˆ 0 + 0.03 = 0.03\n",
    "âˆ‚L/âˆ‚wâ‚ƒ â‰ˆ 0 + 0.5 = 0.5\n",
    "```\n",
    "\n",
    "**Update (learning rate Î± = 0.1):**\n",
    "```\n",
    "wâ‚ := 2.0 - 0.1Ã—0.2 = 2.0 - 0.02 = 1.98\n",
    "wâ‚‚ := 0.3 - 0.1Ã—0.03 = 0.3 - 0.003 = 0.297\n",
    "wâ‚ƒ := 5.0 - 0.1Ã—0.5 = 5.0 - 0.05 = 4.95\n",
    "```\n",
    "\n",
    "**After 100 iterations:**\n",
    "```\n",
    "wâ‚ = 1.23\n",
    "wâ‚‚ = 0.19  â† Still non-zero!\n",
    "wâ‚ƒ = 3.08\n",
    "\n",
    "All weights shrunk proportionally\n",
    "Small weight (wâ‚‚) stayed small but non-zero\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Case 2: L1 Regularization (Î» = 0.1)**\n",
    "\n",
    "**L1 penalty:**\n",
    "```\n",
    "L1 = Î» Ã— (|wâ‚| + |wâ‚‚| + |wâ‚ƒ|)\n",
    "   = 0.1 Ã— (|2.0| + |0.3| + |5.0|)\n",
    "   = 0.1 Ã— (2.0 + 0.3 + 5.0)\n",
    "   = 0.1 Ã— 7.3\n",
    "   = 0.73\n",
    "\n",
    "Total loss:\n",
    "L = 0.00000000006 + 0.73 â‰ˆ 0.73\n",
    "```\n",
    "\n",
    "**L1 gradients:**\n",
    "```\n",
    "âˆ‚L1/âˆ‚wâ‚ = Î» Ã— sign(wâ‚) = 0.1 Ã— sign(2.0) = 0.1 Ã— 1 = 0.1\n",
    "âˆ‚L1/âˆ‚wâ‚‚ = Î» Ã— sign(wâ‚‚) = 0.1 Ã— sign(0.3) = 0.1 Ã— 1 = 0.1\n",
    "âˆ‚L1/âˆ‚wâ‚ƒ = Î» Ã— sign(wâ‚ƒ) = 0.1 Ã— sign(5.0) = 0.1 Ã— 1 = 0.1\n",
    "\n",
    "Notice: All gradients are 0.1 (same magnitude!)\n",
    "Doesn't matter if weight is 0.3 or 5.0!\n",
    "```\n",
    "\n",
    "**Total gradients:**\n",
    "```\n",
    "âˆ‚L/âˆ‚wâ‚ â‰ˆ 0 + 0.1 = 0.1\n",
    "âˆ‚L/âˆ‚wâ‚‚ â‰ˆ 0 + 0.1 = 0.1  â† Same as wâ‚!\n",
    "âˆ‚L/âˆ‚wâ‚ƒ â‰ˆ 0 + 0.1 = 0.1  â† Same as wâ‚!\n",
    "```\n",
    "\n",
    "**Update (learning rate Î± = 0.1):**\n",
    "```\n",
    "wâ‚ := 2.0 - 0.1Ã—0.1 = 2.0 - 0.01 = 1.99\n",
    "wâ‚‚ := 0.3 - 0.1Ã—0.1 = 0.3 - 0.01 = 0.29\n",
    "wâ‚ƒ := 5.0 - 0.1Ã—0.1 = 5.0 - 0.01 = 4.99\n",
    "\n",
    "All decrease by same absolute amount (0.01)\n",
    "Proportionally, wâ‚‚ decreased more!\n",
    "```\n",
    "\n",
    "**After 30 iterations:**\n",
    "```\n",
    "Iteration 1:  wâ‚‚ = 0.29\n",
    "Iteration 10: wâ‚‚ = 0.20\n",
    "Iteration 20: wâ‚‚ = 0.10\n",
    "Iteration 30: wâ‚‚ = 0.00  â† Reached zero!\n",
    "\n",
    "wâ‚‚ hits zero at iteration 30!\n",
    "After that, wâ‚‚ stays at 0 (sparse!)\n",
    "\n",
    "wâ‚ = 1.70 (still large, still useful)\n",
    "wâ‚‚ = 0.00 â† KILLED!\n",
    "wâ‚ƒ = 4.70 (still large, still useful)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Detailed Evolution Over Time:**\n",
    "\n",
    "| Iteration | L2: wâ‚‚ | L1: wâ‚‚ | L2: Gradient | L1: Gradient |\n",
    "|-----------|--------|--------|--------------|--------------|\n",
    "| 0 | 0.300 | 0.300 | 0.030 | 0.100 |\n",
    "| 5 | 0.285 | 0.250 | 0.029 | 0.100 |\n",
    "| 10 | 0.272 | 0.200 | 0.027 | 0.100 |\n",
    "| 15 | 0.259 | 0.150 | 0.026 | 0.100 |\n",
    "| 20 | 0.247 | 0.100 | 0.025 | 0.100 |\n",
    "| 25 | 0.236 | 0.050 | 0.024 | 0.100 |\n",
    "| 30 | 0.225 | **0.000** | 0.023 | **0.000** |\n",
    "| 50 | 0.196 | **0.000** | 0.020 | **0.000** |\n",
    "| 100 | 0.153 | **0.000** | 0.015 | **0.000** |\n",
    "\n",
    "**Key observations:**\n",
    "\n",
    "```\n",
    "L2 Behavior:\n",
    "- Gradient decreases as weight shrinks\n",
    "- Takes forever to reach zero\n",
    "- Weight asymptotically approaches zero\n",
    "- Never exactly zero\n",
    "\n",
    "L1 Behavior:\n",
    "- Gradient constant until weight hits zero\n",
    "- Reaches zero in finite time!\n",
    "- Once zero, gradient becomes zero (stays dead)\n",
    "- Sparse solution!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. The \"Soft Thresholding\" Effect\n",
    "\n",
    "### **What Happens Near Zero:**\n",
    "\n",
    "**L1 has a special property near zero:**\n",
    "\n",
    "```\n",
    "If |âˆ‚L_pred/âˆ‚w| < Î»:\n",
    "  â†’ L1 penalty dominates\n",
    "  â†’ Weight gets pushed to exactly zero!\n",
    "  \n",
    "Example:\n",
    "âˆ‚L_pred/âˆ‚w = 0.05\n",
    "Î» = 0.1\n",
    "\n",
    "Total gradient = 0.05 + 0.1Ã—sign(w) = 0.15\n",
    "Weight decreases until it crosses zero,\n",
    "then the sign flips and it gets pushed back to zero!\n",
    "Net effect: w = 0 (stuck at zero)\n",
    "```\n",
    "\n",
    "**Visualization:**\n",
    "\n",
    "```\n",
    "    âˆ‚L/âˆ‚w\n",
    "      â†‘\n",
    "  0.2 â”‚         â•± L_pred gradient only\n",
    "      â”‚       â•±\n",
    "  0.1 â”‚â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€ Î» = 0.1 (L1 penalty line)\n",
    "      â”‚   â•±   â•²\n",
    "    0 â”‚â”€â—â”€â”€â”€â”€â”€â”€â”€â—â”€â”€ w\n",
    "      â”‚         â•²\n",
    " -0.1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "      \n",
    "Between the two â—'s: L1 penalty > L_pred gradient\n",
    "â†’ Weight gets pushed to zero and stays there!\n",
    "\n",
    "This zone is called the \"sparse region\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Complete Example: Cat vs Dog with L1\n",
    "\n",
    "### **Network Architecture (Same as before):**\n",
    "\n",
    "```\n",
    "Input: 12,288 pixels\n",
    "Hidden: 100 neurons  \n",
    "Output: 2 neurons\n",
    "\n",
    "Total weights: 1,229,000\n",
    "```\n",
    "\n",
    "### **Training with L1 (Î» = 0.001)**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Epoch 1:**\n",
    "\n",
    "**Initial weights (random, small):**\n",
    "```\n",
    "Sample of 10 weights:\n",
    "w = [0.02, -0.01, 0.03, -0.02, 0.01, \n",
    "     0.04, -0.03, 0.02, -0.01, 0.03]\n",
    "\n",
    "All weights non-zero: 1,229,000 / 1,229,000 = 100%\n",
    "```\n",
    "\n",
    "**Forward pass (one training image: cat):**\n",
    "```\n",
    "Prediction: Å· = [0.51, 0.49]  (51% cat - barely right)\n",
    "True label: y = [1, 0]\n",
    "\n",
    "L_pred = CrossEntropyLoss = 0.673\n",
    "```\n",
    "\n",
    "**L1 penalty:**\n",
    "```\n",
    "Sum of absolute weights:\n",
    "Î£|w| = 6,145.3\n",
    "\n",
    "L1_penalty = Î» Ã— Î£|w|\n",
    "           = 0.001 Ã— 6,145.3\n",
    "           = 6.145\n",
    "\n",
    "Total loss:\n",
    "L = 0.673 + 6.145 = 6.818\n",
    "```\n",
    "\n",
    "**L1 gradients (sample):**\n",
    "```\n",
    "For w = 0.02:  âˆ‚L1/âˆ‚w = 0.001 Ã— sign(0.02) = 0.001\n",
    "For w = -0.01: âˆ‚L1/âˆ‚w = 0.001 Ã— sign(-0.01) = -0.001\n",
    "For w = 0.03:  âˆ‚L1/âˆ‚w = 0.001 Ã— sign(0.03) = 0.001\n",
    "\n",
    "All have magnitude 0.001!\n",
    "```\n",
    "\n",
    "**After update:**\n",
    "```\n",
    "Small weights that contribute little:\n",
    "- If |âˆ‚L_pred/âˆ‚w| < 0.001: Weight moves toward zero\n",
    "- Some weights hit zero!\n",
    "\n",
    "Active weights: 1,227,500 / 1,229,000 = 99.9%\n",
    "(1,500 weights already zeroed out!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Epoch 10:**\n",
    "\n",
    "```\n",
    "Training accuracy: 78%\n",
    "Test accuracy: 76%\n",
    "\n",
    "Sample weights:\n",
    "w = [0.00, 0.00, 1.23, -0.00, 0.45,\n",
    "     2.10, 0.00, 0.87, -0.00, 1.56]\n",
    "     \n",
    "Active weights: 982,000 / 1,229,000 = 79.9%\n",
    "Zero weights: 247,000 (20% are dead!)\n",
    "\n",
    "L_pred = 0.421\n",
    "L1_penalty = 0.001 Ã— 3,892.1 = 3.892\n",
    "Total loss = 4.313\n",
    "```\n",
    "\n",
    "**What's happening:**\n",
    "```\n",
    "Network is learning:\n",
    "- Important features (cat's ears, whiskers): Large weights\n",
    "- Unimportant features (background noise): Zero weights\n",
    "\n",
    "Feature selection in progress!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Epoch 50:**\n",
    "\n",
    "```\n",
    "Training accuracy: 94%\n",
    "Test accuracy: 91%\n",
    "\n",
    "Active weights: 412,000 / 1,229,000 = 33.5%\n",
    "Zero weights: 817,000 (66.5% are dead!)\n",
    "\n",
    "Network uses only 1/3 of original features!\n",
    "\n",
    "Sample weights:\n",
    "w = [0.00, 0.00, 3.45, 0.00, 0.00,\n",
    "     5.23, 0.00, 2.87, 0.00, 4.12]\n",
    "\n",
    "Important pixel features:\n",
    "- Pixel (23, 34): w = 5.23  (cat ear detector!)\n",
    "- Pixel (45, 12): w = 4.12  (whisker detector!)\n",
    "- Pixel (67, 89): w = 0.00  (random background - ignored)\n",
    "\n",
    "L_pred = 0.095\n",
    "L1_penalty = 0.001 Ã— 2,145.7 = 2.146\n",
    "Total loss = 2.241\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Final Model (Epoch 100):**\n",
    "\n",
    "```\n",
    "Training accuracy: 96%\n",
    "Test accuracy: 93%\n",
    "\n",
    "Active weights: 287,000 / 1,229,000 = 23.4%\n",
    "Zero weights: 942,000 (76.6% sparse!)\n",
    "\n",
    "Network uses only 287K parameters instead of 1.23M!\n",
    "- 76.6% reduction in model size!\n",
    "- Faster inference!\n",
    "- Better interpretability!\n",
    "\n",
    "Weight distribution:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Weight Value  â”‚  Count          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  0.0 (exactly)â”‚  942,000 (76.6%)â”‚ â† Sparsity!\n",
    "â”‚  0.0 to 1.0   â”‚   98,000        â”‚\n",
    "â”‚  1.0 to 3.0   â”‚  142,000        â”‚\n",
    "â”‚  3.0 to 5.0   â”‚   38,000        â”‚\n",
    "â”‚  5.0+         â”‚    9,000        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Most important features:\n",
    "Top 5 non-zero weights:\n",
    "1. w[234,567] = 8.9   (strong cat ear signal)\n",
    "2. w[123,456] = 7.2   (whisker pattern)\n",
    "3. w[789,012] = 6.8   (dog nose shape)\n",
    "4. w[345,678] = 6.1   (fur texture)\n",
    "5. w[901,234] = 5.9   (eye position)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Comparing L1 vs L2 on Same Network\n",
    "\n",
    "### **Same Cat vs Dog Network, Same Data:**\n",
    "\n",
    "| Metric | L2 (Î»=0.01) | L1 (Î»=0.001) |\n",
    "|--------|-------------|--------------|\n",
    "| **Training Acc** | 94% | 96% |\n",
    "| **Test Acc** | 92% | 93% |\n",
    "| **Non-zero weights** | 1,229,000 (100%) | 287,000 (23%) |\n",
    "| **Zero weights** | 0 (0%) | 942,000 (77%) |\n",
    "| **Model size** | 4.9 MB | 1.1 MB â†“ |\n",
    "| **Inference time** | 12 ms | 4 ms â†“ |\n",
    "| **Weight magnitude** | Avg: 0.8 | Avg: 2.1 (for non-zero) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Weight Distribution Comparison:**\n",
    "\n",
    "```\n",
    "L2 Distribution:                L1 Distribution:\n",
    "\n",
    "  Count                          Count\n",
    "    â†‘                              â†‘\n",
    "500Kâ”‚    â•±â•²                    900Kâ”‚â–ˆ\n",
    "    â”‚   â•±  â•²                       â”‚â–ˆ\n",
    "300Kâ”‚  â•±    â•²                      â”‚â–ˆ\n",
    "    â”‚ â•±      â•²                     â”‚â–ˆ\n",
    "100Kâ”‚â•±        â•²                100Kâ”‚ â•±â•²\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Weight        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Weight\n",
    "   -2  -1  0  1  2               0   2   4   6\n",
    "\n",
    "Bell curve around 0            Spike at exactly 0!\n",
    "All weights used              + Few large weights\n",
    "                              \"Sparse\" solution\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Feature Importance:**\n",
    "\n",
    "**L2 Result:**\n",
    "```\n",
    "All 12,288 pixels used with small weights:\n",
    "Pixel 1:    w = 0.03  (ear - important)\n",
    "Pixel 2:    w = 0.02  (whisker - important)\n",
    "...\n",
    "Pixel 5000: w = 0.005 (background noise - useless but included)\n",
    "...\n",
    "Pixel 12288: w = 0.001 (corner - useless but included)\n",
    "\n",
    "Cannot tell which features are important!\n",
    "All features contribute a little.\n",
    "```\n",
    "\n",
    "**L1 Result:**\n",
    "```\n",
    "Only 2,156 pixels (17%) have non-zero weights:\n",
    "Pixel 1:    w = 2.34  (ear - IMPORTANT! âœ“)\n",
    "Pixel 2:    w = 1.89  (whisker - IMPORTANT! âœ“)\n",
    "...\n",
    "Pixel 5000: w = 0.00  (background noise - IGNORED! âœ“)\n",
    "...\n",
    "Pixel 12288: w = 0.00  (corner - IGNORED! âœ“)\n",
    "\n",
    "Clear feature selection!\n",
    "Easy to interpret: \"Model uses these 2,156 pixels\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Geometric Intuition: Why L1 Creates Sparsity\n",
    "\n",
    "### **Constraint Regions:**\n",
    "\n",
    "**L2 Constraint: $w_1^2 + w_2^2 \\leq C$**\n",
    "\n",
    "```\n",
    "    wâ‚‚\n",
    "     â†‘\n",
    "   1 â”‚   â—â”€â”€â”€â”€â”€â—\n",
    "     â”‚ â—         â—\n",
    "   0 â”œâ—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â†’ wâ‚\n",
    "     â”‚ â—         â—\n",
    "  -1 â”‚   â—â”€â”€â”€â”€â”€â—\n",
    "  \n",
    "Circle (smooth, round)\n",
    "Solution can be anywhere on circle\n",
    "Rarely hits axes (wâ‚=0 or wâ‚‚=0)\n",
    "```\n",
    "\n",
    "**L1 Constraint: $|w_1| + |w_2| \\leq C$**\n",
    "\n",
    "```\n",
    "    wâ‚‚\n",
    "     â†‘\n",
    "   1 â”‚      â—\n",
    "     â”‚     â•± â•²\n",
    "   0 â”œâ”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â†’ wâ‚\n",
    "     â”‚     â•² â•±\n",
    "  -1 â”‚      â—\n",
    "  \n",
    "Diamond (sharp corners!)\n",
    "Solution tends to hit corners\n",
    "Corners are on axes â†’ sparse!\n",
    "(Either wâ‚=0 or wâ‚‚=0)\n",
    "```\n",
    "\n",
    "### **Optimization with Contours:**\n",
    "\n",
    "```\n",
    "L2 Case:                       L1 Case:\n",
    "\n",
    "    wâ‚‚                            wâ‚‚\n",
    "     â†‘                             â†‘\n",
    "     â”‚  â•±â—‹â—‹â—‹â•²                      â”‚  â•±â—‹â—‹â—‹â•²\n",
    "     â”‚ â—‹     â—‹ â† Loss contours     â”‚ â—‹     â—‹\n",
    "     â”‚â—‹   â—   â—‹                    â”‚â—‹   â—   â—‹\n",
    "     â”‚ â—‹ â•±â—â•² â—‹                     â”‚ â—‹     â—‹\n",
    "     â”‚  â—â”€â”€â”€â—                      â”‚  â—â”€â”€â”€â”€â”€â—\n",
    "     â”‚    L2                       â”‚     â•±â”‚â•²\n",
    "     â”‚  circle                     â”‚   â•±  â”‚  â•²\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ wâ‚              â””â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’ wâ‚\n",
    "         â†‘                               â—\n",
    "    Solution:                      Solution hits axis!\n",
    "    w=[0.3, 0.5]                   w=[0, 0.7] â† Sparse!\n",
    "    Both non-zero                  wâ‚ exactly zero!\n",
    "```\n",
    "\n",
    "**Why corners matter:**\n",
    "```\n",
    "Probability solution hits axis:\n",
    "- Circle (L2): Low (must hit exact point)\n",
    "- Diamond (L1): High (corners ARE on axes!)\n",
    "\n",
    "In high dimensions:\n",
    "- L2: Almost never hits axis (no sparsity)\n",
    "- L1: Frequently hits axes (lots of sparsity!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Proximal Gradient Descent (How to Handle L1's Non-Differentiability)\n",
    "\n",
    "### **The Problem:**\n",
    "\n",
    "L1 penalty $|w|$ is not differentiable at $w=0$!\n",
    "\n",
    "```\n",
    "    |w|\n",
    "     â†‘\n",
    "   2 â”‚      â•±\n",
    "   1 â”‚    â•±\n",
    "   0 â”‚â”€â”€â—â”€â”€â”€â”€â”€â”€â†’ w\n",
    "    -2  0  2\n",
    "        â†‘\n",
    "    Not smooth at zero!\n",
    "    Gradient is undefined!\n",
    "```\n",
    "\n",
    "### **The Solution: Soft Thresholding**\n",
    "\n",
    "Instead of regular gradient descent, use:\n",
    "\n",
    "$$w^{new} = \\text{SoftThreshold}(w^{old} - \\alpha \\nabla L_{pred}, \\alpha\\lambda)$$\n",
    "\n",
    "**Soft Threshold Function:**\n",
    "\n",
    "$$\\text{SoftThreshold}(x, \\theta) = \\begin{cases}\n",
    "x - \\theta & \\text{if } x > \\theta \\\\\n",
    "0 & \\text{if } |x| \\leq \\theta \\\\\n",
    "x + \\theta & \\text{if } x < -\\theta\n",
    "\\end{cases}$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Example:**\n",
    "\n",
    "```\n",
    "Current weight: w = 0.5\n",
    "Learning rate: Î± = 0.1\n",
    "L1 strength: Î» = 0.01\n",
    "Prediction gradient: âˆ‚L_pred/âˆ‚w = 2.0\n",
    "\n",
    "Step 1: Regular gradient step\n",
    "w_temp = w - Î± Ã— âˆ‚L_pred/âˆ‚w\n",
    "       = 0.5 - 0.1 Ã— 2.0\n",
    "       = 0.5 - 0.2\n",
    "       = 0.3\n",
    "\n",
    "Step 2: Apply soft threshold with Î±Î» = 0.1 Ã— 0.01 = 0.001\n",
    "w_new = SoftThreshold(0.3, 0.001)\n",
    "      = 0.3 - 0.001  (since 0.3 > 0.001)\n",
    "      = 0.299\n",
    "\n",
    "If w_temp was 0.0005:\n",
    "w_new = SoftThreshold(0.0005, 0.001)\n",
    "      = 0  (since |0.0005| < 0.001)\n",
    "      Weight gets killed!\n",
    "```\n",
    "\n",
    "### **Numerical Comparison:**\n",
    "\n",
    "| w_temp | Threshold (Î±Î») | w_new | What Happened |\n",
    "|--------|----------------|-------|---------------|\n",
    "| 2.0 | 0.01 | 1.99 | Shrunk slightly |\n",
    "| 0.5 | 0.01 | 0.49 | Shrunk slightly |\n",
    "| 0.1 | 0.01 | 0.09 | Shrunk slightly |\n",
    "| 0.02 | 0.01 | 0.01 | Shrunk to near zero |\n",
    "| 0.005 | 0.01 | **0.00** | Killed! (sparse) |\n",
    "| -0.008 | 0.01 | **0.00** | Killed! (sparse) |\n",
    "| -0.5 | 0.01 | -0.49 | Shrunk slightly |\n",
    "\n",
    "**Visual representation:**\n",
    "\n",
    "```\n",
    "Before soft threshold:          After soft threshold:\n",
    "w = [...0.5, 0.02, 0.005,      w = [...0.49, 0.01, 0.00,\n",
    "     -0.008, -0.5, 0.1...]          0.00, -0.49, 0.09...]\n",
    "                                     â†‘     â†‘\n",
    "                                Killed!  Killed!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Practical Implementation\n",
    "\n",
    "### **PyTorch-style Code:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class L1Regularization:\n",
    "    def __init__(self, lambda_l1=0.001):\n",
    "        self.lambda_l1 = lambda_l1\n",
    "    \n",
    "    def __call__(self, model):\n",
    "        \"\"\"Compute L1 penalty for all model parameters\"\"\"\n",
    "        l1_penalty = 0\n",
    "        for param in model.parameters():\n",
    "            l1_penalty += torch.sum(torch.abs(param))\n",
    "        return self.lambda_l1 * l1_penalty\n",
    "\n",
    "# Training loop\n",
    "model = CatDogClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "l1_reg = L1Regularization(lambda_l1=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Prediction loss\n",
    "        pred_loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # L1 penalty\n",
    "        l1_penalty = l1_reg(model)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = pred_loss + l1_penalty\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Optional: Hard threshold very small weights to exactly zero\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param[torch.abs(param) < 1e-6] = 0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Complete Training Example with Logging:**\n",
    "\n",
    "```python\n",
    "# Training one epoch with detailed logging\n",
    "lambda_l1 = 0.001\n",
    "alpha = 0.1\n",
    "\n",
    "print(\"Before training:\")\n",
    "non_zero_before = sum((p != 0).sum().item() \n",
    "                     for p in model.parameters())\n",
    "print(f\"  Non-zero weights: {non_zero_before:,}\")\n",
    "\n",
    "for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "    # Forward\n",
    "    outputs = model(images)  # Shape: (batch_size, 2)\n",
    "    # outputs = [[0.7, 0.3],  # 70% cat\n",
    "    #            [0.2, 0.8],  # 80% dog\n",
    "    #            [0.6, 0.4]]  # 60% cat\n",
    "    \n",
    "    # Prediction loss\n",
    "    pred_loss = F.cross_entropy(outputs, labels)\n",
    "    # pred_loss = 0.357\n",
    "    \n",
    "    # L1 penalty\n",
    "    l1_penalty = 0\n",
    "    for param in model.parameters():\n",
    "        l1_penalty += torch.abs(param).sum()\n",
    "    l1_penalty = lambda_l1 * l1_penalty\n",
    "    # l1_penalty = 0.001 Ã— 1,128,945 = 1,128.945\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = pred_loss + l1_penalty\n",
    "    # total_loss = 0.357 + 1,128.945 = 1,129.302\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Soft thresholding (built into the gradient)\n",
    "    # PyTorch automatically handles this through the L1 gradient\n",
    "    \n",
    "    # Update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Hard thresholding (optional, for true sparsity)\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            # Kill weights smaller than threshold\n",
    "            mask = torch.abs(param) < 1e-4\n",
    "            param[mask] = 0\n",
    "    \n",
    "    if batch_idx % 10 == 0:\n",
    "        # Count non-zero weights\n",
    "        non_zero = sum((p != 0).sum().item() \n",
    "                      for p in model.parameters())\n",
    "        sparsity = (1 - non_zero / non_zero_before) * 100\n",
    "        \n",
    "        print(f\"Batch {batch_idx}:\")\n",
    "        print(f\"  Loss: {total_loss.item():.4f}\")\n",
    "        print(f\"  Non-zero weights: {non_zero:,}\")\n",
    "        print(f\"  Sparsity: {sparsity:.1f}%\")\n",
    "\n",
    "print(\"\\nAfter training:\")\n",
    "non_zero_after = sum((p != 0).sum().item() \n",
    "                    for p in model.parameters())\n",
    "sparsity_final = (1 - non_zero_after / non_zero_before) * 100\n",
    "print(f\"  Non-zero weights: {non_zero_after:,}\")\n",
    "print(f\"  Final sparsity: {sparsity_final:.1f}%\")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Before training:\n",
    "  Non-zero weights: 1,229,000\n",
    "\n",
    "Batch 0:\n",
    "  Loss: 1,129.3024\n",
    "  Non-zero weights: 1,227,834\n",
    "  Sparsity: 0.1%\n",
    "\n",
    "Batch 10:\n",
    "  Loss: 845.2134\n",
    "  Non-zero weights: 1,198,456\n",
    "  Sparsity: 2.5%\n",
    "\n",
    "Batch 20:\n",
    "  Loss: 623.4521\n",
    "  Non-zero weights: 1,145,789\n",
    "  Sparsity: 6.8%\n",
    "\n",
    "...\n",
    "\n",
    "Batch 100:\n",
    "  Loss: 234.5678\n",
    "  Non-zero weights: 892,341\n",
    "  Sparsity: 27.4%\n",
    "\n",
    "After training:\n",
    "  Non-zero weights: 287,234\n",
    "  Final sparsity: 76.6%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 11. When to Use L1 vs L2\n",
    "\n",
    "### **Decision Guide:**\n",
    "\n",
    "| Scenario | Best Choice | Reason |\n",
    "|----------|-------------|--------|\n",
    "| **High-dimensional data** (many features) | L1 | Feature selection needed |\n",
    "| **Most features relevant** | L2 | Keep all features |\n",
    "| **Want interpretability** | L1 | See which features matter |\n",
    "| **Want faster inference** | L1 | Fewer non-zero weights |\n",
    "| **Want smaller model** | L1 | Sparsity reduces size |\n",
    "| **Features correlated** | L2 | L1 picks one randomly |\n",
    "| **Stable gradient flow** | L2 | Smooth everywhere |\n",
    "| **Don't care about size** | L2 | Easier to optimize |\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Examples:**\n",
    "\n",
    "**Use L1 for:**\n",
    "\n",
    "```\n",
    "1. Medical diagnosis (1000s of gene expressions)\n",
    "   â†’ Want to know: \"Which 20 genes matter?\"\n",
    "   â†’ L1 gives sparse solution: 980 weights = 0\n",
    "\n",
    "2. Text classification (100,000 vocabulary)\n",
    "   â†’ Want to know: \"Which 500 words are most indicative?\"\n",
    "   â†’ L1 selects important words, zeros out rare ones\n",
    "\n",
    "3. Mobile deployment\n",
    "   â†’ Need small model (limited memory/compute)\n",
    "   â†’ L1 gives 70% sparsity â†’ 3Ã— smaller model\n",
    "\n",
    "4. Image features (12,288 pixels)\n",
    "   â†’ Want to know: \"Which pixels detect cats?\"\n",
    "   â†’ L1 shows: \"These 2,000 pixels around ears/whiskers\"\n",
    "```\n",
    "\n",
    "**Use L2 for:**\n",
    "\n",
    "```\n",
    "1. Image classification (already few features after conv layers)\n",
    "   â†’ All features important\n",
    "   â†’ L2 keeps all, makes smooth\n",
    "\n",
    "2. General-purpose model\n",
    "   â†’ No specific feature selection needed\n",
    "   â†’ L2 easier to train, more stable\n",
    "\n",
    "3. Small networks\n",
    "   â†’ Model size not a concern\n",
    "   â†’ L2 gives slight performance edge\n",
    "\n",
    "4. Highly correlated features\n",
    "   â†’ L1 would pick one arbitrarily\n",
    "   â†’ L2 distributes weights fairly across correlates\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Elastic Net (Combining L1 and L2):**\n",
    "\n",
    "Sometimes best to use BOTH!\n",
    "\n",
    "$$L_{elastic} = L_{pred} + \\lambda_1 \\sum|w_i| + \\frac{\\lambda_2}{2}\\sum w_i^2$$\n",
    "\n",
    "```\n",
    "Gets benefits of both:\n",
    "- L1: Sparsity, feature selection\n",
    "- L2: Stability, handles correlated features\n",
    "\n",
    "Example:\n",
    "Î»â‚ = 0.001 (L1 for sparsity)\n",
    "Î»â‚‚ = 0.01 (L2 for stability)\n",
    "\n",
    "Result:\n",
    "- 60% sparsity (from L1)\n",
    "- Stable training (from L2)\n",
    "- Best of both worlds!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Summary: L1 Regularization\n",
    "\n",
    "### **What L1 Does:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   L1 Regularization (Lasso)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ADDS: Penalty term Î» Ã— Î£|w|\n",
    "\n",
    "EFFECT: Shrinks weights to EXACTLY zero\n",
    "\n",
    "GRADIENT: âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w + Î»Ã—sign(w)\n",
    "                                â†‘\n",
    "                          Constant magnitude!\n",
    "\n",
    "UPDATE: w := w - Î±Ã—âˆ‚L_pred/âˆ‚w - Î±Ã—Î»Ã—sign(w)\n",
    "\n",
    "RESULT: Sparse weights (many exactly zero)\n",
    "        â†’ Feature selection\n",
    "        â†’ Smaller models\n",
    "        â†’ Better interpretability\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **L1 vs L2 Comparison:**\n",
    "\n",
    "| Aspect | L2 (Ridge) | L1 (Lasso) |\n",
    "|--------|------------|------------|\n",
    "| **Penalty** | $\\frac{\\lambda}{2}\\sum w^2$ | $\\lambda \\sum \\|w\\|$ |\n",
    "| **Gradient** | $\\lambda w$ (proportional) | $\\lambda \\cdot \\text{sign}(w)$ (constant) |\n",
    "| **Sparsity** | No | Yes âœ“ |\n",
    "| **Feature Selection** | No | Yes âœ“ |\n",
    "| **Differentiability** | Smooth | Non-smooth at 0 |\n",
    "| **Model Size** | Full | Reduced âœ“ |\n",
    "| **Interpretability** | Hard | Easy âœ“ |\n",
    "| **Training Speed** | Faster | Slightly slower |\n",
    "| **Correlated Features** | Keeps all | Picks one |\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **L1 creates sparsity** because constant gradient pushes small weights to exactly zero\n",
    "\n",
    "2. **Geometric interpretation:** Diamond-shaped constraint with corners on axes\n",
    "\n",
    "3. **Feature selection:** Automatically identifies important features\n",
    "\n",
    "4. **Practical benefit:** Smaller models, faster inference, better interpretability\n",
    "\n",
    "5. **Trade-off:** Slightly harder to optimize (non-smooth), but worth it for sparsity\n",
    "\n",
    "---\n",
    "\n",
    "**Next: Dropout - A completely different approach to regularization!** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b5000",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
