{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc63ead5",
   "metadata": {},
   "source": [
    "# Regularization Techniques: Complete Explanation\n",
    "## L2 (Ridge/Weight Decay), L1 (Lasso), and Dropout\n",
    "### (Detailed Step-by-Step with Cat vs Dog Classification)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— **Connection to Previous Topics**\n",
    "\n",
    "### **What We Know So Far:**\n",
    "\n",
    "**From Neural Networks:**\n",
    "```\n",
    "Training process:\n",
    "1. Forward pass: Make predictions\n",
    "2. Calculate loss: How wrong we are\n",
    "3. Backward pass: Compute gradients\n",
    "4. Update weights: w := w - Î±Â·âˆ‚L/âˆ‚w\n",
    "```\n",
    "\n",
    "**From CNNs:**\n",
    "```\n",
    "Cat vs Dog classifier:\n",
    "Input (64Ã—64Ã—3 image) â†’ Conv layers â†’ Dense layers â†’ Output [P(cat), P(dog)]\n",
    "\n",
    "Network might have:\n",
    "- 50,000 weights in conv layers\n",
    "- 100,000 weights in dense layers\n",
    "- Total: 150,000 parameters!\n",
    "```\n",
    "\n",
    "**The New Problem: OVERFITTING**\n",
    "\n",
    "---\n",
    "\n",
    "# Part 1: Understanding Overfitting\n",
    "\n",
    "## 1. What is Overfitting?\n",
    "\n",
    "### **Simple Analogy**\n",
    "\n",
    "Imagine a student preparing for an exam:\n",
    "\n",
    "**Good Student (Generalization):**\n",
    "```\n",
    "Studies: Understands concepts\n",
    "Exam: Solves new problems correctly\n",
    "âœ“ Can apply knowledge to unseen questions\n",
    "```\n",
    "\n",
    "**Overfit Student (Memorization):**\n",
    "```\n",
    "Studies: Memorizes every practice problem exactly\n",
    "Exam: Fails on slightly different questions\n",
    "âœ— Only knows exact problems, can't generalize\n",
    "```\n",
    "\n",
    "**Neural networks can make the same mistake!**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Overfitting in Cat vs Dog Classification\n",
    "\n",
    "### **Our Dataset:**\n",
    "\n",
    "```\n",
    "Training Set: 100 images\n",
    "- 50 cats\n",
    "- 50 dogs\n",
    "\n",
    "Test Set: 20 images (never seen before)\n",
    "- 10 cats  \n",
    "- 10 dogs\n",
    "```\n",
    "\n",
    "### **Scenario 1: Healthy Model (Good Generalization)**\n",
    "\n",
    "```\n",
    "Training Accuracy: 95%\n",
    "Test Accuracy: 93%\n",
    "\n",
    "The model learned general features:\n",
    "- \"Pointy ears\" â†’ Cat\n",
    "- \"Floppy ears\" â†’ Dog\n",
    "- \"Whiskers\" â†’ Cat\n",
    "- \"Long snout\" â†’ Dog\n",
    "\n",
    "âœ“ Performs well on new images!\n",
    "```\n",
    "\n",
    "### **Scenario 2: Overfit Model (Memorization)**\n",
    "\n",
    "```\n",
    "Training Accuracy: 100%\n",
    "Test Accuracy: 65%\n",
    "\n",
    "The model memorized specific images:\n",
    "- \"This exact pixel pattern at position (23,45)\" â†’ Cat\n",
    "- \"This specific noise pattern\" â†’ Dog\n",
    "- \"Training image #37's exact colors\" â†’ Cat\n",
    "\n",
    "âœ— Fails on new images! Too specific!\n",
    "```\n",
    "\n",
    "**Visualize the problem:**\n",
    "\n",
    "```\n",
    "         UNDERFITTING          GOOD FIT           OVERFITTING\n",
    "         \n",
    "Train:   â—  â—  â—‹  â—‹          â—  â—  â—‹  â—‹          â—  â—  â—‹  â—‹\n",
    "         â—  â—  â—‹  â—‹          â—  â—  â—‹  â—‹          â—  â—  â—‹  â—‹\n",
    "         \n",
    "Learned: â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â•±â•²â•±â•²â•±â•²â•±â•²â•±â•²\n",
    "         (too simple)       (just right)        (too complex)\n",
    "         \n",
    "Test:    â—  ?  â—‹  ?         â—  â—  â—‹  â—‹          â—  ?  â—‹  ?\n",
    "         Poor on both       Good on both!       Good train,\n",
    "         train & test       85-95%              bad test!\n",
    "         <70%                                   Train: 100%\n",
    "                                                Test: 60%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Real Numbers: Watching Overfitting Happen\n",
    "\n",
    "Let's train a Cat vs Dog classifier and watch it overfit!\n",
    "\n",
    "### **Network Architecture:**\n",
    "\n",
    "```\n",
    "Input: 64Ã—64Ã—3 = 12,288 pixels\n",
    "â†“\n",
    "Hidden Layer 1: 1000 neurons (12,288,000 weights!)\n",
    "â†“\n",
    "Hidden Layer 2: 500 neurons (500,000 weights)\n",
    "â†“\n",
    "Output Layer: 2 neurons (1,000 weights)\n",
    "\n",
    "Total: ~12.8 MILLION parameters\n",
    "Training samples: Only 100 images!\n",
    "\n",
    "Ratio: 128,000 parameters per training sample!\n",
    "This is a recipe for overfitting! ğŸš¨\n",
    "```\n",
    "\n",
    "### **Training Progress (Epoch by Epoch):**\n",
    "\n",
    "| Epoch | Training Loss | Training Acc | Test Loss | Test Acc | What's Happening |\n",
    "|-------|--------------|--------------|-----------|----------|------------------|\n",
    "| 1 | 0.693 | 50% | 0.695 | 48% | Random guessing |\n",
    "| 5 | 0.420 | 78% | 0.435 | 76% | Learning general features |\n",
    "| 10 | 0.210 | 92% | 0.235 | 89% | Good generalization! |\n",
    "| 20 | 0.085 | 98% | 0.315 | 85% | Starting to overfit... |\n",
    "| 30 | 0.025 | 100% | 0.520 | 78% | Overfitting badly! |\n",
    "| 50 | 0.005 | 100% | 0.890 | 65% | Memorized training set |\n",
    "| 100 | 0.001 | 100% | 1.450 | 62% | Complete overfitting |\n",
    "\n",
    "**The Warning Signs:**\n",
    "\n",
    "```\n",
    "    Loss\n",
    "     â†‘\n",
    "  1.5â”‚              â•±â”€ Test loss rising\n",
    "  1.0â”‚            â•±  (BAD SIGN!)\n",
    "  0.5â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±\n",
    "  0.0â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Training loss falling\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs\n",
    "         â†‘ \n",
    "    Best point! (Epoch 10)\n",
    "    Should stop here!\n",
    "```\n",
    "\n",
    "**What the weights look like:**\n",
    "\n",
    "```\n",
    "Epoch 10 (Good):\n",
    "Weights: Small, smooth\n",
    "w = [0.23, -0.15, 0.08, -0.31, 0.19, ...]\n",
    "Most weights: -1 to +1\n",
    "\n",
    "Epoch 100 (Overfit):\n",
    "Weights: Large, chaotic\n",
    "w = [15.3, -23.7, 8.9, -45.2, 31.8, ...]\n",
    "Many weights: -50 to +50!\n",
    "\n",
    "The network became too sensitive!\n",
    "Tiny changes in input â†’ huge changes in output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Why Does Overfitting Happen?\n",
    "\n",
    "### **Reason 1: Too Many Parameters**\n",
    "\n",
    "```\n",
    "100 training images\n",
    "12,800,000 parameters\n",
    "\n",
    "It's like trying to fit 100 data points with a \n",
    "12-million-degree polynomial! You can fit perfectly\n",
    "but it's meaningless.\n",
    "\n",
    "Example with simple data:\n",
    "\n",
    "3 points:  (1,2), (2,4), (3,5)\n",
    "\n",
    "Fit with line (2 parameters): y = 1.5x + 0.5\n",
    "  â†’ Smooth, generalizes well âœ“\n",
    "  \n",
    "Fit with degree-10 polynomial (11 parameters):\n",
    "  â†’ Wiggly, passes through exactly but crazy between points âœ—\n",
    "```\n",
    "\n",
    "### **Reason 2: Not Enough Data**\n",
    "\n",
    "```\n",
    "Network capacity: 12M parameters\n",
    "Training samples: 100\n",
    "\n",
    "The network has too much \"freedom\"\n",
    "Many different weight configurations\n",
    "can all perfectly fit 100 images!\n",
    "\n",
    "Like having 100 equations with 12 million unknowns\n",
    "â†’ Infinite solutions!\n",
    "```\n",
    "\n",
    "### **Reason 3: Training Too Long**\n",
    "\n",
    "```\n",
    "Early training: Learning general patterns\n",
    "Later training: Memorizing noise and specifics\n",
    "\n",
    "It's like studying:\n",
    "- First hour: Understand concepts (good!)\n",
    "- Hour 10: Start memorizing exact wording\n",
    "- Hour 100: Memorized every comma, can't adapt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part 2: L2 Regularization (Ridge / Weight Decay)\n",
    "\n",
    "## 1. Plain English Explanation\n",
    "\n",
    "### **The Core Idea**\n",
    "\n",
    "**L2 regularization says:** \"Keep weights small!\"\n",
    "\n",
    "**Why?** Small weights = simpler model = better generalization\n",
    "\n",
    "```\n",
    "Without L2:                    With L2:\n",
    "Weights can grow huge          Penalty for large weights\n",
    "w = [50, -80, 120, -200]      w = [0.5, -0.8, 1.2, -2.0]\n",
    "     â†‘                              â†‘\n",
    "Over-sensitive!                Reasonable!\n",
    "\n",
    "Small input change â†’ HUGE output  Small input change â†’ Small output\n",
    "Overfits to training noise        Generalizes to new data\n",
    "```\n",
    "\n",
    "### **The Intuition: Financial Penalty**\n",
    "\n",
    "Think of it like a tax on large weights:\n",
    "\n",
    "```\n",
    "Original loss: \"How wrong are predictions?\"\n",
    "L2 loss: \"How wrong are predictions? + Penalty for large weights\"\n",
    "\n",
    "Network must balance:\n",
    "- Fitting training data (low prediction error)\n",
    "- Keeping weights small (low weight penalty)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Mathematics\n",
    "\n",
    "### **Original Loss Function:**\n",
    "\n",
    "$$L = \\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Just measures prediction errors.\n",
    "\n",
    "### **L2 Regularized Loss:**\n",
    "\n",
    "$$L_{L2} = \\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n}w_j^2$$\n",
    "\n",
    "**Components:**\n",
    "\n",
    "| Part | Name | Meaning |\n",
    "|------|------|---------|\n",
    "| $\\frac{1}{m}\\sum(y_i - \\hat{y}_i)^2$ | Prediction loss | How wrong predictions are |\n",
    "| $\\frac{\\lambda}{2}\\sum w_j^2$ | L2 penalty | Sum of squared weights |\n",
    "| $\\lambda$ | Regularization strength | How much to penalize (0.001 to 0.1 typical) |\n",
    "| $w_j$ | Weight j | A specific parameter in the network |\n",
    "| $n$ | Number of weights | Total parameters |\n",
    "\n",
    "**Key insight:** Squaring weights means:\n",
    "- Large weights get HUGE penalty (10Â² = 100)\n",
    "- Small weights get tiny penalty (0.1Â² = 0.01)\n",
    "- Network prefers many small weights over few large ones\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Step-by-Step Numerical Example\n",
    "\n",
    "### **Scenario: Single Neuron**\n",
    "\n",
    "```\n",
    "Input: x = [2.0, 3.0]\n",
    "Weights: w = [wâ‚, wâ‚‚]\n",
    "Bias: b = 0.5\n",
    "True label: y = 1.0\n",
    "\n",
    "Forward pass:\n",
    "z = wâ‚Ã—2.0 + wâ‚‚Ã—3.0 + 0.5\n",
    "Å· = sigmoid(z)\n",
    "```\n",
    "\n",
    "### **Training Step 1: No Regularization (Î» = 0)**\n",
    "\n",
    "**Current weights:**\n",
    "```\n",
    "wâ‚ = 5.0\n",
    "wâ‚‚ = 8.0\n",
    "b = 0.5\n",
    "```\n",
    "\n",
    "**Forward pass:**\n",
    "```\n",
    "z = 5.0Ã—2.0 + 8.0Ã—3.0 + 0.5\n",
    "  = 10.0 + 24.0 + 0.5\n",
    "  = 34.5\n",
    "\n",
    "Å· = sigmoid(34.5) = 1/(1 + e^(-34.5)) â‰ˆ 0.99999999\n",
    "\n",
    "Prediction loss:\n",
    "L_pred = (1.0 - 0.99999999)Â² = 0.00000001\n",
    "\n",
    "Total loss (no regularization):\n",
    "L = 0.00000001 âœ“ (seems perfect!)\n",
    "```\n",
    "\n",
    "**Gradient (no regularization):**\n",
    "```\n",
    "âˆ‚L/âˆ‚wâ‚ = (Å· - y) Ã— xâ‚\n",
    "       = (0.99999999 - 1.0) Ã— 2.0\n",
    "       = -0.00000002\n",
    "\n",
    "âˆ‚L/âˆ‚wâ‚‚ = (Å· - y) Ã— xâ‚‚\n",
    "       = (0.99999999 - 1.0) Ã— 3.0\n",
    "       = -0.00000003\n",
    "```\n",
    "\n",
    "**Update (Î± = 0.1):**\n",
    "```\n",
    "wâ‚ := 5.0 - 0.1Ã—(-0.00000002) = 5.00000000002\n",
    "wâ‚‚ := 8.0 - 0.1Ã—(-0.00000003) = 8.00000000003\n",
    "\n",
    "Weights barely change! They're stuck at large values!\n",
    "Network is overconfident and overfit.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Step 2: With L2 Regularization (Î» = 0.01)**\n",
    "\n",
    "**Same starting weights:**\n",
    "```\n",
    "wâ‚ = 5.0\n",
    "wâ‚‚ = 8.0\n",
    "```\n",
    "\n",
    "**Forward pass (same as before):**\n",
    "```\n",
    "Å· = 0.99999999\n",
    "\n",
    "Prediction loss:\n",
    "L_pred = (1.0 - 0.99999999)Â² = 0.00000001\n",
    "```\n",
    "\n",
    "**L2 penalty:**\n",
    "```\n",
    "L2_penalty = (Î»/2) Ã— (wâ‚Â² + wâ‚‚Â²)\n",
    "           = (0.01/2) Ã— (5.0Â² + 8.0Â²)\n",
    "           = 0.005 Ã— (25 + 64)\n",
    "           = 0.005 Ã— 89\n",
    "           = 0.445\n",
    "\n",
    "Total loss:\n",
    "L = L_pred + L2_penalty\n",
    "  = 0.00000001 + 0.445\n",
    "  = 0.445\n",
    "\n",
    "Loss dominated by regularization!\n",
    "Network says: \"Your weights are too large!\"\n",
    "```\n",
    "\n",
    "**Gradient with L2:**\n",
    "```\n",
    "âˆ‚L/âˆ‚wâ‚ = âˆ‚L_pred/âˆ‚wâ‚ + âˆ‚L2_penalty/âˆ‚wâ‚\n",
    "       = -0.00000002 + Î»Ã—wâ‚\n",
    "       = -0.00000002 + 0.01Ã—5.0\n",
    "       = -0.00000002 + 0.05\n",
    "       = 0.05 (positive! wants to decrease wâ‚)\n",
    "\n",
    "âˆ‚L/âˆ‚wâ‚‚ = âˆ‚L_pred/âˆ‚wâ‚‚ + âˆ‚L2_penalty/âˆ‚wâ‚‚\n",
    "       = -0.00000003 + Î»Ã—wâ‚‚\n",
    "       = -0.00000003 + 0.01Ã—8.0\n",
    "       = -0.00000003 + 0.08\n",
    "       = 0.08 (positive! wants to decrease wâ‚‚)\n",
    "```\n",
    "\n",
    "**Update (Î± = 0.1):**\n",
    "```\n",
    "wâ‚ := 5.0 - 0.1Ã—0.05 = 5.0 - 0.005 = 4.995\n",
    "wâ‚‚ := 8.0 - 0.1Ã—0.08 = 8.0 - 0.008 = 7.992\n",
    "\n",
    "Weights are shrinking!\n",
    "This is called \"weight decay\"\n",
    "```\n",
    "\n",
    "**After 100 iterations:**\n",
    "```\n",
    "Without L2: w = [5.00, 8.00] (stuck, overfit)\n",
    "With L2:    w = [1.23, 1.98] (smaller, generalized)\n",
    "\n",
    "The L2 penalty drove weights down!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Complete Example: Cat vs Dog Network\n",
    "\n",
    "### **Network Architecture:**\n",
    "\n",
    "```\n",
    "Input: 12,288 features (64Ã—64Ã—3 image)\n",
    "Hidden: 100 neurons\n",
    "Output: 2 neurons (cat, dog)\n",
    "\n",
    "Total weights: 12,288Ã—100 + 100Ã—2 = 1,229,000 weights!\n",
    "```\n",
    "\n",
    "### **Training: 3 Different Î» Values**\n",
    "\n",
    "**Dataset:**\n",
    "- 100 training images (50 cats, 50 dogs)\n",
    "- 20 test images (10 cats, 10 dogs)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Case 1: No Regularization (Î» = 0)**\n",
    "\n",
    "**Epoch 1:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [0.02, -0.01, 0.03, -0.02, 0.01]\n",
    "\n",
    "Training loss: 0.693\n",
    "Test loss: 0.701\n",
    "\n",
    "Weight sum: Î£wÂ² = 0.0007 (small)\n",
    "```\n",
    "\n",
    "**Epoch 50:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [45.2, -67.3, 89.1, -123.5, 156.8]\n",
    "\n",
    "Training loss: 0.001 (perfect fit!)\n",
    "Test loss: 1.850 (terrible!)\n",
    "\n",
    "Weight sum: Î£wÂ² = 45,892 (HUGE!)\n",
    "\n",
    "Network memorized training set!\n",
    "```\n",
    "\n",
    "**Weight evolution:**\n",
    "```\n",
    "    |Weight|\n",
    "      â†‘\n",
    "  150â”‚                     â•±\n",
    "  100â”‚                  â•±\n",
    "   50â”‚              â•±\n",
    "    0â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs\n",
    "      \n",
    "Weights exploded!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Case 2: Moderate Regularization (Î» = 0.01)**\n",
    "\n",
    "**Epoch 1:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [0.02, -0.01, 0.03, -0.02, 0.01]\n",
    "\n",
    "L_pred = 0.693\n",
    "L2_penalty = 0.01/2 Ã— 0.0007 = 0.0000035\n",
    "L_total = 0.693\n",
    "\n",
    "Negligible penalty at start (weights small)\n",
    "```\n",
    "\n",
    "**Epoch 50:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [2.3, -1.8, 3.1, -2.7, 1.9]\n",
    "\n",
    "Training loss: 0.085 (good fit)\n",
    "Test loss: 0.095 (generalizes well!)\n",
    "\n",
    "Weight sum: Î£wÂ² = 85.2\n",
    "L2_penalty = 0.01/2 Ã— 85.2 = 0.426\n",
    "\n",
    "Network balanced fitting vs weight size!\n",
    "```\n",
    "\n",
    "**Weight evolution:**\n",
    "```\n",
    "    |Weight|\n",
    "      â†‘\n",
    "    5â”‚      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    0â”‚â”€â”€â”€â”€â•±\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs\n",
    "      \n",
    "Weights grew but stabilized!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Case 3: Too Strong Regularization (Î» = 1.0)**\n",
    "\n",
    "**Epoch 50:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [0.05, -0.03, 0.08, -0.06, 0.04]\n",
    "\n",
    "Training loss: 0.520 (underfit!)\n",
    "Test loss: 0.535 (consistent but poor)\n",
    "\n",
    "Weight sum: Î£wÂ² = 0.14\n",
    "L2_penalty = 1.0/2 Ã— 0.14 = 0.07\n",
    "\n",
    "L2 penalty dominated!\n",
    "Weights stayed too small.\n",
    "Network couldn't fit even training data!\n",
    "```\n",
    "\n",
    "**Weight evolution:**\n",
    "```\n",
    "    |Weight|\n",
    "      â†‘\n",
    " 0.10â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    " 0.05â”‚â•±\n",
    " 0.00â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs\n",
    "      \n",
    "Weights barely grew!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Table:**\n",
    "\n",
    "| Î» | Training Acc | Test Acc | Weight Magnitude | Status |\n",
    "|---|-------------|----------|------------------|--------|\n",
    "| **0** | 100% | 65% | Very large (45-150) | Overfit |\n",
    "| **0.001** | 98% | 88% | Large (5-15) | Slight overfit |\n",
    "| **0.01** | 94% | 92% | Medium (1-5) | **Good!** âœ“ |\n",
    "| **0.1** | 85% | 84% | Small (0.3-2) | Slight underfit |\n",
    "| **1.0** | 68% | 67% | Very small (0.01-0.1) | Underfit |\n",
    "\n",
    "**The sweet spot: Î» = 0.01**\n",
    "\n",
    "---\n",
    "\n",
    "## 5. How Gradient Descent Changes with L2\n",
    "\n",
    "### **Without L2:**\n",
    "\n",
    "```\n",
    "âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w\n",
    "\n",
    "Update:\n",
    "w := w - Î± Ã— âˆ‚L_pred/âˆ‚w\n",
    "\n",
    "Only cares about prediction error!\n",
    "```\n",
    "\n",
    "### **With L2:**\n",
    "\n",
    "```\n",
    "âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w + Î»w\n",
    "\n",
    "Update:\n",
    "w := w - Î± Ã— (âˆ‚L_pred/âˆ‚w + Î»w)\n",
    "  = w - Î±Ã—âˆ‚L_pred/âˆ‚w - Î±Ã—Î»w\n",
    "  = w(1 - Î±Î») - Î±Ã—âˆ‚L_pred/âˆ‚w\n",
    "      â†‘           â†‘\n",
    "   \"decay\"    usual gradient\n",
    "\n",
    "Weight decay factor: (1 - Î±Î»)\n",
    "```\n",
    "\n",
    "**Numerical example:**\n",
    "```\n",
    "w = 10.0\n",
    "Î± = 0.1\n",
    "Î» = 0.01\n",
    "âˆ‚L_pred/âˆ‚w = 2.0\n",
    "\n",
    "Without L2:\n",
    "w := 10.0 - 0.1Ã—2.0 = 10.0 - 0.2 = 9.8\n",
    "\n",
    "With L2:\n",
    "w := 10.0 - 0.1Ã—2.0 - 0.1Ã—0.01Ã—10.0\n",
    "  = 10.0 - 0.2 - 0.01\n",
    "  = 9.79\n",
    "\n",
    "Extra 0.01 from weight decay!\n",
    "```\n",
    "\n",
    "**Over many iterations:**\n",
    "```\n",
    "Iteration 0:   w = 10.00\n",
    "Iteration 1:   w = 9.79\n",
    "Iteration 10:  w = 8.52\n",
    "Iteration 100: w = 3.21\n",
    "Iteration 500: w = 1.15\n",
    "\n",
    "Weight gradually shrinks!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Visualizing L2's Effect\n",
    "\n",
    "### **Loss Landscape Without L2:**\n",
    "\n",
    "```\n",
    "         wâ‚‚\n",
    "          â†‘\n",
    "        5 â”‚     â•±â•²â•±â•²â•±â•²\n",
    "          â”‚   â•±        â•²\n",
    "        0 â”‚â”€â•±   (min)   â•²â”€â†’ wâ‚\n",
    "          â”‚              \n",
    "       -5 â”‚\n",
    "       \n",
    "Network can reach minimum\n",
    "with huge weights (overfit)\n",
    "Many equivalent solutions\n",
    "```\n",
    "\n",
    "### **Loss Landscape With L2:**\n",
    "\n",
    "```\n",
    "         wâ‚‚\n",
    "          â†‘\n",
    "        5 â”‚\n",
    "          â”‚     â•±â”€â•²\n",
    "        0 â”‚â”€â”€â”€â—â”€â”€â”€â”€â”€â†’ wâ‚\n",
    "          â”‚   min\n",
    "       -5 â”‚\n",
    "       \n",
    "L2 adds a \"bowl\" penalty\n",
    "centered at origin\n",
    "Network prefers solutions\n",
    "near (0,0) - small weights!\n",
    "```\n",
    "\n",
    "**Combined landscape:**\n",
    "\n",
    "```\n",
    "         Loss\n",
    "          â†‘\n",
    "          â”‚      â”Œâ”€ L2 penalty (bowl shape)\n",
    "          â”‚     â•±â”‚â•²\n",
    "          â”‚   â•±  â”‚  â•²   â† Combined loss\n",
    "          â”‚ â•±   â—â”‚    â•²  (shifted minimum)\n",
    "          â”‚â•±_____â”‚_____â•²\n",
    "          â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â†’ Weight magnitude\n",
    "                 â†‘\n",
    "            New minimum\n",
    "         (smaller weights)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Why L2 Prevents Overfitting\n",
    "\n",
    "### **Reason 1: Limits Model Complexity**\n",
    "\n",
    "```\n",
    "Without L2:\n",
    "Any weight configuration allowed\n",
    "â†’ Can memorize training data\n",
    "\n",
    "With L2:\n",
    "Large weights heavily penalized\n",
    "â†’ Forces simpler model\n",
    "â†’ Simpler model = better generalization\n",
    "```\n",
    "\n",
    "### **Reason 2: Reduces Sensitivity**\n",
    "\n",
    "```\n",
    "Output = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™\n",
    "\n",
    "Large weights:\n",
    "If wâ‚ = 100, small noise in xâ‚ (Â±0.01)\n",
    "causes huge output change (Â±1.0)\n",
    "â†’ Overly sensitive to noise!\n",
    "\n",
    "Small weights:\n",
    "If wâ‚ = 1.0, noise in xâ‚ (Â±0.01)\n",
    "causes small output change (Â±0.01)\n",
    "â†’ Robust to noise!\n",
    "```\n",
    "\n",
    "### **Reason 3: Spreads Information**\n",
    "\n",
    "```\n",
    "Without L2:\n",
    "Model might use: w = [100, 0, 0, 0, 0]\n",
    "Relies on single feature (brittle!)\n",
    "\n",
    "With L2:\n",
    "Model forced to use: w = [0.5, 0.4, 0.3, 0.6, 0.5]\n",
    "Uses many features (robust!)\n",
    "\n",
    "If one feature fails â†’ Others compensate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Practical Implementation\n",
    "\n",
    "### **PyTorch-style Code:**\n",
    "\n",
    "```python\n",
    "# Without L2\n",
    "loss = criterion(outputs, targets)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# With L2 (Method 1: Manual)\n",
    "lambda_l2 = 0.01\n",
    "l2_penalty = 0\n",
    "for param in model.parameters():\n",
    "    l2_penalty += torch.sum(param ** 2)\n",
    "\n",
    "loss = criterion(outputs, targets) + (lambda_l2 / 2) * l2_penalty\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# With L2 (Method 2: Built-in weight_decay)\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    weight_decay=0.01  # This is Î»!\n",
    ")\n",
    "\n",
    "loss = criterion(outputs, targets)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()  # Automatically applies weight decay\n",
    "```\n",
    "\n",
    "### **Step-by-step: One training batch**\n",
    "\n",
    "```python\n",
    "# Forward pass\n",
    "batch_x = [cat_image_1, dog_image_1, cat_image_2]  # 3 images\n",
    "batch_y = [1, 0, 1]  # 1=cat, 0=dog\n",
    "\n",
    "outputs = model(batch_x)\n",
    "# outputs = [[0.8, 0.2],   # 80% cat - correct!\n",
    "#            [0.3, 0.7],   # 70% dog - correct!\n",
    "#            [0.6, 0.4]]   # 60% cat - correct but uncertain\n",
    "\n",
    "# Prediction loss\n",
    "pred_loss = CrossEntropyLoss(outputs, batch_y)\n",
    "# pred_loss = 0.223 + 0.357 + 0.511 = 1.091 / 3 = 0.364\n",
    "\n",
    "# L2 penalty (Î» = 0.01)\n",
    "all_weights = model.parameters()  # 1.23 million weights\n",
    "weight_sum_sq = sum(w**2 for w in all_weights)  # = 45,200\n",
    "\n",
    "l2_penalty = (0.01 / 2) * 45,200 = 226\n",
    "\n",
    "# Total loss\n",
    "total_loss = 0.364 + 226 = 226.364\n",
    "\n",
    "# Backward and update\n",
    "total_loss.backward()  # Computes âˆ‚L/âˆ‚w for ALL weights\n",
    "optimizer.step()       # Updates: w := w - Î±Ã—(âˆ‚L_pred/âˆ‚w + Î»w)\n",
    "\n",
    "# After update\n",
    "weight_sum_sq = 44,870  # Decreased! Weight decay at work!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Choosing Î» (Hyperparameter Tuning)\n",
    "\n",
    "### **Rule of Thumb:**\n",
    "\n",
    "| Î» value | Effect | When to use |\n",
    "|---------|--------|-------------|\n",
    "| **0** | No regularization | Lots of data, simple model |\n",
    "| **0.0001-0.001** | Very weak | Slight overfitting |\n",
    "| **0.01** | Moderate | **Default starting point** |\n",
    "| **0.1** | Strong | Heavy overfitting |\n",
    "| **1.0+** | Very strong | Extreme overfitting |\n",
    "\n",
    "### **Grid Search Example:**\n",
    "\n",
    "```python\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1.0]\n",
    "best_lambda = None\n",
    "best_val_acc = 0\n",
    "\n",
    "for lam in lambdas:\n",
    "    model = train_model(lambda_l2=lam)\n",
    "    val_acc = evaluate(model, validation_set)\n",
    "    \n",
    "    print(f\"Î»={lam}: Val Acc = {val_acc:.2%}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_lambda = lam\n",
    "\n",
    "# Output:\n",
    "# Î»=0:     Val Acc = 68%  (overfit)\n",
    "# Î»=0.001: Val Acc = 89%\n",
    "# Î»=0.01:  Val Acc = 93%  â† Best!\n",
    "# Î»=0.1:   Val Acc = 86%\n",
    "# Î»=1.0:   Val Acc = 72%  (underfit)\n",
    "\n",
    "print(f\"Best Î»: {best_lambda}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Summary: L2 Regularization\n",
    "\n",
    "### **What L2 Does:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ L2 Regularization (Ridge / Weight Decay) â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ADDS: Penalty term Î»/2 Ã— Î£wÂ²\n",
    "\n",
    "EFFECT: Shrinks weights toward zero\n",
    "\n",
    "GRADIENT: âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w + Î»w\n",
    "\n",
    "UPDATE: w := w(1-Î±Î») - Î±Ã—âˆ‚L_pred/âˆ‚w\n",
    "             â†‘\n",
    "        Weight decay!\n",
    "\n",
    "RESULT: Smaller, more stable weights\n",
    "        â†’ Better generalization\n",
    "```\n",
    "\n",
    "### **Pros and Cons:**\n",
    "\n",
    "**Pros:**\n",
    "- âœ“ Simple to implement\n",
    "- âœ“ Computationally cheap\n",
    "- âœ“ Works well in practice\n",
    "- âœ“ Smooth, differentiable\n",
    "- âœ“ All weights shrink (distributes info)\n",
    "\n",
    "**Cons:**\n",
    "- âœ— Doesn't set weights to exactly zero\n",
    "- âœ— Adds hyperparameter (Î») to tune\n",
    "- âœ— Can underfit if Î» too large\n",
    "\n",
    "---\n",
    "\n",
    "**Next up: L1 Regularization (Lasso) - Similar idea but with a twist!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629ccc2",
   "metadata": {},
   "source": [
    "# Part 3: L1 Regularization (Lasso)\n",
    "\n",
    "## 1. Plain English Explanation\n",
    "\n",
    "### **The Core Difference from L2**\n",
    "\n",
    "**L2 (Ridge):** \"Keep weights small\"\n",
    "- Penalty = Î»/2 Ã— (wâ‚Â² + wâ‚‚Â² + wâ‚ƒÂ² + ...)\n",
    "- Shrinks all weights toward zero\n",
    "- Weights get small but rarely exactly zero\n",
    "\n",
    "**L1 (Lasso):** \"Keep weights small AND prefer sparsity\"\n",
    "- Penalty = Î» Ã— (|wâ‚| + |wâ‚‚| + |wâ‚ƒ| + ...)\n",
    "- Shrinks weights toward zero\n",
    "- Many weights become EXACTLY zero!\n",
    "\n",
    "```\n",
    "L2 Result:                    L1 Result:\n",
    "w = [0.3, 0.5, 0.2, 0.8,     w = [0, 0.7, 0, 1.2,\n",
    "     0.1, 0.4, 0.6, 0.3]          0, 0, 0.9, 0]\n",
    "     \n",
    "All weights small            Half the weights are ZERO!\n",
    "All features used            Only important features used!\n",
    "```\n",
    "\n",
    "### **Why Is This Useful?**\n",
    "\n",
    "**Feature Selection Automatically!**\n",
    "\n",
    "```\n",
    "Cat vs Dog Classifier:\n",
    "64Ã—64Ã—3 = 12,288 pixel features\n",
    "\n",
    "With L2: All 12,288 features have small weights\n",
    "         â†’ Uses all pixels (even noisy ones)\n",
    "\n",
    "With L1: Maybe only 500 features have non-zero weights\n",
    "         â†’ Uses only important pixels!\n",
    "         â†’ \"Cat has whiskers\" âœ“\n",
    "         â†’ \"Random noise in corner\" âœ— (weight = 0)\n",
    "```\n",
    "\n",
    "### **Real-World Analogy**\n",
    "\n",
    "**Hiring for a job:**\n",
    "\n",
    "**L2 approach:** \n",
    "```\n",
    "Give everyone small tasks\n",
    "- Expert: 20% time\n",
    "- Good person: 15% time  \n",
    "- Mediocre: 10% time\n",
    "- Bad person: 5% time\n",
    "\n",
    "Everyone works a little!\n",
    "```\n",
    "\n",
    "**L1 approach:**\n",
    "```\n",
    "Fire the bad performers!\n",
    "- Expert: 40% time\n",
    "- Good person: 35% time\n",
    "- Mediocre: 25% time\n",
    "- Bad person: 0% (FIRED!)\n",
    "\n",
    "Only keep the best!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Mathematics\n",
    "\n",
    "### **L1 Regularized Loss:**\n",
    "\n",
    "$$L_{L1} = \\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2 + \\lambda\\sum_{j=1}^{n}|w_j|$$\n",
    "\n",
    "**Compare with L2:**\n",
    "\n",
    "| Aspect | L2 | L1 |\n",
    "|--------|----|----|\n",
    "| **Formula** | $\\lambda/2 \\times \\sum w_j^2$ | $\\lambda \\times \\sum \\|w_j\\|$ |\n",
    "| **Penalty** | Squared weights | Absolute values |\n",
    "| **Gradient** | $\\lambda w$ (proportional to weight) | $\\lambda \\times \\text{sign}(w)$ (constant!) |\n",
    "| **Effect** | Weights â†’ small | Weights â†’ zero |\n",
    "| **Sparsity** | No | Yes |\n",
    "\n",
    "**Key difference in gradient:**\n",
    "\n",
    "```\n",
    "L2 gradient: âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w + Î»w\n",
    "             If w=10  â†’ adds +10Î»\n",
    "             If w=1   â†’ adds +1Î»\n",
    "             If w=0.1 â†’ adds +0.1Î»\n",
    "             (Gentle push, proportional to size)\n",
    "\n",
    "L1 gradient: âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w + Î»Ã—sign(w)\n",
    "             If w=10  â†’ adds +Î»\n",
    "             If w=1   â†’ adds +Î»  \n",
    "             If w=0.1 â†’ adds +Î»\n",
    "             (Constant push, same for any positive w!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why L1 Creates Sparsity (Intuition)\n",
    "\n",
    "### **The Gradient Behavior**\n",
    "\n",
    "```\n",
    "L2 Gradient vs Weight:        L1 Gradient vs Weight:\n",
    "\n",
    " âˆ‚L/âˆ‚w                         âˆ‚L/âˆ‚w\n",
    "   â†‘                             â†‘\n",
    "10 â”‚        â•±                  1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    " 5 â”‚      â•±                    0 â”‚\n",
    " 0 â”‚â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â†’ w              -1â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "-5 â”‚  â•±                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ w\n",
    "-10â”‚â•±                             \n",
    "   \n",
    "Gradient grows with w         Gradient is constant!\n",
    "Large w â†’ large push          Any w â†’ same push\n",
    "Small w â†’ tiny push           Keeps pushing until w=0!\n",
    "```\n",
    "\n",
    "**What this means:**\n",
    "\n",
    "**L2:**\n",
    "```\n",
    "w = 10  â†’ Push of -10Î» â†’ w becomes 9.9\n",
    "w = 1   â†’ Push of -1Î»  â†’ w becomes 0.99\n",
    "w = 0.1 â†’ Push of -0.1Î» â†’ w becomes 0.099\n",
    "w = 0.01 â†’ Push of -0.01Î» â†’ w becomes 0.0099\n",
    "...\n",
    "Never quite reaches zero!\n",
    "```\n",
    "\n",
    "**L1:**\n",
    "```\n",
    "w = 10  â†’ Push of -Î» â†’ w becomes 9.9\n",
    "w = 1   â†’ Push of -Î» â†’ w becomes 0.9\n",
    "w = 0.1 â†’ Push of -Î» â†’ w becomes 0.0\n",
    "        (if Î»=0.1 and learning rate makes this happen)\n",
    "        \n",
    "Can hit zero exactly!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Step-by-Step Numerical Example\n",
    "\n",
    "### **Scenario: Single Neuron (Comparing L1 vs L2)**\n",
    "\n",
    "```\n",
    "Input: x = [2.0, 1.5, 3.0]\n",
    "Weights: w = [wâ‚, wâ‚‚, wâ‚ƒ]\n",
    "Bias: b = 0.5\n",
    "True label: y = 1.0\n",
    "\n",
    "Forward: z = wâ‚Ã—2.0 + wâ‚‚Ã—1.5 + wâ‚ƒÃ—3.0 + 0.5\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Initial State:**\n",
    "\n",
    "```\n",
    "wâ‚ = 2.0\n",
    "wâ‚‚ = 0.3  (small weight)\n",
    "wâ‚ƒ = 5.0\n",
    "b = 0.5\n",
    "```\n",
    "\n",
    "**Forward pass:**\n",
    "```\n",
    "z = 2.0Ã—2.0 + 0.3Ã—1.5 + 5.0Ã—3.0 + 0.5\n",
    "  = 4.0 + 0.45 + 15.0 + 0.5\n",
    "  = 19.95\n",
    "\n",
    "Å· = sigmoid(19.95) â‰ˆ 0.9999999975\n",
    "\n",
    "Prediction loss:\n",
    "L_pred = (1.0 - 0.9999999975)Â² â‰ˆ 0.00000000006\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Case 1: L2 Regularization (Î» = 0.1)**\n",
    "\n",
    "**L2 penalty:**\n",
    "```\n",
    "L2 = (Î»/2) Ã— (wâ‚Â² + wâ‚‚Â² + wâ‚ƒÂ²)\n",
    "   = (0.1/2) Ã— (2.0Â² + 0.3Â² + 5.0Â²)\n",
    "   = 0.05 Ã— (4.0 + 0.09 + 25.0)\n",
    "   = 0.05 Ã— 29.09\n",
    "   = 1.4545\n",
    "\n",
    "Total loss:\n",
    "L = 0.00000000006 + 1.4545 â‰ˆ 1.4545\n",
    "```\n",
    "\n",
    "**Gradients:**\n",
    "```\n",
    "âˆ‚L_pred/âˆ‚wâ‚ â‰ˆ 0 (prediction almost perfect)\n",
    "âˆ‚L_pred/âˆ‚wâ‚‚ â‰ˆ 0\n",
    "âˆ‚L_pred/âˆ‚wâ‚ƒ â‰ˆ 0\n",
    "\n",
    "L2 gradients:\n",
    "âˆ‚L2/âˆ‚wâ‚ = Î»wâ‚ = 0.1 Ã— 2.0 = 0.2\n",
    "âˆ‚L2/âˆ‚wâ‚‚ = Î»wâ‚‚ = 0.1 Ã— 0.3 = 0.03\n",
    "âˆ‚L2/âˆ‚wâ‚ƒ = Î»wâ‚ƒ = 0.1 Ã— 5.0 = 0.5\n",
    "\n",
    "Total gradients:\n",
    "âˆ‚L/âˆ‚wâ‚ â‰ˆ 0 + 0.2 = 0.2\n",
    "âˆ‚L/âˆ‚wâ‚‚ â‰ˆ 0 + 0.03 = 0.03\n",
    "âˆ‚L/âˆ‚wâ‚ƒ â‰ˆ 0 + 0.5 = 0.5\n",
    "```\n",
    "\n",
    "**Update (learning rate Î± = 0.1):**\n",
    "```\n",
    "wâ‚ := 2.0 - 0.1Ã—0.2 = 2.0 - 0.02 = 1.98\n",
    "wâ‚‚ := 0.3 - 0.1Ã—0.03 = 0.3 - 0.003 = 0.297\n",
    "wâ‚ƒ := 5.0 - 0.1Ã—0.5 = 5.0 - 0.05 = 4.95\n",
    "```\n",
    "\n",
    "**After 100 iterations:**\n",
    "```\n",
    "wâ‚ = 1.23\n",
    "wâ‚‚ = 0.19  â† Still non-zero!\n",
    "wâ‚ƒ = 3.08\n",
    "\n",
    "All weights shrunk proportionally\n",
    "Small weight (wâ‚‚) stayed small but non-zero\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Case 2: L1 Regularization (Î» = 0.1)**\n",
    "\n",
    "**L1 penalty:**\n",
    "```\n",
    "L1 = Î» Ã— (|wâ‚| + |wâ‚‚| + |wâ‚ƒ|)\n",
    "   = 0.1 Ã— (|2.0| + |0.3| + |5.0|)\n",
    "   = 0.1 Ã— (2.0 + 0.3 + 5.0)\n",
    "   = 0.1 Ã— 7.3\n",
    "   = 0.73\n",
    "\n",
    "Total loss:\n",
    "L = 0.00000000006 + 0.73 â‰ˆ 0.73\n",
    "```\n",
    "\n",
    "**L1 gradients:**\n",
    "```\n",
    "âˆ‚L1/âˆ‚wâ‚ = Î» Ã— sign(wâ‚) = 0.1 Ã— sign(2.0) = 0.1 Ã— 1 = 0.1\n",
    "âˆ‚L1/âˆ‚wâ‚‚ = Î» Ã— sign(wâ‚‚) = 0.1 Ã— sign(0.3) = 0.1 Ã— 1 = 0.1\n",
    "âˆ‚L1/âˆ‚wâ‚ƒ = Î» Ã— sign(wâ‚ƒ) = 0.1 Ã— sign(5.0) = 0.1 Ã— 1 = 0.1\n",
    "\n",
    "Notice: All gradients are 0.1 (same magnitude!)\n",
    "Doesn't matter if weight is 0.3 or 5.0!\n",
    "```\n",
    "\n",
    "**Total gradients:**\n",
    "```\n",
    "âˆ‚L/âˆ‚wâ‚ â‰ˆ 0 + 0.1 = 0.1\n",
    "âˆ‚L/âˆ‚wâ‚‚ â‰ˆ 0 + 0.1 = 0.1  â† Same as wâ‚!\n",
    "âˆ‚L/âˆ‚wâ‚ƒ â‰ˆ 0 + 0.1 = 0.1  â† Same as wâ‚!\n",
    "```\n",
    "\n",
    "**Update (learning rate Î± = 0.1):**\n",
    "```\n",
    "wâ‚ := 2.0 - 0.1Ã—0.1 = 2.0 - 0.01 = 1.99\n",
    "wâ‚‚ := 0.3 - 0.1Ã—0.1 = 0.3 - 0.01 = 0.29\n",
    "wâ‚ƒ := 5.0 - 0.1Ã—0.1 = 5.0 - 0.01 = 4.99\n",
    "\n",
    "All decrease by same absolute amount (0.01)\n",
    "Proportionally, wâ‚‚ decreased more!\n",
    "```\n",
    "\n",
    "**After 30 iterations:**\n",
    "```\n",
    "Iteration 1:  wâ‚‚ = 0.29\n",
    "Iteration 10: wâ‚‚ = 0.20\n",
    "Iteration 20: wâ‚‚ = 0.10\n",
    "Iteration 30: wâ‚‚ = 0.00  â† Reached zero!\n",
    "\n",
    "wâ‚‚ hits zero at iteration 30!\n",
    "After that, wâ‚‚ stays at 0 (sparse!)\n",
    "\n",
    "wâ‚ = 1.70 (still large, still useful)\n",
    "wâ‚‚ = 0.00 â† KILLED!\n",
    "wâ‚ƒ = 4.70 (still large, still useful)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Detailed Evolution Over Time:**\n",
    "\n",
    "| Iteration | L2: wâ‚‚ | L1: wâ‚‚ | L2: Gradient | L1: Gradient |\n",
    "|-----------|--------|--------|--------------|--------------|\n",
    "| 0 | 0.300 | 0.300 | 0.030 | 0.100 |\n",
    "| 5 | 0.285 | 0.250 | 0.029 | 0.100 |\n",
    "| 10 | 0.272 | 0.200 | 0.027 | 0.100 |\n",
    "| 15 | 0.259 | 0.150 | 0.026 | 0.100 |\n",
    "| 20 | 0.247 | 0.100 | 0.025 | 0.100 |\n",
    "| 25 | 0.236 | 0.050 | 0.024 | 0.100 |\n",
    "| 30 | 0.225 | **0.000** | 0.023 | **0.000** |\n",
    "| 50 | 0.196 | **0.000** | 0.020 | **0.000** |\n",
    "| 100 | 0.153 | **0.000** | 0.015 | **0.000** |\n",
    "\n",
    "**Key observations:**\n",
    "\n",
    "```\n",
    "L2 Behavior:\n",
    "- Gradient decreases as weight shrinks\n",
    "- Takes forever to reach zero\n",
    "- Weight asymptotically approaches zero\n",
    "- Never exactly zero\n",
    "\n",
    "L1 Behavior:\n",
    "- Gradient constant until weight hits zero\n",
    "- Reaches zero in finite time!\n",
    "- Once zero, gradient becomes zero (stays dead)\n",
    "- Sparse solution!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. The \"Soft Thresholding\" Effect\n",
    "\n",
    "### **What Happens Near Zero:**\n",
    "\n",
    "**L1 has a special property near zero:**\n",
    "\n",
    "```\n",
    "If |âˆ‚L_pred/âˆ‚w| < Î»:\n",
    "  â†’ L1 penalty dominates\n",
    "  â†’ Weight gets pushed to exactly zero!\n",
    "  \n",
    "Example:\n",
    "âˆ‚L_pred/âˆ‚w = 0.05\n",
    "Î» = 0.1\n",
    "\n",
    "Total gradient = 0.05 + 0.1Ã—sign(w) = 0.15\n",
    "Weight decreases until it crosses zero,\n",
    "then the sign flips and it gets pushed back to zero!\n",
    "Net effect: w = 0 (stuck at zero)\n",
    "```\n",
    "\n",
    "**Visualization:**\n",
    "\n",
    "```\n",
    "    âˆ‚L/âˆ‚w\n",
    "      â†‘\n",
    "  0.2 â”‚         â•± L_pred gradient only\n",
    "      â”‚       â•±\n",
    "  0.1 â”‚â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€ Î» = 0.1 (L1 penalty line)\n",
    "      â”‚   â•±   â•²\n",
    "    0 â”‚â”€â—â”€â”€â”€â”€â”€â”€â”€â—â”€â”€ w\n",
    "      â”‚         â•²\n",
    " -0.1 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "      \n",
    "Between the two â—'s: L1 penalty > L_pred gradient\n",
    "â†’ Weight gets pushed to zero and stays there!\n",
    "\n",
    "This zone is called the \"sparse region\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Complete Example: Cat vs Dog with L1\n",
    "\n",
    "### **Network Architecture (Same as before):**\n",
    "\n",
    "```\n",
    "Input: 12,288 pixels\n",
    "Hidden: 100 neurons  \n",
    "Output: 2 neurons\n",
    "\n",
    "Total weights: 1,229,000\n",
    "```\n",
    "\n",
    "### **Training with L1 (Î» = 0.001)**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Epoch 1:**\n",
    "\n",
    "**Initial weights (random, small):**\n",
    "```\n",
    "Sample of 10 weights:\n",
    "w = [0.02, -0.01, 0.03, -0.02, 0.01, \n",
    "     0.04, -0.03, 0.02, -0.01, 0.03]\n",
    "\n",
    "All weights non-zero: 1,229,000 / 1,229,000 = 100%\n",
    "```\n",
    "\n",
    "**Forward pass (one training image: cat):**\n",
    "```\n",
    "Prediction: Å· = [0.51, 0.49]  (51% cat - barely right)\n",
    "True label: y = [1, 0]\n",
    "\n",
    "L_pred = CrossEntropyLoss = 0.673\n",
    "```\n",
    "\n",
    "**L1 penalty:**\n",
    "```\n",
    "Sum of absolute weights:\n",
    "Î£|w| = 6,145.3\n",
    "\n",
    "L1_penalty = Î» Ã— Î£|w|\n",
    "           = 0.001 Ã— 6,145.3\n",
    "           = 6.145\n",
    "\n",
    "Total loss:\n",
    "L = 0.673 + 6.145 = 6.818\n",
    "```\n",
    "\n",
    "**L1 gradients (sample):**\n",
    "```\n",
    "For w = 0.02:  âˆ‚L1/âˆ‚w = 0.001 Ã— sign(0.02) = 0.001\n",
    "For w = -0.01: âˆ‚L1/âˆ‚w = 0.001 Ã— sign(-0.01) = -0.001\n",
    "For w = 0.03:  âˆ‚L1/âˆ‚w = 0.001 Ã— sign(0.03) = 0.001\n",
    "\n",
    "All have magnitude 0.001!\n",
    "```\n",
    "\n",
    "**After update:**\n",
    "```\n",
    "Small weights that contribute little:\n",
    "- If |âˆ‚L_pred/âˆ‚w| < 0.001: Weight moves toward zero\n",
    "- Some weights hit zero!\n",
    "\n",
    "Active weights: 1,227,500 / 1,229,000 = 99.9%\n",
    "(1,500 weights already zeroed out!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Epoch 10:**\n",
    "\n",
    "```\n",
    "Training accuracy: 78%\n",
    "Test accuracy: 76%\n",
    "\n",
    "Sample weights:\n",
    "w = [0.00, 0.00, 1.23, -0.00, 0.45,\n",
    "     2.10, 0.00, 0.87, -0.00, 1.56]\n",
    "     \n",
    "Active weights: 982,000 / 1,229,000 = 79.9%\n",
    "Zero weights: 247,000 (20% are dead!)\n",
    "\n",
    "L_pred = 0.421\n",
    "L1_penalty = 0.001 Ã— 3,892.1 = 3.892\n",
    "Total loss = 4.313\n",
    "```\n",
    "\n",
    "**What's happening:**\n",
    "```\n",
    "Network is learning:\n",
    "- Important features (cat's ears, whiskers): Large weights\n",
    "- Unimportant features (background noise): Zero weights\n",
    "\n",
    "Feature selection in progress!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Epoch 50:**\n",
    "\n",
    "```\n",
    "Training accuracy: 94%\n",
    "Test accuracy: 91%\n",
    "\n",
    "Active weights: 412,000 / 1,229,000 = 33.5%\n",
    "Zero weights: 817,000 (66.5% are dead!)\n",
    "\n",
    "Network uses only 1/3 of original features!\n",
    "\n",
    "Sample weights:\n",
    "w = [0.00, 0.00, 3.45, 0.00, 0.00,\n",
    "     5.23, 0.00, 2.87, 0.00, 4.12]\n",
    "\n",
    "Important pixel features:\n",
    "- Pixel (23, 34): w = 5.23  (cat ear detector!)\n",
    "- Pixel (45, 12): w = 4.12  (whisker detector!)\n",
    "- Pixel (67, 89): w = 0.00  (random background - ignored)\n",
    "\n",
    "L_pred = 0.095\n",
    "L1_penalty = 0.001 Ã— 2,145.7 = 2.146\n",
    "Total loss = 2.241\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Final Model (Epoch 100):**\n",
    "\n",
    "```\n",
    "Training accuracy: 96%\n",
    "Test accuracy: 93%\n",
    "\n",
    "Active weights: 287,000 / 1,229,000 = 23.4%\n",
    "Zero weights: 942,000 (76.6% sparse!)\n",
    "\n",
    "Network uses only 287K parameters instead of 1.23M!\n",
    "- 76.6% reduction in model size!\n",
    "- Faster inference!\n",
    "- Better interpretability!\n",
    "\n",
    "Weight distribution:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Weight Value  â”‚  Count          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  0.0 (exactly)â”‚  942,000 (76.6%)â”‚ â† Sparsity!\n",
    "â”‚  0.0 to 1.0   â”‚   98,000        â”‚\n",
    "â”‚  1.0 to 3.0   â”‚  142,000        â”‚\n",
    "â”‚  3.0 to 5.0   â”‚   38,000        â”‚\n",
    "â”‚  5.0+         â”‚    9,000        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Most important features:\n",
    "Top 5 non-zero weights:\n",
    "1. w[234,567] = 8.9   (strong cat ear signal)\n",
    "2. w[123,456] = 7.2   (whisker pattern)\n",
    "3. w[789,012] = 6.8   (dog nose shape)\n",
    "4. w[345,678] = 6.1   (fur texture)\n",
    "5. w[901,234] = 5.9   (eye position)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Comparing L1 vs L2 on Same Network\n",
    "\n",
    "### **Same Cat vs Dog Network, Same Data:**\n",
    "\n",
    "| Metric | L2 (Î»=0.01) | L1 (Î»=0.001) |\n",
    "|--------|-------------|--------------|\n",
    "| **Training Acc** | 94% | 96% |\n",
    "| **Test Acc** | 92% | 93% |\n",
    "| **Non-zero weights** | 1,229,000 (100%) | 287,000 (23%) |\n",
    "| **Zero weights** | 0 (0%) | 942,000 (77%) |\n",
    "| **Model size** | 4.9 MB | 1.1 MB â†“ |\n",
    "| **Inference time** | 12 ms | 4 ms â†“ |\n",
    "| **Weight magnitude** | Avg: 0.8 | Avg: 2.1 (for non-zero) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Weight Distribution Comparison:**\n",
    "\n",
    "```\n",
    "L2 Distribution:                L1 Distribution:\n",
    "\n",
    "  Count                          Count\n",
    "    â†‘                              â†‘\n",
    "500Kâ”‚    â•±â•²                    900Kâ”‚â–ˆ\n",
    "    â”‚   â•±  â•²                       â”‚â–ˆ\n",
    "300Kâ”‚  â•±    â•²                      â”‚â–ˆ\n",
    "    â”‚ â•±      â•²                     â”‚â–ˆ\n",
    "100Kâ”‚â•±        â•²                100Kâ”‚ â•±â•²\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Weight        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Weight\n",
    "   -2  -1  0  1  2               0   2   4   6\n",
    "\n",
    "Bell curve around 0            Spike at exactly 0!\n",
    "All weights used              + Few large weights\n",
    "                              \"Sparse\" solution\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Feature Importance:**\n",
    "\n",
    "**L2 Result:**\n",
    "```\n",
    "All 12,288 pixels used with small weights:\n",
    "Pixel 1:    w = 0.03  (ear - important)\n",
    "Pixel 2:    w = 0.02  (whisker - important)\n",
    "...\n",
    "Pixel 5000: w = 0.005 (background noise - useless but included)\n",
    "...\n",
    "Pixel 12288: w = 0.001 (corner - useless but included)\n",
    "\n",
    "Cannot tell which features are important!\n",
    "All features contribute a little.\n",
    "```\n",
    "\n",
    "**L1 Result:**\n",
    "```\n",
    "Only 2,156 pixels (17%) have non-zero weights:\n",
    "Pixel 1:    w = 2.34  (ear - IMPORTANT! âœ“)\n",
    "Pixel 2:    w = 1.89  (whisker - IMPORTANT! âœ“)\n",
    "...\n",
    "Pixel 5000: w = 0.00  (background noise - IGNORED! âœ“)\n",
    "...\n",
    "Pixel 12288: w = 0.00  (corner - IGNORED! âœ“)\n",
    "\n",
    "Clear feature selection!\n",
    "Easy to interpret: \"Model uses these 2,156 pixels\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Geometric Intuition: Why L1 Creates Sparsity\n",
    "\n",
    "### **Constraint Regions:**\n",
    "\n",
    "**L2 Constraint: $w_1^2 + w_2^2 \\leq C$**\n",
    "\n",
    "```\n",
    "    wâ‚‚\n",
    "     â†‘\n",
    "   1 â”‚   â—â”€â”€â”€â”€â”€â—\n",
    "     â”‚ â—         â—\n",
    "   0 â”œâ—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â†’ wâ‚\n",
    "     â”‚ â—         â—\n",
    "  -1 â”‚   â—â”€â”€â”€â”€â”€â—\n",
    "  \n",
    "Circle (smooth, round)\n",
    "Solution can be anywhere on circle\n",
    "Rarely hits axes (wâ‚=0 or wâ‚‚=0)\n",
    "```\n",
    "\n",
    "**L1 Constraint: $|w_1| + |w_2| \\leq C$**\n",
    "\n",
    "```\n",
    "    wâ‚‚\n",
    "     â†‘\n",
    "   1 â”‚      â—\n",
    "     â”‚     â•± â•²\n",
    "   0 â”œâ”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â†’ wâ‚\n",
    "     â”‚     â•² â•±\n",
    "  -1 â”‚      â—\n",
    "  \n",
    "Diamond (sharp corners!)\n",
    "Solution tends to hit corners\n",
    "Corners are on axes â†’ sparse!\n",
    "(Either wâ‚=0 or wâ‚‚=0)\n",
    "```\n",
    "\n",
    "### **Optimization with Contours:**\n",
    "\n",
    "```\n",
    "L2 Case:                       L1 Case:\n",
    "\n",
    "    wâ‚‚                            wâ‚‚\n",
    "     â†‘                             â†‘\n",
    "     â”‚  â•±â—‹â—‹â—‹â•²                      â”‚  â•±â—‹â—‹â—‹â•²\n",
    "     â”‚ â—‹     â—‹ â† Loss contours     â”‚ â—‹     â—‹\n",
    "     â”‚â—‹   â—   â—‹                    â”‚â—‹   â—   â—‹\n",
    "     â”‚ â—‹ â•±â—â•² â—‹                     â”‚ â—‹     â—‹\n",
    "     â”‚  â—â”€â”€â”€â—                      â”‚  â—â”€â”€â”€â”€â”€â—\n",
    "     â”‚    L2                       â”‚     â•±â”‚â•²\n",
    "     â”‚  circle                     â”‚   â•±  â”‚  â•²\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ wâ‚              â””â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’ wâ‚\n",
    "         â†‘                               â—\n",
    "    Solution:                      Solution hits axis!\n",
    "    w=[0.3, 0.5]                   w=[0, 0.7] â† Sparse!\n",
    "    Both non-zero                  wâ‚ exactly zero!\n",
    "```\n",
    "\n",
    "**Why corners matter:**\n",
    "```\n",
    "Probability solution hits axis:\n",
    "- Circle (L2): Low (must hit exact point)\n",
    "- Diamond (L1): High (corners ARE on axes!)\n",
    "\n",
    "In high dimensions:\n",
    "- L2: Almost never hits axis (no sparsity)\n",
    "- L1: Frequently hits axes (lots of sparsity!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Proximal Gradient Descent (How to Handle L1's Non-Differentiability)\n",
    "\n",
    "### **The Problem:**\n",
    "\n",
    "L1 penalty $|w|$ is not differentiable at $w=0$!\n",
    "\n",
    "```\n",
    "    |w|\n",
    "     â†‘\n",
    "   2 â”‚      â•±\n",
    "   1 â”‚    â•±\n",
    "   0 â”‚â”€â”€â—â”€â”€â”€â”€â”€â”€â†’ w\n",
    "    -2  0  2\n",
    "        â†‘\n",
    "    Not smooth at zero!\n",
    "    Gradient is undefined!\n",
    "```\n",
    "\n",
    "### **The Solution: Soft Thresholding**\n",
    "\n",
    "Instead of regular gradient descent, use:\n",
    "\n",
    "$$w^{new} = \\text{SoftThreshold}(w^{old} - \\alpha \\nabla L_{pred}, \\alpha\\lambda)$$\n",
    "\n",
    "**Soft Threshold Function:**\n",
    "\n",
    "$$\\text{SoftThreshold}(x, \\theta) = \\begin{cases}\n",
    "x - \\theta & \\text{if } x > \\theta \\\\\n",
    "0 & \\text{if } |x| \\leq \\theta \\\\\n",
    "x + \\theta & \\text{if } x < -\\theta\n",
    "\\end{cases}$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Example:**\n",
    "\n",
    "```\n",
    "Current weight: w = 0.5\n",
    "Learning rate: Î± = 0.1\n",
    "L1 strength: Î» = 0.01\n",
    "Prediction gradient: âˆ‚L_pred/âˆ‚w = 2.0\n",
    "\n",
    "Step 1: Regular gradient step\n",
    "w_temp = w - Î± Ã— âˆ‚L_pred/âˆ‚w\n",
    "       = 0.5 - 0.1 Ã— 2.0\n",
    "       = 0.5 - 0.2\n",
    "       = 0.3\n",
    "\n",
    "Step 2: Apply soft threshold with Î±Î» = 0.1 Ã— 0.01 = 0.001\n",
    "w_new = SoftThreshold(0.3, 0.001)\n",
    "      = 0.3 - 0.001  (since 0.3 > 0.001)\n",
    "      = 0.299\n",
    "\n",
    "If w_temp was 0.0005:\n",
    "w_new = SoftThreshold(0.0005, 0.001)\n",
    "      = 0  (since |0.0005| < 0.001)\n",
    "      Weight gets killed!\n",
    "```\n",
    "\n",
    "### **Numerical Comparison:**\n",
    "\n",
    "| w_temp | Threshold (Î±Î») | w_new | What Happened |\n",
    "|--------|----------------|-------|---------------|\n",
    "| 2.0 | 0.01 | 1.99 | Shrunk slightly |\n",
    "| 0.5 | 0.01 | 0.49 | Shrunk slightly |\n",
    "| 0.1 | 0.01 | 0.09 | Shrunk slightly |\n",
    "| 0.02 | 0.01 | 0.01 | Shrunk to near zero |\n",
    "| 0.005 | 0.01 | **0.00** | Killed! (sparse) |\n",
    "| -0.008 | 0.01 | **0.00** | Killed! (sparse) |\n",
    "| -0.5 | 0.01 | -0.49 | Shrunk slightly |\n",
    "\n",
    "**Visual representation:**\n",
    "\n",
    "```\n",
    "Before soft threshold:          After soft threshold:\n",
    "w = [...0.5, 0.02, 0.005,      w = [...0.49, 0.01, 0.00,\n",
    "     -0.008, -0.5, 0.1...]          0.00, -0.49, 0.09...]\n",
    "                                     â†‘     â†‘\n",
    "                                Killed!  Killed!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Practical Implementation\n",
    "\n",
    "### **PyTorch-style Code:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class L1Regularization:\n",
    "    def __init__(self, lambda_l1=0.001):\n",
    "        self.lambda_l1 = lambda_l1\n",
    "    \n",
    "    def __call__(self, model):\n",
    "        \"\"\"Compute L1 penalty for all model parameters\"\"\"\n",
    "        l1_penalty = 0\n",
    "        for param in model.parameters():\n",
    "            l1_penalty += torch.sum(torch.abs(param))\n",
    "        return self.lambda_l1 * l1_penalty\n",
    "\n",
    "# Training loop\n",
    "model = CatDogClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "l1_reg = L1Regularization(lambda_l1=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Prediction loss\n",
    "        pred_loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # L1 penalty\n",
    "        l1_penalty = l1_reg(model)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = pred_loss + l1_penalty\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Optional: Hard threshold very small weights to exactly zero\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param[torch.abs(param) < 1e-6] = 0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Complete Training Example with Logging:**\n",
    "\n",
    "```python\n",
    "# Training one epoch with detailed logging\n",
    "lambda_l1 = 0.001\n",
    "alpha = 0.1\n",
    "\n",
    "print(\"Before training:\")\n",
    "non_zero_before = sum((p != 0).sum().item() \n",
    "                     for p in model.parameters())\n",
    "print(f\"  Non-zero weights: {non_zero_before:,}\")\n",
    "\n",
    "for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "    # Forward\n",
    "    outputs = model(images)  # Shape: (batch_size, 2)\n",
    "    # outputs = [[0.7, 0.3],  # 70% cat\n",
    "    #            [0.2, 0.8],  # 80% dog\n",
    "    #            [0.6, 0.4]]  # 60% cat\n",
    "    \n",
    "    # Prediction loss\n",
    "    pred_loss = F.cross_entropy(outputs, labels)\n",
    "    # pred_loss = 0.357\n",
    "    \n",
    "    # L1 penalty\n",
    "    l1_penalty = 0\n",
    "    for param in model.parameters():\n",
    "        l1_penalty += torch.abs(param).sum()\n",
    "    l1_penalty = lambda_l1 * l1_penalty\n",
    "    # l1_penalty = 0.001 Ã— 1,128,945 = 1,128.945\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = pred_loss + l1_penalty\n",
    "    # total_loss = 0.357 + 1,128.945 = 1,129.302\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Soft thresholding (built into the gradient)\n",
    "    # PyTorch automatically handles this through the L1 gradient\n",
    "    \n",
    "    # Update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Hard thresholding (optional, for true sparsity)\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            # Kill weights smaller than threshold\n",
    "            mask = torch.abs(param) < 1e-4\n",
    "            param[mask] = 0\n",
    "    \n",
    "    if batch_idx % 10 == 0:\n",
    "        # Count non-zero weights\n",
    "        non_zero = sum((p != 0).sum().item() \n",
    "                      for p in model.parameters())\n",
    "        sparsity = (1 - non_zero / non_zero_before) * 100\n",
    "        \n",
    "        print(f\"Batch {batch_idx}:\")\n",
    "        print(f\"  Loss: {total_loss.item():.4f}\")\n",
    "        print(f\"  Non-zero weights: {non_zero:,}\")\n",
    "        print(f\"  Sparsity: {sparsity:.1f}%\")\n",
    "\n",
    "print(\"\\nAfter training:\")\n",
    "non_zero_after = sum((p != 0).sum().item() \n",
    "                    for p in model.parameters())\n",
    "sparsity_final = (1 - non_zero_after / non_zero_before) * 100\n",
    "print(f\"  Non-zero weights: {non_zero_after:,}\")\n",
    "print(f\"  Final sparsity: {sparsity_final:.1f}%\")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Before training:\n",
    "  Non-zero weights: 1,229,000\n",
    "\n",
    "Batch 0:\n",
    "  Loss: 1,129.3024\n",
    "  Non-zero weights: 1,227,834\n",
    "  Sparsity: 0.1%\n",
    "\n",
    "Batch 10:\n",
    "  Loss: 845.2134\n",
    "  Non-zero weights: 1,198,456\n",
    "  Sparsity: 2.5%\n",
    "\n",
    "Batch 20:\n",
    "  Loss: 623.4521\n",
    "  Non-zero weights: 1,145,789\n",
    "  Sparsity: 6.8%\n",
    "\n",
    "...\n",
    "\n",
    "Batch 100:\n",
    "  Loss: 234.5678\n",
    "  Non-zero weights: 892,341\n",
    "  Sparsity: 27.4%\n",
    "\n",
    "After training:\n",
    "  Non-zero weights: 287,234\n",
    "  Final sparsity: 76.6%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 11. When to Use L1 vs L2\n",
    "\n",
    "### **Decision Guide:**\n",
    "\n",
    "| Scenario | Best Choice | Reason |\n",
    "|----------|-------------|--------|\n",
    "| **High-dimensional data** (many features) | L1 | Feature selection needed |\n",
    "| **Most features relevant** | L2 | Keep all features |\n",
    "| **Want interpretability** | L1 | See which features matter |\n",
    "| **Want faster inference** | L1 | Fewer non-zero weights |\n",
    "| **Want smaller model** | L1 | Sparsity reduces size |\n",
    "| **Features correlated** | L2 | L1 picks one randomly |\n",
    "| **Stable gradient flow** | L2 | Smooth everywhere |\n",
    "| **Don't care about size** | L2 | Easier to optimize |\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Examples:**\n",
    "\n",
    "**Use L1 for:**\n",
    "\n",
    "```\n",
    "1. Medical diagnosis (1000s of gene expressions)\n",
    "   â†’ Want to know: \"Which 20 genes matter?\"\n",
    "   â†’ L1 gives sparse solution: 980 weights = 0\n",
    "\n",
    "2. Text classification (100,000 vocabulary)\n",
    "   â†’ Want to know: \"Which 500 words are most indicative?\"\n",
    "   â†’ L1 selects important words, zeros out rare ones\n",
    "\n",
    "3. Mobile deployment\n",
    "   â†’ Need small model (limited memory/compute)\n",
    "   â†’ L1 gives 70% sparsity â†’ 3Ã— smaller model\n",
    "\n",
    "4. Image features (12,288 pixels)\n",
    "   â†’ Want to know: \"Which pixels detect cats?\"\n",
    "   â†’ L1 shows: \"These 2,000 pixels around ears/whiskers\"\n",
    "```\n",
    "\n",
    "**Use L2 for:**\n",
    "\n",
    "```\n",
    "1. Image classification (already few features after conv layers)\n",
    "   â†’ All features important\n",
    "   â†’ L2 keeps all, makes smooth\n",
    "\n",
    "2. General-purpose model\n",
    "   â†’ No specific feature selection needed\n",
    "   â†’ L2 easier to train, more stable\n",
    "\n",
    "3. Small networks\n",
    "   â†’ Model size not a concern\n",
    "   â†’ L2 gives slight performance edge\n",
    "\n",
    "4. Highly correlated features\n",
    "   â†’ L1 would pick one arbitrarily\n",
    "   â†’ L2 distributes weights fairly across correlates\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Elastic Net (Combining L1 and L2):**\n",
    "\n",
    "Sometimes best to use BOTH!\n",
    "\n",
    "$$L_{elastic} = L_{pred} + \\lambda_1 \\sum|w_i| + \\frac{\\lambda_2}{2}\\sum w_i^2$$\n",
    "\n",
    "```\n",
    "Gets benefits of both:\n",
    "- L1: Sparsity, feature selection\n",
    "- L2: Stability, handles correlated features\n",
    "\n",
    "Example:\n",
    "Î»â‚ = 0.001 (L1 for sparsity)\n",
    "Î»â‚‚ = 0.01 (L2 for stability)\n",
    "\n",
    "Result:\n",
    "- 60% sparsity (from L1)\n",
    "- Stable training (from L2)\n",
    "- Best of both worlds!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Summary: L1 Regularization\n",
    "\n",
    "### **What L1 Does:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   L1 Regularization (Lasso)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ADDS: Penalty term Î» Ã— Î£|w|\n",
    "\n",
    "EFFECT: Shrinks weights to EXACTLY zero\n",
    "\n",
    "GRADIENT: âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w + Î»Ã—sign(w)\n",
    "                                â†‘\n",
    "                          Constant magnitude!\n",
    "\n",
    "UPDATE: w := w - Î±Ã—âˆ‚L_pred/âˆ‚w - Î±Ã—Î»Ã—sign(w)\n",
    "\n",
    "RESULT: Sparse weights (many exactly zero)\n",
    "        â†’ Feature selection\n",
    "        â†’ Smaller models\n",
    "        â†’ Better interpretability\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **L1 vs L2 Comparison:**\n",
    "\n",
    "| Aspect | L2 (Ridge) | L1 (Lasso) |\n",
    "|--------|------------|------------|\n",
    "| **Penalty** | $\\frac{\\lambda}{2}\\sum w^2$ | $\\lambda \\sum \\|w\\|$ |\n",
    "| **Gradient** | $\\lambda w$ (proportional) | $\\lambda \\cdot \\text{sign}(w)$ (constant) |\n",
    "| **Sparsity** | No | Yes âœ“ |\n",
    "| **Feature Selection** | No | Yes âœ“ |\n",
    "| **Differentiability** | Smooth | Non-smooth at 0 |\n",
    "| **Model Size** | Full | Reduced âœ“ |\n",
    "| **Interpretability** | Hard | Easy âœ“ |\n",
    "| **Training Speed** | Faster | Slightly slower |\n",
    "| **Correlated Features** | Keeps all | Picks one |\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **L1 creates sparsity** because constant gradient pushes small weights to exactly zero\n",
    "\n",
    "2. **Geometric interpretation:** Diamond-shaped constraint with corners on axes\n",
    "\n",
    "3. **Feature selection:** Automatically identifies important features\n",
    "\n",
    "4. **Practical benefit:** Smaller models, faster inference, better interpretability\n",
    "\n",
    "5. **Trade-off:** Slightly harder to optimize (non-smooth), but worth it for sparsity\n",
    "\n",
    "---\n",
    "\n",
    "**Next: Dropout - A completely different approach to regularization!** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b06e32",
   "metadata": {},
   "source": [
    "# Part 4: Dropout Regularization\n",
    "\n",
    "## 1. Plain English Explanation\n",
    "\n",
    "### **The Core Idea**\n",
    "\n",
    "**Dropout:** \"Randomly turn off neurons during training\"\n",
    "\n",
    "```\n",
    "Normal Training:              Dropout Training:\n",
    "All neurons active           Random neurons disabled each step\n",
    "\n",
    "   â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—                  â—â”€â”€â”€â”€â—‹â”€â”€â”€â”€â—\n",
    "   â”‚\\  /â”‚\\  /â”‚                  â”‚\\       /â”‚\n",
    "   â— \\/  â— \\/ â—                 â— Ã—   â—‹   â—\n",
    "   â”‚ /\\  â”‚ /\\ â”‚                 â”‚    Ã—    â”‚\n",
    "   â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—                  â—â”€â”€â”€â”€â—‹â”€â”€â”€â”€â—\n",
    "   \n",
    "All connections               Some randomly removed!\n",
    "used every time              (50% dropout shown)\n",
    "```\n",
    "\n",
    "**During training:**\n",
    "- Randomly set 50% of neurons to zero\n",
    "- Different neurons dropped each batch\n",
    "- Forces network to not rely on any single neuron\n",
    "\n",
    "**During testing:**\n",
    "- Use ALL neurons\n",
    "- Scale outputs appropriately\n",
    "- Get ensemble effect\n",
    "\n",
    "---\n",
    "\n",
    "### **Real-World Analogy**\n",
    "\n",
    "**Learning to play basketball:**\n",
    "\n",
    "**Without Dropout (Regular Training):**\n",
    "```\n",
    "Team always has all 5 players:\n",
    "- Player 1 (star): Always scores\n",
    "- Player 2-5: Just pass to Player 1\n",
    "\n",
    "Team becomes dependent on Player 1!\n",
    "If Player 1 injured â†’ Team fails!\n",
    "```\n",
    "\n",
    "**With Dropout:**\n",
    "```\n",
    "Training with random players missing:\n",
    "\n",
    "Game 1: Players 1, 3, 4 play (2, 5 out)\n",
    "  â†’ Players 3, 4 must learn to score!\n",
    "\n",
    "Game 2: Players 2, 3, 5 play (1, 4 out)\n",
    "  â†’ Star player out! Others must step up!\n",
    "\n",
    "Game 3: Players 1, 2, 4 play (3, 5 out)\n",
    "  â†’ Different combination learns teamwork\n",
    "\n",
    "Result: Every player becomes competent\n",
    "        Team not dependent on any single player\n",
    "        Robust to injuries!\n",
    "```\n",
    "\n",
    "**Same with neural networks!**\n",
    "- Every neuron learns useful features\n",
    "- Network not dependent on any single neuron\n",
    "- Robust to noise/missing information\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Prevents Overfitting**\n",
    "\n",
    "**Without Dropout:**\n",
    "```\n",
    "Network: \"I'll memorize training data using this exact \n",
    "         combination of neurons!\"\n",
    "\n",
    "Neuron 47: Detects \"training image #3's exact pixel pattern\"\n",
    "Neuron 92: Detects \"training image #7's noise\"\n",
    "â†’ Overfit to training set!\n",
    "```\n",
    "\n",
    "**With Dropout:**\n",
    "```\n",
    "Network: \"I can't rely on specific neurons being there!\n",
    "         I need multiple redundant ways to detect features.\"\n",
    "\n",
    "Neurons 12, 47, 89: All learn to detect \"cat ears\"\n",
    "Neurons 23, 56, 91: All learn to detect \"whiskers\"\n",
    "â†’ Robust, general features!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Mathematics\n",
    "\n",
    "### **Dropout During Training:**\n",
    "\n",
    "For dropout rate $p$ (probability of dropping):\n",
    "\n",
    "$$h^{dropout} = \\begin{cases}\n",
    "0 & \\text{with probability } p \\\\\n",
    "h & \\text{with probability } (1-p)\n",
    "\\end{cases}$$\n",
    "\n",
    "Or equivalently:\n",
    "$$h^{dropout} = h \\odot m$$\n",
    "\n",
    "Where:\n",
    "- $m \\sim \\text{Bernoulli}(1-p)$ is a mask (0 or 1 for each neuron)\n",
    "- $\\odot$ is element-wise multiplication\n",
    "\n",
    "**During Testing (Inference):**\n",
    "\n",
    "$$h^{test} = (1-p) \\cdot h$$\n",
    "\n",
    "Or if using **inverted dropout** (more common):\n",
    "\n",
    "**Training:**\n",
    "$$h^{dropout} = \\frac{h \\odot m}{1-p}$$\n",
    "\n",
    "**Testing:**\n",
    "$$h^{test} = h$$\n",
    "(No change needed!)\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Components:**\n",
    "\n",
    "| Symbol | Name | Meaning |\n",
    "|--------|------|---------|\n",
    "| **p** | Dropout rate | Probability of dropping a neuron (typically 0.5) |\n",
    "| **h** | Hidden activations | Output of layer before dropout |\n",
    "| **m** | Binary mask | Random 0/1 for each neuron |\n",
    "| **âŠ™** | Element-wise product | Multiply corresponding elements |\n",
    "| **Bernoulli(1-p)** | Binary distribution | Outputs 1 with probability (1-p), 0 with probability p |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Step-by-Step Numerical Example\n",
    "\n",
    "### **Scenario: Cat vs Dog Classifier**\n",
    "\n",
    "```\n",
    "Network Architecture:\n",
    "Input: 12,288 pixels (64Ã—64Ã—3)\n",
    "Hidden Layer 1: 1000 neurons\n",
    "Hidden Layer 2: 100 neurons  â† We'll apply dropout HERE\n",
    "Output: 2 neurons (cat, dog)\n",
    "\n",
    "Dropout rate: p = 0.5 (drop 50% of neurons)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Training: Forward Pass WITHOUT Dropout**\n",
    "\n",
    "**Batch: 1 cat image**\n",
    "\n",
    "**Input to Hidden Layer 2:**\n",
    "```\n",
    "100 neurons, sample of first 10:\n",
    "h = [2.3, 0.5, 3.1, 0.0, 1.8, 2.7, 0.3, 1.2, 3.5, 0.8]\n",
    "\n",
    "All neurons active!\n",
    "```\n",
    "\n",
    "**Output computation (simplified, just first output neuron):**\n",
    "```\n",
    "Weights connecting to output neuron 0 (cat detector):\n",
    "w = [0.5, 0.3, 0.8, 0.1, 0.6, 0.7, 0.2, 0.4, 0.9, 0.3, ...]\n",
    "\n",
    "Output before softmax:\n",
    "zâ‚€ = Î£(h[i] Ã— w[i])\n",
    "   = 2.3Ã—0.5 + 0.5Ã—0.3 + 3.1Ã—0.8 + 0.0Ã—0.1 + 1.8Ã—0.6 + ...\n",
    "   = 1.15 + 0.15 + 2.48 + 0.0 + 1.08 + ...\n",
    "   = 15.7 (total from all 100 neurons)\n",
    "\n",
    "After softmax: P(cat) = 0.92 (92% confident)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Training: Forward Pass WITH Dropout (p=0.5)**\n",
    "\n",
    "**Step 1: Generate random mask**\n",
    "\n",
    "```\n",
    "For each of 100 neurons, flip a coin:\n",
    "Heads (50% chance) â†’ Keep (m[i] = 1)\n",
    "Tails (50% chance) â†’ Drop (m[i] = 0)\n",
    "\n",
    "Random mask m:\n",
    "m = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0, ...]\n",
    "     â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘\n",
    "    Keep Drop Keep Drop Drop Keep Keep Drop Keep Drop\n",
    "\n",
    "Count: 48 kept, 52 dropped (roughly 50-50)\n",
    "```\n",
    "\n",
    "**Step 2: Apply mask (element-wise multiply)**\n",
    "\n",
    "```\n",
    "Original activations:\n",
    "h = [2.3, 0.5, 3.1, 0.0, 1.8, 2.7, 0.3, 1.2, 3.5, 0.8, ...]\n",
    "\n",
    "Mask:\n",
    "m = [1,   0,   1,   0,   0,   1,   1,   0,   1,   0,   ...]\n",
    "\n",
    "After dropout:\n",
    "h_drop = h âŠ™ m\n",
    "       = [2.3, 0.0, 3.1, 0.0, 0.0, 2.7, 0.3, 0.0, 3.5, 0.0, ...]\n",
    "         â†‘    â†‘    â†‘    â†‘    â†‘    â†‘    â†‘    â†‘    â†‘    â†‘\n",
    "        Kept Dead Kept Dead Dead Kept Kept Dead Kept Dead\n",
    "```\n",
    "\n",
    "**Step 3: Scale up (inverted dropout)**\n",
    "\n",
    "```\n",
    "Why scale? We dropped 50% of neurons, so sum is now half!\n",
    "To maintain same expected value, divide by (1-p) = 0.5\n",
    "\n",
    "h_drop_scaled = h_drop / (1 - p)\n",
    "              = h_drop / 0.5\n",
    "              = 2 Ã— h_drop\n",
    "\n",
    "h_drop_scaled = [4.6, 0.0, 6.2, 0.0, 0.0, 5.4, 0.6, 0.0, 7.0, 0.0, ...]\n",
    "```\n",
    "\n",
    "**Step 4: Compute output with dropped neurons**\n",
    "\n",
    "```\n",
    "Same weights as before:\n",
    "w = [0.5, 0.3, 0.8, 0.1, 0.6, 0.7, 0.2, 0.4, 0.9, 0.3, ...]\n",
    "\n",
    "Output:\n",
    "zâ‚€ = Î£(h_drop_scaled[i] Ã— w[i])\n",
    "   = 4.6Ã—0.5 + 0.0Ã—0.3 + 6.2Ã—0.8 + 0.0Ã—0.1 + 0.0Ã—0.6 + 5.4Ã—0.7 + ...\n",
    "   = 2.30 + 0.0 + 4.96 + 0.0 + 0.0 + 3.78 + ...\n",
    "   = 16.1 (total from ~48 active neurons, scaled up)\n",
    "\n",
    "After softmax: P(cat) = 0.93\n",
    "\n",
    "Similar output! Scaling compensated for dropped neurons.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Detailed Comparison:**\n",
    "\n",
    "| Neuron | Original h | Mask | Dropped h | Scaled h | Contribution (Ã—w) |\n",
    "|--------|-----------|------|-----------|----------|-------------------|\n",
    "| 0 | 2.3 | 1 | 2.3 | 4.6 | 4.6Ã—0.5 = 2.30 |\n",
    "| 1 | 0.5 | 0 | 0.0 | 0.0 | 0.0Ã—0.3 = 0.00 |\n",
    "| 2 | 3.1 | 1 | 3.1 | 6.2 | 6.2Ã—0.8 = 4.96 |\n",
    "| 3 | 0.0 | 0 | 0.0 | 0.0 | 0.0Ã—0.1 = 0.00 |\n",
    "| 4 | 1.8 | 0 | 0.0 | 0.0 | 0.0Ã—0.6 = 0.00 |\n",
    "| 5 | 2.7 | 1 | 2.7 | 5.4 | 5.4Ã—0.7 = 3.78 |\n",
    "| 6 | 0.3 | 1 | 0.3 | 0.6 | 0.6Ã—0.2 = 0.12 |\n",
    "| 7 | 1.2 | 0 | 0.0 | 0.0 | 0.0Ã—0.4 = 0.00 |\n",
    "| 8 | 3.5 | 1 | 3.5 | 7.0 | 7.0Ã—0.9 = 6.30 |\n",
    "| 9 | 0.8 | 0 | 0.0 | 0.0 | 0.0Ã—0.3 = 0.00 |\n",
    "\n",
    "**Summary:**\n",
    "- Without dropout: All 10 contribute â†’ Sum â‰ˆ 15.7\n",
    "- With dropout: Only 5 contribute (scaled) â†’ Sum â‰ˆ 16.1\n",
    "- Similar results due to scaling!\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Training Over Multiple Batches\n",
    "\n",
    "Let's see how dropout changes each batch:\n",
    "\n",
    "### **Batch 1: Cat Image**\n",
    "\n",
    "```\n",
    "Mask 1: [1, 0, 1, 0, 0, 1, 1, 0, 1, 0, ...]\n",
    "Active neurons: 0, 2, 5, 6, 8, ... (48 total)\n",
    "\n",
    "Network uses: Neurons 0, 2, 5, 6, 8 to classify\n",
    "Learns: \"These specific neurons must detect cats\"\n",
    "```\n",
    "\n",
    "**Batch 2: Dog Image**\n",
    "\n",
    "```\n",
    "Mask 2: [0, 1, 0, 1, 1, 0, 1, 1, 0, 1, ...]\n",
    "Active neurons: 1, 3, 4, 6, 7, 9, ... (51 total)\n",
    "\n",
    "Network uses: DIFFERENT neurons! (1, 3, 4, 6, 7, 9)\n",
    "Learns: \"These OTHER neurons must also detect dogs\"\n",
    "\n",
    "Notice: Neuron 6 is only common active neuron!\n",
    "        Forces redundancy!\n",
    "```\n",
    "\n",
    "**Batch 3: Cat Image**\n",
    "\n",
    "```\n",
    "Mask 3: [1, 1, 0, 0, 1, 1, 0, 1, 1, 1, ...]\n",
    "Active neurons: 0, 1, 4, 5, 7, 8, 9, ... (49 total)\n",
    "\n",
    "Network uses: YET ANOTHER combination!\n",
    "Learns: \"Need multiple ways to detect cats\"\n",
    "\n",
    "Across batches:\n",
    "- Neuron 0: Active in batches 1, 3 â†’ Learns cat features\n",
    "- Neuron 1: Active in batches 2, 3 â†’ Learns both features\n",
    "- Neuron 2: Active in batch 1 only â†’ Must be reliable!\n",
    "- ...\n",
    "\n",
    "Every neuron becomes useful!\n",
    "No single neuron is critical!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Accumulative Effect:**\n",
    "\n",
    "**After 1000 batches with dropout:**\n",
    "\n",
    "```\n",
    "Neuron 0 was active in: 487 batches (48.7%)\n",
    "  â†’ Learned robust cat ear features\n",
    "  â†’ Can't rely on other specific neurons\n",
    "\n",
    "Neuron 1 was active in: 512 batches (51.2%)\n",
    "  â†’ Learned robust dog nose features\n",
    "  â†’ Also forced to be redundant\n",
    "\n",
    "Neuron 2 was active in: 495 batches (49.5%)\n",
    "  â†’ Learned whisker detection\n",
    "  â†’ Multiple ways to combine with others\n",
    "\n",
    "...\n",
    "\n",
    "Result: Each neuron learned general, robust features\n",
    "        No co-adaptation (dependency on specific neurons)\n",
    "        Network can handle missing information\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Testing (Inference) Phase\n",
    "\n",
    "### **Why We Don't Use Dropout at Test Time:**\n",
    "\n",
    "```\n",
    "Training: Need randomness for regularization\n",
    "Testing: Want consistent, best predictions\n",
    "```\n",
    "\n",
    "**Two approaches:**\n",
    "\n",
    "---\n",
    "\n",
    "### **Approach 1: Standard Dropout (Scale at Test)**\n",
    "\n",
    "**Training:**\n",
    "```\n",
    "Apply dropout (p=0.5):\n",
    "h_train = h âŠ™ m  (50% neurons dropped, not scaled)\n",
    "```\n",
    "\n",
    "**Testing:**\n",
    "```\n",
    "Use all neurons, but scale down:\n",
    "h_test = (1-p) Ã— h = 0.5 Ã— h\n",
    "\n",
    "Why? Expected value during training was:\n",
    "E[h_train] = (1-p) Ã— h = 0.5 Ã— h\n",
    "So at test, we use that expected value directly.\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Training (one batch):\n",
    "h = [2.3, 0.5, 3.1, 0.0, 1.8, 2.7, 0.3, 1.2, 3.5, 0.8]\n",
    "m = [1,   0,   1,   0,   0,   1,   1,   0,   1,   0  ]\n",
    "h_train = [2.3, 0.0, 3.1, 0.0, 0.0, 2.7, 0.3, 0.0, 3.5, 0.0]\n",
    "\n",
    "Testing:\n",
    "h_test = 0.5 Ã— [2.3, 0.5, 3.1, 0.0, 1.8, 2.7, 0.3, 1.2, 3.5, 0.8]\n",
    "       = [1.15, 0.25, 1.55, 0.0, 0.9, 1.35, 0.15, 0.6, 1.75, 0.4]\n",
    "\n",
    "All neurons active, but scaled down to match training expectation\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Approach 2: Inverted Dropout (Scale at Train) âœ“ Preferred**\n",
    "\n",
    "**Training:**\n",
    "```\n",
    "Apply dropout AND scale up:\n",
    "h_train = (h âŠ™ m) / (1-p) = (h âŠ™ m) / 0.5 = 2 Ã— (h âŠ™ m)\n",
    "\n",
    "Maintains same expected value as original h!\n",
    "```\n",
    "\n",
    "**Testing:**\n",
    "```\n",
    "Use all neurons, no scaling:\n",
    "h_test = h  (simple!)\n",
    "\n",
    "No changes needed at test time!\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Training (one batch):\n",
    "h = [2.3, 0.5, 3.1, 0.0, 1.8, 2.7, 0.3, 1.2, 3.5, 0.8]\n",
    "m = [1,   0,   1,   0,   0,   1,   1,   0,   1,   0  ]\n",
    "h_dropped = [2.3, 0.0, 3.1, 0.0, 0.0, 2.7, 0.3, 0.0, 3.5, 0.0]\n",
    "h_train = 2 Ã— h_dropped\n",
    "        = [4.6, 0.0, 6.2, 0.0, 0.0, 5.4, 0.6, 0.0, 7.0, 0.0]\n",
    "\n",
    "Testing:\n",
    "h_test = [2.3, 0.5, 3.1, 0.0, 1.8, 2.7, 0.3, 1.2, 3.5, 0.8]\n",
    "(No change! Already matches expected value)\n",
    "\n",
    "Advantage: Testing is simpler, faster\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Complete Cat vs Dog Example\n",
    "\n",
    "### **Full Network Training with Dropout:**\n",
    "\n",
    "```\n",
    "Architecture:\n",
    "Input: 12,288 pixels\n",
    "Hidden 1: 1000 neurons (dropout p=0.5)\n",
    "Hidden 2: 100 neurons (dropout p=0.5)\n",
    "Output: 2 neurons (no dropout on output!)\n",
    "\n",
    "Training: 100 images (50 cats, 50 dogs)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Epoch 1, Batch 1:**\n",
    "\n",
    "**Forward pass:**\n",
    "\n",
    "```\n",
    "1. Input layer: 12,288 pixels of cat image\n",
    "   x = [0.12, 0.45, 0.89, 0.23, ...]\n",
    "\n",
    "2. Hidden layer 1 (before dropout):\n",
    "   h1 = ReLU(W1 Ã— x + b1)\n",
    "   h1 = [2.1, 0.0, 3.4, 1.2, 0.5, 3.8, ...]  (1000 values)\n",
    "\n",
    "3. Dropout on h1 (p=0.5):\n",
    "   Random mask: m1 = [1, 0, 1, 1, 0, 1, ...]\n",
    "   h1_drop = 2 Ã— (h1 âŠ™ m1)\n",
    "          = [4.2, 0.0, 6.8, 2.4, 0.0, 7.6, ...]\n",
    "   \n",
    "   ~500 neurons active, ~500 dead\n",
    "\n",
    "4. Hidden layer 2 (before dropout):\n",
    "   h2 = ReLU(W2 Ã— h1_drop + b2)\n",
    "   h2 = [3.2, 1.1, 0.0, 2.7, 0.8, ...]  (100 values)\n",
    "\n",
    "5. Dropout on h2 (p=0.5):\n",
    "   Random mask: m2 = [1, 0, 1, 1, 0, ...]\n",
    "   h2_drop = 2 Ã— (h2 âŠ™ m2)\n",
    "          = [6.4, 0.0, 0.0, 5.4, 0.0, ...]\n",
    "   \n",
    "   ~50 neurons active, ~50 dead\n",
    "\n",
    "6. Output layer (no dropout):\n",
    "   z = W_out Ã— h2_drop + b_out\n",
    "   z = [8.3, 2.1]  (cat score, dog score)\n",
    "   \n",
    "   After softmax:\n",
    "   P(cat) = 0.997, P(dog) = 0.003\n",
    "   \n",
    "   Prediction: Cat âœ“ Correct!\n",
    "\n",
    "7. Loss:\n",
    "   Cross-entropy = -log(0.997) = 0.003 (very low, good!)\n",
    "```\n",
    "\n",
    "**Backward pass:**\n",
    "\n",
    "```\n",
    "Gradients flow back through network:\n",
    "âˆ‚L/âˆ‚W_out, âˆ‚L/âˆ‚b_out â†’ computed normally\n",
    "\n",
    "At h2 dropout:\n",
    "- Gradients only flow through active neurons!\n",
    "- Dead neurons (mask=0) get âˆ‚L/âˆ‚h2=0\n",
    "- Active neurons get full gradient (scaled)\n",
    "\n",
    "Example:\n",
    "If neuron 0 was active (m2[0]=1):\n",
    "  âˆ‚L/âˆ‚h2[0] = âˆ‚L/âˆ‚h2_drop[0] Ã— 2  (undo scaling)\n",
    "  Update weights normally\n",
    "\n",
    "If neuron 1 was dead (m2[1]=0):\n",
    "  âˆ‚L/âˆ‚h2[1] = 0\n",
    "  No weight updates for this neuron this batch!\n",
    "\n",
    "Same for h1 dropout layer...\n",
    "```\n",
    "\n",
    "**Weight updates:**\n",
    "\n",
    "```\n",
    "Only weights connected to ACTIVE neurons get updated!\n",
    "\n",
    "W_out: All weights update (no dropout on output)\n",
    "W2: Only ~50 columns update (corresponding to active h1 neurons)\n",
    "W1: Only ~500 rows update (corresponding to active h1 neurons)\n",
    "\n",
    "Specific neurons don't always get to \"learn\"\n",
    "â†’ Forces each neuron to be independently useful!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Epoch 1, Batch 2:**\n",
    "\n",
    "**Same cat image, but DIFFERENT random masks!**\n",
    "\n",
    "```\n",
    "1. Hidden layer 1 dropout:\n",
    "   NEW mask: m1 = [0, 1, 1, 0, 1, 0, ...]\n",
    "   Different neurons active!\n",
    "   \n",
    "   Previously active: Neuron 0, 2, 3, 5...\n",
    "   Now active: Neuron 1, 2, 4...\n",
    "   \n",
    "   Only neuron 2 in common!\n",
    "\n",
    "2. Hidden layer 2 dropout:\n",
    "   NEW mask: m2 = [0, 1, 0, 1, 1, ...]\n",
    "   Again, different combination!\n",
    "\n",
    "3. Network forced to use different neurons\n",
    "   Must learn redundant representations!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **After 10 Epochs:**\n",
    "\n",
    "**Statistics:**\n",
    "\n",
    "```\n",
    "Neuron activity (Hidden Layer 1, 1000 neurons):\n",
    "\n",
    "Neuron 0: Active 492/1000 batches (49.2%)\n",
    "  Average activation when active: 3.2\n",
    "  Learned: Cat ear detector (robust)\n",
    "\n",
    "Neuron 1: Active 507/1000 batches (50.7%)\n",
    "  Average activation when active: 2.8\n",
    "  Learned: Dog nose detector (robust)\n",
    "\n",
    "Neuron 2: Active 489/1000 batches (48.9%)\n",
    "  Average activation when active: 4.1\n",
    "  Learned: Whisker detector (robust)\n",
    "\n",
    "...\n",
    "\n",
    "All neurons used roughly equally!\n",
    "No single neuron dominates!\n",
    "Each learned independent, useful features!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Testing Phase:**\n",
    "\n",
    "**Test image: New cat**\n",
    "\n",
    "```\n",
    "1. Input: x_test = [...]\n",
    "\n",
    "2. Hidden layer 1 (NO dropout):\n",
    "   h1_test = ReLU(W1 Ã— x_test + b1)\n",
    "   h1_test = [2.1, 1.5, 3.4, 1.2, 0.5, 3.8, ...]\n",
    "   \n",
    "   All 1000 neurons active!\n",
    "   No scaling needed (inverted dropout)\n",
    "\n",
    "3. Hidden layer 2 (NO dropout):\n",
    "   h2_test = ReLU(W2 Ã— h1_test + b2)\n",
    "   h2_test = [3.2, 1.1, 2.5, 2.7, 0.8, ...]\n",
    "   \n",
    "   All 100 neurons active!\n",
    "\n",
    "4. Output:\n",
    "   z = W_out Ã— h2_test + b_out\n",
    "   z = [7.8, 1.9]\n",
    "   \n",
    "   P(cat) = 0.995\n",
    "   \n",
    "   Prediction: Cat âœ“\n",
    "```\n",
    "\n",
    "**Ensemble effect:**\n",
    "```\n",
    "During training, network saw many \"sub-networks\":\n",
    "- Batch 1: Neurons [0,2,3,5,...] and [0,2,4,...]\n",
    "- Batch 2: Neurons [1,2,4,6,...] and [1,3,5,...]\n",
    "- ...\n",
    "\n",
    "At test time: Average over all possible sub-networks\n",
    "             = Using all neurons with scaled weights\n",
    "             = Ensemble of 2^1100 models!\n",
    "             \n",
    "This is why dropout works so well!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Comparing Dropout to L1/L2\n",
    "\n",
    "### **Conceptual Differences:**\n",
    "\n",
    "| Aspect | L1/L2 | Dropout |\n",
    "|--------|-------|---------|\n",
    "| **What it does** | Penalizes large weights | Randomly drops neurons |\n",
    "| **Where applied** | Loss function | Network architecture |\n",
    "| **How it regularizes** | Weight shrinking | Ensemble averaging |\n",
    "| **Deterministic?** | Yes | No (stochastic) |\n",
    "| **At test time** | Use all weights | Use all neurons (scaled) |\n",
    "| **Sparsity** | L1: Yes, L2: No | No |\n",
    "| **Computation** | Adds to gradient | Adds masking operation |\n",
    "\n",
    "---\n",
    "\n",
    "### **Same Network, Different Regularizations:**\n",
    "\n",
    "**Cat vs Dog classifier, 100 training images:**\n",
    "\n",
    "---\n",
    "\n",
    "#### **No Regularization:**\n",
    "\n",
    "```\n",
    "After 50 epochs:\n",
    "Training accuracy: 100%\n",
    "Test accuracy: 68%\n",
    "\n",
    "Weight statistics:\n",
    "- Max weight: 127.3\n",
    "- Min weight: -98.6\n",
    "- Average magnitude: 12.4\n",
    "\n",
    "Network behavior:\n",
    "- Memorized training images\n",
    "- Co-adapted neurons (work only together)\n",
    "- Brittle, doesn't generalize\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **L2 Regularization (Î»=0.01):**\n",
    "\n",
    "```\n",
    "After 50 epochs:\n",
    "Training accuracy: 94%\n",
    "Test accuracy: 91%\n",
    "\n",
    "Weight statistics:\n",
    "- Max weight: 4.2\n",
    "- Min weight: -3.8\n",
    "- Average magnitude: 0.8\n",
    "\n",
    "Network behavior:\n",
    "- Smooth, small weights\n",
    "- All weights contribute\n",
    "- Good generalization\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **L1 Regularization (Î»=0.001):**\n",
    "\n",
    "```\n",
    "After 50 epochs:\n",
    "Training accuracy: 96%\n",
    "Test accuracy: 92%\n",
    "\n",
    "Weight statistics:\n",
    "- Max weight: 6.1\n",
    "- Min weight: 0 (many zeros!)\n",
    "- Average magnitude: 1.2 (non-zero only)\n",
    "- Sparsity: 73% weights are zero\n",
    "\n",
    "Network behavior:\n",
    "- Feature selection\n",
    "- Only important connections kept\n",
    "- Interpretable\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Dropout (p=0.5):**\n",
    "\n",
    "```\n",
    "After 50 epochs:\n",
    "Training accuracy: 89%\n",
    "Test accuracy: 93%\n",
    "\n",
    "Weight statistics:\n",
    "- Max weight: 8.7\n",
    "- Min weight: -7.3\n",
    "- Average magnitude: 2.1\n",
    "\n",
    "Network behavior:\n",
    "- Redundant representations\n",
    "- No co-adaptation\n",
    "- Ensemble effect\n",
    "- Best generalization!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Combining Regularizations:**\n",
    "\n",
    "**Best practice: Use multiple together!**\n",
    "\n",
    "```python\n",
    "model = CatDogClassifier()\n",
    "\n",
    "# L2 on weights\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    weight_decay=0.01  # L2\n",
    ")\n",
    "\n",
    "# Dropout in architecture\n",
    "class CatDogClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(12288, 1000)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)  # Dropout\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)  # Dropout\n",
    "        self.fc3 = nn.Linear(100, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Applied here\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)  # And here\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Result: L2 + Dropout\n",
    "# Training accuracy: 88%\n",
    "# Test accuracy: 95% â† Best of both worlds!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Why Dropout Works: Theoretical Insights\n",
    "\n",
    "### **Reason 1: Prevents Co-adaptation**\n",
    "\n",
    "**Without dropout:**\n",
    "```\n",
    "Neuron A: Detects \"cat ears\"\n",
    "Neuron B: Detects \"pointy shape, but only if A is active\"\n",
    "Neuron C: Detects \"fur, but only if A and B are active\"\n",
    "\n",
    "Co-adapted! B and C rely on A.\n",
    "If A fails â†’ Everything fails!\n",
    "```\n",
    "\n",
    "**With dropout:**\n",
    "```\n",
    "Training batch 1: A active, B dropped, C active\n",
    "  â†’ C must learn to detect fur WITHOUT B!\n",
    "  \n",
    "Training batch 2: A dropped, B active, C active\n",
    "  â†’ B and C must work WITHOUT A!\n",
    "  \n",
    "Training batch 3: A active, B active, C dropped\n",
    "  â†’ A and B must work WITHOUT C!\n",
    "\n",
    "Result: Each neuron learns independently useful features!\n",
    "No dependencies!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Reason 2: Ensemble Learning**\n",
    "\n",
    "**Dropout = Training exponentially many models!**\n",
    "\n",
    "```\n",
    "With 100 neurons and p=0.5:\n",
    "Number of possible dropout masks = 2^100 â‰ˆ 10^30\n",
    "\n",
    "Each training batch uses different mask\n",
    "â†’ Training different sub-network\n",
    "â†’ Like training 10^30 different models!\n",
    "\n",
    "At test time:\n",
    "Use all neurons = Approximate average of all models\n",
    "                = Ensemble prediction\n",
    "                = Much more robust!\n",
    "```\n",
    "\n",
    "**Visualize:**\n",
    "\n",
    "```\n",
    "Sub-network 1:      Sub-network 2:      Sub-network 3:\n",
    " â—â”€â”€â—â”€â”€â—‹            â—‹â”€â”€â—â”€â”€â—             â—â”€â”€â—‹â”€â”€â—\n",
    " â”‚  â”‚               â”‚  â”‚                â”‚     â”‚\n",
    " â—â”€â”€â—‹â”€â”€â—            â—â”€â”€â—‹â”€â”€â—‹             â—‹â”€â”€â—â”€â”€â—\n",
    "\n",
    "Each sees different parts of network\n",
    "Each learns different strategy\n",
    "Test time: Average all strategies\n",
    "â†’ Robust ensemble!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Reason 3: Adding Noise**\n",
    "\n",
    "**Dropout adds multiplicative noise to activations:**\n",
    "\n",
    "```\n",
    "Without dropout: h = f(x)\n",
    "With dropout: h = f(x) Ã— m, where m âˆˆ {0, 2}\n",
    "\n",
    "This noise:\n",
    "- Prevents overfitting to exact training values\n",
    "- Makes network robust to perturbations\n",
    "- Similar to data augmentation\n",
    "\n",
    "Like training with noisy data:\n",
    "â†’ Network learns to ignore noise\n",
    "â†’ Focuses on robust features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Reason 4: Implicit Regularization**\n",
    "\n",
    "**Mathematical equivalence (approximately):**\n",
    "\n",
    "Dropout â‰ˆ L2 regularization on activations\n",
    "\n",
    "```\n",
    "Minimizing with dropout â‰ˆ Minimizing:\n",
    "L_pred + Î» Ã— Î£||h||^2\n",
    "\n",
    "Where Î» depends on dropout rate p.\n",
    "\n",
    "But dropout is MORE than just L2:\n",
    "- Stochastic (different each batch)\n",
    "- Acts on activations, not just weights\n",
    "- Creates ensemble effect\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Practical Implementation Details\n",
    "\n",
    "### **Complete PyTorch Implementation:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DropoutExample(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(12288, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, 2)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)  # Dropout after activation\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)  # Dropout after activation\n",
    "        \n",
    "        # Output (no dropout!)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Training mode\n",
    "model = DropoutExample()\n",
    "model.train()  # Enables dropout\n",
    "\n",
    "# Testing mode\n",
    "model.eval()   # Disables dropout\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Manual Dropout Implementation:**\n",
    "\n",
    "```python\n",
    "def dropout(x, p=0.5, training=True):\n",
    "    \"\"\"\n",
    "    Manual dropout implementation\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        p: Dropout probability\n",
    "        training: If True, apply dropout; if False, no dropout\n",
    "    \n",
    "    Returns:\n",
    "        Output tensor\n",
    "    \"\"\"\n",
    "    if not training:\n",
    "        return x  # No dropout at test time\n",
    "    \n",
    "    # Generate random mask\n",
    "    mask = (torch.rand_like(x) > p).float()\n",
    "    # mask: 0 with probability p, 1 with probability (1-p)\n",
    "    \n",
    "    # Apply mask and scale (inverted dropout)\n",
    "    return x * mask / (1 - p)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(32, 100)  # Batch of 32, 100 features\n",
    "\n",
    "# Training\n",
    "x_train = dropout(x, p=0.5, training=True)\n",
    "print(\"Training:\")\n",
    "print(f\"  Original mean: {x.mean():.3f}\")\n",
    "print(f\"  After dropout mean: {x_train.mean():.3f}\")\n",
    "print(f\"  Zeros: {(x_train == 0).sum().item()}/{x_train.numel()}\")\n",
    "\n",
    "# Testing\n",
    "x_test = dropout(x, p=0.5, training=False)\n",
    "print(\"\\nTesting:\")\n",
    "print(f\"  Mean: {x_test.mean():.3f}\")\n",
    "print(f\"  Zeros: {(x_test == 0).sum().item()}/{x_test.numel()}\")\n",
    "\n",
    "# Output:\n",
    "# Training:\n",
    "#   Original mean: 0.023\n",
    "#   After dropout mean: 0.019\n",
    "#   Zeros: 1587/3200 (49.6%)\n",
    "#\n",
    "# Testing:\n",
    "#   Mean: 0.023\n",
    "#   Zeros: 0/3200 (0%)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Detailed Training Loop:**\n",
    "\n",
    "```python\n",
    "model = DropoutExample(dropout_rate=0.5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# TRAINING\n",
    "model.train()  # Important! Enables dropout\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Forward pass (dropout active)\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Check dropout effect\n",
    "        with torch.no_grad():\n",
    "            # Run same batch twice with dropout\n",
    "            out1 = model(batch_x[:1])  # Same image\n",
    "            out2 = model(batch_x[:1])  # Same image again\n",
    "            \n",
    "            print(f\"Same input, different dropout:\")\n",
    "            print(f\"  Output 1: {out1}\")\n",
    "            print(f\"  Output 2: {out2}\")\n",
    "            print(f\"  Different: {not torch.allclose(out1, out2)}\")\n",
    "            # Output: Different: True (stochastic!)\n",
    "\n",
    "# TESTING\n",
    "model.eval()  # Important! Disables dropout\n",
    "\n",
    "test_acc = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        # Forward pass (no dropout)\n",
    "        outputs = model(batch_x)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        test_acc += (preds == batch_y).sum().item()\n",
    "        \n",
    "        # Run same batch twice\n",
    "        out1 = model(batch_x[:1])\n",
    "        out2 = model(batch_x[:1])\n",
    "        \n",
    "        print(f\"Same input, no dropout:\")\n",
    "        print(f\"  Output 1: {out1}\")\n",
    "        print(f\"  Output 2: {out2}\")\n",
    "        print(f\"  Same: {torch.allclose(out1, out2)}\")\n",
    "        # Output: Same: True (deterministic!)\n",
    "\n",
    "test_acc /= len(test_loader.dataset)\n",
    "print(f\"Test accuracy: {test_acc:.2%}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Choosing Dropout Rate (p)\n",
    "\n",
    "### **Common Values:**\n",
    "\n",
    "| Layer Type | Typical p | Reason |\n",
    "|------------|-----------|--------|\n",
    "| **Input layer** | 0.0-0.2 | Rarely drop input features |\n",
    "| **Hidden layers** | 0.5 | Default, works well |\n",
    "| **Last hidden layer** | 0.2-0.5 | Less aggressive |\n",
    "| **Output layer** | 0.0 | Never drop output! |\n",
    "| **Convolutional layers** | 0.0-0.2 | Already regularized spatially |\n",
    "| **Fully connected layers** | 0.5 | Most prone to overfitting |\n",
    "\n",
    "---\n",
    "\n",
    "### **Effect of Different p Values:**\n",
    "\n",
    "**Our Cat vs Dog network with different dropout rates:**\n",
    "\n",
    "| p | Training Acc | Test Acc | Training Time | Notes |\n",
    "|---|--------------|----------|---------------|-------|\n",
    "| **0.0** | 100% | 68% | Fast | No regularization, overfit |\n",
    "| **0.2** | 98% | 89% | Fast | Mild regularization |\n",
    "| **0.5** | 89% | 93% | Medium | **Optimal!** |\n",
    "| **0.7** | 78% | 85% | Slow | Too aggressive |\n",
    "| **0.9** | 62% | 64% | Very slow | Can't learn, underfit |\n",
    "\n",
    "**Visualization:**\n",
    "\n",
    "```\n",
    "    Accuracy\n",
    "      â†‘\n",
    "  100â”‚â—                 â— Training\n",
    "   90â”‚        â—‹\n",
    "   80â”‚             â—‹    â—‹ Test\n",
    "   70â”‚                  â—\n",
    "   60â”‚                      â—â—‹\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ p\n",
    "       0   0.2   0.5  0.7  0.9\n",
    "            â†‘\n",
    "         Sweet spot!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Guidelines:**\n",
    "\n",
    "```\n",
    "Start with p=0.5 for fully connected layers\n",
    "\n",
    "If overfitting persists:\n",
    "  â†’ Increase p (e.g., 0.6 or 0.7)\n",
    "\n",
    "If underfitting:\n",
    "  â†’ Decrease p (e.g., 0.3 or 0.4)\n",
    "  â†’ Or remove dropout entirely\n",
    "\n",
    "For convolutional networks:\n",
    "  â†’ Use p=0.2 or 0.3 (less aggressive)\n",
    "  â†’ Only on fully connected layers at end\n",
    "\n",
    "For very deep networks:\n",
    "  â†’ Use lower p (0.2-0.3) on all layers\n",
    "  â†’ Easier gradient flow\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Variants of Dropout\n",
    "\n",
    "### **DropConnect**\n",
    "\n",
    "Instead of dropping neurons, drop **individual weights!**\n",
    "\n",
    "```\n",
    "Regular Dropout:              DropConnect:\n",
    "Drop entire neurons          Drop individual connections\n",
    "\n",
    "  â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—                  â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—\n",
    "  â”‚\\  /â”‚\\  /â”‚                  â”‚\\  /â”‚  â•± â”‚\n",
    "  â— Ã—   â— Ã—  â—                  â— â”€  â— Ã—  â—\n",
    "  â”‚    Ã—    â”‚                  â”‚ â•²  â”‚ â•²  â”‚\n",
    "  â—â”€â”€â”€â”€â—‹â”€â”€â”€â”€â—                  â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—\n",
    "       â†‘                            â†‘â†‘â†‘\n",
    "  Dead neuron              Dead connections\n",
    "\n",
    "More fine-grained!\n",
    "```\n",
    "\n",
    "**Formula:**\n",
    "```python\n",
    "# Regular dropout\n",
    "h = dropout(activation)\n",
    "\n",
    "# DropConnect\n",
    "W_dropped = W * mask  # Mask on weights!\n",
    "h = activation(W_dropped Ã— x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Spatial Dropout (for CNNs)**\n",
    "\n",
    "Drop entire **feature maps** instead of individual neurons:\n",
    "\n",
    "```\n",
    "Regular Dropout:              Spatial Dropout:\n",
    "Random neurons in map         Entire maps\n",
    "\n",
    "  Feature Map 1:               Feature Map 1:\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚â— â—‹ â— â—‹ â”‚                  â”‚â— â— â— â—â”‚  â† Kept\n",
    "  â”‚â—‹ â— â—‹ â— â”‚                  â”‚â— â— â— â—â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "  Feature Map 2:               Feature Map 2:\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚â—‹ â— â—‹ â— â”‚                  â”‚â—‹ â—‹ â—‹ â—‹â”‚  â† Dropped\n",
    "  â”‚â— â—‹ â— â—‹ â”‚                  â”‚â—‹ â—‹ â—‹ â—‹â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Why?** Neighboring pixels are correlated; dropping individual pixels doesn't help much.\n",
    "\n",
    "---\n",
    "\n",
    "### **Variational Dropout**\n",
    "\n",
    "Use **same mask** for all timesteps in RNNs:\n",
    "\n",
    "```\n",
    "Regular Dropout in RNN:       Variational Dropout:\n",
    "Different mask each step     Same mask all steps\n",
    "\n",
    "  hâ‚ with mask [1,0,1,0]        hâ‚ with mask [1,0,1,0]\n",
    "  hâ‚‚ with mask [0,1,1,0]        hâ‚‚ with mask [1,0,1,0]\n",
    "  hâ‚ƒ with mask [1,1,0,0]        hâ‚ƒ with mask [1,0,1,0]\n",
    "  \n",
    "  Inconsistent over time!       Consistent over time!\n",
    "```\n",
    "\n",
    "**Why?** RNNs process sequences; same features should be dropped across entire sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Alpha Dropout (for SELU activation)**\n",
    "\n",
    "Maintains mean and variance for SELU activations:\n",
    "\n",
    "```\n",
    "Regular Dropout: Can break SELU properties\n",
    "Alpha Dropout: Preserves mean=0, var=1\n",
    "\n",
    "Designed specifically for:\n",
    "- SELU activation functions\n",
    "- Self-normalizing neural networks\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 12. When NOT to Use Dropout\n",
    "\n",
    "### **Cases to Avoid:**\n",
    "\n",
    "**1. Small Datasets**\n",
    "```\n",
    "With only 20 training samples:\n",
    "- Dropout throws away half the data each batch!\n",
    "- Only ~10 samples per update\n",
    "- Not enough to learn anything\n",
    "\n",
    "Solution: Use L2 regularization instead\n",
    "```\n",
    "\n",
    "**2. Convolutional Layers**\n",
    "```\n",
    "Conv layers already have:\n",
    "- Weight sharing (regularization)\n",
    "- Spatial structure (regularization)\n",
    "- Translation invariance\n",
    "\n",
    "Dropout can hurt more than help!\n",
    "\n",
    "Solution: Only use dropout on fully connected layers\n",
    "```\n",
    "\n",
    "**3. Batch Normalization Present**\n",
    "```\n",
    "Batch norm already regularizes:\n",
    "- Normalizes activations\n",
    "- Adds noise during training\n",
    "- Provides similar benefits to dropout\n",
    "\n",
    "Dropout + Batch norm can conflict!\n",
    "\n",
    "Solution: Use one or the other, not both in same layer\n",
    "```\n",
    "\n",
    "**4. When Training is Already Slow**\n",
    "```\n",
    "Dropout effectively cuts training data in half\n",
    "â†’ Need ~2Ã— more epochs to converge\n",
    "\n",
    "If training already takes days:\n",
    "- Consider L2 instead\n",
    "- Or use lower dropout rate (p=0.2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Complete Comparison: L2 vs L1 vs Dropout\n",
    "\n",
    "### **Summary Table:**\n",
    "\n",
    "| Aspect | L2 | L1 | Dropout |\n",
    "|--------|----|----|---------|\n",
    "| **Mechanism** | Penalize large weights | Penalize any weights | Drop neurons randomly |\n",
    "| **Effect** | Shrink weights | Sparse weights | Ensemble averaging |\n",
    "| **Deterministic?** | Yes | Yes | No |\n",
    "| **Adds computation** | Minimal | Minimal | Some (masking) |\n",
    "| **Feature selection** | No | Yes | No |\n",
    "| **Co-adaptation** | Still possible | Still possible | Prevented âœ“ |\n",
    "| **Interpretability** | Hard | Easy (sparse) | Hard |\n",
    "| **Model size** | Same | Reduced | Same |\n",
    "| **Test time** | Normal | Normal | Normal (after setup) |\n",
    "| **Works with small data** | Yes | Yes | Not great |\n",
    "| **Best for** | General | Feature selection | Deep networks |\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use What:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚        Regularization Decision Tree     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Is your network deep (>3 layers)?\n",
    "â”œâ”€ YES â†’ Use Dropout (p=0.5)\n",
    "â”‚        + L2 (Î»=0.01) for weights\n",
    "â”‚        = Best generalization!\n",
    "â”‚\n",
    "â””â”€ NO â†’ Is it a convolutional network?\n",
    "    â”œâ”€ YES â†’ Use L2 (Î»=0.01)\n",
    "    â”‚        Maybe light dropout (p=0.2) on FC layers\n",
    "    â”‚\n",
    "    â””â”€ NO â†’ Do you need feature selection?\n",
    "        â”œâ”€ YES â†’ Use L1 (Î»=0.001)\n",
    "        â”‚        Get sparse, interpretable model\n",
    "        â”‚\n",
    "        â””â”€ NO â†’ Use L2 (Î»=0.01)\n",
    "                Simple and effective\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Summary: Dropout\n",
    "\n",
    "### **What Dropout Does:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚            Dropout                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "TRAINING: Randomly set p% of neurons to 0\n",
    "\n",
    "EFFECT: \n",
    "- Prevents co-adaptation\n",
    "- Creates ensemble of sub-networks\n",
    "- Forces redundant representations\n",
    "\n",
    "TESTING: Use all neurons (scaled)\n",
    "\n",
    "RESULT: \n",
    "- Better generalization\n",
    "- More robust features\n",
    "- Ensemble prediction\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **The Three Key Ideas:**\n",
    "\n",
    "**1. Break Co-dependencies**\n",
    "```\n",
    "Without dropout:\n",
    "Neuron A â†depends onâ†’ Neuron B â†depends onâ†’ Neuron C\n",
    "(Fragile!)\n",
    "\n",
    "With dropout:\n",
    "Neuron A (independent)\n",
    "Neuron B (independent)  \n",
    "Neuron C (independent)\n",
    "(Robust!)\n",
    "```\n",
    "\n",
    "**2. Ensemble Learning**\n",
    "```\n",
    "Train 2^n different sub-networks\n",
    "Test: Average over all of them\n",
    "= Powerful ensemble for free!\n",
    "```\n",
    "\n",
    "**3. Noise as Regularization**\n",
    "```\n",
    "Adding multiplicative noise:\n",
    "- Prevents overfitting\n",
    "- Makes network robust\n",
    "- Similar to data augmentation\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Recommendations:**\n",
    "\n",
    "```\n",
    "âœ“ Use p=0.5 for fully connected layers\n",
    "âœ“ Use p=0.2 for convolutional layers (or none)\n",
    "âœ“ Never dropout output layer\n",
    "âœ“ Combine with L2 for best results\n",
    "âœ“ Remember model.train() and model.eval()!\n",
    "\n",
    "âœ— Don't use with very small datasets\n",
    "âœ— Don't combine with batch norm in same layer\n",
    "âœ— Don't use p>0.7 (too aggressive)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 15. Final Example: All Three Together\n",
    "\n",
    "### **Complete Cat vs Dog Classifier with L1 + L2 + Dropout:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RegularizedCatDogNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Architecture\n",
    "        self.fc1 = nn.Linear(12288, 1000)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)  # Dropout!\n",
    "        \n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)  # Dropout!\n",
    "        \n",
    "        self.fc3 = nn.Linear(100, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = RegularizedCatDogNet()\n",
    "\n",
    "# Optimizer with L2\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    weight_decay=0.01  # L2 regularization!\n",
    ")\n",
    "\n",
    "# L1 regularization function\n",
    "def l1_regularization(model, lambda_l1=0.001):\n",
    "    l1_loss = 0\n",
    "    for param in model.parameters():\n",
    "        l1_loss += torch.sum(torch.abs(param))\n",
    "    return lambda_l1 * l1_loss\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Forward\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Loss = Prediction + L1 + L2 (L2 in optimizer)\n",
    "        pred_loss = F.cross_entropy(outputs, batch_y)\n",
    "        l1_loss = l1_regularization(model, lambda_l1=0.0001)\n",
    "        total_loss = pred_loss + l1_loss\n",
    "        \n",
    "        # Backward and update\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.2%}\")\n",
    "\n",
    "# Result with all three:\n",
    "# Training Acc: 87%\n",
    "# Test Acc: 96%  â† Excellent generalization!\n",
    "#\n",
    "# L1 created 68% sparsity\n",
    "# L2 kept weights small\n",
    "# Dropout prevented co-adaptation\n",
    "# = Best of all worlds!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**You now understand all three major regularization techniques! ğŸ‰**\n",
    "\n",
    "- **L2:** Shrinks all weights, smooth and stable\n",
    "- **L1:** Creates sparsity, automatic feature selection  \n",
    "- **Dropout:** Prevents co-adaptation, ensemble learning\n",
    "\n",
    "Each tackles overfitting from a different angle, and they work great together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4eaa8",
   "metadata": {},
   "source": [
    "# Vanishing and Exploding Gradients: Complete Explanation\n",
    "## Why They Happen, When They Occur, and How to Fix Them\n",
    "### (Detailed Step-by-Step with Cat vs Dog Classification)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— **Connection to Previous Topics**\n",
    "\n",
    "### **What We Know So Far:**\n",
    "\n",
    "**From Backpropagation:**\n",
    "```\n",
    "Training process:\n",
    "1. Forward pass: Compute predictions\n",
    "2. Compute loss: Measure error\n",
    "3. Backward pass: Compute âˆ‚L/âˆ‚w for all weights\n",
    "4. Update: w := w - Î±Â·âˆ‚L/âˆ‚w\n",
    "```\n",
    "\n",
    "**From CNNs and Regularization:**\n",
    "```\n",
    "We can build deep networks:\n",
    "Input â†’ Layer 1 â†’ Layer 2 â†’ ... â†’ Layer 10 â†’ Output\n",
    "\n",
    "We use gradient descent to train them.\n",
    "```\n",
    "\n",
    "**The New Problem:**\n",
    "\n",
    "```\n",
    "What if gradients become too small or too large\n",
    "as they flow backward through many layers?\n",
    "\n",
    "Too small (vanishing): Learning stops!\n",
    "Too large (exploding): Training diverges!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part 1: What Are Vanishing and Exploding Gradients?\n",
    "\n",
    "## 1. Plain English Explanation\n",
    "\n",
    "### **The Core Problem**\n",
    "\n",
    "Imagine a deep network as a chain of people passing a message:\n",
    "\n",
    "```\n",
    "Person 1 â†’ Person 2 â†’ Person 3 â†’ ... â†’ Person 10\n",
    "(Layer 1)   (Layer 2)   (Layer 3)       (Layer 10)\n",
    "```\n",
    "\n",
    "**During training, we need to pass error signals BACKWARD:**\n",
    "```\n",
    "Person 1 â† Person 2 â† Person 3 â† ... â† Person 10\n",
    "(Update)    (Update)    (Update)        (Error)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Vanishing Gradients (The Whisper Problem)**\n",
    "\n",
    "```\n",
    "Person 10: Shouts \"ERROR IS 100!\"\n",
    "Person 9:  Hears 50, passes on \"error is 50\"\n",
    "Person 8:  Hears 25, passes on \"error is 25\"\n",
    "Person 7:  Hears 12, passes on \"error is 12\"\n",
    "...\n",
    "Person 2:  Hears 0.01, passes on \"error is 0.01\"\n",
    "Person 1:  Hears 0.0001 â€” can't hear anything!\n",
    "\n",
    "Early layers can't learn!\n",
    "```\n",
    "\n",
    "**In neural networks:**\n",
    "```\n",
    "Layer 10: Gradient = 1.0\n",
    "Layer 9:  Gradient = 0.5\n",
    "Layer 8:  Gradient = 0.25\n",
    "Layer 7:  Gradient = 0.125\n",
    "Layer 6:  Gradient = 0.0625\n",
    "Layer 5:  Gradient = 0.031\n",
    "Layer 4:  Gradient = 0.016\n",
    "Layer 3:  Gradient = 0.008\n",
    "Layer 2:  Gradient = 0.004\n",
    "Layer 1:  Gradient = 0.002 â† Too small to learn!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Exploding Gradients (The Megaphone Problem)**\n",
    "\n",
    "```\n",
    "Person 10: Says \"error is 2\"\n",
    "Person 9:  Amplifies to 4, passes \"error is 4\"\n",
    "Person 8:  Amplifies to 8, passes \"error is 8\"\n",
    "Person 7:  Amplifies to 16, passes \"error is 16\"\n",
    "...\n",
    "Person 2:  Amplifies to 512, passes \"error is 512\"\n",
    "Person 1:  Hears 1024 â€” OVERWHELMING!\n",
    "\n",
    "Signal becomes meaningless noise!\n",
    "```\n",
    "\n",
    "**In neural networks:**\n",
    "```\n",
    "Layer 10: Gradient = 1.0\n",
    "Layer 9:  Gradient = 2.0\n",
    "Layer 8:  Gradient = 4.0\n",
    "Layer 7:  Gradient = 8.0\n",
    "Layer 6:  Gradient = 16.0\n",
    "Layer 5:  Gradient = 32.0\n",
    "Layer 4:  Gradient = 64.0\n",
    "Layer 3:  Gradient = 128.0\n",
    "Layer 2:  Gradient = 256.0\n",
    "Layer 1:  Gradient = 512.0 â† Explodes! Training fails!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Mathematical Cause\n",
    "\n",
    "### **Chain Rule Through Multiple Layers**\n",
    "\n",
    "**Forward pass through 3 layers:**\n",
    "```\n",
    "x â†’ [Layer 1] â†’ hâ‚ â†’ [Layer 2] â†’ hâ‚‚ â†’ [Layer 3] â†’ output\n",
    "```\n",
    "\n",
    "**Backward pass (chain rule):**\n",
    "```\n",
    "âˆ‚L/âˆ‚Wâ‚ = âˆ‚L/âˆ‚hâ‚‚ Ã— âˆ‚hâ‚‚/âˆ‚hâ‚ Ã— âˆ‚hâ‚/âˆ‚Wâ‚\n",
    "         â†‘        â†‘        â†‘\n",
    "      Layer 3  Layer 2  Layer 1\n",
    "```\n",
    "\n",
    "**The problem:** We multiply many terms!\n",
    "\n",
    "**If each term is < 1:** Product vanishes\n",
    "```\n",
    "âˆ‚hâ‚‚/âˆ‚hâ‚ = 0.5\n",
    "âˆ‚hâ‚/âˆ‚Wâ‚ = 0.5\n",
    "\n",
    "âˆ‚L/âˆ‚Wâ‚ = (something) Ã— 0.5 Ã— 0.5 = (something) Ã— 0.25\n",
    "```\n",
    "\n",
    "**If each term is > 1:** Product explodes\n",
    "```\n",
    "âˆ‚hâ‚‚/âˆ‚hâ‚ = 2.0\n",
    "âˆ‚hâ‚/âˆ‚Wâ‚ = 2.0\n",
    "\n",
    "âˆ‚L/âˆ‚Wâ‚ = (something) Ã— 2.0 Ã— 2.0 = (something) Ã— 4.0\n",
    "```\n",
    "\n",
    "**With 10 layers:**\n",
    "```\n",
    "Vanishing: 0.5^10 = 0.00097 (nearly zero!)\n",
    "Exploding: 2.0^10 = 1024 (huge!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part 2: Detailed Numerical Example - Vanishing Gradients\n",
    "\n",
    "## Setup: Deep Cat vs Dog Classifier\n",
    "\n",
    "```\n",
    "Network Architecture (10 layers deep):\n",
    "\n",
    "Input: 64Ã—64Ã—3 = 12,288 pixels\n",
    "â†“\n",
    "Layer 1: 1000 neurons + sigmoid\n",
    "â†“\n",
    "Layer 2: 1000 neurons + sigmoid\n",
    "â†“\n",
    "Layer 3: 1000 neurons + sigmoid\n",
    "â†“\n",
    "Layer 4: 1000 neurons + sigmoid\n",
    "â†“\n",
    "Layer 5: 1000 neurons + sigmoid\n",
    "â†“\n",
    "Layer 6: 1000 neurons + sigmoid\n",
    "â†“\n",
    "Layer 7: 1000 neurons + sigmoid\n",
    "â†“\n",
    "Layer 8: 1000 neurons + sigmoid\n",
    "â†“\n",
    "Layer 9: 1000 neurons + sigmoid\n",
    "â†“\n",
    "Layer 10: 2 neurons (cat, dog)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Forward Pass: One Training Example\n",
    "\n",
    "**Input:** Cat image (x)\n",
    "\n",
    "**Layer 1:**\n",
    "```\n",
    "Sample of 5 neurons:\n",
    "\n",
    "zâ‚ = Wâ‚x + bâ‚ = [0.5, -0.3, 1.2, -0.8, 0.6]\n",
    "\n",
    "Apply sigmoid: hâ‚ = Ïƒ(zâ‚)\n",
    "hâ‚[0] = Ïƒ(0.5)  = 1/(1+e^(-0.5))  = 0.622\n",
    "hâ‚[1] = Ïƒ(-0.3) = 1/(1+e^(0.3))   = 0.426\n",
    "hâ‚[2] = Ïƒ(1.2)  = 1/(1+e^(-1.2))  = 0.768\n",
    "hâ‚[3] = Ïƒ(-0.8) = 1/(1+e^(0.8))   = 0.310\n",
    "hâ‚[4] = Ïƒ(0.6)  = 1/(1+e^(-0.6))  = 0.646\n",
    "\n",
    "hâ‚ = [0.622, 0.426, 0.768, 0.310, 0.646, ...]\n",
    "```\n",
    "\n",
    "**Layer 2:**\n",
    "```\n",
    "zâ‚‚ = Wâ‚‚hâ‚ + bâ‚‚ = [0.3, -0.2, 0.8, -0.5, 0.4]\n",
    "\n",
    "hâ‚‚ = Ïƒ(zâ‚‚)\n",
    "hâ‚‚ = [0.574, 0.450, 0.689, 0.378, 0.599, ...]\n",
    "```\n",
    "\n",
    "**Notice:** Activations are all in range (0, 1) and clustering around 0.5\n",
    "\n",
    "**Layer 3:**\n",
    "```\n",
    "hâ‚ƒ = [0.556, 0.472, 0.623, 0.412, 0.587, ...]\n",
    "```\n",
    "\n",
    "**Layer 4:**\n",
    "```\n",
    "hâ‚„ = [0.542, 0.485, 0.601, 0.438, 0.573, ...]\n",
    "```\n",
    "\n",
    "**Pattern emerging:** All activations converging toward 0.5!\n",
    "\n",
    "**Layer 5-9:** Continue this pattern...\n",
    "\n",
    "**Layer 10 (output):**\n",
    "```\n",
    "zâ‚â‚€ = [0.52, 0.48]\n",
    "After softmax: [0.51, 0.49]\n",
    "\n",
    "Prediction: 51% cat, 49% dog\n",
    "Network is basically guessing randomly!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Backward Pass: Watching Gradients Vanish\n",
    "\n",
    "**At output (Layer 10):**\n",
    "```\n",
    "True label: Cat (y = [1, 0])\n",
    "Prediction: Å· = [0.51, 0.49]\n",
    "\n",
    "Gradient at output:\n",
    "âˆ‚L/âˆ‚zâ‚â‚€ = Å· - y = [0.51-1, 0.49-0] = [-0.49, 0.49]\n",
    "\n",
    "Magnitude: ~0.5\n",
    "```\n",
    "\n",
    "**Backward to Layer 9:**\n",
    "\n",
    "```\n",
    "Need to compute: âˆ‚L/âˆ‚hâ‚‰\n",
    "\n",
    "Using chain rule:\n",
    "âˆ‚L/âˆ‚hâ‚‰ = âˆ‚L/âˆ‚zâ‚â‚€ Ã— âˆ‚zâ‚â‚€/âˆ‚hâ‚‰\n",
    "       = âˆ‚L/âˆ‚zâ‚â‚€ Ã— Wâ‚â‚€\n",
    "\n",
    "But we also need âˆ‚L/âˆ‚zâ‚‰ for the weight updates:\n",
    "âˆ‚L/âˆ‚zâ‚‰ = âˆ‚L/âˆ‚hâ‚‰ Ã— âˆ‚hâ‚‰/âˆ‚zâ‚‰\n",
    "       = âˆ‚L/âˆ‚hâ‚‰ Ã— Ïƒ'(zâ‚‰)\n",
    "\n",
    "Sigmoid derivative: Ïƒ'(z) = Ïƒ(z)(1-Ïƒ(z))\n",
    "```\n",
    "\n",
    "**Calculate sigmoid derivatives at Layer 9:**\n",
    "```\n",
    "hâ‚‰ = [0.542, 0.485, 0.601, 0.438, 0.573, ...]\n",
    "\n",
    "Sigmoid derivatives:\n",
    "Ïƒ'(zâ‚‰[0]) = hâ‚‰[0] Ã— (1 - hâ‚‰[0]) = 0.542 Ã— 0.458 = 0.248\n",
    "Ïƒ'(zâ‚‰[1]) = hâ‚‰[1] Ã— (1 - hâ‚‰[1]) = 0.485 Ã— 0.515 = 0.250\n",
    "Ïƒ'(zâ‚‰[2]) = hâ‚‰[2] Ã— (1 - hâ‚‰[2]) = 0.601 Ã— 0.399 = 0.240\n",
    "Ïƒ'(zâ‚‰[3]) = hâ‚‰[3] Ã— (1 - hâ‚‰[3]) = 0.438 Ã— 0.562 = 0.246\n",
    "Ïƒ'(zâ‚‰[4]) = hâ‚‰[4] Ã— (1 - hâ‚‰[4]) = 0.573 Ã— 0.427 = 0.245\n",
    "\n",
    "All derivatives â‰ˆ 0.25 (one quarter!)\n",
    "```\n",
    "\n",
    "**Pass gradient backward:**\n",
    "```\n",
    "âˆ‚L/âˆ‚zâ‚‰ â‰ˆ (gradient from layer 10) Ã— Wâ‚â‚€ Ã— 0.25\n",
    "\n",
    "If gradient from layer 10 was 0.5:\n",
    "âˆ‚L/âˆ‚zâ‚‰ â‰ˆ 0.5 Ã— (weights) Ã— 0.25\n",
    "       â‰ˆ 0.5 Ã— 1.0 Ã— 0.25  (assuming weight â‰ˆ 1)\n",
    "       â‰ˆ 0.125\n",
    "\n",
    "Gradient reduced by 4Ã— just from sigmoid derivative!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Backward to Layer 8:**\n",
    "\n",
    "```\n",
    "âˆ‚L/âˆ‚zâ‚ˆ = (gradient from layer 9) Ã— Wâ‚‰ Ã— Ïƒ'(zâ‚ˆ)\n",
    "       â‰ˆ 0.125 Ã— 1.0 Ã— 0.25\n",
    "       â‰ˆ 0.031\n",
    "\n",
    "Gradient reduced by another 4Ã—!\n",
    "```\n",
    "\n",
    "**Continue backward through all layers:**\n",
    "\n",
    "| Layer | Gradient Magnitude | Factor from Previous |\n",
    "|-------|-------------------|---------------------|\n",
    "| 10 (output) | 0.500 | - |\n",
    "| 9 | 0.125 | Ã—0.25 |\n",
    "| 8 | 0.031 | Ã—0.25 |\n",
    "| 7 | 0.008 | Ã—0.25 |\n",
    "| 6 | 0.002 | Ã—0.25 |\n",
    "| 5 | 0.0005 | Ã—0.25 |\n",
    "| 4 | 0.0001 | Ã—0.25 |\n",
    "| 3 | 0.00003 | Ã—0.25 |\n",
    "| 2 | 0.000008 | Ã—0.25 |\n",
    "| 1 | 0.000002 | Ã—0.25 |\n",
    "\n",
    "**Gradient at Layer 1: 0.000002 (essentially zero!)**\n",
    "\n",
    "```\n",
    "Total reduction: 0.25^9 â‰ˆ 0.000004\n",
    "\n",
    "The gradient vanished!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Weight Updates with Vanished Gradients\n",
    "\n",
    "**Layer 10 (close to output):**\n",
    "```\n",
    "Gradient: 0.125\n",
    "Learning rate: Î± = 0.1\n",
    "\n",
    "Weight update:\n",
    "Î”Wâ‚â‚€ = Î± Ã— gradient Ã— activations\n",
    "     = 0.1 Ã— 0.125 Ã— (hâ‚‰)\n",
    "     â‰ˆ 0.0125 Ã— (activations)\n",
    "\n",
    "For a weight of 1.0:\n",
    "W_new = 1.0 - 0.0125 = 0.9875\n",
    "Change: 1.25% âœ“ Reasonable learning\n",
    "```\n",
    "\n",
    "**Layer 5 (middle):**\n",
    "```\n",
    "Gradient: 0.0005\n",
    "Learning rate: Î± = 0.1\n",
    "\n",
    "Weight update:\n",
    "Î”Wâ‚… = 0.1 Ã— 0.0005 Ã— (hâ‚„)\n",
    "    â‰ˆ 0.00005 Ã— (activations)\n",
    "\n",
    "For a weight of 1.0:\n",
    "W_new = 1.0 - 0.00005 = 0.99995\n",
    "Change: 0.005% âœ“ Very slow learning\n",
    "```\n",
    "\n",
    "**Layer 1 (early layer):**\n",
    "```\n",
    "Gradient: 0.000002\n",
    "Learning rate: Î± = 0.1\n",
    "\n",
    "Weight update:\n",
    "Î”Wâ‚ = 0.1 Ã— 0.000002 Ã— (x)\n",
    "    â‰ˆ 0.0000002 Ã— (activations)\n",
    "\n",
    "For a weight of 1.0:\n",
    "W_new = 1.0 - 0.0000002 = 0.9999998\n",
    "Change: 0.00002% âœ— Practically no learning!\n",
    "```\n",
    "\n",
    "**Result:**\n",
    "```\n",
    "After 1000 iterations:\n",
    "\n",
    "Layer 10: Learned well, weights changed significantly\n",
    "Layer 5:  Learned slowly\n",
    "Layer 1:  Barely changed at all!\n",
    "\n",
    "Early layers stuck with random initialization!\n",
    "Network can't learn deep representations!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part 3: Detailed Numerical Example - Exploding Gradients\n",
    "\n",
    "## Setup: Same Network, Different Initialization\n",
    "\n",
    "```\n",
    "Same architecture, but:\n",
    "- Weights initialized LARGE (mean=0, std=2.0)\n",
    "- Using ReLU instead of sigmoid (for demonstration)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Forward Pass with Large Weights\n",
    "\n",
    "**Layer 1:**\n",
    "```\n",
    "zâ‚ = Wâ‚x + bâ‚\n",
    "\n",
    "With large weights (Wâ‚ elements ~2.0):\n",
    "zâ‚ = [5.2, -3.8, 8.1, -6.5, 4.3, ...]\n",
    "\n",
    "Apply ReLU: hâ‚ = max(0, zâ‚)\n",
    "hâ‚ = [5.2, 0.0, 8.1, 0.0, 4.3, ...]\n",
    "\n",
    "Large activations!\n",
    "```\n",
    "\n",
    "**Layer 2:**\n",
    "```\n",
    "zâ‚‚ = Wâ‚‚hâ‚ + bâ‚‚\n",
    "\n",
    "Wâ‚‚ also large (~2.0), hâ‚ is large (5-8):\n",
    "zâ‚‚ = 2.0 Ã— [5.2, 0, 8.1, ...] (simplified)\n",
    "zâ‚‚ = [18.7, -12.3, 29.4, -21.8, 15.6, ...]\n",
    "\n",
    "After ReLU:\n",
    "hâ‚‚ = [18.7, 0.0, 29.4, 0.0, 15.6, ...]\n",
    "\n",
    "Even larger!\n",
    "```\n",
    "\n",
    "**Layer 3:**\n",
    "```\n",
    "zâ‚ƒ â‰ˆ 2.0 Ã— [18.7, 0, 29.4, ...]\n",
    "zâ‚ƒ = [67.8, -45.2, 106.3, -78.9, 56.4, ...]\n",
    "\n",
    "hâ‚ƒ = [67.8, 0.0, 106.3, 0.0, 56.4, ...]\n",
    "\n",
    "Exploding!\n",
    "```\n",
    "\n",
    "**Layer 4:**\n",
    "```\n",
    "hâ‚„ = [245.1, 0.0, 384.2, 0.0, 203.8, ...]\n",
    "```\n",
    "\n",
    "**Layer 5:**\n",
    "```\n",
    "hâ‚… = [886.7, 0.0, 1389.5, 0.0, 737.2, ...]\n",
    "```\n",
    "\n",
    "**By Layer 10:**\n",
    "```\n",
    "hâ‚â‚€ = [3.8Ã—10â·, 0.0, 5.9Ã—10â·, 0.0, ...]\n",
    "\n",
    "OVERFLOW! Numbers too large to represent!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Backward Pass with Exploding Gradients\n",
    "\n",
    "**At output:**\n",
    "```\n",
    "Prediction went to infinity, loss is NaN\n",
    "Or if we catch it earlier:\n",
    "\n",
    "âˆ‚L/âˆ‚zâ‚â‚€ â‰ˆ [huge, huge]\n",
    "```\n",
    "\n",
    "**Backward to Layer 9:**\n",
    "```\n",
    "âˆ‚L/âˆ‚zâ‚‰ = âˆ‚L/âˆ‚hâ‚‰ Ã— ReLU'(zâ‚‰)\n",
    "\n",
    "ReLU derivative:\n",
    "ReLU'(z) = 1 if z > 0, else 0\n",
    "\n",
    "If neuron was active:\n",
    "âˆ‚L/âˆ‚zâ‚‰ = (large gradient from layer 10) Ã— Wâ‚â‚€ Ã— 1\n",
    "       â‰ˆ (1000) Ã— (2.0) Ã— 1\n",
    "       â‰ˆ 2000\n",
    "\n",
    "Gradient doubled!\n",
    "```\n",
    "\n",
    "**Backward through layers:**\n",
    "\n",
    "| Layer | Gradient Magnitude | Factor from Previous |\n",
    "|-------|-------------------|---------------------|\n",
    "| 10 | 1,000 | - |\n",
    "| 9 | 2,000 | Ã—2 |\n",
    "| 8 | 4,000 | Ã—2 |\n",
    "| 7 | 8,000 | Ã—2 |\n",
    "| 6 | 16,000 | Ã—2 |\n",
    "| 5 | 32,000 | Ã—2 |\n",
    "| 4 | 64,000 | Ã—2 |\n",
    "| 3 | 128,000 | Ã—2 |\n",
    "| 2 | 256,000 | Ã—2 |\n",
    "| 1 | 512,000 | Ã—2 |\n",
    "\n",
    "**Gradient at Layer 1: 512,000 (exploded!)**\n",
    "\n",
    "---\n",
    "\n",
    "## Weight Updates with Exploded Gradients\n",
    "\n",
    "**Layer 1:**\n",
    "```\n",
    "Gradient: 512,000\n",
    "Learning rate: Î± = 0.1\n",
    "\n",
    "Weight update:\n",
    "Î”Wâ‚ = 0.1 Ã— 512,000 Ã— (x)\n",
    "    = 51,200 Ã— (activations)\n",
    "\n",
    "For a weight of 2.0:\n",
    "W_new = 2.0 - 51,200 Ã— (some value)\n",
    "      = -25,598 or +25,602 (random sign)\n",
    "\n",
    "Weight jumped wildly!\n",
    "```\n",
    "\n",
    "**Consequences:**\n",
    "```\n",
    "Iteration 1: Weights explode to thousands\n",
    "Iteration 2: Loss becomes NaN\n",
    "Iteration 3: All predictions are NaN\n",
    "\n",
    "Training failed completely!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part 4: Why Sigmoid Causes Vanishing Gradients\n",
    "\n",
    "## The Sigmoid Gradient Problem\n",
    "\n",
    "### **Sigmoid Function:**\n",
    "\n",
    "```\n",
    "Ïƒ(z) = 1/(1 + e^(-z))\n",
    "\n",
    "Derivative:\n",
    "Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))\n",
    "```\n",
    "\n",
    "### **The Issue: Maximum Derivative is 0.25**\n",
    "\n",
    "```\n",
    "When is derivative maximum?\n",
    "Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))\n",
    "\n",
    "This is maximized when Ïƒ(z) = 0.5\n",
    "â†’ Ïƒ'(z) = 0.5 Ã— 0.5 = 0.25\n",
    "\n",
    "Graph of Ïƒ'(z):\n",
    "\n",
    "   Ïƒ'(z)\n",
    "    â†‘\n",
    "0.25â”‚    â•±â”€â•²\n",
    "    â”‚   â•±   â•²\n",
    "0.20â”‚  â•±     â•²\n",
    "    â”‚ â•±       â•²\n",
    "0.10â”‚â•±         â•²\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ z\n",
    "   -4  0   4\n",
    "\n",
    "Maximum is 0.25 at z=0\n",
    "Goes to 0 as |z| increases\n",
    "```\n",
    "\n",
    "### **Numerical Examples:**\n",
    "\n",
    "```\n",
    "z = 0:   Ïƒ(z) = 0.500, Ïƒ'(z) = 0.500Ã—0.500 = 0.250 â† Maximum!\n",
    "z = 1:   Ïƒ(z) = 0.731, Ïƒ'(z) = 0.731Ã—0.269 = 0.197\n",
    "z = 2:   Ïƒ(z) = 0.881, Ïƒ'(z) = 0.881Ã—0.119 = 0.105\n",
    "z = 3:   Ïƒ(z) = 0.953, Ïƒ'(z) = 0.953Ã—0.047 = 0.045\n",
    "z = 4:   Ïƒ(z) = 0.982, Ïƒ'(z) = 0.982Ã—0.018 = 0.018\n",
    "z = 5:   Ïƒ(z) = 0.993, Ïƒ'(z) = 0.993Ã—0.007 = 0.007\n",
    "\n",
    "As activation saturates (z large):\n",
    "â†’ Derivative approaches zero\n",
    "â†’ \"Saturated neurons\"\n",
    "```\n",
    "\n",
    "### **Through 10 Layers:**\n",
    "\n",
    "```\n",
    "Best case (all neurons at z=0, maximum derivative):\n",
    "Gradient reduction = (0.25)^10 = 0.00000095367\n",
    "\n",
    "Even with perfect conditions, gradient is essentially zero!\n",
    "\n",
    "Typical case (neurons saturated, z=2):\n",
    "Gradient reduction = (0.1)^10 = 0.0000000001\n",
    "\n",
    "Completely vanished!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Why Tanh is Slightly Better\n",
    "\n",
    "### **Tanh Function:**\n",
    "\n",
    "```\n",
    "tanh(z) = (e^z - e^(-z))/(e^z + e^(-z))\n",
    "\n",
    "Derivative:\n",
    "tanh'(z) = 1 - tanhÂ²(z)\n",
    "```\n",
    "\n",
    "### **Maximum Derivative: 1.0**\n",
    "\n",
    "```\n",
    "tanh'(0) = 1 - 0Â² = 1.0 â† Better than sigmoid!\n",
    "\n",
    "But still problems:\n",
    "z = 2:  tanh'(z) = 0.071\n",
    "z = 3:  tanh'(z) = 0.010\n",
    "z = 4:  tanh'(z) = 0.001\n",
    "```\n",
    "\n",
    "### **Through 10 Layers:**\n",
    "\n",
    "```\n",
    "Best case (all z=0):\n",
    "Gradient reduction = (1.0)^10 = 1.0 â† Perfect!\n",
    "\n",
    "But this never happens in practice...\n",
    "\n",
    "Typical case (z=1.5):\n",
    "Gradient reduction = (0.18)^10 = 0.00000003570\n",
    "\n",
    "Still vanishes, but slower than sigmoid\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part 5: Why ReLU Helps (But Not Completely)\n",
    "\n",
    "## ReLU Derivative\n",
    "\n",
    "```\n",
    "ReLU(z) = max(0, z)\n",
    "\n",
    "Derivative:\n",
    "ReLU'(z) = 1 if z > 0\n",
    "           0 if z â‰¤ 0\n",
    "```\n",
    "\n",
    "### **The Good News:**\n",
    "\n",
    "```\n",
    "Active neurons (z > 0): derivative = 1\n",
    "â†’ No multiplication factor!\n",
    "â†’ Gradient passes through unchanged!\n",
    "\n",
    "Through 10 layers (if all active):\n",
    "Gradient reduction = (1)^10 = 1.0 âœ“\n",
    "\n",
    "No vanishing!\n",
    "```\n",
    "\n",
    "### **The Bad News:**\n",
    "\n",
    "```\n",
    "Dead neurons (z â‰¤ 0): derivative = 0\n",
    "â†’ Gradient completely blocked!\n",
    "â†’ \"Dying ReLU\" problem\n",
    "\n",
    "If 50% neurons die:\n",
    "Gradient = (1)^5 Ã— (0)^5 = 0\n",
    "\n",
    "Still vanishes if too many neurons die!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part 6: Weight Initialization - The Solution\n",
    "\n",
    "## The Core Problem with Random Initialization\n",
    "\n",
    "### **Naive Initialization:**\n",
    "\n",
    "```python\n",
    "# Random initialization from standard normal\n",
    "W = np.random.randn(n_in, n_out)\n",
    "```\n",
    "\n",
    "**Why this fails:**\n",
    "\n",
    "```\n",
    "If n_in = 1000 (input dimension):\n",
    "\n",
    "Output for one neuron:\n",
    "z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚â‚€â‚€â‚€xâ‚â‚€â‚€â‚€\n",
    "\n",
    "If w ~ N(0,1) and x ~ N(0,1):\n",
    "Variance of z = Var(wâ‚xâ‚) + Var(wâ‚‚xâ‚‚) + ... + Var(wâ‚â‚€â‚€â‚€xâ‚â‚€â‚€â‚€)\n",
    "              = 1000 Ã— Var(w)Var(x)  (assuming independence)\n",
    "              = 1000 Ã— 1 Ã— 1\n",
    "              = 1000\n",
    "\n",
    "Standard deviation of z = âˆš1000 â‰ˆ 31.6\n",
    "\n",
    "z could easily be -100 to +100!\n",
    "```\n",
    "\n",
    "**With sigmoid:**\n",
    "```\n",
    "z = 100  â†’ Ïƒ(100) â‰ˆ 1.0  â†’ Saturated!\n",
    "z = -100 â†’ Ïƒ(-100) â‰ˆ 0.0 â†’ Saturated!\n",
    "\n",
    "Ïƒ'(100) â‰ˆ 0 â†’ Vanishing gradient!\n",
    "```\n",
    "\n",
    "**With ReLU:**\n",
    "```\n",
    "z = 100 â†’ ReLU(100) = 100 â†’ Huge activation!\n",
    "Next layer gets even larger â†’ Exploding!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Solution 1: Xavier/Glorot Initialization\n",
    "\n",
    "### **The Goal:**\n",
    "\n",
    "Keep variance of activations AND gradients roughly constant across layers.\n",
    "\n",
    "### **The Formula:**\n",
    "\n",
    "**For sigmoid/tanh activations:**\n",
    "\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in} + n_{out}}}\\right)$$\n",
    "\n",
    "Or equivalently:\n",
    "$$W \\sim \\text{Uniform}\\left(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $n_{in}$ = number of input units\n",
    "- $n_{out}$ = number of output units\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Works:**\n",
    "\n",
    "**Mathematical derivation (simplified):**\n",
    "\n",
    "```\n",
    "Forward pass variance:\n",
    "For z = Wx + b, we want Var(z) â‰ˆ Var(x)\n",
    "\n",
    "If weights have variance ÏƒÂ²_w:\n",
    "Var(z) = n_in Ã— ÏƒÂ²_w Ã— Var(x)\n",
    "\n",
    "To keep variance constant:\n",
    "n_in Ã— ÏƒÂ²_w = 1\n",
    "â†’ ÏƒÂ²_w = 1/n_in\n",
    "\n",
    "Backward pass variance:\n",
    "For gradient âˆ‚L/âˆ‚x = W^T Ã— âˆ‚L/âˆ‚z:\n",
    "Var(âˆ‚L/âˆ‚x) = n_out Ã— ÏƒÂ²_w Ã— Var(âˆ‚L/âˆ‚z)\n",
    "\n",
    "To keep gradient variance constant:\n",
    "n_out Ã— ÏƒÂ²_w = 1\n",
    "â†’ ÏƒÂ²_w = 1/n_out\n",
    "\n",
    "Compromise between forward and backward:\n",
    "ÏƒÂ²_w = 2/(n_in + n_out)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Numerical Example:**\n",
    "\n",
    "**Layer with n_in=1000, n_out=500:**\n",
    "\n",
    "**Bad initialization:**\n",
    "```python\n",
    "W = np.random.randn(1000, 500)  # Ïƒ = 1.0\n",
    "\n",
    "Input: x ~ N(0, 1), n_in=1000\n",
    "\n",
    "Output variance:\n",
    "Var(z) = 1000 Ã— 1.0Â² Ã— 1 = 1000\n",
    "Ïƒ(z) = 31.6\n",
    "\n",
    "With sigmoid:\n",
    "z typically in range [-100, 100]\n",
    "Neurons saturated!\n",
    "Ïƒ'(z) â‰ˆ 0\n",
    "Vanishing gradient!\n",
    "```\n",
    "\n",
    "**Xavier initialization:**\n",
    "```python\n",
    "std = np.sqrt(2 / (1000 + 500))\n",
    "W = np.random.randn(1000, 500) * std\n",
    "# std = âˆš(2/1500) = 0.0365\n",
    "\n",
    "Output variance:\n",
    "Var(z) = 1000 Ã— (0.0365)Â² Ã— 1 = 1.33\n",
    "Ïƒ(z) = 1.15\n",
    "\n",
    "With sigmoid:\n",
    "z typically in range [-3, 3]\n",
    "Neurons active in good range!\n",
    "Ïƒ'(z) â‰ˆ 0.2 (good)\n",
    "Gradients flow!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Solution 2: He Initialization (for ReLU)\n",
    "\n",
    "### **The Problem with Xavier for ReLU:**\n",
    "\n",
    "```\n",
    "ReLU kills half the neurons (z < 0 â†’ output = 0)\n",
    "â†’ Effective fan-in is actually n_in/2\n",
    "\n",
    "Xavier assumes all neurons active\n",
    "â†’ Underestimates needed variance for ReLU\n",
    "```\n",
    "\n",
    "### **He Initialization Formula:**\n",
    "\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in}}}\\right)$$\n",
    "\n",
    "**Why factor of 2?**\n",
    "- ReLU zeros out half the neurons\n",
    "- Need 2Ã— variance to compensate\n",
    "- Keeps variance constant despite killing neurons\n",
    "\n",
    "---\n",
    "\n",
    "### **Numerical Example:**\n",
    "\n",
    "**Layer with n_in=1000, n_out=500:**\n",
    "\n",
    "**Xavier with ReLU (suboptimal):**\n",
    "```python\n",
    "std_xavier = np.sqrt(2 / (1000 + 500))  # 0.0365\n",
    "W = np.random.randn(1000, 500) * std_xavier\n",
    "\n",
    "Forward pass:\n",
    "z = Wx\n",
    "Var(z) = 1000 Ã— (0.0365)Â² = 1.33\n",
    "After ReLU (kills half):\n",
    "Var(ReLU(z)) â‰ˆ 0.665\n",
    "\n",
    "Variance decreased!\n",
    "Activations get smaller through layers!\n",
    "```\n",
    "\n",
    "**He with ReLU (optimal):**\n",
    "```python\n",
    "std_he = np.sqrt(2 / 1000)  # 0.0447\n",
    "W = np.random.randn(1000, 500) * std_he\n",
    "\n",
    "Forward pass:\n",
    "z = Wx  \n",
    "Var(z) = 1000 Ã— (0.0447)Â² = 2.0\n",
    "After ReLU (kills half):\n",
    "Var(ReLU(z)) â‰ˆ 1.0\n",
    "\n",
    "Variance maintained! âœ“\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part 7: Complete Example - Fixing the Deep Network\n",
    "\n",
    "## Our Cat vs Dog Network (10 layers)\n",
    "\n",
    "### **Attempt 1: Bad Initialization**\n",
    "\n",
    "```python\n",
    "class DeepCatDogNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # 10 layers, each 1000 neurons\n",
    "        sizes = [12288, 1000, 1000, 1000, 1000, 1000, \n",
    "                 1000, 1000, 1000, 1000, 2]\n",
    "        \n",
    "        for i in range(len(sizes)-1):\n",
    "            layer = nn.Linear(sizes[i], sizes[i+1])\n",
    "            \n",
    "            # BAD: Default initialization (too large)\n",
    "            # PyTorch default: Uniform(-1/âˆšn_in, 1/âˆšn_in)\n",
    "            # For n_in=1000: std â‰ˆ 0.028\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.sigmoid(layer(x))  # Sigmoid activation\n",
    "        x = self.layers[-1](x)  # Output layer\n",
    "        return x\n",
    "\n",
    "model = DeepCatDogNet()\n",
    "```\n",
    "\n",
    "**Training:**\n",
    "```\n",
    "Epoch 1:\n",
    "  Forward pass through layer 1: mean=0.501, std=0.092\n",
    "  Forward pass through layer 2: mean=0.500, std=0.043\n",
    "  Forward pass through layer 3: mean=0.500, std=0.021\n",
    "  Forward pass through layer 4: mean=0.500, std=0.011\n",
    "  Forward pass through layer 5: mean=0.500, std=0.005\n",
    "  Forward pass through layer 10: mean=0.500, std=0.0001\n",
    "  \n",
    "  All neurons converging to 0.5 (saturated)!\n",
    "  \n",
    "  Backward pass gradients:\n",
    "  Layer 10: 0.125\n",
    "  Layer 9:  0.031\n",
    "  Layer 8:  0.008\n",
    "  Layer 5:  0.0005\n",
    "  Layer 1:  0.000002 â† Vanished!\n",
    "  \n",
    "Loss: 0.693 (random guessing)\n",
    "\n",
    "Epoch 100:\n",
    "  Loss: 0.693 (no improvement!)\n",
    "  Train accuracy: 50%\n",
    "  Test accuracy: 50%\n",
    "  \n",
    "Network never learned!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Attempt 2: Xavier Initialization**\n",
    "\n",
    "```python\n",
    "class DeepCatDogNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        sizes = [12288, 1000, 1000, 1000, 1000, 1000, \n",
    "                 1000, 1000, 1000, 1000, 2]\n",
    "        \n",
    "        for i in range(len(sizes)-1):\n",
    "            layer = nn.Linear(sizes[i], sizes[i+1])\n",
    "            \n",
    "            # Xavier initialization\n",
    "            nn.init.xavier_normal_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.sigmoid(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "model = DeepCatDogNet()\n",
    "```\n",
    "\n",
    "**Check initialization:**\n",
    "```python\n",
    "for i, layer in enumerate(model.layers[:-1]):\n",
    "    W = layer.weight.data\n",
    "    n_in, n_out = W.shape[1], W.shape[0]\n",
    "    expected_std = np.sqrt(2 / (n_in + n_out))\n",
    "    actual_std = W.std().item()\n",
    "    \n",
    "    print(f\"Layer {i+1}: Expected Ïƒ={expected_std:.4f}, \"\n",
    "          f\"Actual Ïƒ={actual_std:.4f}\")\n",
    "\n",
    "# Output:\n",
    "# Layer 1: Expected Ïƒ=0.0123, Actual Ïƒ=0.0122 âœ“\n",
    "# Layer 2: Expected Ïƒ=0.0316, Actual Ïƒ=0.0317 âœ“\n",
    "# Layer 3: Expected Ïƒ=0.0316, Actual Ïƒ=0.0314 âœ“\n",
    "# ...\n",
    "```\n",
    "\n",
    "**Training:**\n",
    "```\n",
    "Epoch 1:\n",
    "  Forward pass activations:\n",
    "  Layer 1: mean=0.498, std=0.234 âœ“ (good spread)\n",
    "  Layer 2: mean=0.501, std=0.228 âœ“\n",
    "  Layer 3: mean=0.499, std=0.225 âœ“\n",
    "  Layer 4: mean=0.502, std=0.223 âœ“\n",
    "  Layer 10: mean=0.497, std=0.210 âœ“\n",
    "  \n",
    "  Activations stay in healthy range!\n",
    "  \n",
    "  Backward pass gradients:\n",
    "  Layer 10: 0.125\n",
    "  Layer 9:  0.092\n",
    "  Layer 8:  0.068\n",
    "  Layer 5:  0.031\n",
    "  Layer 1:  0.008 â† Still learning! âœ“\n",
    "  \n",
    "Loss: 0.623 (learning started!)\n",
    "\n",
    "Epoch 10:\n",
    "  Loss: 0.285\n",
    "  Train accuracy: 78%\n",
    "\n",
    "Epoch 50:\n",
    "  Loss: 0.092\n",
    "  Train accuracy: 94%\n",
    "  Test accuracy: 89%\n",
    "\n",
    "Network learned successfully! âœ“\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Attempt 3: He Initialization with ReLU**\n",
    "\n",
    "```python\n",
    "class DeepCatDogNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        sizes = [12288, 1000, 1000, 1000, 1000, 1000, \n",
    "                 1000, 1000, 1000, 1000, 2]\n",
    "        \n",
    "        for i in range(len(sizes)-1):\n",
    "            layer = nn.Linear(sizes[i], sizes[i+1])\n",
    "            \n",
    "            # He initialization\n",
    "            nn.init.kaiming_normal_(layer.weight, \n",
    "                                   mode='fan_in',\n",
    "                                   nonlinearity='relu')\n",
    "            nn.init.zeros_(layer.bias)\n",
    "            \n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))  # ReLU activation\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "model = DeepCatDogNet()\n",
    "```\n",
    "\n",
    "**Check initialization:**\n",
    "```python\n",
    "for i, layer in enumerate(model.layers[:-1]):\n",
    "    W = layer.weight.data\n",
    "    n_in = W.shape[1]\n",
    "    expected_std = np.sqrt(2 / n_in)\n",
    "    actual_std = W.std().item()\n",
    "    \n",
    "    print(f\"Layer {i+1}: Expected Ïƒ={expected_std:.4f}, \"\n",
    "          f\"Actual Ïƒ={actual_std:.4f}\")\n",
    "\n",
    "# Output:\n",
    "# Layer 1: Expected Ïƒ=0.0127, Actual Ïƒ=0.0128 âœ“\n",
    "# Layer 2: Expected Ïƒ=0.0447, Actual Ïƒ=0.0448 âœ“\n",
    "# Layer 3: Expected Ïƒ=0.0447, Actual Ïƒ=0.0446 âœ“\n",
    "```\n",
    "\n",
    "**Training:**\n",
    "```\n",
    "Epoch 1:\n",
    "  Forward pass activations (after ReLU):\n",
    "  Layer 1: mean=0.892, std=1.123 âœ“\n",
    "  Layer 2: mean=0.878, std=1.108 âœ“\n",
    "  Layer 3: mean=0.885, std=1.115 âœ“\n",
    "  Layer 10: mean=0.891, std=1.119 âœ“\n",
    "  \n",
    "  Variance maintained across layers!\n",
    "  \n",
    "  Backward pass gradients:\n",
    "  Layer 10: 0.234\n",
    "  Layer 9:  0.221\n",
    "  Layer 8:  0.208\n",
    "  Layer 5:  0.176\n",
    "  Layer 1:  0.142 â† Excellent gradient flow! âœ“\n",
    "  \n",
    "Loss: 0.487\n",
    "\n",
    "Epoch 10:\n",
    "  Loss: 0.145\n",
    "  Train accuracy: 92%\n",
    "\n",
    "Epoch 50:\n",
    "  Loss: 0.032\n",
    "  Train accuracy: 98%\n",
    "  Test accuracy: 94%\n",
    "\n",
    "Best performance! âœ“\n",
    "Faster convergence than Xavier+Sigmoid!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Initialization | Activation | Epoch 50 Loss | Train Acc | Test Acc | Gradient @ Layer 1 |\n",
    "|----------------|-----------|---------------|-----------|----------|-------------------|\n",
    "| **Random (std=1.0)** | Sigmoid | 0.693 | 50% | 50% | 0.000002 |\n",
    "| **Default PyTorch** | Sigmoid | 0.693 | 52% | 51% | 0.00001 |\n",
    "| **Xavier** | Sigmoid | 0.092 | 94% | 89% | 0.008 |\n",
    "| **He** | ReLU | **0.032** | **98%** | **94%** | **0.142** |\n",
    "\n",
    "**He + ReLU wins!**\n",
    "\n",
    "---\n",
    "\n",
    "# Part 8: All Initialization Methods\n",
    "\n",
    "## Summary of Initialization Techniques\n",
    "\n",
    "### **1. Zero Initialization (DON'T USE)**\n",
    "\n",
    "```python\n",
    "W = np.zeros((n_in, n_out))\n",
    "```\n",
    "\n",
    "**Problem:**\n",
    "```\n",
    "All weights identical â†’ All neurons compute same thing\n",
    "Symmetry never broken â†’ Network can't learn\n",
    "No differentiation between neurons\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Random Small Numbers (BASIC)**\n",
    "\n",
    "```python\n",
    "W = np.random.randn(n_in, n_out) * 0.01\n",
    "```\n",
    "\n",
    "**Pros:** Simple, prevents saturation\n",
    "**Cons:** Too small for deep networks, activations vanish\n",
    "\n",
    "**Use case:** Shallow networks (1-3 layers) only\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Xavier/Glorot Initialization**\n",
    "\n",
    "```python\n",
    "# Normal distribution\n",
    "std = np.sqrt(2 / (n_in + n_out))\n",
    "W = np.random.randn(n_in, n_out) * std\n",
    "\n",
    "# Uniform distribution\n",
    "limit = np.sqrt(6 / (n_in + n_out))\n",
    "W = np.random.uniform(-limit, limit, (n_in, n_out))\n",
    "```\n",
    "\n",
    "**Best for:** Sigmoid, Tanh activations\n",
    "**Formula:** $\\text{Var}(W) = \\frac{2}{n_{in} + n_{out}}$\n",
    "\n",
    "---\n",
    "\n",
    "### **4. He/Kaiming Initialization**\n",
    "\n",
    "```python\n",
    "# Normal distribution\n",
    "std = np.sqrt(2 / n_in)\n",
    "W = np.random.randn(n_in, n_out) * std\n",
    "\n",
    "# Uniform distribution\n",
    "limit = np.sqrt(6 / n_in)\n",
    "W = np.random.uniform(-limit, limit, (n_in, n_out))\n",
    "```\n",
    "\n",
    "**Best for:** ReLU, Leaky ReLU, ELU activations\n",
    "**Formula:** $\\text{Var}(W) = \\frac{2}{n_{in}}$\n",
    "\n",
    "---\n",
    "\n",
    "### **5. LeCun Initialization**\n",
    "\n",
    "```python\n",
    "std = np.sqrt(1 / n_in)\n",
    "W = np.random.randn(n_in, n_out) * std\n",
    "```\n",
    "\n",
    "**Best for:** SELU activation (self-normalizing networks)\n",
    "**Formula:** $\\text{Var}(W) = \\frac{1}{n_{in}}$\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Tree for Initialization\n",
    "\n",
    "```\n",
    "What activation function are you using?\n",
    "\n",
    "â”œâ”€ Sigmoid or Tanh\n",
    "â”‚  â†’ Use Xavier Initialization\n",
    "â”‚    W ~ N(0, âˆš(2/(n_in + n_out)))\n",
    "â”‚\n",
    "â”œâ”€ ReLU, Leaky ReLU, PReLU\n",
    "â”‚  â†’ Use He Initialization\n",
    "â”‚    W ~ N(0, âˆš(2/n_in))\n",
    "â”‚\n",
    "â”œâ”€ SELU\n",
    "â”‚  â†’ Use LeCun Initialization\n",
    "â”‚    W ~ N(0, âˆš(1/n_in))\n",
    "â”‚\n",
    "â””â”€ Linear (no activation)\n",
    "   â†’ Use Xavier Initialization\n",
    "     W ~ N(0, âˆš(2/(n_in + n_out)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part 9: Practical PyTorch Implementation\n",
    "\n",
    "## Manual Initialization\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class ProperlyInitializedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(12288, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 2)\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # He initialization for ReLU\n",
    "                nn.init.kaiming_normal_(\n",
    "                    module.weight,\n",
    "                    mode='fan_in',\n",
    "                    nonlinearity='relu'\n",
    "                )\n",
    "                # Zero biases\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ProperlyInitializedNet()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Using Built-in Initializers\n",
    "\n",
    "```python\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Xavier/Glorot initialization\n",
    "init.xavier_normal_(layer.weight, gain=1.0)\n",
    "init.xavier_uniform_(layer.weight, gain=1.0)\n",
    "\n",
    "# He/Kaiming initialization\n",
    "init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "# LeCun initialization\n",
    "init.normal_(layer.weight, mean=0, std=np.sqrt(1/n_in))\n",
    "\n",
    "# Constant initialization\n",
    "init.constant_(layer.weight, 0.5)\n",
    "init.zeros_(layer.bias)\n",
    "init.ones_(layer.bias)\n",
    "\n",
    "# Orthogonal initialization (for RNNs)\n",
    "init.orthogonal_(layer.weight, gain=1.0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Training Example\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define network\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        # 10 hidden layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(12288, 1000),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.Linear(1000, 2)\n",
    "        ])\n",
    "        \n",
    "        # Initialize based on activation\n",
    "        self._initialize()\n",
    "    \n",
    "    def _initialize(self):\n",
    "        for layer in self.layers[:-1]:  # All but output\n",
    "            if self.activation == 'relu':\n",
    "                nn.init.kaiming_normal_(\n",
    "                    layer.weight,\n",
    "                    mode='fan_in',\n",
    "                    nonlinearity='relu'\n",
    "                )\n",
    "            elif self.activation == 'sigmoid':\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "            \n",
    "            nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        # Output layer\n",
    "        nn.init.xavier_normal_(self.layers[-1].weight)\n",
    "        nn.init.zeros_(self.layers[-1].bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "            if self.activation == 'relu':\n",
    "                x = torch.relu(x)\n",
    "            elif self.activation == 'sigmoid':\n",
    "                x = torch.sigmoid(x)\n",
    "        \n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Create models with different configurations\n",
    "model_bad = DeepNet(activation='sigmoid')\n",
    "# Don't initialize - use PyTorch defaults\n",
    "\n",
    "model_good = DeepNet(activation='relu')\n",
    "# Already initialized properly in __init__\n",
    "\n",
    "# Training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_good.parameters(), lr=0.1)\n",
    "\n",
    "# Monitor gradients during training\n",
    "def check_gradients(model):\n",
    "    \"\"\"Check gradient magnitudes at each layer\"\"\"\n",
    "    gradients = []\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if layer.weight.grad is not None:\n",
    "            grad_norm = layer.weight.grad.norm().item()\n",
    "            gradients.append(grad_norm)\n",
    "            print(f\"  Layer {i+1}: gradient norm = {grad_norm:.6f}\")\n",
    "    return gradients\n",
    "\n",
    "# Training loop\n",
    "model_good.train()\n",
    "for epoch in range(10):\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Forward\n",
    "        outputs = model_good(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Check gradients (first batch only)\n",
    "        if epoch == 0:\n",
    "            print(f\"\\nEpoch {epoch+1}, Batch 1:\")\n",
    "            check_gradients(model_good)\n",
    "        \n",
    "        # Update\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Output:\n",
    "# Epoch 1, Batch 1:\n",
    "#   Layer 1: gradient norm = 0.142356  âœ“\n",
    "#   Layer 2: gradient norm = 0.138421  âœ“\n",
    "#   Layer 3: gradient norm = 0.134892  âœ“\n",
    "#   Layer 4: gradient norm = 0.131234  âœ“\n",
    "#   Layer 5: gradient norm = 0.127845  âœ“\n",
    "#   Layer 6: gradient norm = 0.124567  âœ“\n",
    "#   Layer 7: gradient norm = 0.121432  âœ“\n",
    "#   Layer 8: gradient norm = 0.118234  âœ“\n",
    "#   Layer 9: gradient norm = 0.115123  âœ“\n",
    "#   Layer 10: gradient norm = 0.112456 âœ“\n",
    "# \n",
    "# Gradients flow well through all layers!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part 10: Other Solutions to Gradient Problems\n",
    "\n",
    "## Batch Normalization\n",
    "\n",
    "**Normalizes activations at each layer:**\n",
    "\n",
    "```python\n",
    "class BNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(12288, 1000)\n",
    "        self.bn1 = nn.BatchNorm1d(1000)  # Batch Norm!\n",
    "        \n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.bn2 = nn.BatchNorm1d(1000)\n",
    "        \n",
    "        self.fc3 = nn.Linear(1000, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)  # Normalize before activation\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "**How it helps:**\n",
    "```\n",
    "Before BN:\n",
    "Layer 3 input: mean=15.2, std=8.9 (varied)\n",
    "Layer 5 input: mean=28.7, std=15.3 (growing)\n",
    "\n",
    "After BN:\n",
    "Layer 3 input: mean=0.0, std=1.0 (normalized)\n",
    "Layer 5 input: mean=0.0, std=1.0 (normalized)\n",
    "\n",
    "Activations stay in healthy range!\n",
    "Gradients flow better!\n",
    "Less sensitive to initialization!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Residual Connections (ResNet)\n",
    "\n",
    "**Skip connections allow gradients to bypass layers:**\n",
    "\n",
    "```python\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(size, size)\n",
    "        self.fc2 = nn.Linear(size, size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x  # Save input\n",
    "        \n",
    "        out = torch.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        out = out + residual  # Add skip connection!\n",
    "        out = torch.relu(out)\n",
    "        \n",
    "        return out\n",
    "```\n",
    "\n",
    "**Gradient flow:**\n",
    "```\n",
    "Without skip connection:\n",
    "âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚out Ã— âˆ‚out/âˆ‚fc2 Ã— âˆ‚fc2/âˆ‚fc1 Ã— âˆ‚fc1/âˆ‚x\n",
    "(Many multiplications â†’ vanishing)\n",
    "\n",
    "With skip connection:\n",
    "âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚out Ã— (âˆ‚out/âˆ‚fc2 Ã— ... + 1)\n",
    "                    â†‘             â†‘\n",
    "              Complex path    Direct path!\n",
    "\n",
    "Direct path allows gradient to flow unchanged!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Clipping\n",
    "\n",
    "**Prevent exploding gradients:**\n",
    "\n",
    "```python\n",
    "# After loss.backward(), before optimizer.step()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "# This clips gradient if norm exceeds 1.0\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "If gradient norm > max_norm:\n",
    "    gradient = gradient Ã— (max_norm / gradient_norm)\n",
    "\n",
    "Example:\n",
    "Gradient: [100, -50, 200] (norm = 229)\n",
    "After clipping (max=1.0):\n",
    "Gradient: [0.437, -0.218, 0.873] (norm = 1.0)\n",
    "\n",
    "Direction preserved, magnitude controlled!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## LSTM/GRU for Sequences\n",
    "\n",
    "**Special architectures for RNNs:**\n",
    "\n",
    "```\n",
    "Problem with vanilla RNN:\n",
    "h_t = tanh(W_h h_{t-1} + W_x x_t)\n",
    "\n",
    "Gradient through T timesteps:\n",
    "âˆ‚h_1/âˆ‚h_0 involves multiplying W_h many times\n",
    "â†’ Vanishing/exploding\n",
    "\n",
    "LSTM solution:\n",
    "- Gating mechanism\n",
    "- Additive updates (not multiplicative)\n",
    "- Cell state provides gradient highway\n",
    "\n",
    "c_t = f_t âŠ™ c_{t-1} + i_t âŠ™ g_t\n",
    "      â†‘\n",
    "  Direct connection! Gradient flows easily.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part 11: Summary\n",
    "\n",
    "## The Complete Picture\n",
    "\n",
    "### **Vanishing Gradients:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚       Vanishing Gradients           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "CAUSE:\n",
    "- Sigmoid/tanh with small derivatives\n",
    "- Poor weight initialization\n",
    "- Very deep networks\n",
    "\n",
    "SYMPTOM:\n",
    "- Early layers don't learn\n",
    "- Loss plateaus quickly\n",
    "- Weights barely change\n",
    "\n",
    "SOLUTION:\n",
    "âœ“ Use ReLU activation\n",
    "âœ“ Proper initialization (He/Xavier)\n",
    "âœ“ Batch normalization\n",
    "âœ“ Residual connections\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Exploding Gradients:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚       Exploding Gradients           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "CAUSE:\n",
    "- Large weight initialization\n",
    "- Deep networks without normalization\n",
    "- RNNs with long sequences\n",
    "\n",
    "SYMPTOM:\n",
    "- Loss becomes NaN\n",
    "- Weights oscillate wildly\n",
    "- Training diverges\n",
    "\n",
    "SOLUTION:\n",
    "âœ“ Proper initialization (He/Xavier)\n",
    "âœ“ Gradient clipping\n",
    "âœ“ Batch normalization\n",
    "âœ“ Lower learning rate\n",
    "âœ“ LSTM/GRU for sequences\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Initialization Decision Guide:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Choose Your Initialization         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Activation: ReLU or variants\n",
    "  â†’ He initialization\n",
    "    W ~ N(0, âˆš(2/n_in))\n",
    "\n",
    "Activation: Sigmoid or Tanh\n",
    "  â†’ Xavier initialization\n",
    "    W ~ N(0, âˆš(2/(n_in + n_out)))\n",
    "\n",
    "Activation: SELU\n",
    "  â†’ LeCun initialization\n",
    "    W ~ N(0, âˆš(1/n_in))\n",
    "\n",
    "Very deep network (>20 layers):\n",
    "  â†’ Also add:\n",
    "    â€¢ Batch Normalization\n",
    "    â€¢ Residual connections\n",
    "\n",
    "Recurrent network (RNN/LSTM):\n",
    "  â†’ Orthogonal initialization\n",
    "    â€¢ Or use pre-built LSTM/GRU\n",
    "    â€¢ Add gradient clipping\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **Gradient flow is crucial** for deep network training\n",
    "\n",
    "2. **Proper initialization** keeps variance constant across layers\n",
    "\n",
    "3. **Activation functions matter:**\n",
    "   - Sigmoid/Tanh: Vanishing gradients (max derivative 0.25/1.0)\n",
    "   - ReLU: Better gradient flow (derivative 1.0 when active)\n",
    "\n",
    "4. **Match initialization to activation:**\n",
    "   - Xavier for Sigmoid/Tanh\n",
    "   - He for ReLU\n",
    "\n",
    "5. **Modern solutions work together:**\n",
    "   - Good initialization\n",
    "   - Batch normalization\n",
    "   - Residual connections\n",
    "   - Gradient clipping\n",
    "\n",
    "6. **Always monitor gradients** during training!\n",
    "\n",
    "---\n",
    "\n",
    "**You now understand why deep networks were hard to train, and how modern techniques solved it! ğŸ‰**\n",
    "\n",
    "The combination of:\n",
    "- **He initialization** (proper weight scaling)\n",
    "- **ReLU activations** (gradient flow)\n",
    "- **Batch normalization** (stable activations)  \n",
    "- **Residual connections** (gradient highways)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b128f173",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
