{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc63ead5",
   "metadata": {},
   "source": [
    "# Regularization Techniques: Complete Explanation\n",
    "## L2 (Ridge/Weight Decay), L1 (Lasso), and Dropout\n",
    "### (Detailed Step-by-Step with Cat vs Dog Classification)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— **Connection to Previous Topics**\n",
    "\n",
    "### **What We Know So Far:**\n",
    "\n",
    "**From Neural Networks:**\n",
    "```\n",
    "Training process:\n",
    "1. Forward pass: Make predictions\n",
    "2. Calculate loss: How wrong we are\n",
    "3. Backward pass: Compute gradients\n",
    "4. Update weights: w := w - Î±Â·âˆ‚L/âˆ‚w\n",
    "```\n",
    "\n",
    "**From CNNs:**\n",
    "```\n",
    "Cat vs Dog classifier:\n",
    "Input (64Ã—64Ã—3 image) â†’ Conv layers â†’ Dense layers â†’ Output [P(cat), P(dog)]\n",
    "\n",
    "Network might have:\n",
    "- 50,000 weights in conv layers\n",
    "- 100,000 weights in dense layers\n",
    "- Total: 150,000 parameters!\n",
    "```\n",
    "\n",
    "**The New Problem: OVERFITTING**\n",
    "\n",
    "---\n",
    "\n",
    "# Part 1: Understanding Overfitting\n",
    "\n",
    "## 1. What is Overfitting?\n",
    "\n",
    "### **Simple Analogy**\n",
    "\n",
    "Imagine a student preparing for an exam:\n",
    "\n",
    "**Good Student (Generalization):**\n",
    "```\n",
    "Studies: Understands concepts\n",
    "Exam: Solves new problems correctly\n",
    "âœ“ Can apply knowledge to unseen questions\n",
    "```\n",
    "\n",
    "**Overfit Student (Memorization):**\n",
    "```\n",
    "Studies: Memorizes every practice problem exactly\n",
    "Exam: Fails on slightly different questions\n",
    "âœ— Only knows exact problems, can't generalize\n",
    "```\n",
    "\n",
    "**Neural networks can make the same mistake!**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Overfitting in Cat vs Dog Classification\n",
    "\n",
    "### **Our Dataset:**\n",
    "\n",
    "```\n",
    "Training Set: 100 images\n",
    "- 50 cats\n",
    "- 50 dogs\n",
    "\n",
    "Test Set: 20 images (never seen before)\n",
    "- 10 cats  \n",
    "- 10 dogs\n",
    "```\n",
    "\n",
    "### **Scenario 1: Healthy Model (Good Generalization)**\n",
    "\n",
    "```\n",
    "Training Accuracy: 95%\n",
    "Test Accuracy: 93%\n",
    "\n",
    "The model learned general features:\n",
    "- \"Pointy ears\" â†’ Cat\n",
    "- \"Floppy ears\" â†’ Dog\n",
    "- \"Whiskers\" â†’ Cat\n",
    "- \"Long snout\" â†’ Dog\n",
    "\n",
    "âœ“ Performs well on new images!\n",
    "```\n",
    "\n",
    "### **Scenario 2: Overfit Model (Memorization)**\n",
    "\n",
    "```\n",
    "Training Accuracy: 100%\n",
    "Test Accuracy: 65%\n",
    "\n",
    "The model memorized specific images:\n",
    "- \"This exact pixel pattern at position (23,45)\" â†’ Cat\n",
    "- \"This specific noise pattern\" â†’ Dog\n",
    "- \"Training image #37's exact colors\" â†’ Cat\n",
    "\n",
    "âœ— Fails on new images! Too specific!\n",
    "```\n",
    "\n",
    "**Visualize the problem:**\n",
    "\n",
    "```\n",
    "         UNDERFITTING          GOOD FIT           OVERFITTING\n",
    "         \n",
    "Train:   â—  â—  â—‹  â—‹          â—  â—  â—‹  â—‹          â—  â—  â—‹  â—‹\n",
    "         â—  â—  â—‹  â—‹          â—  â—  â—‹  â—‹          â—  â—  â—‹  â—‹\n",
    "         \n",
    "Learned: â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â•±â•²â•±â•²â•±â•²â•±â•²â•±â•²\n",
    "         (too simple)       (just right)        (too complex)\n",
    "         \n",
    "Test:    â—  ?  â—‹  ?         â—  â—  â—‹  â—‹          â—  ?  â—‹  ?\n",
    "         Poor on both       Good on both!       Good train,\n",
    "         train & test       85-95%              bad test!\n",
    "         <70%                                   Train: 100%\n",
    "                                                Test: 60%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Real Numbers: Watching Overfitting Happen\n",
    "\n",
    "Let's train a Cat vs Dog classifier and watch it overfit!\n",
    "\n",
    "### **Network Architecture:**\n",
    "\n",
    "```\n",
    "Input: 64Ã—64Ã—3 = 12,288 pixels\n",
    "â†“\n",
    "Hidden Layer 1: 1000 neurons (12,288,000 weights!)\n",
    "â†“\n",
    "Hidden Layer 2: 500 neurons (500,000 weights)\n",
    "â†“\n",
    "Output Layer: 2 neurons (1,000 weights)\n",
    "\n",
    "Total: ~12.8 MILLION parameters\n",
    "Training samples: Only 100 images!\n",
    "\n",
    "Ratio: 128,000 parameters per training sample!\n",
    "This is a recipe for overfitting! ğŸš¨\n",
    "```\n",
    "\n",
    "### **Training Progress (Epoch by Epoch):**\n",
    "\n",
    "| Epoch | Training Loss | Training Acc | Test Loss | Test Acc | What's Happening |\n",
    "|-------|--------------|--------------|-----------|----------|------------------|\n",
    "| 1 | 0.693 | 50% | 0.695 | 48% | Random guessing |\n",
    "| 5 | 0.420 | 78% | 0.435 | 76% | Learning general features |\n",
    "| 10 | 0.210 | 92% | 0.235 | 89% | Good generalization! |\n",
    "| 20 | 0.085 | 98% | 0.315 | 85% | Starting to overfit... |\n",
    "| 30 | 0.025 | 100% | 0.520 | 78% | Overfitting badly! |\n",
    "| 50 | 0.005 | 100% | 0.890 | 65% | Memorized training set |\n",
    "| 100 | 0.001 | 100% | 1.450 | 62% | Complete overfitting |\n",
    "\n",
    "**The Warning Signs:**\n",
    "\n",
    "```\n",
    "    Loss\n",
    "     â†‘\n",
    "  1.5â”‚              â•±â”€ Test loss rising\n",
    "  1.0â”‚            â•±  (BAD SIGN!)\n",
    "  0.5â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±\n",
    "  0.0â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Training loss falling\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs\n",
    "         â†‘ \n",
    "    Best point! (Epoch 10)\n",
    "    Should stop here!\n",
    "```\n",
    "\n",
    "**What the weights look like:**\n",
    "\n",
    "```\n",
    "Epoch 10 (Good):\n",
    "Weights: Small, smooth\n",
    "w = [0.23, -0.15, 0.08, -0.31, 0.19, ...]\n",
    "Most weights: -1 to +1\n",
    "\n",
    "Epoch 100 (Overfit):\n",
    "Weights: Large, chaotic\n",
    "w = [15.3, -23.7, 8.9, -45.2, 31.8, ...]\n",
    "Many weights: -50 to +50!\n",
    "\n",
    "The network became too sensitive!\n",
    "Tiny changes in input â†’ huge changes in output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Why Does Overfitting Happen?\n",
    "\n",
    "### **Reason 1: Too Many Parameters**\n",
    "\n",
    "```\n",
    "100 training images\n",
    "12,800,000 parameters\n",
    "\n",
    "It's like trying to fit 100 data points with a \n",
    "12-million-degree polynomial! You can fit perfectly\n",
    "but it's meaningless.\n",
    "\n",
    "Example with simple data:\n",
    "\n",
    "3 points:  (1,2), (2,4), (3,5)\n",
    "\n",
    "Fit with line (2 parameters): y = 1.5x + 0.5\n",
    "  â†’ Smooth, generalizes well âœ“\n",
    "  \n",
    "Fit with degree-10 polynomial (11 parameters):\n",
    "  â†’ Wiggly, passes through exactly but crazy between points âœ—\n",
    "```\n",
    "\n",
    "### **Reason 2: Not Enough Data**\n",
    "\n",
    "```\n",
    "Network capacity: 12M parameters\n",
    "Training samples: 100\n",
    "\n",
    "The network has too much \"freedom\"\n",
    "Many different weight configurations\n",
    "can all perfectly fit 100 images!\n",
    "\n",
    "Like having 100 equations with 12 million unknowns\n",
    "â†’ Infinite solutions!\n",
    "```\n",
    "\n",
    "### **Reason 3: Training Too Long**\n",
    "\n",
    "```\n",
    "Early training: Learning general patterns\n",
    "Later training: Memorizing noise and specifics\n",
    "\n",
    "It's like studying:\n",
    "- First hour: Understand concepts (good!)\n",
    "- Hour 10: Start memorizing exact wording\n",
    "- Hour 100: Memorized every comma, can't adapt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part 2: L2 Regularization (Ridge / Weight Decay)\n",
    "\n",
    "## 1. Plain English Explanation\n",
    "\n",
    "### **The Core Idea**\n",
    "\n",
    "**L2 regularization says:** \"Keep weights small!\"\n",
    "\n",
    "**Why?** Small weights = simpler model = better generalization\n",
    "\n",
    "```\n",
    "Without L2:                    With L2:\n",
    "Weights can grow huge          Penalty for large weights\n",
    "w = [50, -80, 120, -200]      w = [0.5, -0.8, 1.2, -2.0]\n",
    "     â†‘                              â†‘\n",
    "Over-sensitive!                Reasonable!\n",
    "\n",
    "Small input change â†’ HUGE output  Small input change â†’ Small output\n",
    "Overfits to training noise        Generalizes to new data\n",
    "```\n",
    "\n",
    "### **The Intuition: Financial Penalty**\n",
    "\n",
    "Think of it like a tax on large weights:\n",
    "\n",
    "```\n",
    "Original loss: \"How wrong are predictions?\"\n",
    "L2 loss: \"How wrong are predictions? + Penalty for large weights\"\n",
    "\n",
    "Network must balance:\n",
    "- Fitting training data (low prediction error)\n",
    "- Keeping weights small (low weight penalty)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Mathematics\n",
    "\n",
    "### **Original Loss Function:**\n",
    "\n",
    "$$L = \\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Just measures prediction errors.\n",
    "\n",
    "### **L2 Regularized Loss:**\n",
    "\n",
    "$$L_{L2} = \\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n}w_j^2$$\n",
    "\n",
    "**Components:**\n",
    "\n",
    "| Part | Name | Meaning |\n",
    "|------|------|---------|\n",
    "| $\\frac{1}{m}\\sum(y_i - \\hat{y}_i)^2$ | Prediction loss | How wrong predictions are |\n",
    "| $\\frac{\\lambda}{2}\\sum w_j^2$ | L2 penalty | Sum of squared weights |\n",
    "| $\\lambda$ | Regularization strength | How much to penalize (0.001 to 0.1 typical) |\n",
    "| $w_j$ | Weight j | A specific parameter in the network |\n",
    "| $n$ | Number of weights | Total parameters |\n",
    "\n",
    "**Key insight:** Squaring weights means:\n",
    "- Large weights get HUGE penalty (10Â² = 100)\n",
    "- Small weights get tiny penalty (0.1Â² = 0.01)\n",
    "- Network prefers many small weights over few large ones\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Step-by-Step Numerical Example\n",
    "\n",
    "### **Scenario: Single Neuron**\n",
    "\n",
    "```\n",
    "Input: x = [2.0, 3.0]\n",
    "Weights: w = [wâ‚, wâ‚‚]\n",
    "Bias: b = 0.5\n",
    "True label: y = 1.0\n",
    "\n",
    "Forward pass:\n",
    "z = wâ‚Ã—2.0 + wâ‚‚Ã—3.0 + 0.5\n",
    "Å· = sigmoid(z)\n",
    "```\n",
    "\n",
    "### **Training Step 1: No Regularization (Î» = 0)**\n",
    "\n",
    "**Current weights:**\n",
    "```\n",
    "wâ‚ = 5.0\n",
    "wâ‚‚ = 8.0\n",
    "b = 0.5\n",
    "```\n",
    "\n",
    "**Forward pass:**\n",
    "```\n",
    "z = 5.0Ã—2.0 + 8.0Ã—3.0 + 0.5\n",
    "  = 10.0 + 24.0 + 0.5\n",
    "  = 34.5\n",
    "\n",
    "Å· = sigmoid(34.5) = 1/(1 + e^(-34.5)) â‰ˆ 0.99999999\n",
    "\n",
    "Prediction loss:\n",
    "L_pred = (1.0 - 0.99999999)Â² = 0.00000001\n",
    "\n",
    "Total loss (no regularization):\n",
    "L = 0.00000001 âœ“ (seems perfect!)\n",
    "```\n",
    "\n",
    "**Gradient (no regularization):**\n",
    "```\n",
    "âˆ‚L/âˆ‚wâ‚ = (Å· - y) Ã— xâ‚\n",
    "       = (0.99999999 - 1.0) Ã— 2.0\n",
    "       = -0.00000002\n",
    "\n",
    "âˆ‚L/âˆ‚wâ‚‚ = (Å· - y) Ã— xâ‚‚\n",
    "       = (0.99999999 - 1.0) Ã— 3.0\n",
    "       = -0.00000003\n",
    "```\n",
    "\n",
    "**Update (Î± = 0.1):**\n",
    "```\n",
    "wâ‚ := 5.0 - 0.1Ã—(-0.00000002) = 5.00000000002\n",
    "wâ‚‚ := 8.0 - 0.1Ã—(-0.00000003) = 8.00000000003\n",
    "\n",
    "Weights barely change! They're stuck at large values!\n",
    "Network is overconfident and overfit.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Step 2: With L2 Regularization (Î» = 0.01)**\n",
    "\n",
    "**Same starting weights:**\n",
    "```\n",
    "wâ‚ = 5.0\n",
    "wâ‚‚ = 8.0\n",
    "```\n",
    "\n",
    "**Forward pass (same as before):**\n",
    "```\n",
    "Å· = 0.99999999\n",
    "\n",
    "Prediction loss:\n",
    "L_pred = (1.0 - 0.99999999)Â² = 0.00000001\n",
    "```\n",
    "\n",
    "**L2 penalty:**\n",
    "```\n",
    "L2_penalty = (Î»/2) Ã— (wâ‚Â² + wâ‚‚Â²)\n",
    "           = (0.01/2) Ã— (5.0Â² + 8.0Â²)\n",
    "           = 0.005 Ã— (25 + 64)\n",
    "           = 0.005 Ã— 89\n",
    "           = 0.445\n",
    "\n",
    "Total loss:\n",
    "L = L_pred + L2_penalty\n",
    "  = 0.00000001 + 0.445\n",
    "  = 0.445\n",
    "\n",
    "Loss dominated by regularization!\n",
    "Network says: \"Your weights are too large!\"\n",
    "```\n",
    "\n",
    "**Gradient with L2:**\n",
    "```\n",
    "âˆ‚L/âˆ‚wâ‚ = âˆ‚L_pred/âˆ‚wâ‚ + âˆ‚L2_penalty/âˆ‚wâ‚\n",
    "       = -0.00000002 + Î»Ã—wâ‚\n",
    "       = -0.00000002 + 0.01Ã—5.0\n",
    "       = -0.00000002 + 0.05\n",
    "       = 0.05 (positive! wants to decrease wâ‚)\n",
    "\n",
    "âˆ‚L/âˆ‚wâ‚‚ = âˆ‚L_pred/âˆ‚wâ‚‚ + âˆ‚L2_penalty/âˆ‚wâ‚‚\n",
    "       = -0.00000003 + Î»Ã—wâ‚‚\n",
    "       = -0.00000003 + 0.01Ã—8.0\n",
    "       = -0.00000003 + 0.08\n",
    "       = 0.08 (positive! wants to decrease wâ‚‚)\n",
    "```\n",
    "\n",
    "**Update (Î± = 0.1):**\n",
    "```\n",
    "wâ‚ := 5.0 - 0.1Ã—0.05 = 5.0 - 0.005 = 4.995\n",
    "wâ‚‚ := 8.0 - 0.1Ã—0.08 = 8.0 - 0.008 = 7.992\n",
    "\n",
    "Weights are shrinking!\n",
    "This is called \"weight decay\"\n",
    "```\n",
    "\n",
    "**After 100 iterations:**\n",
    "```\n",
    "Without L2: w = [5.00, 8.00] (stuck, overfit)\n",
    "With L2:    w = [1.23, 1.98] (smaller, generalized)\n",
    "\n",
    "The L2 penalty drove weights down!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Complete Example: Cat vs Dog Network\n",
    "\n",
    "### **Network Architecture:**\n",
    "\n",
    "```\n",
    "Input: 12,288 features (64Ã—64Ã—3 image)\n",
    "Hidden: 100 neurons\n",
    "Output: 2 neurons (cat, dog)\n",
    "\n",
    "Total weights: 12,288Ã—100 + 100Ã—2 = 1,229,000 weights!\n",
    "```\n",
    "\n",
    "### **Training: 3 Different Î» Values**\n",
    "\n",
    "**Dataset:**\n",
    "- 100 training images (50 cats, 50 dogs)\n",
    "- 20 test images (10 cats, 10 dogs)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Case 1: No Regularization (Î» = 0)**\n",
    "\n",
    "**Epoch 1:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [0.02, -0.01, 0.03, -0.02, 0.01]\n",
    "\n",
    "Training loss: 0.693\n",
    "Test loss: 0.701\n",
    "\n",
    "Weight sum: Î£wÂ² = 0.0007 (small)\n",
    "```\n",
    "\n",
    "**Epoch 50:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [45.2, -67.3, 89.1, -123.5, 156.8]\n",
    "\n",
    "Training loss: 0.001 (perfect fit!)\n",
    "Test loss: 1.850 (terrible!)\n",
    "\n",
    "Weight sum: Î£wÂ² = 45,892 (HUGE!)\n",
    "\n",
    "Network memorized training set!\n",
    "```\n",
    "\n",
    "**Weight evolution:**\n",
    "```\n",
    "    |Weight|\n",
    "      â†‘\n",
    "  150â”‚                     â•±\n",
    "  100â”‚                  â•±\n",
    "   50â”‚              â•±\n",
    "    0â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs\n",
    "      \n",
    "Weights exploded!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Case 2: Moderate Regularization (Î» = 0.01)**\n",
    "\n",
    "**Epoch 1:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [0.02, -0.01, 0.03, -0.02, 0.01]\n",
    "\n",
    "L_pred = 0.693\n",
    "L2_penalty = 0.01/2 Ã— 0.0007 = 0.0000035\n",
    "L_total = 0.693\n",
    "\n",
    "Negligible penalty at start (weights small)\n",
    "```\n",
    "\n",
    "**Epoch 50:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [2.3, -1.8, 3.1, -2.7, 1.9]\n",
    "\n",
    "Training loss: 0.085 (good fit)\n",
    "Test loss: 0.095 (generalizes well!)\n",
    "\n",
    "Weight sum: Î£wÂ² = 85.2\n",
    "L2_penalty = 0.01/2 Ã— 85.2 = 0.426\n",
    "\n",
    "Network balanced fitting vs weight size!\n",
    "```\n",
    "\n",
    "**Weight evolution:**\n",
    "```\n",
    "    |Weight|\n",
    "      â†‘\n",
    "    5â”‚      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    0â”‚â”€â”€â”€â”€â•±\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs\n",
    "      \n",
    "Weights grew but stabilized!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Case 3: Too Strong Regularization (Î» = 1.0)**\n",
    "\n",
    "**Epoch 50:**\n",
    "```\n",
    "Weights (sample of 5):\n",
    "w = [0.05, -0.03, 0.08, -0.06, 0.04]\n",
    "\n",
    "Training loss: 0.520 (underfit!)\n",
    "Test loss: 0.535 (consistent but poor)\n",
    "\n",
    "Weight sum: Î£wÂ² = 0.14\n",
    "L2_penalty = 1.0/2 Ã— 0.14 = 0.07\n",
    "\n",
    "L2 penalty dominated!\n",
    "Weights stayed too small.\n",
    "Network couldn't fit even training data!\n",
    "```\n",
    "\n",
    "**Weight evolution:**\n",
    "```\n",
    "    |Weight|\n",
    "      â†‘\n",
    " 0.10â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    " 0.05â”‚â•±\n",
    " 0.00â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs\n",
    "      \n",
    "Weights barely grew!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Table:**\n",
    "\n",
    "| Î» | Training Acc | Test Acc | Weight Magnitude | Status |\n",
    "|---|-------------|----------|------------------|--------|\n",
    "| **0** | 100% | 65% | Very large (45-150) | Overfit |\n",
    "| **0.001** | 98% | 88% | Large (5-15) | Slight overfit |\n",
    "| **0.01** | 94% | 92% | Medium (1-5) | **Good!** âœ“ |\n",
    "| **0.1** | 85% | 84% | Small (0.3-2) | Slight underfit |\n",
    "| **1.0** | 68% | 67% | Very small (0.01-0.1) | Underfit |\n",
    "\n",
    "**The sweet spot: Î» = 0.01**\n",
    "\n",
    "---\n",
    "\n",
    "## 5. How Gradient Descent Changes with L2\n",
    "\n",
    "### **Without L2:**\n",
    "\n",
    "```\n",
    "âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w\n",
    "\n",
    "Update:\n",
    "w := w - Î± Ã— âˆ‚L_pred/âˆ‚w\n",
    "\n",
    "Only cares about prediction error!\n",
    "```\n",
    "\n",
    "### **With L2:**\n",
    "\n",
    "```\n",
    "âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w + Î»w\n",
    "\n",
    "Update:\n",
    "w := w - Î± Ã— (âˆ‚L_pred/âˆ‚w + Î»w)\n",
    "  = w - Î±Ã—âˆ‚L_pred/âˆ‚w - Î±Ã—Î»w\n",
    "  = w(1 - Î±Î») - Î±Ã—âˆ‚L_pred/âˆ‚w\n",
    "      â†‘           â†‘\n",
    "   \"decay\"    usual gradient\n",
    "\n",
    "Weight decay factor: (1 - Î±Î»)\n",
    "```\n",
    "\n",
    "**Numerical example:**\n",
    "```\n",
    "w = 10.0\n",
    "Î± = 0.1\n",
    "Î» = 0.01\n",
    "âˆ‚L_pred/âˆ‚w = 2.0\n",
    "\n",
    "Without L2:\n",
    "w := 10.0 - 0.1Ã—2.0 = 10.0 - 0.2 = 9.8\n",
    "\n",
    "With L2:\n",
    "w := 10.0 - 0.1Ã—2.0 - 0.1Ã—0.01Ã—10.0\n",
    "  = 10.0 - 0.2 - 0.01\n",
    "  = 9.79\n",
    "\n",
    "Extra 0.01 from weight decay!\n",
    "```\n",
    "\n",
    "**Over many iterations:**\n",
    "```\n",
    "Iteration 0:   w = 10.00\n",
    "Iteration 1:   w = 9.79\n",
    "Iteration 10:  w = 8.52\n",
    "Iteration 100: w = 3.21\n",
    "Iteration 500: w = 1.15\n",
    "\n",
    "Weight gradually shrinks!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Visualizing L2's Effect\n",
    "\n",
    "### **Loss Landscape Without L2:**\n",
    "\n",
    "```\n",
    "         wâ‚‚\n",
    "          â†‘\n",
    "        5 â”‚     â•±â•²â•±â•²â•±â•²\n",
    "          â”‚   â•±        â•²\n",
    "        0 â”‚â”€â•±   (min)   â•²â”€â†’ wâ‚\n",
    "          â”‚              \n",
    "       -5 â”‚\n",
    "       \n",
    "Network can reach minimum\n",
    "with huge weights (overfit)\n",
    "Many equivalent solutions\n",
    "```\n",
    "\n",
    "### **Loss Landscape With L2:**\n",
    "\n",
    "```\n",
    "         wâ‚‚\n",
    "          â†‘\n",
    "        5 â”‚\n",
    "          â”‚     â•±â”€â•²\n",
    "        0 â”‚â”€â”€â”€â—â”€â”€â”€â”€â”€â†’ wâ‚\n",
    "          â”‚   min\n",
    "       -5 â”‚\n",
    "       \n",
    "L2 adds a \"bowl\" penalty\n",
    "centered at origin\n",
    "Network prefers solutions\n",
    "near (0,0) - small weights!\n",
    "```\n",
    "\n",
    "**Combined landscape:**\n",
    "\n",
    "```\n",
    "         Loss\n",
    "          â†‘\n",
    "          â”‚      â”Œâ”€ L2 penalty (bowl shape)\n",
    "          â”‚     â•±â”‚â•²\n",
    "          â”‚   â•±  â”‚  â•²   â† Combined loss\n",
    "          â”‚ â•±   â—â”‚    â•²  (shifted minimum)\n",
    "          â”‚â•±_____â”‚_____â•²\n",
    "          â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â†’ Weight magnitude\n",
    "                 â†‘\n",
    "            New minimum\n",
    "         (smaller weights)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Why L2 Prevents Overfitting\n",
    "\n",
    "### **Reason 1: Limits Model Complexity**\n",
    "\n",
    "```\n",
    "Without L2:\n",
    "Any weight configuration allowed\n",
    "â†’ Can memorize training data\n",
    "\n",
    "With L2:\n",
    "Large weights heavily penalized\n",
    "â†’ Forces simpler model\n",
    "â†’ Simpler model = better generalization\n",
    "```\n",
    "\n",
    "### **Reason 2: Reduces Sensitivity**\n",
    "\n",
    "```\n",
    "Output = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™\n",
    "\n",
    "Large weights:\n",
    "If wâ‚ = 100, small noise in xâ‚ (Â±0.01)\n",
    "causes huge output change (Â±1.0)\n",
    "â†’ Overly sensitive to noise!\n",
    "\n",
    "Small weights:\n",
    "If wâ‚ = 1.0, noise in xâ‚ (Â±0.01)\n",
    "causes small output change (Â±0.01)\n",
    "â†’ Robust to noise!\n",
    "```\n",
    "\n",
    "### **Reason 3: Spreads Information**\n",
    "\n",
    "```\n",
    "Without L2:\n",
    "Model might use: w = [100, 0, 0, 0, 0]\n",
    "Relies on single feature (brittle!)\n",
    "\n",
    "With L2:\n",
    "Model forced to use: w = [0.5, 0.4, 0.3, 0.6, 0.5]\n",
    "Uses many features (robust!)\n",
    "\n",
    "If one feature fails â†’ Others compensate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Practical Implementation\n",
    "\n",
    "### **PyTorch-style Code:**\n",
    "\n",
    "```python\n",
    "# Without L2\n",
    "loss = criterion(outputs, targets)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# With L2 (Method 1: Manual)\n",
    "lambda_l2 = 0.01\n",
    "l2_penalty = 0\n",
    "for param in model.parameters():\n",
    "    l2_penalty += torch.sum(param ** 2)\n",
    "\n",
    "loss = criterion(outputs, targets) + (lambda_l2 / 2) * l2_penalty\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# With L2 (Method 2: Built-in weight_decay)\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    weight_decay=0.01  # This is Î»!\n",
    ")\n",
    "\n",
    "loss = criterion(outputs, targets)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()  # Automatically applies weight decay\n",
    "```\n",
    "\n",
    "### **Step-by-step: One training batch**\n",
    "\n",
    "```python\n",
    "# Forward pass\n",
    "batch_x = [cat_image_1, dog_image_1, cat_image_2]  # 3 images\n",
    "batch_y = [1, 0, 1]  # 1=cat, 0=dog\n",
    "\n",
    "outputs = model(batch_x)\n",
    "# outputs = [[0.8, 0.2],   # 80% cat - correct!\n",
    "#            [0.3, 0.7],   # 70% dog - correct!\n",
    "#            [0.6, 0.4]]   # 60% cat - correct but uncertain\n",
    "\n",
    "# Prediction loss\n",
    "pred_loss = CrossEntropyLoss(outputs, batch_y)\n",
    "# pred_loss = 0.223 + 0.357 + 0.511 = 1.091 / 3 = 0.364\n",
    "\n",
    "# L2 penalty (Î» = 0.01)\n",
    "all_weights = model.parameters()  # 1.23 million weights\n",
    "weight_sum_sq = sum(w**2 for w in all_weights)  # = 45,200\n",
    "\n",
    "l2_penalty = (0.01 / 2) * 45,200 = 226\n",
    "\n",
    "# Total loss\n",
    "total_loss = 0.364 + 226 = 226.364\n",
    "\n",
    "# Backward and update\n",
    "total_loss.backward()  # Computes âˆ‚L/âˆ‚w for ALL weights\n",
    "optimizer.step()       # Updates: w := w - Î±Ã—(âˆ‚L_pred/âˆ‚w + Î»w)\n",
    "\n",
    "# After update\n",
    "weight_sum_sq = 44,870  # Decreased! Weight decay at work!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Choosing Î» (Hyperparameter Tuning)\n",
    "\n",
    "### **Rule of Thumb:**\n",
    "\n",
    "| Î» value | Effect | When to use |\n",
    "|---------|--------|-------------|\n",
    "| **0** | No regularization | Lots of data, simple model |\n",
    "| **0.0001-0.001** | Very weak | Slight overfitting |\n",
    "| **0.01** | Moderate | **Default starting point** |\n",
    "| **0.1** | Strong | Heavy overfitting |\n",
    "| **1.0+** | Very strong | Extreme overfitting |\n",
    "\n",
    "### **Grid Search Example:**\n",
    "\n",
    "```python\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1.0]\n",
    "best_lambda = None\n",
    "best_val_acc = 0\n",
    "\n",
    "for lam in lambdas:\n",
    "    model = train_model(lambda_l2=lam)\n",
    "    val_acc = evaluate(model, validation_set)\n",
    "    \n",
    "    print(f\"Î»={lam}: Val Acc = {val_acc:.2%}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_lambda = lam\n",
    "\n",
    "# Output:\n",
    "# Î»=0:     Val Acc = 68%  (overfit)\n",
    "# Î»=0.001: Val Acc = 89%\n",
    "# Î»=0.01:  Val Acc = 93%  â† Best!\n",
    "# Î»=0.1:   Val Acc = 86%\n",
    "# Î»=1.0:   Val Acc = 72%  (underfit)\n",
    "\n",
    "print(f\"Best Î»: {best_lambda}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Summary: L2 Regularization\n",
    "\n",
    "### **What L2 Does:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ L2 Regularization (Ridge / Weight Decay) â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ADDS: Penalty term Î»/2 Ã— Î£wÂ²\n",
    "\n",
    "EFFECT: Shrinks weights toward zero\n",
    "\n",
    "GRADIENT: âˆ‚L/âˆ‚w = âˆ‚L_pred/âˆ‚w + Î»w\n",
    "\n",
    "UPDATE: w := w(1-Î±Î») - Î±Ã—âˆ‚L_pred/âˆ‚w\n",
    "             â†‘\n",
    "        Weight decay!\n",
    "\n",
    "RESULT: Smaller, more stable weights\n",
    "        â†’ Better generalization\n",
    "```\n",
    "\n",
    "### **Pros and Cons:**\n",
    "\n",
    "**Pros:**\n",
    "- âœ“ Simple to implement\n",
    "- âœ“ Computationally cheap\n",
    "- âœ“ Works well in practice\n",
    "- âœ“ Smooth, differentiable\n",
    "- âœ“ All weights shrink (distributes info)\n",
    "\n",
    "**Cons:**\n",
    "- âœ— Doesn't set weights to exactly zero\n",
    "- âœ— Adds hyperparameter (Î») to tune\n",
    "- âœ— Can underfit if Î» too large\n",
    "\n",
    "---\n",
    "\n",
    "**Next up: L1 Regularization (Lasso) - Similar idea but with a twist!**\n",
    "\n",
    "Should I continue with L1 (Lasso) regularization now, showing how it differs from L2 and why it creates sparse weights (many exactly zero)? Then we'll finish with Dropout!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629ccc2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
