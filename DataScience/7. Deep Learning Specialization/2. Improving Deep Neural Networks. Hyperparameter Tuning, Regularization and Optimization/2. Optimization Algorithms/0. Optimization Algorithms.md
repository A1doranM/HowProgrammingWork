# Mini-Batch Gradient Descent: Complete Explanation
## Understanding Batch Sizes and Their Impact on Training
### (Detailed Step-by-Step with Cat vs Dog Classification)

---

## ğŸ”— **Connection to Previous Topics**

### **What We Know So Far:**

**From Gradient Descent:**
```
Training process:
1. Forward pass: Compute predictions
2. Compute loss: Measure error
3. Compute gradients: âˆ‚L/âˆ‚w
4. Update weights: w := w - Î±Â·âˆ‚L/âˆ‚w
```

**The Question We Haven't Answered:**

```
We have 1000 training images.

Do we:
A) Compute gradients on ALL 1000 images, then update once?
B) Compute gradients on each single image, update 1000 times?
C) Compute gradients on small groups (e.g., 32 images), update many times?

Which is best? Why?
```

---

# Part 1: The Three Flavors of Gradient Descent

## 1. Plain English Explanation

### **The Student Learning Analogy**

Imagine a student preparing for an exam with 1000 practice problems:

---

### **Approach 1: Batch Gradient Descent (Full Batch)**

```
Student process:
1. Solve ALL 1000 problems
2. Check ALL answers
3. Calculate overall performance
4. Study based on total mistakes
5. Repeat

Pros: Gets complete picture of knowledge
Cons: Takes forever to get feedback!
```

**In neural networks:**
```
1. Process ALL 1000 images
2. Compute loss on ALL images
3. Average gradients from ALL images
4. Make ONE weight update
5. Repeat

One epoch = One weight update!
```

---

### **Approach 2: Stochastic Gradient Descent (SGD, Batch Size = 1)**

```
Student process:
1. Solve ONE problem
2. Check ONE answer
3. Study immediately based on that mistake
4. Move to next problem
5. Repeat 1000 times

Pros: Fast feedback, very frequent learning
Cons: Each problem might not be representative, noisy updates
```

**In neural networks:**
```
1. Process ONE image
2. Compute loss on ONE image
3. Compute gradient from ONE image
4. Make ONE weight update
5. Repeat 1000 times

One epoch = 1000 weight updates!
```

---

### **Approach 3: Mini-Batch Gradient Descent (The Sweet Spot)**

```
Student process:
1. Solve 32 problems
2. Check 32 answers
3. Study based on patterns in these 32
4. Move to next 32 problems
5. Repeat 31 times (1000/32 â‰ˆ 31)

Pros: Good balance - gets patterns, frequent feedback
Cons: Need to choose batch size
```

**In neural networks:**
```
1. Process 32 images (one mini-batch)
2. Compute loss on these 32 images
3. Average gradients from these 32 images
4. Make ONE weight update
5. Repeat 31 times

One epoch = 31 weight updates!
```

---

# Part 2: Detailed Numerical Example

## Setup: Cat vs Dog Classifier

```
Dataset: 128 training images (64 cats, 64 dogs)
Network: Simple 2-layer network
Feature dimension: 1000 (after CNN features)
Hidden layer: 100 neurons
Output: 2 neurons (cat, dog)

Total parameters: 1000Ã—100 + 100Ã—2 = 100,200 weights
```

---

## Approach 1: Batch Gradient Descent (Batch Size = 128)

### **Iteration 1: Process Entire Dataset**

**Forward Pass - ALL 128 images:**

```
Image 1 (cat):
zâ‚ = WÃ—xâ‚ + b
Å·â‚ = softmax(zâ‚) = [0.52, 0.48]  (52% cat)
yâ‚ = [1, 0]  (true: cat)
Lossâ‚ = -log(0.52) = 0.654

Image 2 (dog):
Å·â‚‚ = [0.45, 0.55]  (55% dog)
yâ‚‚ = [0, 1]  (true: dog)
Lossâ‚‚ = -log(0.55) = 0.598

Image 3 (cat):
Å·â‚ƒ = [0.61, 0.39]  (61% cat)
yâ‚ƒ = [1, 0]  (true: cat)
Lossâ‚ƒ = -log(0.61) = 0.494

...

Image 128 (dog):
Å·â‚â‚‚â‚ˆ = [0.38, 0.62]  (62% dog)
yâ‚â‚‚â‚ˆ = [0, 1]  (true: dog)
Lossâ‚â‚‚â‚ˆ = -log(0.62) = 0.478
```

**Average Loss:**
```
L_batch = (Lossâ‚ + Lossâ‚‚ + ... + Lossâ‚â‚‚â‚ˆ) / 128
        = (0.654 + 0.598 + 0.494 + ... + 0.478) / 128
        = 72.45 / 128
        = 0.566
```

---

**Backward Pass - Compute Gradients:**

```
For each weight w:

âˆ‚L/âˆ‚w = (âˆ‚Lossâ‚/âˆ‚w + âˆ‚Lossâ‚‚/âˆ‚w + ... + âˆ‚Lossâ‚â‚‚â‚ˆ/âˆ‚w) / 128

Example for weight W[50, 30] (connecting feature 30 to hidden neuron 50):

Image 1: âˆ‚Lossâ‚/âˆ‚W[50,30] = 0.023
Image 2: âˆ‚Lossâ‚‚/âˆ‚W[50,30] = -0.018
Image 3: âˆ‚Lossâ‚ƒ/âˆ‚W[50,30] = 0.031
...
Image 128: âˆ‚Lossâ‚â‚‚â‚ˆ/âˆ‚W[50,30] = -0.015

Average gradient:
âˆ‚L/âˆ‚W[50,30] = (0.023 - 0.018 + 0.031 + ... - 0.015) / 128
              = 0.812 / 128
              = 0.00634

This is the gradient for THIS weight, averaged over ALL 128 images.
```

**Gradient Statistics:**
```
For all 100,200 weights:
âˆ‚L/âˆ‚Wâ‚ = [0.00634, -0.00821, 0.01234, ...]

Mean gradient magnitude: 0.0087
Standard deviation: 0.0034
Min gradient: -0.0245
Max gradient: 0.0298

Smooth, well-averaged gradients!
```

---

**Weight Update:**

```
Learning rate: Î± = 0.1

Update ALL weights:
W[50,30] := W[50,30] - Î± Ã— âˆ‚L/âˆ‚W[50,30]
         := 0.250 - 0.1 Ã— 0.00634
         := 0.250 - 0.000634
         := 0.249366

Similarly for all 100,200 weights...
```

**One Epoch Progress:**
```
Iteration 1 (all 128 images):
  Loss: 0.566 â†’ 0.562 (after update)
  Time: 2.5 seconds
  Weight updates: 1

That's it! Only ONE update per epoch!
```

---

## Approach 2: Stochastic Gradient Descent (Batch Size = 1)

### **Iteration 1: Process First Image**

**Forward Pass - Image 1 only:**

```
Image 1 (cat):
xâ‚ = [0.12, 0.45, 0.89, 0.23, ...]  (1000 features)

zâ‚ = WÃ—xâ‚ + b
Å·â‚ = softmax(zâ‚) = [0.52, 0.48]
yâ‚ = [1, 0]
Lossâ‚ = -log(0.52) = 0.654
```

**Backward Pass - Compute Gradients:**

```
For weight W[50, 30]:

âˆ‚Lossâ‚/âˆ‚W[50,30] = 0.023

No averaging! Just gradient from this ONE image.
```

**Gradient Statistics (from single image):**
```
Mean gradient magnitude: 0.0234
Standard deviation: 0.0892  â† Much more noisy!
Min gradient: -0.3145
Max gradient: 0.2891

Noisy, high variance gradients!
```

**Weight Update:**

```
W[50,30] := 0.250 - 0.1 Ã— 0.023
         := 0.250 - 0.0023
         := 0.2477
```

---

### **Iteration 2: Process Second Image**

**Forward Pass - Image 2 only:**

```
Image 2 (dog):
Å·â‚‚ = [0.45, 0.55]
yâ‚‚ = [0, 1]
Lossâ‚‚ = 0.598
```

**Gradient:**
```
âˆ‚Lossâ‚‚/âˆ‚W[50,30] = -0.018  (different sign from before!)
```

**Weight Update:**

```
W[50,30] := 0.2477 - 0.1 Ã— (-0.018)
         := 0.2477 + 0.0018
         := 0.2495

Weight went UP! Opposite direction from iteration 1!
```

---

### **Iteration 3: Process Third Image**

**Gradient and Update:**

```
âˆ‚Lossâ‚ƒ/âˆ‚W[50,30] = 0.031

W[50,30] := 0.2495 - 0.1 Ã— 0.031
         := 0.2495 - 0.0031
         := 0.2464

Back down again!
```

---

**One Epoch Progress:**

```
Start: W[50,30] = 0.250

Iteration 1 (image 1):  W = 0.2477, Lossâ‚ = 0.654
Iteration 2 (image 2):  W = 0.2495, Lossâ‚‚ = 0.598
Iteration 3 (image 3):  W = 0.2464, Lossâ‚ƒ = 0.494
...
Iteration 128 (image 128): W = 0.2493, Lossâ‚â‚‚â‚ˆ = 0.478

End: W[50,30] = 0.2493

Average loss over epoch: 0.566
Final loss: 0.478
Time: 0.5 seconds (faster per update, but 128 updates!)
Weight updates: 128

Weights zig-zagged a lot, but average movement similar to batch!
```

---

## Approach 3: Mini-Batch Gradient Descent (Batch Size = 32)

### **Mini-Batch 1: Process Images 1-32**

**Forward Pass - 32 images:**

```
Image 1: Å·â‚ = [0.52, 0.48], Lossâ‚ = 0.654
Image 2: Å·â‚‚ = [0.45, 0.55], Lossâ‚‚ = 0.598
Image 3: Å·â‚ƒ = [0.61, 0.39], Lossâ‚ƒ = 0.494
...
Image 32: Å·â‚ƒâ‚‚ = [0.58, 0.42], Lossâ‚ƒâ‚‚ = 0.543

Average loss for this mini-batch:
L_mini = (Lossâ‚ + Lossâ‚‚ + ... + Lossâ‚ƒâ‚‚) / 32
       = (0.654 + 0.598 + ... + 0.543) / 32
       = 17.89 / 32
       = 0.559
```

**Backward Pass - Average Gradients:**

```
For weight W[50, 30]:

âˆ‚Lossâ‚/âˆ‚W[50,30] = 0.023
âˆ‚Lossâ‚‚/âˆ‚W[50,30] = -0.018
âˆ‚Lossâ‚ƒ/âˆ‚W[50,30] = 0.031
...
âˆ‚Lossâ‚ƒâ‚‚/âˆ‚W[50,30] = 0.019

Average gradient:
âˆ‚L_mini/âˆ‚W[50,30] = (0.023 - 0.018 + 0.031 + ... + 0.019) / 32
                   = 0.197 / 32
                   = 0.00616

Less noisy than single image (SGD)
More noisy than full batch
```

**Gradient Statistics:**
```
Mean gradient magnitude: 0.0091
Standard deviation: 0.0156  â† Between SGD and Batch!
Min gradient: -0.0847
Max gradient: 0.0923

Moderate noise, good signal!
```

**Weight Update:**

```
W[50,30] := 0.250 - 0.1 Ã— 0.00616
         := 0.250 - 0.000616
         := 0.249384
```

---

### **Mini-Batch 2: Process Images 33-64**

**Forward and Backward:**

```
Average loss: 0.571
âˆ‚L_mini/âˆ‚W[50,30] = 0.00682

Update:
W[50,30] := 0.249384 - 0.1 Ã— 0.00682
         := 0.248702
```

---

### **Mini-Batch 3: Process Images 65-96**

```
Average loss: 0.548
âˆ‚L_mini/âˆ‚W[50,30] = 0.00591

Update:
W[50,30] := 0.248702 - 0.1 Ã— 0.00591
         := 0.248111
```

---

### **Mini-Batch 4: Process Images 97-128**

```
Average loss: 0.563
âˆ‚L_mini/âˆ‚W[50,30] = 0.00658

Update:
W[50,30] := 0.248111 - 0.1 Ã— 0.00658
         := 0.247453
```

---

**One Epoch Progress:**

```
Start: W[50,30] = 0.250

Mini-batch 1 (images 1-32):   W = 0.249384, Loss = 0.559
Mini-batch 2 (images 33-64):  W = 0.248702, Loss = 0.571
Mini-batch 3 (images 65-96):  W = 0.248111, Loss = 0.548
Mini-batch 4 (images 97-128): W = 0.247453, Loss = 0.563

End: W[50,30] = 0.247453

Average loss over epoch: 0.560
Time: 1.2 seconds
Weight updates: 4

Smoother than SGD, more frequent than Batch!
```

---

# Part 3: Visual Comparison

## Weight Trajectory During One Epoch

```
W[50,30] value over time:

Batch GD (1 update):
0.250 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ 0.249366
      â†‘                                 â†‘
    Start                            End (1 step)


SGD (128 updates):
0.250 â†˜â†—â†˜â†—â†˜â†˜â†—â†˜â†—â†˜â†—â†˜â†—â†˜â†—â†˜â†˜â†—â†˜â†—â†˜... â†’ 0.2493
      â†‘                              â†‘
    Start                         End (zig-zag path)


Mini-batch (4 updates):
0.250 â”€â”€â†’ 0.249384 â”€â”€â†’ 0.248702 â”€â”€â†’ 0.248111 â”€â”€â†’ 0.247453
      â†‘                                              â†‘
    Start                                          End (smooth path)
```

---

## Loss Trajectory

```
    Loss
     â†‘
  0.7â”‚
  0.6â”‚â—                           Batch GD
  0.5â”‚ â•²___________________________
     â”‚
  0.7â”‚â—â•²  â•±â•²  â•±â•²  â•±â•²              SGD
  0.6â”‚  â•²â•±  â•²â•±  â•²â•±  â•²_â•±â•²_
  0.5â”‚                    â•²___
     â”‚
  0.7â”‚â—                           Mini-batch
  0.6â”‚ â•²___â•²___â•²___
  0.5â”‚           â•²_______________
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Iterations
       1        50       100

â— = Starting point

SGD: Noisy but frequent updates
Batch: Smooth but infrequent
Mini-batch: Best of both! âœ“
```

---

# Part 4: Detailed Comparison

## Metrics Comparison (128 training images)

| Method | Batch Size | Updates/Epoch | Time/Epoch | Memory | Gradient Noise | Convergence |
|--------|-----------|---------------|-----------|---------|----------------|-------------|
| **Batch GD** | 128 (all) | 1 | 2.5s | High (all images in memory) | Very Low | Smooth but slow |
| **Mini-batch** | 32 | 4 | 1.2s | Medium (32 images) | Medium | **Optimal** âœ“ |
| **SGD** | 1 | 128 | 2.8s | Very Low (1 image) | Very High | Fast but noisy |

---

## Update Quality Comparison

**Single weight W[50,30] after one epoch:**

| Method | Start | End | Total Change | Path |
|--------|-------|-----|--------------|------|
| **Batch** | 0.250 | 0.249366 | -0.000634 | Straight line |
| **Mini-batch** | 0.250 | 0.247453 | -0.002547 | Gentle curve |
| **SGD** | 0.250 | 0.2493 | -0.0007 | Zig-zag |

**Observation:** Mini-batch made largest progress! (Due to more updates)

---

## Gradient Statistics

**For the same weight W[50,30] across methods:**

| Method | Mean Gradient | Std Dev | Min | Max | Signal-to-Noise |
|--------|--------------|---------|-----|-----|-----------------|
| **Batch** | 0.00634 | 0.0034 | -0.0245 | 0.0298 | 1.86 |
| **Mini-batch** | 0.00616 | 0.0156 | -0.0847 | 0.0923 | 0.39 |
| **SGD** | 0.00622 | 0.0892 | -0.3145 | 0.2891 | 0.07 |

**Signal-to-Noise Ratio = Mean / Std Dev**
- Higher is better (clearer signal)
- Batch: Best signal quality
- Mini-batch: Good balance
- SGD: Very noisy

---

# Part 5: The Mathematics

## Loss Function for Different Batch Sizes

### **Full Batch:**

$$L_{\text{batch}} = \frac{1}{N}\sum_{i=1}^{N}\mathcal{L}(\hat{y}_i, y_i)$$

Where N = total training set size (128 in our example)

---

### **Mini-Batch:**

$$L_{\text{mini}} = \frac{1}{B}\sum_{i=1}^{B}\mathcal{L}(\hat{y}_i, y_i)$$

Where B = batch size (e.g., 32)

**For one epoch with N samples and batch size B:**
- Number of mini-batches: $M = \lceil N/B \rceil$
- Total updates per epoch: M

---

### **Stochastic (Single Sample):**

$$L_{\text{sgd}} = \mathcal{L}(\hat{y}_i, y_i)$$

Just the loss from ONE sample (B = 1)

---

## Gradient Computation

### **Full Batch Gradient:**

$$\nabla_{\theta}L = \frac{1}{N}\sum_{i=1}^{N}\nabla_{\theta}\mathcal{L}(\hat{y}_i, y_i)$$

**Expanded for one weight:**
$$\frac{\partial L}{\partial w} = \frac{1}{N}\left(\frac{\partial \mathcal{L}_1}{\partial w} + \frac{\partial \mathcal{L}_2}{\partial w} + ... + \frac{\partial \mathcal{L}_N}{\partial w}\right)$$

---

### **Mini-Batch Gradient:**

$$\nabla_{\theta}L = \frac{1}{B}\sum_{i=1}^{B}\nabla_{\theta}\mathcal{L}(\hat{y}_i, y_i)$$

**For mini-batch j:**
$$\frac{\partial L_j}{\partial w} = \frac{1}{B}\sum_{i \in \mathcal{B}_j}\frac{\partial \mathcal{L}_i}{\partial w}$$

Where $\mathcal{B}_j$ is the set of samples in mini-batch j

---

### **Stochastic Gradient:**

$$\nabla_{\theta}L = \nabla_{\theta}\mathcal{L}(\hat{y}_i, y_i)$$

No averaging, just gradient from sample i

---

## Expected Value of Gradients

**Key insight:** All three are **unbiased estimators** of the true gradient!

$$\mathbb{E}[\nabla_{\text{mini-batch}}] = \mathbb{E}[\nabla_{\text{SGD}}] = \nabla_{\text{batch}}$$

**But variance differs:**

$$\text{Var}(\nabla_{\text{batch}}) = 0 \quad \text{(deterministic)}$$

$$\text{Var}(\nabla_{\text{mini-batch}}) = \frac{\sigma^2}{B}$$

$$\text{Var}(\nabla_{\text{SGD}}) = \sigma^2$$

Where $\sigma^2$ is the variance of individual sample gradients

**Relationship:**
$$\text{Var}(\nabla_{\text{mini-batch}}) = \frac{1}{B} \times \text{Var}(\nabla_{\text{SGD}})$$

Larger batch size â†’ Lower variance

---

# Part 6: Choosing Batch Size

## The Trade-offs

### **Small Batch Size (e.g., 1-8):**

```
Pros:
âœ“ Fast updates (more per epoch)
âœ“ Regularization effect (noise helps escape local minima)
âœ“ Low memory usage
âœ“ Better generalization (sometimes)

Cons:
âœ— Noisy gradients
âœ— Can't exploit GPU parallelism well
âœ— Unstable training
âœ— Requires careful learning rate tuning
```

---

### **Large Batch Size (e.g., 256-1024):**

```
Pros:
âœ“ Smooth, stable gradients
âœ“ Better GPU utilization
âœ“ Can use larger learning rates
âœ“ More efficient computation

Cons:
âœ— Slower updates (fewer per epoch)
âœ— More memory required
âœ— Can get stuck in sharp minima
âœ— Worse generalization (sometimes)
âœ— Requires more epochs to converge
```

---

### **Medium Batch Size (e.g., 32-128):**

```
The sweet spot! âœ“

Pros:
âœ“ Good balance of speed and stability
âœ“ Reasonable GPU utilization
âœ“ Manageable memory
âœ“ Good generalization

This is why 32, 64, 128 are most common!
```

---

## Numerical Example: Different Batch Sizes

**Same network, same 1024 images, trained for 10 epochs:**

| Batch Size | Updates/Epoch | Total Updates | Time/Epoch | Final Train Acc | Final Test Acc | Best? |
|-----------|---------------|---------------|-----------|----------------|---------------|-------|
| **1** (SGD) | 1024 | 10,240 | 8.5s | 98% | 88% | âœ— Slow |
| **8** | 128 | 1,280 | 3.2s | 97% | 91% | Good |
| **16** | 64 | 640 | 2.1s | 96% | 92% | Good |
| **32** | 32 | 320 | 1.5s | 95% | **93%** | âœ“ Best! |
| **64** | 16 | 160 | 1.2s | 94% | 92% | Good |
| **128** | 8 | 80 | 1.0s | 93% | 91% | Good |
| **256** | 4 | 40 | 0.9s | 91% | 88% | âœ— Underfit |
| **1024** (Batch) | 1 | 10 | 0.8s | 85% | 82% | âœ— Underfit |

**Observations:**

```
Batch size 32:
- Best test accuracy (93%)
- Reasonable training time (1.5s/epoch)
- Good balance of everything

Batch size 8-16:
- Also excellent
- Slightly better generalization
- But slower training

Batch size 256+:
- Fast per epoch
- But need many more epochs to converge
- Worse generalization
```

---

## Practical Guidelines

### **By Problem Type:**

```
Image Classification (CNNs):
â†’ Batch size: 32-128
  Reason: Good features, parallelizable

Text/Sequences (RNNs/Transformers):
â†’ Batch size: 16-64
  Reason: Variable lengths, memory intensive

Small datasets (<1000 samples):
â†’ Batch size: 8-32
  Reason: More updates per epoch needed

Large datasets (>100K samples):
â†’ Batch size: 64-256
  Reason: More efficient, still many updates
```

---

### **By GPU Memory:**

```
GPU Memory: 4GB
â†’ Batch size: 16-32 (typical)

GPU Memory: 8GB
â†’ Batch size: 32-64

GPU Memory: 16GB+
â†’ Batch size: 64-128+

Rule of thumb:
If GPU memory allows, use batch size in [32, 128]
If memory limited, reduce batch size and adjust learning rate
```

---

### **The "Linear Scaling Rule":**

When you change batch size, adjust learning rate proportionally:

$$\alpha_{\text{new}} = \alpha_{\text{old}} \times \frac{B_{\text{new}}}{B_{\text{old}}}$$

**Example:**
```
Original: Batch size 32, Learning rate 0.1

If you change to batch size 128:
New learning rate = 0.1 Ã— (128/32) = 0.1 Ã— 4 = 0.4

Why? Larger batches â†’ smoother gradients â†’ can take bigger steps
```

---

# Part 7: Complete Training Example

## Cat vs Dog Classifier with Mini-Batch GD

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Generate synthetic data for demonstration
torch.manual_seed(42)
X_train = torch.randn(1024, 1000)  # 1024 images, 1000 features
y_train = torch.randint(0, 2, (1024,))  # Binary labels (cat=0, dog=1)

X_test = torch.randn(256, 1000)
y_test = torch.randint(0, 2, (256,))

# Create datasets and dataloaders
train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

# Try different batch sizes
batch_sizes = [1, 8, 32, 128, 1024]

for batch_size in batch_sizes:
    print(f"\n{'='*50}")
    print(f"Training with Batch Size: {batch_size}")
    print(f"{'='*50}")
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=256,  # Test batch size can be larger
        shuffle=False
    )
    
    # Define model
    class CatDogNet(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(1000, 100)
            self.fc2 = nn.Linear(100, 2)
        
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = self.fc2(x)
            return x
    
    model = CatDogNet()
    
    # Adjust learning rate based on batch size (linear scaling)
    base_lr = 0.01
    lr = base_lr * (batch_size / 32)  # Scale relative to batch size 32
    
    optimizer = optim.SGD(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()
    
    # Track metrics
    import time
    start_time = time.time()
    
    # Training
    model.train()
    n_updates = 0
    
    for epoch in range(3):
        epoch_loss = 0
        n_correct = 0
        n_samples = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # Forward pass
            output = model(data)
            loss = criterion(output, target)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Track metrics
            epoch_loss += loss.item()
            pred = output.argmax(dim=1)
            n_correct += (pred == target).sum().item()
            n_samples += len(target)
            n_updates += 1
            
            # Print first few batches of first epoch
            if epoch == 0 and batch_idx < 3:
                print(f"  Epoch 0, Batch {batch_idx}:")
                print(f"    Batch loss: {loss.item():.4f}")
                print(f"    Batch accuracy: {(pred == target).float().mean():.2%}")
        
        # Epoch summary
        avg_loss = epoch_loss / len(train_loader)
        accuracy = n_correct / n_samples
        print(f"\nEpoch {epoch}: Loss = {avg_loss:.4f}, "
              f"Accuracy = {accuracy:.2%}")
    
    # Testing
    model.eval()
    test_correct = 0
    test_total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            pred = output.argmax(dim=1)
            test_correct += (pred == target).sum().item()
            test_total += len(target)
    
    test_accuracy = test_correct / test_total
    
    # Summary
    elapsed_time = time.time() - start_time
    updates_per_epoch = len(train_loader)
    
    print(f"\n{'='*50}")
    print(f"Summary for Batch Size {batch_size}:")
    print(f"  Updates per epoch: {updates_per_epoch}")
    print(f"  Total updates: {n_updates}")
    print(f"  Training time: {elapsed_time:.2f}s")
    print(f"  Time per epoch: {elapsed_time/3:.2f}s")
    print(f"  Final train accuracy: {accuracy:.2%}")
    print(f"  Test accuracy: {test_accuracy:.2%}")
    print(f"{'='*50}")
```

---

**Expected Output:**

```
==================================================
Training with Batch Size: 1
==================================================
  Epoch 0, Batch 0:
    Batch loss: 0.7234
    Batch accuracy: 50.00%
  Epoch 0, Batch 1:
    Batch loss: 0.6891
    Batch accuracy: 100.00%
  Epoch 0, Batch 2:
    Batch loss: 0.7012
    Batch accuracy: 0.00%

Epoch 0: Loss = 0.6945, Accuracy = 51.27%
Epoch 1: Loss = 0.6823, Accuracy = 58.89%
Epoch 2: Loss = 0.6701, Accuracy = 63.48%

==================================================
Summary for Batch Size 1:
  Updates per epoch: 1024
  Total updates: 3072
  Training time: 8.34s
  Time per epoch: 2.78s
  Final train accuracy: 63.48%
  Test accuracy: 59.77%
==================================================

==================================================
Training with Batch Size: 32
==================================================
  Epoch 0, Batch 0:
    Batch loss: 0.7123
    Batch accuracy: 53.12%
  Epoch 0, Batch 1:
    Batch loss: 0.6945
    Batch accuracy: 59.38%
  Epoch 0, Batch 2:
    Batch loss: 0.6834
    Batch accuracy: 62.50%

Epoch 0: Loss = 0.6891, Accuracy = 55.47%
Epoch 1: Loss = 0.6523, Accuracy = 67.19%
Epoch 2: Loss = 0.6234, Accuracy = 73.83%

==================================================
Summary for Batch Size 32:
  Updates per epoch: 32
  Total updates: 96
  Training time: 1.23s
  Time per epoch: 0.41s
  Final train accuracy: 73.83%
  Test accuracy: 71.48%
==================================================

==================================================
Training with Batch Size: 1024
==================================================
  Epoch 0, Batch 0:
    Batch loss: 0.6932
    Batch accuracy: 50.00%

Epoch 0: Loss = 0.6932, Accuracy = 50.00%
Epoch 1: Loss = 0.6931, Accuracy = 50.10%
Epoch 2: Loss = 0.6929, Accuracy = 50.20%

==================================================
Summary for Batch Size 1024:
  Updates per epoch: 1
  Total updates: 3
  Training time: 0.67s
  Time per epoch: 0.22s
  Final train accuracy: 50.20%
  Test accuracy: 50.78%
==================================================
```

**Analysis:**
```
Batch size 1 (SGD):
- Many updates (1024/epoch)
- Slow and noisy
- Moderate performance

Batch size 32:
- Balanced updates (32/epoch)
- Fast and stable
- BEST performance! âœ“

Batch size 1024 (Full batch):
- Very few updates (1/epoch)
- Fast per epoch but barely learning
- Needs many more epochs
```

---

# Part 8: Advanced Considerations

## Batch Size and Generalization

### **The Sharp vs Flat Minima Hypothesis**

```
Small batch (SGD):           Large batch:
Finds flat minimum           Finds sharp minimum

    Loss                         Loss
     â†‘                            â†‘
     â”‚  â•±â”€â”€â”€â”€â”€â•²                   â”‚  â•±â•²
     â”‚ â•±       â•²                  â”‚ â•±  â•²
     â”‚â•±    â—    â•²                 â”‚â•± â—  â•²
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ w             â””â”€â”€â”€â”€â”€â”€â†’ w
          â†‘                           â†‘
    Flat minimum                Sharp minimum
    (robust to                  (sensitive to
     parameter changes)          parameter changes)

Flat minima â†’ Better generalization!
Sharp minima â†’ Overfit!
```

**Why small batches find flat minima:**
```
Noisy gradients explore the loss landscape
Like annealing in physics
Escape sharp valleys, settle in wide valleys
```

---

## Batch Size and Learning Rate

### **The Critical Batch Size**

There's a point where increasing batch size no longer helps:

```
Test Accuracy vs Batch Size (with optimal LR)

  Acc
   â†‘
 95%â”‚     â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚    â•±
 90%â”‚   â•±
    â”‚  â•±
 85%â”‚ â•±
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Batch Size
      8  32  128  512
           â†‘
      Critical batch size
      (~128 for many problems)

Beyond this, larger batches don't improve generalization
even with perfect LR tuning!
```

---

## Gradient Accumulation (Simulating Large Batches)

**Problem:** Want large effective batch size but limited GPU memory

**Solution:** Accumulate gradients over multiple small batches

```python
# Simulate batch size of 128 with batches of 32
model = CatDogNet()
optimizer = optim.SGD(model.parameters(), lr=0.1)
accumulation_steps = 4  # 32 Ã— 4 = 128 effective batch size

optimizer.zero_grad()

for batch_idx, (data, target) in enumerate(train_loader):
    # Forward pass
    output = model(data)
    loss = criterion(output, target)
    
    # Normalize loss by accumulation steps
    loss = loss / accumulation_steps
    
    # Backward pass (accumulate gradients)
    loss.backward()
    
    # Update weights every 4 batches
    if (batch_idx + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
        print(f"Updated weights at batch {batch_idx + 1}")

# Output:
# Updated weights at batch 4
# Updated weights at batch 8
# Updated weights at batch 12
# ...
```

**Effect:**
```
Without accumulation (batch size 32):
- Update every 1 batch
- 32 samples per update
- Noisy gradients

With accumulation (4 batches):
- Update every 4 batches
- 128 samples per update
- Smoother gradients
- Same memory as batch size 32!
```

---

# Part 9: Batch Normalization and Batch Size

## How Batch Norm Depends on Batch Size

**Batch Normalization computes statistics over the batch:**

$$\mu_B = \frac{1}{B}\sum_{i=1}^{B}x_i$$

$$\sigma^2_B = \frac{1}{B}\sum_{i=1}^{B}(x_i - \mu_B)^2$$

$$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma^2_B + \epsilon}}$$

---

### **The Problem with Small Batches:**

```
Batch size 32:
Î¼_B = 0.05, Ïƒ_B = 1.02  (good estimate)

Batch size 2:
Î¼_B = 0.23, Ïƒ_B = 0.67  (noisy estimate!)

Batch size 1:
Î¼_B = x_1, Ïƒ_B = 0  (undefined!)
Can't normalize!
```

**Guideline:** If using Batch Norm, use batch size â‰¥ 16, preferably â‰¥ 32

---

### **Alternatives for Small Batches:**

**Layer Normalization (LayerNorm):**
```python
# Normalize across features instead of batch
layer_norm = nn.LayerNorm(hidden_size)

# Works with batch size 1!
x = torch.randn(1, hidden_size)  # Batch size 1
normalized = layer_norm(x)  # No problem!
```

**Group Normalization:**
```python
# Normalize within groups of channels
group_norm = nn.GroupNorm(num_groups=8, num_channels=32)

# Also works with batch size 1
```

---

# Part 10: Practical Tips and Best Practices

## Choosing Batch Size: Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      How to Choose Batch Size?          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Start with batch size 32 (good default)

Is training unstable (loss oscillating wildly)?
â”œâ”€ YES â†’ Increase batch size (64 or 128)
â”‚         Larger batches = more stable gradients
â”‚
â””â”€ NO â†’ Continue

Is training too slow (taking forever)?
â”œâ”€ YES â†’ Check GPU utilization
â”‚         â”œâ”€ Low utilization? â†’ Increase batch size
â”‚         â””â”€ High utilization? â†’ Keep current size
â”‚
â””â”€ NO â†’ Continue

Is test accuracy much lower than train?
â”œâ”€ YES â†’ Might be overfitting
â”‚         Try DECREASING batch size (16 or 8)
â”‚         Smaller batches = better generalization
â”‚
â””â”€ NO â†’ You're good! âœ“

Using Batch Normalization?
â”œâ”€ YES â†’ Keep batch size â‰¥ 16, prefer â‰¥ 32
â””â”€ NO â†’ Batch size 8-16 is fine

Memory limited?
â”œâ”€ YES â†’ Use smallest batch size that fits
â”‚         Use gradient accumulation to simulate larger
â””â”€ NO â†’ Enjoy the freedom!
```

---

## Tuning Learning Rate with Batch Size

### **The Learning Rate Range Test:**

```python
# Find optimal learning rate for your batch size
def find_lr(model, train_loader, init_lr=1e-8, final_lr=10):
    optimizer = optim.SGD(model.parameters(), lr=init_lr)
    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(
        optimizer,
        gamma=(final_lr/init_lr)**(1/len(train_loader))
    )
    
    lrs = []
    losses = []
    
    for batch_idx, (data, target) in enumerate(train_loader):
        # Forward and backward
        output = model(data)
        loss = criterion(output, target)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Record
        lrs.append(lr_scheduler.get_last_lr()[0])
        losses.append(loss.item())
        
        # Update LR
        lr_scheduler.step()
        
        # Stop if loss explodes
        if loss.item() > losses[0] * 10:
            break
    
    # Plot
    import matplotlib.pyplot as plt
    plt.plot(lrs, losses)
    plt.xscale('log')
    plt.xlabel('Learning Rate')
    plt.ylabel('Loss')
    plt.title('Learning Rate Range Test')
    plt.show()
    
    # Optimal LR is usually where loss decreases fastest
    return lrs, losses

# Run for different batch sizes
for batch_size in [16, 32, 64, 128]:
    print(f"\nFinding optimal LR for batch size {batch_size}")
    train_loader = DataLoader(train_dataset, batch_size=batch_size)
    model = CatDogNet()
    find_lr(model, train_loader)
```

**Typical results:**
```
Batch size 16:  Optimal LR â‰ˆ 0.01
Batch size 32:  Optimal LR â‰ˆ 0.02
Batch size 64:  Optimal LR â‰ˆ 0.04
Batch size 128: Optimal LR â‰ˆ 0.08

Pattern: LR scales roughly linearly with batch size
```

---

## Common Mistakes to Avoid

### **âŒ Mistake 1: Using batch size that doesn't divide dataset size**

```python
# BAD: 1000 samples, batch size 32
# Last batch has only 8 samples!
train_loader = DataLoader(dataset, batch_size=32)

# GOOD: Either drop last batch or handle gracefully
train_loader = DataLoader(
    dataset,
    batch_size=32,
    drop_last=True  # Drop incomplete batch
)
```

---

### **âŒ Mistake 2: Not adjusting LR when changing batch size**

```python
# BAD: Same LR for different batch sizes
for batch_size in [16, 32, 64]:
    optimizer = optim.SGD(model.parameters(), lr=0.01)  # Always 0.01!

# GOOD: Scale LR with batch size
for batch_size in [16, 32, 64]:
    lr = 0.01 * (batch_size / 32)  # Scale relative to base
    optimizer = optim.SGD(model.parameters(), lr=lr)
```

---

### **âŒ Mistake 3: Forgetting to shuffle**

```python
# BAD: No shuffling
train_loader = DataLoader(dataset, batch_size=32, shuffle=False)
# Model sees images in same order every epoch!
# First batch always cats, second always dogs â†’ poor learning

# GOOD: Shuffle each epoch
train_loader = DataLoader(dataset, batch_size=32, shuffle=True)
# Different mini-batches each epoch â†’ better generalization
```

---

### **âŒ Mistake 4: Different batch sizes for train and validation**

```python
# PROBLEMATIC: If using Batch Norm
train_loader = DataLoader(train_set, batch_size=32)
val_loader = DataLoader(val_set, batch_size=256)  # Different!

# Batch Norm statistics differ greatly
# Can cause validation performance to differ from training

# BETTER: Same or similar batch sizes
train_loader = DataLoader(train_set, batch_size=32)
val_loader = DataLoader(val_set, batch_size=32)

# OR: Use eval mode (running stats instead of batch stats)
model.eval()  # Switches to running mean/var
```

---

# Part 11: Summary

## The Complete Picture

### **Three Approaches Compared:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Gradient Descent Methods                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚ Batch GD (Full Batch):                             â”‚
â”‚   Batch Size = N (all data)                        â”‚
â”‚   Updates/Epoch = 1                                â”‚
â”‚   Pros: Stable, deterministic                       â”‚
â”‚   Cons: Slow, memory intensive                      â”‚
â”‚   Use: Small datasets, when stability critical     â”‚
â”‚                                                      â”‚
â”‚ Mini-Batch GD: âœ“ RECOMMENDED                       â”‚
â”‚   Batch Size = 16-128 (typically 32)              â”‚
â”‚   Updates/Epoch = N/B                              â”‚
â”‚   Pros: Balanced speed/stability, GPU efficient    â”‚
â”‚   Cons: Requires batch size tuning                 â”‚
â”‚   Use: Almost always! Default choice               â”‚
â”‚                                                      â”‚
â”‚ Stochastic GD (SGD):                               â”‚
â”‚   Batch Size = 1                                   â”‚
â”‚   Updates/Epoch = N                                â”‚
â”‚   Pros: Fast per update, regularization effect    â”‚
â”‚   Cons: Very noisy, unstable, slow overall        â”‚
â”‚   Use: Rarely alone, mostly for theory            â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Key Formulas:**

**Mini-Batch Loss:**
$$L_{\text{mini}} = \frac{1}{B}\sum_{i=1}^{B}\mathcal{L}(\hat{y}_i, y_i)$$

**Mini-Batch Gradient:**
$$\nabla_{\theta}L = \frac{1}{B}\sum_{i=1}^{B}\nabla_{\theta}\mathcal{L}(\hat{y}_i, y_i)$$

**Gradient Variance:**
$$\text{Var}(\nabla_{\text{mini}}) = \frac{\sigma^2}{B}$$

**Learning Rate Scaling:**
$$\alpha_{\text{new}} = \alpha_{\text{base}} \times \frac{B_{\text{new}}}{B_{\text{base}}}$$

---

### **Practical Guidelines:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Batch Size Recommendations        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                          â”‚
â”‚ Default: 32                              â”‚
â”‚   Good balance for most problems         â”‚
â”‚                                          â”‚
â”‚ Image Classification: 32-128             â”‚
â”‚   Depends on GPU memory                  â”‚
â”‚                                          â”‚
â”‚ NLP/Sequences: 16-64                     â”‚
â”‚   Variable lengths need more memory      â”‚
â”‚                                          â”‚
â”‚ Small Dataset (<1000): 8-32              â”‚
â”‚   Need more updates per epoch            â”‚
â”‚                                          â”‚
â”‚ Large Dataset (>100K): 64-256            â”‚
â”‚   Efficiency matters more                â”‚
â”‚                                          â”‚
â”‚ With Batch Norm: â‰¥16, prefer â‰¥32        â”‚
â”‚   Need good batch statistics             â”‚
â”‚                                          â”‚
â”‚ GPU Memory Limited: Smallest that fits   â”‚
â”‚   + gradient accumulation                â”‚
â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **The Magic Formula:**

For most problems, this works well:

```python
# The "standard recipe"
batch_size = 32
learning_rate = 0.001  # (with Adam optimizer)
# OR
learning_rate = 0.1    # (with SGD optimizer)

# If you change batch size:
if new_batch_size != 32:
    learning_rate = learning_rate * (new_batch_size / 32)
```

---

### **Debugging Checklist:**

```
Training issues? Check:

â–¡ Batch size in [8, 256] range?
â–¡ Learning rate scaled with batch size?
â–¡ Shuffling enabled for training?
â–¡ Using shuffle=True in DataLoader?
â–¡ Batch Norm with batch size â‰¥16?
â–¡ Last batch size consistent (use drop_last)?
â–¡ Test with larger batches if possible?
â–¡ Monitor gradient norms (too large/small)?
â–¡ Loss oscillating wildly? â†’ Increase batch size
â–¡ Loss barely moving? â†’ Decrease batch size
```

---

**You now understand mini-batch gradient descent completely! ğŸ‰**

The key insights:
- **Mini-batches balance speed and stability**
- **Batch size affects gradient noise and convergence**
- **Larger batches need larger learning rates (linear scaling)**
- **32-128 is the sweet spot for most problems**
- **Small batches help generalization, large batches help efficiency**


