# Mini-Batch Gradient Descent: Complete Explanation
## Understanding Batch Sizes and Their Impact on Training
### (Detailed Step-by-Step with Cat vs Dog Classification)

---

## ğŸ”— **Connection to Previous Topics**

### **What We Know So Far:**

**From Gradient Descent:**
```
Training process:
1. Forward pass: Compute predictions
2. Compute loss: Measure error
3. Compute gradients: âˆ‚L/âˆ‚w
4. Update weights: w := w - Î±Â·âˆ‚L/âˆ‚w
```

**The Question We Haven't Answered:**

```
We have 1000 training images.

Do we:
A) Compute gradients on ALL 1000 images, then update once?
B) Compute gradients on each single image, update 1000 times?
C) Compute gradients on small groups (e.g., 32 images), update many times?

Which is best? Why?
```

---

# Part 1: The Three Flavors of Gradient Descent

## 1. Plain English Explanation

### **The Student Learning Analogy**

Imagine a student preparing for an exam with 1000 practice problems:

---

### **Approach 1: Batch Gradient Descent (Full Batch)**

```
Student process:
1. Solve ALL 1000 problems
2. Check ALL answers
3. Calculate overall performance
4. Study based on total mistakes
5. Repeat

Pros: Gets complete picture of knowledge
Cons: Takes forever to get feedback!
```

**In neural networks:**
```
1. Process ALL 1000 images
2. Compute loss on ALL images
3. Average gradients from ALL images
4. Make ONE weight update
5. Repeat

One epoch = One weight update!
```

---

### **Approach 2: Stochastic Gradient Descent (SGD, Batch Size = 1)**

```
Student process:
1. Solve ONE problem
2. Check ONE answer
3. Study immediately based on that mistake
4. Move to next problem
5. Repeat 1000 times

Pros: Fast feedback, very frequent learning
Cons: Each problem might not be representative, noisy updates
```

**In neural networks:**
```
1. Process ONE image
2. Compute loss on ONE image
3. Compute gradient from ONE image
4. Make ONE weight update
5. Repeat 1000 times

One epoch = 1000 weight updates!
```

---

### **Approach 3: Mini-Batch Gradient Descent (The Sweet Spot)**

```
Student process:
1. Solve 32 problems
2. Check 32 answers
3. Study based on patterns in these 32
4. Move to next 32 problems
5. Repeat 31 times (1000/32 â‰ˆ 31)

Pros: Good balance - gets patterns, frequent feedback
Cons: Need to choose batch size
```

**In neural networks:**
```
1. Process 32 images (one mini-batch)
2. Compute loss on these 32 images
3. Average gradients from these 32 images
4. Make ONE weight update
5. Repeat 31 times

One epoch = 31 weight updates!
```

---

# Part 2: Detailed Numerical Example

## Setup: Cat vs Dog Classifier

```
Dataset: 128 training images (64 cats, 64 dogs)
Network: Simple 2-layer network
Feature dimension: 1000 (after CNN features)
Hidden layer: 100 neurons
Output: 2 neurons (cat, dog)

Total parameters: 1000Ã—100 + 100Ã—2 = 100,200 weights
```

---

## Approach 1: Batch Gradient Descent (Batch Size = 128)

### **Iteration 1: Process Entire Dataset**

**Forward Pass - ALL 128 images:**

```
Image 1 (cat):
zâ‚ = WÃ—xâ‚ + b
Å·â‚ = softmax(zâ‚) = [0.52, 0.48]  (52% cat)
yâ‚ = [1, 0]  (true: cat)
Lossâ‚ = -log(0.52) = 0.654

Image 2 (dog):
Å·â‚‚ = [0.45, 0.55]  (55% dog)
yâ‚‚ = [0, 1]  (true: dog)
Lossâ‚‚ = -log(0.55) = 0.598

Image 3 (cat):
Å·â‚ƒ = [0.61, 0.39]  (61% cat)
yâ‚ƒ = [1, 0]  (true: cat)
Lossâ‚ƒ = -log(0.61) = 0.494

...

Image 128 (dog):
Å·â‚â‚‚â‚ˆ = [0.38, 0.62]  (62% dog)
yâ‚â‚‚â‚ˆ = [0, 1]  (true: dog)
Lossâ‚â‚‚â‚ˆ = -log(0.62) = 0.478
```

**Average Loss:**
```
L_batch = (Lossâ‚ + Lossâ‚‚ + ... + Lossâ‚â‚‚â‚ˆ) / 128
        = (0.654 + 0.598 + 0.494 + ... + 0.478) / 128
        = 72.45 / 128
        = 0.566
```

---

**Backward Pass - Compute Gradients:**

```
For each weight w:

âˆ‚L/âˆ‚w = (âˆ‚Lossâ‚/âˆ‚w + âˆ‚Lossâ‚‚/âˆ‚w + ... + âˆ‚Lossâ‚â‚‚â‚ˆ/âˆ‚w) / 128

Example for weight W[50, 30] (connecting feature 30 to hidden neuron 50):

Image 1: âˆ‚Lossâ‚/âˆ‚W[50,30] = 0.023
Image 2: âˆ‚Lossâ‚‚/âˆ‚W[50,30] = -0.018
Image 3: âˆ‚Lossâ‚ƒ/âˆ‚W[50,30] = 0.031
...
Image 128: âˆ‚Lossâ‚â‚‚â‚ˆ/âˆ‚W[50,30] = -0.015

Average gradient:
âˆ‚L/âˆ‚W[50,30] = (0.023 - 0.018 + 0.031 + ... - 0.015) / 128
              = 0.812 / 128
              = 0.00634

This is the gradient for THIS weight, averaged over ALL 128 images.
```

**Gradient Statistics:**
```
For all 100,200 weights:
âˆ‚L/âˆ‚Wâ‚ = [0.00634, -0.00821, 0.01234, ...]

Mean gradient magnitude: 0.0087
Standard deviation: 0.0034
Min gradient: -0.0245
Max gradient: 0.0298

Smooth, well-averaged gradients!
```

---

**Weight Update:**

```
Learning rate: Î± = 0.1

Update ALL weights:
W[50,30] := W[50,30] - Î± Ã— âˆ‚L/âˆ‚W[50,30]
         := 0.250 - 0.1 Ã— 0.00634
         := 0.250 - 0.000634
         := 0.249366

Similarly for all 100,200 weights...
```

**One Epoch Progress:**
```
Iteration 1 (all 128 images):
  Loss: 0.566 â†’ 0.562 (after update)
  Time: 2.5 seconds
  Weight updates: 1

That's it! Only ONE update per epoch!
```

---

## Approach 2: Stochastic Gradient Descent (Batch Size = 1)

### **Iteration 1: Process First Image**

**Forward Pass - Image 1 only:**

```
Image 1 (cat):
xâ‚ = [0.12, 0.45, 0.89, 0.23, ...]  (1000 features)

zâ‚ = WÃ—xâ‚ + b
Å·â‚ = softmax(zâ‚) = [0.52, 0.48]
yâ‚ = [1, 0]
Lossâ‚ = -log(0.52) = 0.654
```

**Backward Pass - Compute Gradients:**

```
For weight W[50, 30]:

âˆ‚Lossâ‚/âˆ‚W[50,30] = 0.023

No averaging! Just gradient from this ONE image.
```

**Gradient Statistics (from single image):**
```
Mean gradient magnitude: 0.0234
Standard deviation: 0.0892  â† Much more noisy!
Min gradient: -0.3145
Max gradient: 0.2891

Noisy, high variance gradients!
```

**Weight Update:**

```
W[50,30] := 0.250 - 0.1 Ã— 0.023
         := 0.250 - 0.0023
         := 0.2477
```

---

### **Iteration 2: Process Second Image**

**Forward Pass - Image 2 only:**

```
Image 2 (dog):
Å·â‚‚ = [0.45, 0.55]
yâ‚‚ = [0, 1]
Lossâ‚‚ = 0.598
```

**Gradient:**
```
âˆ‚Lossâ‚‚/âˆ‚W[50,30] = -0.018  (different sign from before!)
```

**Weight Update:**

```
W[50,30] := 0.2477 - 0.1 Ã— (-0.018)
         := 0.2477 + 0.0018
         := 0.2495

Weight went UP! Opposite direction from iteration 1!
```

---

### **Iteration 3: Process Third Image**

**Gradient and Update:**

```
âˆ‚Lossâ‚ƒ/âˆ‚W[50,30] = 0.031

W[50,30] := 0.2495 - 0.1 Ã— 0.031
         := 0.2495 - 0.0031
         := 0.2464

Back down again!
```

---

**One Epoch Progress:**

```
Start: W[50,30] = 0.250

Iteration 1 (image 1):  W = 0.2477, Lossâ‚ = 0.654
Iteration 2 (image 2):  W = 0.2495, Lossâ‚‚ = 0.598
Iteration 3 (image 3):  W = 0.2464, Lossâ‚ƒ = 0.494
...
Iteration 128 (image 128): W = 0.2493, Lossâ‚â‚‚â‚ˆ = 0.478

End: W[50,30] = 0.2493

Average loss over epoch: 0.566
Final loss: 0.478
Time: 0.5 seconds (faster per update, but 128 updates!)
Weight updates: 128

Weights zig-zagged a lot, but average movement similar to batch!
```

---

## Approach 3: Mini-Batch Gradient Descent (Batch Size = 32)

### **Mini-Batch 1: Process Images 1-32**

**Forward Pass - 32 images:**

```
Image 1: Å·â‚ = [0.52, 0.48], Lossâ‚ = 0.654
Image 2: Å·â‚‚ = [0.45, 0.55], Lossâ‚‚ = 0.598
Image 3: Å·â‚ƒ = [0.61, 0.39], Lossâ‚ƒ = 0.494
...
Image 32: Å·â‚ƒâ‚‚ = [0.58, 0.42], Lossâ‚ƒâ‚‚ = 0.543

Average loss for this mini-batch:
L_mini = (Lossâ‚ + Lossâ‚‚ + ... + Lossâ‚ƒâ‚‚) / 32
       = (0.654 + 0.598 + ... + 0.543) / 32
       = 17.89 / 32
       = 0.559
```

**Backward Pass - Average Gradients:**

```
For weight W[50, 30]:

âˆ‚Lossâ‚/âˆ‚W[50,30] = 0.023
âˆ‚Lossâ‚‚/âˆ‚W[50,30] = -0.018
âˆ‚Lossâ‚ƒ/âˆ‚W[50,30] = 0.031
...
âˆ‚Lossâ‚ƒâ‚‚/âˆ‚W[50,30] = 0.019

Average gradient:
âˆ‚L_mini/âˆ‚W[50,30] = (0.023 - 0.018 + 0.031 + ... + 0.019) / 32
                   = 0.197 / 32
                   = 0.00616

Less noisy than single image (SGD)
More noisy than full batch
```

**Gradient Statistics:**
```
Mean gradient magnitude: 0.0091
Standard deviation: 0.0156  â† Between SGD and Batch!
Min gradient: -0.0847
Max gradient: 0.0923

Moderate noise, good signal!
```

**Weight Update:**

```
W[50,30] := 0.250 - 0.1 Ã— 0.00616
         := 0.250 - 0.000616
         := 0.249384
```

---

### **Mini-Batch 2: Process Images 33-64**

**Forward and Backward:**

```
Average loss: 0.571
âˆ‚L_mini/âˆ‚W[50,30] = 0.00682

Update:
W[50,30] := 0.249384 - 0.1 Ã— 0.00682
         := 0.248702
```

---

### **Mini-Batch 3: Process Images 65-96**

```
Average loss: 0.548
âˆ‚L_mini/âˆ‚W[50,30] = 0.00591

Update:
W[50,30] := 0.248702 - 0.1 Ã— 0.00591
         := 0.248111
```

---

### **Mini-Batch 4: Process Images 97-128**

```
Average loss: 0.563
âˆ‚L_mini/âˆ‚W[50,30] = 0.00658

Update:
W[50,30] := 0.248111 - 0.1 Ã— 0.00658
         := 0.247453
```

---

**One Epoch Progress:**

```
Start: W[50,30] = 0.250

Mini-batch 1 (images 1-32):   W = 0.249384, Loss = 0.559
Mini-batch 2 (images 33-64):  W = 0.248702, Loss = 0.571
Mini-batch 3 (images 65-96):  W = 0.248111, Loss = 0.548
Mini-batch 4 (images 97-128): W = 0.247453, Loss = 0.563

End: W[50,30] = 0.247453

Average loss over epoch: 0.560
Time: 1.2 seconds
Weight updates: 4

Smoother than SGD, more frequent than Batch!
```

---

# Part 3: Visual Comparison

## Weight Trajectory During One Epoch

```
W[50,30] value over time:

Batch GD (1 update):
0.250 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ 0.249366
      â†‘                                 â†‘
    Start                            End (1 step)


SGD (128 updates):
0.250 â†˜â†—â†˜â†—â†˜â†˜â†—â†˜â†—â†˜â†—â†˜â†—â†˜â†—â†˜â†˜â†—â†˜â†—â†˜... â†’ 0.2493
      â†‘                              â†‘
    Start                         End (zig-zag path)


Mini-batch (4 updates):
0.250 â”€â”€â†’ 0.249384 â”€â”€â†’ 0.248702 â”€â”€â†’ 0.248111 â”€â”€â†’ 0.247453
      â†‘                                              â†‘
    Start                                          End (smooth path)
```

---

## Loss Trajectory

```
    Loss
     â†‘
  0.7â”‚
  0.6â”‚â—                           Batch GD
  0.5â”‚ â•²___________________________
     â”‚
  0.7â”‚â—â•²  â•±â•²  â•±â•²  â•±â•²              SGD
  0.6â”‚  â•²â•±  â•²â•±  â•²â•±  â•²_â•±â•²_
  0.5â”‚                    â•²___
     â”‚
  0.7â”‚â—                           Mini-batch
  0.6â”‚ â•²___â•²___â•²___
  0.5â”‚           â•²_______________
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Iterations
       1        50       100

â— = Starting point

SGD: Noisy but frequent updates
Batch: Smooth but infrequent
Mini-batch: Best of both! âœ“
```

---

# Part 4: Detailed Comparison

## Metrics Comparison (128 training images)

| Method | Batch Size | Updates/Epoch | Time/Epoch | Memory | Gradient Noise | Convergence |
|--------|-----------|---------------|-----------|---------|----------------|-------------|
| **Batch GD** | 128 (all) | 1 | 2.5s | High (all images in memory) | Very Low | Smooth but slow |
| **Mini-batch** | 32 | 4 | 1.2s | Medium (32 images) | Medium | **Optimal** âœ“ |
| **SGD** | 1 | 128 | 2.8s | Very Low (1 image) | Very High | Fast but noisy |

---

## Update Quality Comparison

**Single weight W[50,30] after one epoch:**

| Method | Start | End | Total Change | Path |
|--------|-------|-----|--------------|------|
| **Batch** | 0.250 | 0.249366 | -0.000634 | Straight line |
| **Mini-batch** | 0.250 | 0.247453 | -0.002547 | Gentle curve |
| **SGD** | 0.250 | 0.2493 | -0.0007 | Zig-zag |

**Observation:** Mini-batch made largest progress! (Due to more updates)

---

## Gradient Statistics

**For the same weight W[50,30] across methods:**

| Method | Mean Gradient | Std Dev | Min | Max | Signal-to-Noise |
|--------|--------------|---------|-----|-----|-----------------|
| **Batch** | 0.00634 | 0.0034 | -0.0245 | 0.0298 | 1.86 |
| **Mini-batch** | 0.00616 | 0.0156 | -0.0847 | 0.0923 | 0.39 |
| **SGD** | 0.00622 | 0.0892 | -0.3145 | 0.2891 | 0.07 |

**Signal-to-Noise Ratio = Mean / Std Dev**
- Higher is better (clearer signal)
- Batch: Best signal quality
- Mini-batch: Good balance
- SGD: Very noisy

---

# Part 5: The Mathematics

## Loss Function for Different Batch Sizes

### **Full Batch:**

$$L_{\text{batch}} = \frac{1}{N}\sum_{i=1}^{N}\mathcal{L}(\hat{y}_i, y_i)$$

Where N = total training set size (128 in our example)

---

### **Mini-Batch:**

$$L_{\text{mini}} = \frac{1}{B}\sum_{i=1}^{B}\mathcal{L}(\hat{y}_i, y_i)$$

Where B = batch size (e.g., 32)

**For one epoch with N samples and batch size B:**
- Number of mini-batches: $M = \lceil N/B \rceil$
- Total updates per epoch: M

---

### **Stochastic (Single Sample):**

$$L_{\text{sgd}} = \mathcal{L}(\hat{y}_i, y_i)$$

Just the loss from ONE sample (B = 1)

---

## Gradient Computation

### **Full Batch Gradient:**

$$\nabla_{\theta}L = \frac{1}{N}\sum_{i=1}^{N}\nabla_{\theta}\mathcal{L}(\hat{y}_i, y_i)$$

**Expanded for one weight:**
$$\frac{\partial L}{\partial w} = \frac{1}{N}\left(\frac{\partial \mathcal{L}_1}{\partial w} + \frac{\partial \mathcal{L}_2}{\partial w} + ... + \frac{\partial \mathcal{L}_N}{\partial w}\right)$$

---

### **Mini-Batch Gradient:**

$$\nabla_{\theta}L = \frac{1}{B}\sum_{i=1}^{B}\nabla_{\theta}\mathcal{L}(\hat{y}_i, y_i)$$

**For mini-batch j:**
$$\frac{\partial L_j}{\partial w} = \frac{1}{B}\sum_{i \in \mathcal{B}_j}\frac{\partial \mathcal{L}_i}{\partial w}$$

Where $\mathcal{B}_j$ is the set of samples in mini-batch j

---

### **Stochastic Gradient:**

$$\nabla_{\theta}L = \nabla_{\theta}\mathcal{L}(\hat{y}_i, y_i)$$

No averaging, just gradient from sample i

---

## Expected Value of Gradients

**Key insight:** All three are **unbiased estimators** of the true gradient!

$$\mathbb{E}[\nabla_{\text{mini-batch}}] = \mathbb{E}[\nabla_{\text{SGD}}] = \nabla_{\text{batch}}$$

**But variance differs:**

$$\text{Var}(\nabla_{\text{batch}}) = 0 \quad \text{(deterministic)}$$

$$\text{Var}(\nabla_{\text{mini-batch}}) = \frac{\sigma^2}{B}$$

$$\text{Var}(\nabla_{\text{SGD}}) = \sigma^2$$

Where $\sigma^2$ is the variance of individual sample gradients

**Relationship:**
$$\text{Var}(\nabla_{\text{mini-batch}}) = \frac{1}{B} \times \text{Var}(\nabla_{\text{SGD}})$$

Larger batch size â†’ Lower variance

---

# Part 6: Choosing Batch Size

## The Trade-offs

### **Small Batch Size (e.g., 1-8):**

```
Pros:
âœ“ Fast updates (more per epoch)
âœ“ Regularization effect (noise helps escape local minima)
âœ“ Low memory usage
âœ“ Better generalization (sometimes)

Cons:
âœ— Noisy gradients
âœ— Can't exploit GPU parallelism well
âœ— Unstable training
âœ— Requires careful learning rate tuning
```

---

### **Large Batch Size (e.g., 256-1024):**

```
Pros:
âœ“ Smooth, stable gradients
âœ“ Better GPU utilization
âœ“ Can use larger learning rates
âœ“ More efficient computation

Cons:
âœ— Slower updates (fewer per epoch)
âœ— More memory required
âœ— Can get stuck in sharp minima
âœ— Worse generalization (sometimes)
âœ— Requires more epochs to converge
```

---

### **Medium Batch Size (e.g., 32-128):**

```
The sweet spot! âœ“

Pros:
âœ“ Good balance of speed and stability
âœ“ Reasonable GPU utilization
âœ“ Manageable memory
âœ“ Good generalization

This is why 32, 64, 128 are most common!
```

---

## Numerical Example: Different Batch Sizes

**Same network, same 1024 images, trained for 10 epochs:**

| Batch Size | Updates/Epoch | Total Updates | Time/Epoch | Final Train Acc | Final Test Acc | Best? |
|-----------|---------------|---------------|-----------|----------------|---------------|-------|
| **1** (SGD) | 1024 | 10,240 | 8.5s | 98% | 88% | âœ— Slow |
| **8** | 128 | 1,280 | 3.2s | 97% | 91% | Good |
| **16** | 64 | 640 | 2.1s | 96% | 92% | Good |
| **32** | 32 | 320 | 1.5s | 95% | **93%** | âœ“ Best! |
| **64** | 16 | 160 | 1.2s | 94% | 92% | Good |
| **128** | 8 | 80 | 1.0s | 93% | 91% | Good |
| **256** | 4 | 40 | 0.9s | 91% | 88% | âœ— Underfit |
| **1024** (Batch) | 1 | 10 | 0.8s | 85% | 82% | âœ— Underfit |

**Observations:**

```
Batch size 32:
- Best test accuracy (93%)
- Reasonable training time (1.5s/epoch)
- Good balance of everything

Batch size 8-16:
- Also excellent
- Slightly better generalization
- But slower training

Batch size 256+:
- Fast per epoch
- But need many more epochs to converge
- Worse generalization
```

---

## Practical Guidelines

### **By Problem Type:**

```
Image Classification (CNNs):
â†’ Batch size: 32-128
  Reason: Good features, parallelizable

Text/Sequences (RNNs/Transformers):
â†’ Batch size: 16-64
  Reason: Variable lengths, memory intensive

Small datasets (<1000 samples):
â†’ Batch size: 8-32
  Reason: More updates per epoch needed

Large datasets (>100K samples):
â†’ Batch size: 64-256
  Reason: More efficient, still many updates
```

---

### **By GPU Memory:**

```
GPU Memory: 4GB
â†’ Batch size: 16-32 (typical)

GPU Memory: 8GB
â†’ Batch size: 32-64

GPU Memory: 16GB+
â†’ Batch size: 64-128+

Rule of thumb:
If GPU memory allows, use batch size in [32, 128]
If memory limited, reduce batch size and adjust learning rate
```

---

### **The "Linear Scaling Rule":**

When you change batch size, adjust learning rate proportionally:

$$\alpha_{\text{new}} = \alpha_{\text{old}} \times \frac{B_{\text{new}}}{B_{\text{old}}}$$

**Example:**
```
Original: Batch size 32, Learning rate 0.1

If you change to batch size 128:
New learning rate = 0.1 Ã— (128/32) = 0.1 Ã— 4 = 0.4

Why? Larger batches â†’ smoother gradients â†’ can take bigger steps
```

---

# Part 7: Complete Training Example

## Cat vs Dog Classifier with Mini-Batch GD

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Generate synthetic data for demonstration
torch.manual_seed(42)
X_train = torch.randn(1024, 1000)  # 1024 images, 1000 features
y_train = torch.randint(0, 2, (1024,))  # Binary labels (cat=0, dog=1)

X_test = torch.randn(256, 1000)
y_test = torch.randint(0, 2, (256,))

# Create datasets and dataloaders
train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

# Try different batch sizes
batch_sizes = [1, 8, 32, 128, 1024]

for batch_size in batch_sizes:
    print(f"\n{'='*50}")
    print(f"Training with Batch Size: {batch_size}")
    print(f"{'='*50}")
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=256,  # Test batch size can be larger
        shuffle=False
    )
    
    # Define model
    class CatDogNet(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(1000, 100)
            self.fc2 = nn.Linear(100, 2)
        
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = self.fc2(x)
            return x
    
    model = CatDogNet()
    
    # Adjust learning rate based on batch size (linear scaling)
    base_lr = 0.01
    lr = base_lr * (batch_size / 32)  # Scale relative to batch size 32
    
    optimizer = optim.SGD(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()
    
    # Track metrics
    import time
    start_time = time.time()
    
    # Training
    model.train()
    n_updates = 0
    
    for epoch in range(3):
        epoch_loss = 0
        n_correct = 0
        n_samples = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # Forward pass
            output = model(data)
            loss = criterion(output, target)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Track metrics
            epoch_loss += loss.item()
            pred = output.argmax(dim=1)
            n_correct += (pred == target).sum().item()
            n_samples += len(target)
            n_updates += 1
            
            # Print first few batches of first epoch
            if epoch == 0 and batch_idx < 3:
                print(f"  Epoch 0, Batch {batch_idx}:")
                print(f"    Batch loss: {loss.item():.4f}")
                print(f"    Batch accuracy: {(pred == target).float().mean():.2%}")
        
        # Epoch summary
        avg_loss = epoch_loss / len(train_loader)
        accuracy = n_correct / n_samples
        print(f"\nEpoch {epoch}: Loss = {avg_loss:.4f}, "
              f"Accuracy = {accuracy:.2%}")
    
    # Testing
    model.eval()
    test_correct = 0
    test_total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            pred = output.argmax(dim=1)
            test_correct += (pred == target).sum().item()
            test_total += len(target)
    
    test_accuracy = test_correct / test_total
    
    # Summary
    elapsed_time = time.time() - start_time
    updates_per_epoch = len(train_loader)
    
    print(f"\n{'='*50}")
    print(f"Summary for Batch Size {batch_size}:")
    print(f"  Updates per epoch: {updates_per_epoch}")
    print(f"  Total updates: {n_updates}")
    print(f"  Training time: {elapsed_time:.2f}s")
    print(f"  Time per epoch: {elapsed_time/3:.2f}s")
    print(f"  Final train accuracy: {accuracy:.2%}")
    print(f"  Test accuracy: {test_accuracy:.2%}")
    print(f"{'='*50}")
```

---

**Expected Output:**

```
==================================================
Training with Batch Size: 1
==================================================
  Epoch 0, Batch 0:
    Batch loss: 0.7234
    Batch accuracy: 50.00%
  Epoch 0, Batch 1:
    Batch loss: 0.6891
    Batch accuracy: 100.00%
  Epoch 0, Batch 2:
    Batch loss: 0.7012
    Batch accuracy: 0.00%

Epoch 0: Loss = 0.6945, Accuracy = 51.27%
Epoch 1: Loss = 0.6823, Accuracy = 58.89%
Epoch 2: Loss = 0.6701, Accuracy = 63.48%

==================================================
Summary for Batch Size 1:
  Updates per epoch: 1024
  Total updates: 3072
  Training time: 8.34s
  Time per epoch: 2.78s
  Final train accuracy: 63.48%
  Test accuracy: 59.77%
==================================================

==================================================
Training with Batch Size: 32
==================================================
  Epoch 0, Batch 0:
    Batch loss: 0.7123
    Batch accuracy: 53.12%
  Epoch 0, Batch 1:
    Batch loss: 0.6945
    Batch accuracy: 59.38%
  Epoch 0, Batch 2:
    Batch loss: 0.6834
    Batch accuracy: 62.50%

Epoch 0: Loss = 0.6891, Accuracy = 55.47%
Epoch 1: Loss = 0.6523, Accuracy = 67.19%
Epoch 2: Loss = 0.6234, Accuracy = 73.83%

==================================================
Summary for Batch Size 32:
  Updates per epoch: 32
  Total updates: 96
  Training time: 1.23s
  Time per epoch: 0.41s
  Final train accuracy: 73.83%
  Test accuracy: 71.48%
==================================================

==================================================
Training with Batch Size: 1024
==================================================
  Epoch 0, Batch 0:
    Batch loss: 0.6932
    Batch accuracy: 50.00%

Epoch 0: Loss = 0.6932, Accuracy = 50.00%
Epoch 1: Loss = 0.6931, Accuracy = 50.10%
Epoch 2: Loss = 0.6929, Accuracy = 50.20%

==================================================
Summary for Batch Size 1024:
  Updates per epoch: 1
  Total updates: 3
  Training time: 0.67s
  Time per epoch: 0.22s
  Final train accuracy: 50.20%
  Test accuracy: 50.78%
==================================================
```

**Analysis:**
```
Batch size 1 (SGD):
- Many updates (1024/epoch)
- Slow and noisy
- Moderate performance

Batch size 32:
- Balanced updates (32/epoch)
- Fast and stable
- BEST performance! âœ“

Batch size 1024 (Full batch):
- Very few updates (1/epoch)
- Fast per epoch but barely learning
- Needs many more epochs
```

---

# Part 8: Advanced Considerations

## Batch Size and Generalization

### **The Sharp vs Flat Minima Hypothesis**

```
Small batch (SGD):           Large batch:
Finds flat minimum           Finds sharp minimum

    Loss                         Loss
     â†‘                            â†‘
     â”‚  â•±â”€â”€â”€â”€â”€â•²                   â”‚  â•±â•²
     â”‚ â•±       â•²                  â”‚ â•±  â•²
     â”‚â•±    â—    â•²                 â”‚â•± â—  â•²
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ w             â””â”€â”€â”€â”€â”€â”€â†’ w
          â†‘                           â†‘
    Flat minimum                Sharp minimum
    (robust to                  (sensitive to
     parameter changes)          parameter changes)

Flat minima â†’ Better generalization!
Sharp minima â†’ Overfit!
```

**Why small batches find flat minima:**
```
Noisy gradients explore the loss landscape
Like annealing in physics
Escape sharp valleys, settle in wide valleys
```

---

## Batch Size and Learning Rate

### **The Critical Batch Size**

There's a point where increasing batch size no longer helps:

```
Test Accuracy vs Batch Size (with optimal LR)

  Acc
   â†‘
 95%â”‚     â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚    â•±
 90%â”‚   â•±
    â”‚  â•±
 85%â”‚ â•±
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Batch Size
      8  32  128  512
           â†‘
      Critical batch size
      (~128 for many problems)

Beyond this, larger batches don't improve generalization
even with perfect LR tuning!
```

---

## Gradient Accumulation (Simulating Large Batches)

**Problem:** Want large effective batch size but limited GPU memory

**Solution:** Accumulate gradients over multiple small batches

```python
# Simulate batch size of 128 with batches of 32
model = CatDogNet()
optimizer = optim.SGD(model.parameters(), lr=0.1)
accumulation_steps = 4  # 32 Ã— 4 = 128 effective batch size

optimizer.zero_grad()

for batch_idx, (data, target) in enumerate(train_loader):
    # Forward pass
    output = model(data)
    loss = criterion(output, target)
    
    # Normalize loss by accumulation steps
    loss = loss / accumulation_steps
    
    # Backward pass (accumulate gradients)
    loss.backward()
    
    # Update weights every 4 batches
    if (batch_idx + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
        print(f"Updated weights at batch {batch_idx + 1}")

# Output:
# Updated weights at batch 4
# Updated weights at batch 8
# Updated weights at batch 12
# ...
```

**Effect:**
```
Without accumulation (batch size 32):
- Update every 1 batch
- 32 samples per update
- Noisy gradients

With accumulation (4 batches):
- Update every 4 batches
- 128 samples per update
- Smoother gradients
- Same memory as batch size 32!
```

---

# Part 9: Batch Normalization and Batch Size

## How Batch Norm Depends on Batch Size

**Batch Normalization computes statistics over the batch:**

$$\mu_B = \frac{1}{B}\sum_{i=1}^{B}x_i$$

$$\sigma^2_B = \frac{1}{B}\sum_{i=1}^{B}(x_i - \mu_B)^2$$

$$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma^2_B + \epsilon}}$$

---

### **The Problem with Small Batches:**

```
Batch size 32:
Î¼_B = 0.05, Ïƒ_B = 1.02  (good estimate)

Batch size 2:
Î¼_B = 0.23, Ïƒ_B = 0.67  (noisy estimate!)

Batch size 1:
Î¼_B = x_1, Ïƒ_B = 0  (undefined!)
Can't normalize!
```

**Guideline:** If using Batch Norm, use batch size â‰¥ 16, preferably â‰¥ 32

---

### **Alternatives for Small Batches:**

**Layer Normalization (LayerNorm):**
```python
# Normalize across features instead of batch
layer_norm = nn.LayerNorm(hidden_size)

# Works with batch size 1!
x = torch.randn(1, hidden_size)  # Batch size 1
normalized = layer_norm(x)  # No problem!
```

**Group Normalization:**
```python
# Normalize within groups of channels
group_norm = nn.GroupNorm(num_groups=8, num_channels=32)

# Also works with batch size 1
```

---

# Part 10: Practical Tips and Best Practices

## Choosing Batch Size: Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      How to Choose Batch Size?          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Start with batch size 32 (good default)

Is training unstable (loss oscillating wildly)?
â”œâ”€ YES â†’ Increase batch size (64 or 128)
â”‚         Larger batches = more stable gradients
â”‚
â””â”€ NO â†’ Continue

Is training too slow (taking forever)?
â”œâ”€ YES â†’ Check GPU utilization
â”‚         â”œâ”€ Low utilization? â†’ Increase batch size
â”‚         â””â”€ High utilization? â†’ Keep current size
â”‚
â””â”€ NO â†’ Continue

Is test accuracy much lower than train?
â”œâ”€ YES â†’ Might be overfitting
â”‚         Try DECREASING batch size (16 or 8)
â”‚         Smaller batches = better generalization
â”‚
â””â”€ NO â†’ You're good! âœ“

Using Batch Normalization?
â”œâ”€ YES â†’ Keep batch size â‰¥ 16, prefer â‰¥ 32
â””â”€ NO â†’ Batch size 8-16 is fine

Memory limited?
â”œâ”€ YES â†’ Use smallest batch size that fits
â”‚         Use gradient accumulation to simulate larger
â””â”€ NO â†’ Enjoy the freedom!
```

---

## Tuning Learning Rate with Batch Size

### **The Learning Rate Range Test:**

```python
# Find optimal learning rate for your batch size
def find_lr(model, train_loader, init_lr=1e-8, final_lr=10):
    optimizer = optim.SGD(model.parameters(), lr=init_lr)
    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(
        optimizer,
        gamma=(final_lr/init_lr)**(1/len(train_loader))
    )
    
    lrs = []
    losses = []
    
    for batch_idx, (data, target) in enumerate(train_loader):
        # Forward and backward
        output = model(data)
        loss = criterion(output, target)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Record
        lrs.append(lr_scheduler.get_last_lr()[0])
        losses.append(loss.item())
        
        # Update LR
        lr_scheduler.step()
        
        # Stop if loss explodes
        if loss.item() > losses[0] * 10:
            break
    
    # Plot
    import matplotlib.pyplot as plt
    plt.plot(lrs, losses)
    plt.xscale('log')
    plt.xlabel('Learning Rate')
    plt.ylabel('Loss')
    plt.title('Learning Rate Range Test')
    plt.show()
    
    # Optimal LR is usually where loss decreases fastest
    return lrs, losses

# Run for different batch sizes
for batch_size in [16, 32, 64, 128]:
    print(f"\nFinding optimal LR for batch size {batch_size}")
    train_loader = DataLoader(train_dataset, batch_size=batch_size)
    model = CatDogNet()
    find_lr(model, train_loader)
```

**Typical results:**
```
Batch size 16:  Optimal LR â‰ˆ 0.01
Batch size 32:  Optimal LR â‰ˆ 0.02
Batch size 64:  Optimal LR â‰ˆ 0.04
Batch size 128: Optimal LR â‰ˆ 0.08

Pattern: LR scales roughly linearly with batch size
```

---

## Common Mistakes to Avoid

### **âŒ Mistake 1: Using batch size that doesn't divide dataset size**

```python
# BAD: 1000 samples, batch size 32
# Last batch has only 8 samples!
train_loader = DataLoader(dataset, batch_size=32)

# GOOD: Either drop last batch or handle gracefully
train_loader = DataLoader(
    dataset,
    batch_size=32,
    drop_last=True  # Drop incomplete batch
)
```

---

### **âŒ Mistake 2: Not adjusting LR when changing batch size**

```python
# BAD: Same LR for different batch sizes
for batch_size in [16, 32, 64]:
    optimizer = optim.SGD(model.parameters(), lr=0.01)  # Always 0.01!

# GOOD: Scale LR with batch size
for batch_size in [16, 32, 64]:
    lr = 0.01 * (batch_size / 32)  # Scale relative to base
    optimizer = optim.SGD(model.parameters(), lr=lr)
```

---

### **âŒ Mistake 3: Forgetting to shuffle**

```python
# BAD: No shuffling
train_loader = DataLoader(dataset, batch_size=32, shuffle=False)
# Model sees images in same order every epoch!
# First batch always cats, second always dogs â†’ poor learning

# GOOD: Shuffle each epoch
train_loader = DataLoader(dataset, batch_size=32, shuffle=True)
# Different mini-batches each epoch â†’ better generalization
```

---

### **âŒ Mistake 4: Different batch sizes for train and validation**

```python
# PROBLEMATIC: If using Batch Norm
train_loader = DataLoader(train_set, batch_size=32)
val_loader = DataLoader(val_set, batch_size=256)  # Different!

# Batch Norm statistics differ greatly
# Can cause validation performance to differ from training

# BETTER: Same or similar batch sizes
train_loader = DataLoader(train_set, batch_size=32)
val_loader = DataLoader(val_set, batch_size=32)

# OR: Use eval mode (running stats instead of batch stats)
model.eval()  # Switches to running mean/var
```

---

# Part 11: Summary

## The Complete Picture

### **Three Approaches Compared:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Gradient Descent Methods                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚ Batch GD (Full Batch):                             â”‚
â”‚   Batch Size = N (all data)                        â”‚
â”‚   Updates/Epoch = 1                                â”‚
â”‚   Pros: Stable, deterministic                       â”‚
â”‚   Cons: Slow, memory intensive                      â”‚
â”‚   Use: Small datasets, when stability critical     â”‚
â”‚                                                      â”‚
â”‚ Mini-Batch GD: âœ“ RECOMMENDED                       â”‚
â”‚   Batch Size = 16-128 (typically 32)              â”‚
â”‚   Updates/Epoch = N/B                              â”‚
â”‚   Pros: Balanced speed/stability, GPU efficient    â”‚
â”‚   Cons: Requires batch size tuning                 â”‚
â”‚   Use: Almost always! Default choice               â”‚
â”‚                                                      â”‚
â”‚ Stochastic GD (SGD):                               â”‚
â”‚   Batch Size = 1                                   â”‚
â”‚   Updates/Epoch = N                                â”‚
â”‚   Pros: Fast per update, regularization effect    â”‚
â”‚   Cons: Very noisy, unstable, slow overall        â”‚
â”‚   Use: Rarely alone, mostly for theory            â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Key Formulas:**

**Mini-Batch Loss:**
$$L_{\text{mini}} = \frac{1}{B}\sum_{i=1}^{B}\mathcal{L}(\hat{y}_i, y_i)$$

**Mini-Batch Gradient:**
$$\nabla_{\theta}L = \frac{1}{B}\sum_{i=1}^{B}\nabla_{\theta}\mathcal{L}(\hat{y}_i, y_i)$$

**Gradient Variance:**
$$\text{Var}(\nabla_{\text{mini}}) = \frac{\sigma^2}{B}$$

**Learning Rate Scaling:**
$$\alpha_{\text{new}} = \alpha_{\text{base}} \times \frac{B_{\text{new}}}{B_{\text{base}}}$$

---

### **Practical Guidelines:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Batch Size Recommendations        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                          â”‚
â”‚ Default: 32                              â”‚
â”‚   Good balance for most problems         â”‚
â”‚                                          â”‚
â”‚ Image Classification: 32-128             â”‚
â”‚   Depends on GPU memory                  â”‚
â”‚                                          â”‚
â”‚ NLP/Sequences: 16-64                     â”‚
â”‚   Variable lengths need more memory      â”‚
â”‚                                          â”‚
â”‚ Small Dataset (<1000): 8-32              â”‚
â”‚   Need more updates per epoch            â”‚
â”‚                                          â”‚
â”‚ Large Dataset (>100K): 64-256            â”‚
â”‚   Efficiency matters more                â”‚
â”‚                                          â”‚
â”‚ With Batch Norm: â‰¥16, prefer â‰¥32        â”‚
â”‚   Need good batch statistics             â”‚
â”‚                                          â”‚
â”‚ GPU Memory Limited: Smallest that fits   â”‚
â”‚   + gradient accumulation                â”‚
â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **The Magic Formula:**

For most problems, this works well:

```python
# The "standard recipe"
batch_size = 32
learning_rate = 0.001  # (with Adam optimizer)
# OR
learning_rate = 0.1    # (with SGD optimizer)

# If you change batch size:
if new_batch_size != 32:
    learning_rate = learning_rate * (new_batch_size / 32)
```

---

### **Debugging Checklist:**

```
Training issues? Check:

â–¡ Batch size in [8, 256] range?
â–¡ Learning rate scaled with batch size?
â–¡ Shuffling enabled for training?
â–¡ Using shuffle=True in DataLoader?
â–¡ Batch Norm with batch size â‰¥16?
â–¡ Last batch size consistent (use drop_last)?
â–¡ Test with larger batches if possible?
â–¡ Monitor gradient norms (too large/small)?
â–¡ Loss oscillating wildly? â†’ Increase batch size
â–¡ Loss barely moving? â†’ Decrease batch size
```

---

**You now understand mini-batch gradient descent completely! ğŸ‰**

The key insights:
- **Mini-batches balance speed and stability**
- **Batch size affects gradient noise and convergence**
- **Larger batches need larger learning rates (linear scaling)**
- **32-128 is the sweet spot for most problems**
- **Small batches help generalization, large batches help efficiency**




---

# Exponentially Weighted Averages: Complete Explanation
## The Foundation for Modern Optimizers
### (Detailed Step-by-Step with Temperature Example)

---

## ğŸ”— **Connection to Previous Topics**

### **What We Know So Far:**

**From Mini-Batch Gradient Descent:**
```
We update weights using:
w := w - Î±Â·âˆ‚L/âˆ‚w

But gradients can be noisy (especially with small batches)
Can we smooth them out somehow?
```

**The New Concept:**

```
Instead of using raw gradients directly,
what if we use a SMOOTHED version?

Exponentially Weighted Average (EWA) = Moving average
that gives more weight to recent values!
```

---

# Part 1: Understanding Exponentially Weighted Averages

## 1. Plain English Explanation

### The Core Idea

**Exponentially Weighted Average:** "Remember the past, but recent values matter more"

### Real-World Analogy: Temperature Tracking

Imagine tracking daily temperature in a city:

```
Raw Daily Temperatures (Â°C):
Day 1:  18Â°C
Day 2:  23Â°C  â† Sudden spike!
Day 3:  19Â°C
Day 4:  20Â°C
Day 5:  17Â°C
Day 6:  21Â°C
Day 7:  19Â°C
Day 8:  18Â°C
Day 9:  22Â°C  â† Another spike!
Day 10: 20Â°C
```

**Problem:** Daily temperatures jump around a lot (noisy!)

**Solution:** Use a smoothed average!

---

### Simple Average (Last N Days)

```
Average of last 3 days:

Day 3 avg = (18 + 23 + 19) / 3 = 20.0Â°C
Day 4 avg = (23 + 19 + 20) / 3 = 20.7Â°C
Day 5 avg = (19 + 20 + 17) / 3 = 18.7Â°C
Day 6 avg = (20 + 17 + 21) / 3 = 19.3Â°C
...

Smoother, but ALL 3 days weighted equally!
Day 1 and Day 3 have same importance!
```

---

### Exponentially Weighted Average (EWA)

```
Give MORE weight to recent days!

Day 1:  vâ‚ = 18Â°C (starting value)

Day 2:  vâ‚‚ = 0.9 Ã— vâ‚ + 0.1 Ã— 23Â°C
           = 0.9 Ã— 18 + 0.1 Ã— 23
           = 16.2 + 2.3
           = 18.5Â°C

Day 3:  vâ‚ƒ = 0.9 Ã— vâ‚‚ + 0.1 Ã— 19Â°C
           = 0.9 Ã— 18.5 + 0.1 Ã— 19
           = 16.65 + 1.9
           = 18.55Â°C

Day 4:  vâ‚„ = 0.9 Ã— vâ‚ƒ + 0.1 Ã— 20Â°C
           = 0.9 Ã— 18.55 + 0.1 Ã— 20
           = 16.695 + 2.0
           = 18.695Â°C

Day 5:  vâ‚… = 0.9 Ã— vâ‚„ + 0.1 Ã— 17Â°C
           = 0.9 Ã— 18.695 + 0.1 Ã— 17
           = 16.8255 + 1.7
           = 18.5255Â°C
```

**Notice:**
- Recent days matter more (weight 0.1 = 10%)
- Past values decay (weight 0.9 = 90% of previous average)
- Creates smooth curve!

---

## 2. The Mathematics

### General Formula:

$$v_t = \beta v_{t-1} + (1-\beta)\theta_t$$

Where:
- $v_t$ = Exponentially weighted average at time t
- $\theta_t$ = Actual value at time t (e.g., temperature, gradient)
- $\beta$ = Decay parameter (typically 0.9 to 0.999)
- $(1-\beta)$ = Weight given to current value

**Expanded form:**
$$v_t = (1-\beta)\theta_t + \beta v_{t-1}$$
$$v_t = (1-\beta)\theta_t + \beta[(1-\beta)\theta_{t-1} + \beta v_{t-2}]$$
$$v_t = (1-\beta)\theta_t + (1-\beta)\beta\theta_{t-1} + \beta^2v_{t-2}$$

Continuing this expansion:
$$v_t = (1-\beta)\sum_{i=0}^{t-1}\beta^i\theta_{t-i} + \beta^tv_0$$

---

### Key Components:

| Symbol | Name | Meaning |
|--------|------|---------|
| $v_t$ | EWA at time t | Running average up to time t |
| $\theta_t$ | Current value | New data point (temperature, gradient, etc.) |
| $\beta$ | Decay factor | How much to weight previous average (0.9-0.999) |
| $(1-\beta)$ | Current weight | How much to weight new value (0.001-0.1) |

---

### What Does Î² Control?

**Approximate number of values being averaged:**

$$\text{Effective window} \approx \frac{1}{1-\beta}$$

**Examples:**

```
Î² = 0.9   â†’ 1/(1-0.9) = 1/0.1 = 10 days
Î² = 0.95  â†’ 1/(1-0.95) = 1/0.05 = 20 days
Î² = 0.98  â†’ 1/(1-0.98) = 1/0.02 = 50 days
Î² = 0.99  â†’ 1/(1-0.99) = 1/0.01 = 100 days
Î² = 0.999 â†’ 1/(1-0.999) = 1/0.001 = 1000 days
```

**Intuition:**
- Small Î² (e.g., 0.5): Fast adaptation, follows recent values closely
- Large Î² (e.g., 0.999): Slow adaptation, very smooth but lags behind

---

## 3. Complete Numerical Example: Temperature Data

### Setup:

```
Daily temperatures for 10 days:
Day 1:  Î¸â‚ = 18Â°C
Day 2:  Î¸â‚‚ = 23Â°C
Day 3:  Î¸â‚ƒ = 19Â°C
Day 4:  Î¸â‚„ = 20Â°C
Day 5:  Î¸â‚… = 17Â°C
Day 6:  Î¸â‚† = 21Â°C
Day 7:  Î¸â‚‡ = 19Â°C
Day 8:  Î¸â‚ˆ = 18Â°C
Day 9:  Î¸â‚‰ = 22Â°C
Day 10: Î¸â‚â‚€ = 20Â°C

We'll compute EWA with different Î² values.
```

---

### Case 1: Î² = 0.9 (Fast Adaptation)

**Initialization:**
```
vâ‚€ = 0  (or we can use Î¸â‚ = 18)
Let's use vâ‚€ = 0 to show the issue
```

**Day 1:**
```
vâ‚ = Î²Â·vâ‚€ + (1-Î²)Â·Î¸â‚
   = 0.9 Ã— 0 + 0.1 Ã— 18
   = 0 + 1.8
   = 1.8Â°C  â† Way too low! (true value is 18Â°C)
```

**Day 2:**
```
vâ‚‚ = 0.9 Ã— 1.8 + 0.1 Ã— 23
   = 1.62 + 2.3
   = 3.92Â°C  â† Still too low!
```

**Day 3:**
```
vâ‚ƒ = 0.9 Ã— 3.92 + 0.1 Ã— 19
   = 3.528 + 1.9
   = 5.428Â°C  â† Getting closer but slow...
```

**Day 4:**
```
vâ‚„ = 0.9 Ã— 5.428 + 0.1 Ã— 20
   = 4.8852 + 2.0
   = 6.8852Â°C
```

**Day 5:**
```
vâ‚… = 0.9 Ã— 6.8852 + 0.1 Ã— 17
   = 6.19668 + 1.7
   = 7.89668Â°C
```

**Continue for remaining days:**

| Day | Î¸ (actual) | v (EWA, Î²=0.9) | Error |
|-----|-----------|---------------|-------|
| 1 | 18Â°C | 1.8Â°C | -16.2Â°C |
| 2 | 23Â°C | 3.92Â°C | -19.08Â°C |
| 3 | 19Â°C | 5.428Â°C | -13.572Â°C |
| 4 | 20Â°C | 6.8852Â°C | -13.115Â°C |
| 5 | 17Â°C | 7.89668Â°C | -9.103Â°C |
| 6 | 21Â°C | 9.40701Â°C | -11.593Â°C |
| 7 | 19Â°C | 10.36631Â°C | -8.634Â°C |
| 8 | 18Â°C | 11.12968Â°C | -6.870Â°C |
| 9 | 22Â°C | 12.21671Â°C | -9.783Â°C |
| 10 | 20Â°C | 12.99504Â°C | -7.005Â°C |

**Problem:** Takes ~10 days to reach reasonable values!
This is the **initialization bias** problem.

---

### Case 2: Î² = 0.98 (Slow Adaptation)

**Same calculation:**

| Day | Î¸ (actual) | v (EWA, Î²=0.98) | Error |
|-----|-----------|----------------|-------|
| 1 | 18Â°C | 0.36Â°C | -17.64Â°C |
| 2 | 23Â°C | 0.8128Â°C | -22.187Â°C |
| 3 | 19Â°C | 1.1765Â°C | -17.824Â°C |
| 4 | 20Â°C | 1.5530Â°C | -18.447Â°C |
| 5 | 17Â°C | 1.8619Â°C | -15.138Â°C |
| 6 | 21Â°C | 2.2647Â°C | -18.735Â°C |
| 7 | 19Â°C | 2.5994Â°C | -16.401Â°C |
| 8 | 18Â°C | 2.8674Â°C | -15.133Â°C |
| 9 | 22Â°C | 3.2501Â°C | -18.750Â°C |
| 10 | 20Â°C | 3.5851Â°C | -16.415Â°C |

**Even worse!** Larger Î² = slower warmup!

---

## 4. Why This Happens (The Math Behind It)

### Expansion of v_t:

Starting from vâ‚€ = 0:

$$v_1 = (1-\beta)\theta_1$$
$$v_2 = (1-\beta)\theta_2 + \beta(1-\beta)\theta_1$$
$$v_3 = (1-\beta)\theta_3 + \beta(1-\beta)\theta_2 + \beta^2(1-\beta)\theta_1$$

**General pattern:**
$$v_t = (1-\beta)\sum_{i=1}^{t}\beta^{t-i}\theta_i$$

**The problem:**
```
For small t, the sum of coefficients < 1:

Sum of weights = (1-Î²)(1 + Î² + Î²Â² + ... + Î²^(t-1))
               = (1-Î²) Ã— (1-Î²^t)/(1-Î²)
               = 1 - Î²^t

For t=1: Sum = 1 - Î²Â¹ = 1 - 0.9 = 0.1  â† Only 10% weight!
For t=2: Sum = 1 - Î²Â² = 1 - 0.81 = 0.19 â† Only 19% weight!
For t=5: Sum = 1 - Î²âµ = 1 - 0.59 = 0.41 â† Only 41% weight!
For t=10: Sum = 1 - Î²Â¹â° = 1 - 0.35 = 0.65 â† Getting better
For t=100: Sum = 1 - Î²Â¹â°â° â‰ˆ 0.9999 â† Finally close to 1!

Early estimates are systematically too low!
```

---

## 5. Visualizing the Problem

### Temperature Example:

```
    Temperature (Â°C)
         â†‘
      25â”‚
        â”‚    â—                    â—
      20â”‚  â—   â—   â—     â—   â—     â— â† Actual temperatures
        â”‚        â—     â—   â—   â—
      15â”‚
        â”‚
      10â”‚
        â”‚
       5â”‚      â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ EWA without bias correction
        â”‚     â•±
       0â”‚â”€â”€â”€â”€â•±
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Days
         0   2   4   6   8   10

The EWA starts at 0 and slowly climbs up!
Heavily biased downward initially!
```

---

# Part 2: Bias Correction

## 1. The Solution

### Bias-Corrected Formula:

$$\hat{v}_t = \frac{v_t}{1 - \beta^t}$$

Where:
- $v_t$ = Raw EWA (biased)
- $\hat{v}_t$ = Bias-corrected EWA (unbiased)
- $\beta^t$ = Î² raised to power t

**Why this works:**

```
Remember: Sum of weights in v_t = 1 - Î²^t

To make it sum to 1, divide by (1 - Î²^t)!

Early on (small t):
- Î²^t is large (e.g., 0.9Â¹ = 0.9)
- 1 - Î²^t is small (e.g., 0.1)
- Division by small number â†’ scales UP significantly

Later (large t):
- Î²^t is tiny (e.g., 0.9Â¹â°â° â‰ˆ 0)
- 1 - Î²^t â‰ˆ 1
- Division by ~1 â†’ no scaling needed
```

---

## 2. Complete Numerical Example with Bias Correction

### Temperature Data with Î² = 0.9

**Without bias correction (same as before):**

| Day t | Î¸_t | v_t (biased) | 1-Î²^t | vÌ‚_t (corrected) | True avg |
|-------|-----|-------------|-------|----------------|----------|
| 1 | 18 | 1.8 | 0.1 | 1.8/0.1 = **18.0** | 18.0 âœ“ |
| 2 | 23 | 3.92 | 0.19 | 3.92/0.19 = **20.63** | 20.5 âœ“ |
| 3 | 19 | 5.428 | 0.271 | 5.428/0.271 = **20.03** | 20.0 âœ“ |
| 4 | 20 | 6.8852 | 0.344 | 6.8852/0.344 = **20.01** | 20.0 âœ“ |
| 5 | 17 | 7.897 | 0.410 | 7.897/0.410 = **19.26** | 19.4 âœ“ |
| 6 | 21 | 9.407 | 0.469 | 9.407/0.469 = **20.06** | 19.8 âœ“ |
| 7 | 19 | 10.366 | 0.522 | 10.366/0.522 = **19.86** | 19.7 âœ“ |
| 8 | 18 | 11.130 | 0.570 | 11.130/0.570 = **19.52** | 19.5 âœ“ |
| 9 | 22 | 12.217 | 0.613 | 12.217/0.613 = **19.93** | 19.8 âœ“ |
| 10 | 20 | 12.995 | 0.651 | 12.995/0.651 = **19.96** | 19.9 âœ“ |

**Amazing!** Bias correction fixed the initialization problem!

---

### Detailed Calculation for Day 1:

```
Raw EWA:
vâ‚ = 0.9 Ã— 0 + 0.1 Ã— 18 = 1.8Â°C

Bias correction factor:
1 - Î²^t = 1 - 0.9Â¹ = 1 - 0.9 = 0.1

Corrected EWA:
vÌ‚â‚ = vâ‚ / (1 - Î²^t)
   = 1.8 / 0.1
   = 18.0Â°C  â† Exactly the true value! âœ“
```

---

### Detailed Calculation for Day 5:

```
Raw EWA (from previous calculation):
vâ‚… = 7.89668Â°C

Bias correction factor:
Î²^t = 0.9âµ = 0.59049
1 - Î²^t = 1 - 0.59049 = 0.40951

Corrected EWA:
vÌ‚â‚… = 7.89668 / 0.40951
   = 19.28Â°C

True average of first 5 days:
(18 + 23 + 19 + 20 + 17) / 5 = 97 / 5 = 19.4Â°C

Very close! Error = 0.12Â°C
```

---

## 3. Visualizing Bias Correction

### Temperature Example:

```
    Temperature (Â°C)
         â†‘
      25â”‚
        â”‚    â—                    â—
      20â”‚  â—   â—   â—     â—   â—     â— â† Actual temperatures
        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Bias-corrected EWA
      15â”‚        â—     â—   â—   â—
        â”‚
      10â”‚
        â”‚
       5â”‚      â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Raw EWA (biased)
        â”‚     â•±
       0â”‚â”€â”€â”€â”€â•±
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Days
         0   2   4   6   8   10

With bias correction:
- Starts at correct level immediately!
- Tracks true average closely from day 1
- No "warm-up" period needed
```

---

### Evolution of Bias Correction Factor:

```
    1 - Î²^t
         â†‘
      1.0â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (no correction needed)
         â”‚
      0.8â”‚               â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â”‚             â•±
      0.6â”‚           â•±
         â”‚         â•±
      0.4â”‚       â•±
         â”‚     â•±
      0.2â”‚   â•±
         â”‚  â•±
      0.0â”‚â”€â•±
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Days (t)
          0   5   10  15  20

For Î² = 0.9:
- Day 1: Correct by dividing by 0.1 (Ã—10 multiplier!)
- Day 5: Correct by dividing by 0.41 (Ã—2.4 multiplier)
- Day 10: Correct by dividing by 0.65 (Ã—1.5 multiplier)
- Day 20: Correct by dividing by 0.88 (Ã—1.14 multiplier)
- Day 50: Correct by dividing by ~1.0 (Ã—1.0 - no correction needed)

Correction fades away as t increases!
```

---

## 4. Comparing Different Î² Values

### Temperature Data with Multiple Î²:

**Setup:** Same 10 days of temperature

| Day | Actual | Î²=0.5 | Î²=0.9 (corrected) | Î²=0.98 (corrected) | True Avg |
|-----|--------|-------|------------------|-------------------|----------|
| 1 | 18 | 18.0 | 18.0 | 18.0 | 18.0 |
| 2 | 23 | 20.5 | 20.6 | 20.9 | 20.5 |
| 3 | 19 | 19.8 | 20.0 | 20.6 | 20.0 |
| 4 | 20 | 19.9 | 20.0 | 20.4 | 20.0 |
| 5 | 17 | 18.4 | 19.3 | 19.9 | 19.4 |
| 6 | 21 | 19.7 | 20.1 | 20.2 | 19.8 |
| 7 | 19 | 19.4 | 19.9 | 19.9 | 19.7 |
| 8 | 18 | 18.7 | 19.5 | 19.7 | 19.5 |
| 9 | 22 | 20.3 | 19.9 | 20.1 | 19.8 |
| 10 | 20 | 20.2 | 20.0 | 20.0 | 19.9 |

**Observations:**

```
Î² = 0.5:
- Follows recent values very closely
- Reacts quickly to changes
- More volatile (wiggly)
- Window: ~2 days

Î² = 0.9:
- Balanced smoothing
- Moderate reaction time
- Reasonably stable
- Window: ~10 days

Î² = 0.98:
- Very smooth
- Slow to react to changes
- Very stable
- Window: ~50 days
```

---

### Visual Comparison:

```
    Temperature
         â†‘
      24â”‚        â—
        â”‚      â•± | â•²
      22â”‚    â—  |  â—                    â— Actual
        â”‚   â•±   |   â•²   
      20â”‚  â•±â”€â”€â”€â”€â”¼â”€â”€â”€â”€â•²â”€â”€â”€â”€â”€â”€           â”€â”€ Î²=0.9 (balanced)
        â”‚â—â•±     |     â•²â—    â—
      18â”‚       |      â—  â—             Â·Â·Â·Â· Î²=0.5 (reactive)
        â”‚       |
      16â”‚       |                       â”€ â”€ Î²=0.98 (smooth)
        â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Days
                â†‘
           Big spike - see how
           each Î² reacts differently!
           
Î²=0.5:  Follows spike closely (reactive)
Î²=0.9:  Moderate response (balanced)
Î²=0.98: Barely notices spike (smooth)
```

---

## 5. Application to Neural Network Gradients

### The Problem: Noisy Gradients

**Training a network with mini-batch gradient descent:**

```
Batch 1: âˆ‚L/âˆ‚w = 0.023
Batch 2: âˆ‚L/âˆ‚w = -0.018  â† Different sign!
Batch 3: âˆ‚L/âˆ‚w = 0.031
Batch 4: âˆ‚L/âˆ‚w = 0.012
Batch 5: âˆ‚L/âˆ‚w = -0.008
Batch 6: âˆ‚L/âˆ‚w = 0.025
Batch 7: âˆ‚L/âˆ‚w = 0.019
Batch 8: âˆ‚L/âˆ‚w = -0.005
Batch 9: âˆ‚L/âˆ‚w = 0.028
Batch 10: âˆ‚L/âˆ‚w = 0.015

Gradients oscillate! Hard to make consistent progress.
```

---

### Solution: EWA of Gradients

**Standard gradient descent:**
```
w := w - Î±Â·âˆ‚L/âˆ‚w
```

**With EWA of gradients:**
```
v := Î²Â·v + (1-Î²)Â·âˆ‚L/âˆ‚w  (compute EWA)
vÌ‚ := v / (1 - Î²^t)      (bias correction)
w := w - Î±Â·vÌ‚            (update using smoothed gradient)
```

---

### Numerical Example:

**Setup:**
```
Weight: w = 0.250
Learning rate: Î± = 0.1
Î² = 0.9
vâ‚€ = 0 (initial EWA)
```

**Batch 1:**
```
Gradient: âˆ‚L/âˆ‚w = 0.023

EWA:
vâ‚ = 0.9 Ã— 0 + 0.1 Ã— 0.023 = 0.0023

Bias correction:
vÌ‚â‚ = 0.0023 / (1 - 0.9Â¹) = 0.0023 / 0.1 = 0.023

Update:
w := 0.250 - 0.1 Ã— 0.023 = 0.250 - 0.0023 = 0.2477
```

**Batch 2:**
```
Gradient: âˆ‚L/âˆ‚w = -0.018

EWA:
vâ‚‚ = 0.9 Ã— 0.0023 + 0.1 Ã— (-0.018)
   = 0.00207 - 0.0018
   = 0.00027

Bias correction:
vÌ‚â‚‚ = 0.00027 / (1 - 0.9Â²)
   = 0.00027 / 0.19
   = 0.00142

Update:
w := 0.2477 - 0.1 Ã— 0.00142 = 0.2477 - 0.000142 = 0.247558
```

**Batch 3:**
```
Gradient: âˆ‚L/âˆ‚w = 0.031

EWA:
vâ‚ƒ = 0.9 Ã— 0.00027 + 0.1 Ã— 0.031
   = 0.000243 + 0.0031
   = 0.003343

Bias correction:
vÌ‚â‚ƒ = 0.003343 / (1 - 0.9Â³)
   = 0.003343 / 0.271
   = 0.01234

Update:
w := 0.247558 - 0.1 Ã— 0.01234 = 0.247558 - 0.001234 = 0.246324
```

---

### Complete Table (First 10 Batches):

| Batch | Gradient | v (raw) | 1-Î²^t | vÌ‚ (corrected) | Weight Update |
|-------|----------|---------|-------|--------------|---------------|
| 1 | 0.023 | 0.0023 | 0.1 | 0.0230 | w -= 0.0023 |
| 2 | -0.018 | 0.00027 | 0.19 | 0.0014 | w -= 0.00014 |
| 3 | 0.031 | 0.003343 | 0.271 | 0.0123 | w -= 0.00123 |
| 4 | 0.012 | 0.004209 | 0.344 | 0.0122 | w -= 0.00122 |
| 5 | -0.008 | 0.002988 | 0.410 | 0.0073 | w -= 0.00073 |
| 6 | 0.025 | 0.005189 | 0.469 | 0.0111 | w -= 0.00111 |
| 7 | 0.019 | 0.006570 | 0.522 | 0.0126 | w -= 0.00126 |
| 8 | -0.005 | 0.005413 | 0.570 | 0.0095 | w -= 0.00095 |
| 9 | 0.028 | 0.007672 | 0.613 | 0.0125 | w -= 0.00125 |
| 10 | 0.015 | 0.008404 | 0.651 | 0.0129 | w -= 0.00129 |

---

### Analysis:

**Without bias correction:**
```
Batch 1: vâ‚ = 0.0023 (way too small!)
Batch 2: vâ‚‚ = 0.00027 (even smaller!)
â†’ Tiny weight updates initially
â†’ Slow learning at start
```

**With bias correction:**
```
Batch 1: vÌ‚â‚ = 0.023 (correct magnitude!)
Batch 2: vÌ‚â‚‚ = 0.0014 (reasonable)
â†’ Proper-sized weight updates from start
â†’ Fast learning immediately
```

**After ~20 batches:**
```
1 - Î²Â²â° = 1 - 0.9Â²â° â‰ˆ 0.878

Bias correction factor â‰ˆ 1.14 (minimal)
Raw EWA is good enough!
Bias correction becomes unnecessary!
```

---

## 6. When Is Bias Correction Important?

### Comparison:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Bias Correction: When Needed?       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CRITICAL:
âœ“ Beginning of training (first ~20 iterations)
âœ“ When Î² is large (0.98, 0.99, 0.999)
âœ“ When initial value vâ‚€ = 0

OPTIONAL:
â€¢ After many iterations (>100)
â€¢ When Î² is small (0.5, 0.7)
â€¢ When initialized with good vâ‚€

UNNECESSARY:
âœ— After convergence
âœ— In practice for Adam (often omitted)
âœ— When using large datasets with many updates
```

---

### Numerical Impact:

**Early iterations (t = 1 to 10):**

| Î² | t=1 correction | t=5 correction | t=10 correction |
|---|---------------|----------------|-----------------|
| **0.5** | Ã—2.0 | Ã—1.03 | Ã—1.0 |
| **0.9** | Ã—10 | Ã—2.4 | Ã—1.5 |
| **0.98** | Ã—50 | Ã—10.5 | Ã—5.5 |
| **0.999** | Ã—1000 | Ã—200 | Ã—100 |

**Larger Î² â†’ more critical to correct bias!**

---

**Late iterations (t = 100 to 1000):**

| Î² | t=100 correction | t=500 correction | t=1000 correction |
|---|-----------------|------------------|-------------------|
| **0.5** | Ã—1.0 | Ã—1.0 | Ã—1.0 |
| **0.9** | Ã—1.0 | Ã—1.0 | Ã—1.0 |
| **0.98** | Ã—1.1 | Ã—1.0 | Ã—1.0 |
| **0.999** | Ã—1.6 | Ã—1.0 | Ã—1.0 |

**After enough iterations, correction negligible!**

---

## 7. Complete Implementation

### Python Implementation:

```python
import numpy as np

class ExponentiallyWeightedAverage:
    """
    Compute exponentially weighted average with bias correction
    """
    def __init__(self, beta=0.9):
        """
        Args:
            beta: Decay parameter (0 to 1)
                  Larger beta = more smoothing
        """
        self.beta = beta
        self.v = 0  # Running average
        self.t = 0  # Time step
    
    def update(self, theta, use_bias_correction=True):
        """
        Update EWA with new value
        
        Args:
            theta: New value to incorporate
            use_bias_correction: Whether to apply bias correction
        
        Returns:
            Bias-corrected (or raw) EWA
        """
        self.t += 1
        
        # Update running average
        self.v = self.beta * self.v + (1 - self.beta) * theta
        
        # Bias correction
        if use_bias_correction:
            v_corrected = self.v / (1 - self.beta ** self.t)
            return v_corrected
        else:
            return self.v
    
    def get_value(self, use_bias_correction=True):
        """Get current EWA value"""
        if use_bias_correction and self.t > 0:
            return self.v / (1 - self.beta ** self.t)
        return self.v


# Example: Temperature tracking
temperatures = [18, 23, 19, 20, 17, 21, 19, 18, 22, 20]

# Without bias correction
ewa_no_correction = ExponentiallyWeightedAverage(beta=0.9)
print("Without Bias Correction:")
for day, temp in enumerate(temperatures, 1):
    avg = ewa_no_correction.update(temp, use_bias_correction=False)
    print(f"  Day {day}: Temp={temp}Â°C, EWA={avg:.2f}Â°C")

print("\n" + "="*50 + "\n")

# With bias correction
ewa_corrected = ExponentiallyWeightedAverage(beta=0.9)
print("With Bias Correction:")
for day, temp in enumerate(temperatures, 1):
    avg = ewa_corrected.update(temp, use_bias_correction=True)
    true_avg = np.mean(temperatures[:day])
    error = abs(avg - true_avg)
    print(f"  Day {day}: Temp={temp}Â°C, EWA={avg:.2f}Â°C, "
          f"True Avg={true_avg:.2f}Â°C, Error={error:.2f}Â°C")
```

---

**Output:**

```
Without Bias Correction:
  Day 1: Temp=18Â°C, EWA=1.80Â°C
  Day 2: Temp=23Â°C, EWA=3.92Â°C
  Day 3: Temp=19Â°C, EWA=5.43Â°C
  Day 4: Temp=20Â°C, EWA=6.89Â°C
  Day 5: Temp=17Â°C, EWA=7.90Â°C
  Day 6: Temp=21Â°C, EWA=9.41Â°C
  Day 7: Temp=19Â°C, EWA=10.37Â°C
  Day 8: Temp=18Â°C, EWA=11.13Â°C
  Day 9: Temp=22Â°C, EWA=12.22Â°C
  Day 10: Temp=20Â°C, EWA=13.00Â°C

==================================================

With Bias Correction:
  Day 1: Temp=18Â°C, EWA=18.00Â°C, True Avg=18.00Â°C, Error=0.00Â°C
  Day 2: Temp=23Â°C, EWA=20.63Â°C, True Avg=20.50Â°C, Error=0.13Â°C
  Day 3: Temp=19Â°C, EWA=20.03Â°C, True Avg=20.00Â°C, Error=0.03Â°C
  Day 4: Temp=20Â°C, EWA=20.01Â°C, True Avg=20.00Â°C, Error=0.01Â°C
  Day 5: Temp=17Â°C, EWA=19.28Â°C, True Avg=19.40Â°C, Error=0.12Â°C
  Day 6: Temp=21Â°C, EWA=20.06Â°C, True Avg=19.83Â°C, Error=0.23Â°C
  Day 7: Temp=19Â°C, EWA=19.86Â°C, True Avg=19.71Â°C, Error=0.15Â°C
  Day 8: Temp=18Â°C, EWA=19.53Â°C, True Avg=19.50Â°C, Error=0.03Â°C
  Day 9: Temp=22Â°C, EWA=19.93Â°C, True Avg=19.78Â°C, Error=0.15Â°C
  Day 10: Temp=20Â°C, EWA=19.96Â°C, True Avg=19.90Â°C, Error=0.06Â°C

Bias correction gives accurate estimates from Day 1! âœ“
```

---

## 8. PyTorch Implementation for Gradients

### Using EWA in Training Loop:

```python
import torch
import torch.nn as nn

class EWAGradientDescent:
    """Gradient descent with exponentially weighted average of gradients"""
    
    def __init__(self, parameters, lr=0.01, beta=0.9, use_bias_correction=True):
        self.parameters = list(parameters)
        self.lr = lr
        self.beta = beta
        self.use_bias_correction = use_bias_correction
        
        # Initialize EWA for each parameter
        self.v = [torch.zeros_like(p) for p in self.parameters]
        self.t = 0
    
    def step(self):
        """Perform one optimization step"""
        self.t += 1
        
        for i, param in enumerate(self.parameters):
            if param.grad is None:
                continue
            
            # Current gradient
            grad = param.grad.data
            
            # Update EWA
            self.v[i] = self.beta * self.v[i] + (1 - self.beta) * grad
            
            # Bias correction
            if self.use_bias_correction:
                v_corrected = self.v[i] / (1 - self.beta ** self.t)
            else:
                v_corrected = self.v[i]
            
            # Update parameter
            param.data = param.data - self.lr * v_corrected
    
    def zero_grad(self):
        """Zero out gradients"""
        for param in self.parameters:
            if param.grad is not None:
                param.grad.zero_()


# Example usage
model = nn.Linear(1000, 100)
optimizer = EWAGradientDescent(
    model.parameters(),
    lr=0.01,
    beta=0.9,
    use_bias_correction=True
)

# Training loop
for epoch in range(10):
    for batch_x, batch_y in train_loader:
        # Forward pass
        output = model(batch_x)
        loss = criterion(output, batch_y)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        
        # Update with EWA (includes bias correction)
        optimizer.step()
        
        print(f"Epoch {epoch}, Step {optimizer.t}: Loss={loss.item():.4f}")
```

---

## 9. Effect on Training

### Comparing Standard GD vs EWA

**Same Cat vs Dog network, batch size 32:**

---

#### Standard Gradient Descent:

```
Epoch 1:
  Batch 1: gradient=0.023, w=0.2477,  loss=0.654
  Batch 2: gradient=-0.018, w=0.2495, loss=0.598  â† Oscillating!
  Batch 3: gradient=0.031, w=0.2464,  loss=0.494
  Batch 4: gradient=0.012, w=0.2452,  loss=0.512
  ...
  
Weight trajectory:
0.250 â†’ 0.2477 â†’ 0.2495 â†’ 0.2464 â†’ 0.2452 â†’ ...
        â†˜       â†—       â†˜       â†˜

Zig-zag pattern!
Average progress: 0.0048 per 4 batches
```

---

#### With EWA (Î²=0.9, bias-corrected):

```
Epoch 1:
  Batch 1: grad=0.023,  vÌ‚=0.023,  w=0.2477, loss=0.654
  Batch 2: grad=-0.018, vÌ‚=0.0014, w=0.2476, loss=0.598  â† Smoother!
  Batch 3: grad=0.031,  vÌ‚=0.0123, w=0.2463, loss=0.494
  Batch 4: grad=0.012,  vÌ‚=0.0118, w=0.2452, loss=0.512
  ...
  
Weight trajectory:
0.250 â†’ 0.2477 â†’ 0.2476 â†’ 0.2463 â†’ 0.2452 â†’ ...
        â†˜       â†˜       â†˜       â†˜

Smooth descent!
Average progress: 0.0048 per 4 batches (same endpoint!)

But path is much smoother!
```

---

### Training Curves Comparison:

```
    Loss
     â†‘
  0.7â”‚â—
     â”‚ â•²  â•±â•² â•±â•²               Standard GD (noisy)
  0.6â”‚  â•²â•±  â•²â•±  â•²
     â”‚          â•²â•±â•²
  0.5â”‚     â—       â•²___        With EWA (smooth) âœ“
     â”‚      â•²___
  0.4â”‚          â•²___
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Batches
      0   10   20   30

EWA smooths the descent!
More predictable training!
```

---

## 10. Practical Guidelines

### Choosing Î²:

| Î² Value | Effective Window | Use Case | Smoothness |
|---------|-----------------|----------|------------|
| **0.5** | ~2 values | Fast adaptation needed | Low |
| **0.7** | ~3 values | Moderate smoothing | Medium-Low |
| **0.9** | ~10 values | **Default choice** âœ“ | Medium |
| **0.95** | ~20 values | More smoothing | Medium-High |
| **0.98** | ~50 values | High smoothing | High |
| **0.99** | ~100 values | Very high smoothing | Very High |
| **0.999** | ~1000 values | Extreme smoothing | Extreme |

---

### Decision Guide:

```
For gradient averaging (momentum-based optimizers):
â”œâ”€ Î² = 0.9 (default)
â”‚   Good balance for most problems
â”‚
â”œâ”€ Î² = 0.95-0.98
â”‚   If gradients very noisy (small batches)
â”‚
â””â”€ Î² = 0.99-0.999
    For second moment estimation (RMSprop, Adam)
    Need longer history for variance estimates

For exponential learning rate decay:
â”œâ”€ Î² = 0.95-0.99
    Smooth learning rate reduction

For validation metrics tracking:
â”œâ”€ Î² = 0.9-0.95
    Smooth but responsive to changes
```

---

### When to Use Bias Correction:

```
ALWAYS use bias correction when:
âœ“ Training from scratch (first epoch)
âœ“ Using large Î² (â‰¥0.98)
âœ“ Making critical decisions based on EWA
âœ“ Î²^t won't be negligible for your training length

Can skip bias correction when:
â€¢ After warm-up period (~20-50 iterations)
â€¢ Using small Î² (â‰¤0.9)
â€¢ Training for many iterations (>1000)
â€¢ Implementation simplicity matters more than early accuracy
```

---

## 11. Common Mistakes

### âŒ Mistake 1: Not Using Bias Correction Early

```python
# BAD: No bias correction
v = 0
beta = 0.99
for t, gradient in enumerate(gradients, 1):
    v = beta * v + (1 - beta) * gradient
    w = w - lr * v  # v is severely biased at start!

# GOOD: With bias correction
v = 0
beta = 0.99
for t, gradient in enumerate(gradients, 1):
    v = beta * v + (1 - beta) * gradient
    v_corrected = v / (1 - beta ** t)
    w = w - lr * v_corrected  # v_corrected is accurate!
```

---

### âŒ Mistake 2: Wrong Initial Value

```python
# BAD: Initialize with first value
v = gradients[0]  # vâ‚€ = Î¸â‚
# This introduces different bias!

# GOOD: Initialize with zero
v = 0
# Then use bias correction
```

---

### âŒ Mistake 3: Using Î²=1.0

```python
# BAD: Î² = 1.0
v = 1.0 * v + (1 - 1.0) * theta
  = v + 0
  = v  # Never updates!

# Î² must be < 1.0!
```

---

### âŒ Mistake 4: Forgetting to Increment t

```python
# BAD: Fixed t
v = beta * v + (1 - beta) * theta
v_corrected = v / (1 - beta ** 1)  # Always using t=1!

# GOOD: Increment t each step
t += 1
v = beta * v + (1 - beta) * theta
v_corrected = v / (1 - beta ** t)
```

---

## 12. Advanced: Why the Formula Works

### Theoretical Justification:

**Expected value of v_t:**

Given $v_0 = 0$ and assuming $\mathbb{E}[\theta_i] = \mu$ (constant):

$$\mathbb{E}[v_t] = (1-\beta)\sum_{i=1}^{t}\beta^{t-i}\mu$$

$$= (1-\beta)\mu\sum_{i=0}^{t-1}\beta^i$$

$$= (1-\beta)\mu \cdot \frac{1-\beta^t}{1-\beta}$$

$$= \mu(1-\beta^t)$$

**So v_t is biased by factor (1-Î²^t)!**

Dividing by (1-Î²^t) removes this bias:

$$\mathbb{E}\left[\frac{v_t}{1-\beta^t}\right] = \frac{\mu(1-\beta^t)}{1-\beta^t} = \mu$$

**Unbiased! âœ“**

---

### Proof by Example (Î²=0.9, Î¼=20):

```
Expected value of raw v_t:

t=1:  E[vâ‚] = 20 Ã— (1 - 0.9Â¹) = 20 Ã— 0.1 = 2.0
t=2:  E[vâ‚‚] = 20 Ã— (1 - 0.9Â²) = 20 Ã— 0.19 = 3.8
t=3:  E[vâ‚ƒ] = 20 Ã— (1 - 0.9Â³) = 20 Ã— 0.271 = 5.42
t=5:  E[vâ‚…] = 20 Ã— (1 - 0.9âµ) = 20 Ã— 0.410 = 8.2
t=10: E[vâ‚â‚€] = 20 Ã— (1 - 0.9Â¹â°) = 20 Ã— 0.651 = 13.02

All biased downward!

Expected value of corrected vÌ‚_t:

E[vÌ‚_t] = E[v_t] / (1 - Î²^t)
       = Î¼(1-Î²^t) / (1-Î²^t)
       = Î¼
       = 20  â† Correct for all t! âœ“
```

---

## 13. Summary

### What EWA Does:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Exponentially Weighted Average (EWA)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FORMULA: v_t = Î²Â·v_{t-1} + (1-Î²)Â·Î¸_t

EFFECT: 
- Smooths noisy sequences
- Recent values weighted more
- Old values decay exponentially

PARAMETERS:
- Î² âˆˆ (0,1): Decay parameter
  â€¢ Large Î² (0.98): Very smooth, slow adaptation
  â€¢ Small Î² (0.5): Less smooth, fast adaptation
  â€¢ Typical: 0.9 (balances smoothing and responsiveness)

WINDOW: Approximates averaging over 1/(1-Î²) values

APPLICATION:
- Gradient smoothing (momentum)
- Variance estimation (RMSprop, Adam)
- Learning rate scheduling
- Metric tracking
```

---

### Bias Correction:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Bias Correction                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PROBLEM: vâ‚€ = 0 â†’ early estimates biased low

FORMULA: vÌ‚_t = v_t / (1 - Î²^t)

EFFECT:
- Removes initialization bias
- Critical for first ~20 iterations
- Becomes negligible as t â†’ âˆ

WHEN TO USE:
âœ“ Beginning of training
âœ“ Large Î² values (â‰¥0.98)
âœ“ When accuracy matters early on

WHEN TO SKIP:
â€¢ After warm-up (~50 iterations)
â€¢ Small Î² (â‰¤0.9)
â€¢ Computational efficiency critical
```

---

### Key Formulas:

**Standard EWA:**
$$v_t = \beta v_{t-1} + (1-\beta)\theta_t$$

**Bias-Corrected EWA:**
$$\hat{v}_t = \frac{v_t}{1 - \beta^t}$$

**Effective Window Size:**
$$N_{eff} \approx \frac{1}{1-\beta}$$

**Expected Value (if Î¸_t â‰ˆ constant Î¼):**
$$\mathbb{E}[v_t] = \mu(1-\beta^t)$$
$$\mathbb{E}[\hat{v}_t] = \mu$$

---

### Practical Recommendations:

```
âœ“ Use Î²=0.9 as default (averages ~10 values)
âœ“ Always use bias correction for first ~20 steps
âœ“ For variance estimates, use larger Î² (0.99 or 0.999)
âœ“ Initialize vâ‚€ = 0 (simpler than vâ‚€ = Î¸â‚)
âœ“ Monitor convergence - adjust Î² if needed

âœ— Don't use Î² â‰¥ 1.0 (won't update!)
âœ— Don't use Î² < 0.5 (loses smoothing benefit)
âœ— Don't forget to increment t for bias correction
âœ— Don't skip bias correction with large Î² values
```

---

### Use Cases in Deep Learning:

```
1. Momentum Optimization
   EWA of gradients: v_t = Î²Â·v_{t-1} + (1-Î²)Â·âˆ‡L
   Î² = 0.9 (typical)
   
2. RMSprop
   EWA of squared gradients: s_t = Î²Â·s_{t-1} + (1-Î²)Â·(âˆ‡L)Â²
   Î² = 0.999 (typical)
   
3. Adam Optimizer
   First moment (momentum): m_t with Î²â‚ = 0.9
   Second moment (RMSprop): v_t with Î²â‚‚ = 0.999
   Both use bias correction!
   
4. Learning Rate Scheduling
   Smooth decay: lr_t = Î²Â·lr_{t-1} + (1-Î²)Â·lr_target
   Î² = 0.95 (typical)
```

---

**You now understand exponentially weighted averages and bias correction! ğŸ‰**

This is the foundation for:
- **Momentum** (EWA of gradients)
- **RMSprop** (EWA of squared gradients)
- **Adam** (EWA of both first and second moments)

The bias correction ensures these optimizers work correctly from iteration 1!
