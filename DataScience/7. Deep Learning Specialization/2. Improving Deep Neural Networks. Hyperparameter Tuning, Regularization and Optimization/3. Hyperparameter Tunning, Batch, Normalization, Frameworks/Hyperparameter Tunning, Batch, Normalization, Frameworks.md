# Hyperparameter Tuning, Batch Normalization, and Frameworks
## Advanced Techniques for Training Deep Neural Networks
### (Detailed Step-by-Step with Complete Examples)

## Table of Contents

### Part 1: Normalizing Activations in a Network (Batch Normalization)
- [Overview & Connection to Previous Topics](#-connection-to-previous-topics)
- [1. Understanding the Problem](#part-1-understanding-the-problem)
  - [Plain English Explanation](#1-plain-english-explanation)
  - [The Covariate Shift Problem](#the-covariate-shift-problem)
  - [Assembly Line Analogy](#real-world-analogy-assembly-line)
  - [Neural Network Analogy](#neural-network-analogy)
- [2. The Mathematics of Batch Normalization](#2-the-mathematics-of-batch-normalization)
  - [The Algorithm](#batch-normalization-algorithm)
  - [Key Components](#key-components)
  - [Why It Works](#why-normalization-works)
- [3. Complete Numerical Example](#3-complete-numerical-example)
  - [Setup: Simple Network](#setup-simple-3-layer-network)
  - [Without Batch Norm](#without-batch-normalization)
  - [With Batch Norm](#with-batch-normalization)
  - [Step-by-Step Calculations](#detailed-calculation-for-batch-1)
- [4. Visual Comparison](#4-visual-comparison)
  - [Activation Distributions](#activation-distributions)
  - [Gradient Flow](#gradient-flow)
- [5. Training and Test Time](#5-batch-norm-training-vs-test-time)
  - [Training Mode](#training-mode)
  - [Test Mode](#test-mode-inference)
  - [Running Statistics](#maintaining-running-statistics)
- [6. Complete Implementation](#6-complete-pytorch-implementation)
  - [Manual Implementation](#manual-batch-normalization)
  - [Built-in Implementation](#using-pytorch-built-in)
- [7. Placement in Networks](#7-where-to-place-batch-norm)
  - [Before or After Activation](#before-vs-after-activation)
  - [Convolutional Networks](#batch-norm-in-cnns)
  - [Recurrent Networks](#batch-norm-in-rnns)
- [8. Effects on Training](#8-effects-on-training)
  - [Learning Rate](#learning-rate-with-batch-norm)
  - [Regularization Effect](#regularization-effect)
  - [Convergence Speed](#convergence-speed)
- [9. Batch Size Considerations](#9-batch-size-and-batch-norm)
  - [Minimum Batch Size](#minimum-batch-size-requirements)
  - [Small Batch Problems](#problems-with-small-batches)
  - [Solutions](#solutions-for-small-batches)
- [10. Alternatives to Batch Norm](#10-alternatives-to-batch-normalization)
  - [Layer Normalization](#layer-normalization)
  - [Instance Normalization](#instance-normalization)
  - [Group Normalization](#group-normalization)
  - [Comparison](#comparison-of-normalization-techniques)
- [11. Advanced Topics](#11-advanced-topics)
  - [Batch Renormalization](#batch-renormalization)
  - [Weight Normalization](#weight-normalization)
  - [Spectral Normalization](#spectral-normalization)
- [12. Practical Guidelines](#12-practical-guidelines)
  - [When to Use Batch Norm](#when-to-use-batch-norm)
  - [Hyperparameter Guidelines](#hyperparameter-guidelines)
  - [Common Mistakes](#common-mistakes)
- [13. Batch Norm Summary](#13-summary-batch-normalization)

---

---

## ğŸ”— **Connection to Previous Topics**

### **What We Know So Far:**

**From Optimization Algorithms:**
```
We learned how to train networks efficiently:
- Mini-batch gradient descent
- Momentum (accelerates convergence)
- RMSprop (adapts learning rates)
- Adam (combines both)

But we haven't addressed a fundamental problem:
Internal Covariate Shift!
```

**The New Problem:**

```
As we train deeper networks, we face a challenge:

Input to layer 3 depends on layers 1 and 2
When layers 1 and 2 update their weights,
the distribution of layer 3's input CHANGES!

This is like trying to hit a moving target!

Layer 3 has to constantly adapt to:
- Different mean values
- Different variance/scale
- Different distributions

This slows down training significantly!
```

**The Question:**

```
Can we STABILIZE the inputs to each layer?
Make training faster and more stable?
Allow us to use higher learning rates?

Answer: Batch Normalization!
```

---

# Part 1: Understanding the Problem

## 1. Plain English Explanation

### The Covariate Shift Problem

**Covariate Shift:** "The distribution of inputs keeps changing during training"

### Real-World Analogy: Assembly Line

Imagine a factory assembly line with 5 stations:

**Without Stabilization:**
```
Station 1 â†’ Station 2 â†’ Station 3 â†’ Station 4 â†’ Station 5
(Raw)      (Polish)     (Paint)     (Inspect)   (Package)

Day 1: Station 1 outputs parts ranging 10-20cm
Station 3 adjusts paint spray for this size range

Day 2: Station 1 improves, outputs parts 15-25cm
Station 3's spray pattern now WRONG for this range!
Must readjust everything!

Day 3: Station 1 changes again to 12-18cm
Station 3 must readjust AGAIN!

Each station constantly adapting to changing inputs!
Slow, inefficient, never stabilizes!
```

**With Stabilization (Batch Normalization):**
```
Station 1 â†’ [Normalizer] â†’ Station 2 â†’ [Normalizer] â†’ Station 3 ...

Day 1: Parts from Station 1: 10-20cm
Normalizer: Scales all parts to standard 0-10cm range
Station 2 receives: 0-10cm (consistent!)

Day 2: Parts from Station 1: 15-25cm (changed!)
Normalizer: Still scales to 0-10cm range
Station 2 receives: 0-10cm (same as before!)

Day 3: Parts from Station 1: 12-18cm (changed again!)
Normalizer: Still scales to 0-10cm range
Station 2 receives: 0-10cm (still consistent!)

Each station works with STABLE inputs!
Fast adaptation, better quality!
```

---

### Neural Network Analogy

**The Problem Without Batch Norm:**

```
Layer 1: Wâ‚, bâ‚
   â†“ (outputs: zâ‚ = Wâ‚x + bâ‚)
   â†“ (activation: aâ‚ = ReLU(zâ‚))
   
Layer 2: Wâ‚‚, bâ‚‚ (receives aâ‚ as input)
   â†“ (outputs: zâ‚‚ = Wâ‚‚aâ‚ + bâ‚‚)
   â†“ (activation: aâ‚‚ = ReLU(zâ‚‚))
   
Layer 3: Wâ‚ƒ, bâ‚ƒ (receives aâ‚‚ as input)

Training iteration 1:
aâ‚ has mean=5.2, std=2.3
Layer 2 learns based on this distribution

Training iteration 100:
Wâ‚ has changed significantly!
aâ‚ now has mean=12.7, std=8.9 (completely different!)
Layer 2's learned weights are now mismatched!
Layer 2 has to "relearn" for new distribution!

This happens at EVERY layer!
Each layer chasing moving targets!
```

**With Batch Normalization:**

```
Layer 1: Wâ‚, bâ‚
   â†“ (outputs: zâ‚)
   â†“ (activation: aâ‚ = ReLU(zâ‚))
   â†“ [BATCH NORM] â† Normalize here!
   â†“ (normalized: Ã£â‚ has mean=0, std=1 ALWAYS)
   
Layer 2: Wâ‚‚, bâ‚‚ (receives Ã£â‚ as input)
   â†“ (outputs: zâ‚‚)
   â†“ (activation: aâ‚‚ = ReLU(zâ‚‚))
   â†“ [BATCH NORM] â† Normalize here!
   â†“ (normalized: Ã£â‚‚ has mean=0, std=1 ALWAYS)
   
Layer 3: Wâ‚ƒ, bâ‚ƒ (receives Ã£â‚‚ as input)

Training iteration 1:
Ã£â‚ has mean=0, std=1
Layer 2 learns based on this

Training iteration 100:
Wâ‚ has changed, BUT:
Ã£â‚ STILL has mean=0, std=1 (normalized!)
Layer 2's input distribution UNCHANGED!
Layer 2 can focus on learning the task!

Stable inputs at every layer! âœ“
```

---

## 2. The Mathematics of Batch Normalization

### Batch Normalization Algorithm:

For a mini-batch of activations $\{x_1, x_2, ..., x_B\}$ where $B$ is batch size:

**Step 1: Compute batch statistics**
$$\mu_B = \frac{1}{B}\sum_{i=1}^{B}x_i$$

$$\sigma^2_B = \frac{1}{B}\sum_{i=1}^{B}(x_i - \mu_B)^2$$

**Step 2: Normalize**
$$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma^2_B + \epsilon}}$$

**Step 3: Scale and shift (learnable parameters)**
$$y_i = \gamma \hat{x}_i + \beta$$

Where:
- $\mu_B$ = Batch mean
- $\sigma^2_B$ = Batch variance
- $\epsilon$ = Small constant (e.g., $10^{-5}$) for numerical stability
- $\gamma$ = Scale parameter (learned)
- $\beta$ = Shift parameter (learned)
- $\hat{x}_i$ = Normalized value (mean=0, std=1)
- $y_i$ = Final output (after scale and shift)

---

### Key Components:

| Symbol | Name | Purpose | Typical Value |
|--------|------|---------|---------------|
| $\mu_B$ | Batch mean | Center of distribution | Computed per batch |
| $\sigma^2_B$ | Batch variance | Spread of distribution | Computed per batch |
| $\epsilon$ | Epsilon | Numerical stability | $10^{-5}$ |
| $\gamma$ | Gamma (scale) | Learned: controls spread | Initialized to 1 |
| $\beta$ | Beta (shift) | Learned: controls center | Initialized to 0 |
| $\hat{x}_i$ | Normalized value | Zero mean, unit variance | - |
| $y_i$ | Output | After learned transformation | - |

---

### Why Normalization Works:

```
Before Normalization:
x = [0.1, 5.2, -2.3, 8.9, 1.2, ...]
Mean â‰ˆ 2.7, Std â‰ˆ 3.8 (varies batch to batch!)

Problem:
- Large values â†’ Large gradients â†’ Instability
- Values shift â†’ Distribution changes â†’ Slow learning
- Different scales â†’ Hard to choose learning rate

After Normalization:
xÌ‚ = [-0.68, 0.66, -1.32, 1.63, -0.39, ...]
Mean = 0, Std = 1 (stable across batches!)

Benefits:
âœ“ Consistent scale â†’ Stable gradients
âœ“ Centered around 0 â†’ Faster convergence
âœ“ Unit variance â†’ Can use higher learning rates
âœ“ Distribution stable â†’ Layers learn efficiently
```

---

### Why Scale and Shift (Î³, Î²)?

```
Pure normalization forces:
xÌ‚ with mean=0, std=1

But what if the network NEEDS a different distribution?
Example: Sigmoid activation works best with inputs near Â±3

Solution: Learn Î³ and Î²!
y = Î³Â·xÌ‚ + Î²

Network can learn optimal mean and std for each layer!

Special cases:
If Î³ = Ïƒ_B and Î² = Î¼_B:
  y = Ïƒ_BÂ·xÌ‚ + Î¼_B = x (identity! Can recover original)
  
Network can decide:
- Use normalization (Î³â‰ˆ1, Î²â‰ˆ0)
- Undo normalization (Î³=Ïƒ_B, Î²=Î¼_B)
- Or anything in between!
```

---

## 3. Complete Numerical Example

### Setup: Simple 3-Layer Network

```
Network architecture:
Input: 4 features
Layer 1: 3 neurons (with Batch Norm)
Layer 2: 2 neurons (with Batch Norm)
Output: 1 neuron

Batch size: 4 images

We'll trace through one forward pass
```

---

### Without Batch Normalization:

**Mini-batch input (4 samples, 4 features each):**
```
X = [
  [0.5, 1.2, -0.3, 2.1],  # Sample 1
  [1.8, 0.4, 1.5, -0.8],  # Sample 2
  [-0.2, 2.3, 0.7, 1.2],  # Sample 3
  [0.9, -1.1, 0.2, 0.5]   # Sample 4
]
```

**Layer 1 forward pass:**
```
Weights Wâ‚ (3Ã—4), biases bâ‚ (3Ã—1)

For sample 1:
zâ‚[1] = Wâ‚[1]Â·x[1] + bâ‚[1]
     = [0.2, -0.1, 0.5, 0.3]Â·[0.5, 1.2, -0.3, 2.1] + 0.1
     = 0.1 - 0.12 - 0.15 + 0.63 + 0.1
     = 0.56

Similarly for all neurons and samples:
Zâ‚ = [
  [0.56, -0.23, 1.45],  # Sample 1
  [2.31, 0.87, -1.12],  # Sample 2
  [1.78, 2.45, 0.34],   # Sample 3
  [-0.45, 0.12, 0.89]   # Sample 4
]

Apply ReLU:
Aâ‚ = ReLU(Zâ‚) = [
  [0.56, 0, 1.45],
  [2.31, 0.87, 0],
  [1.78, 2.45, 0.34],
  [0, 0.12, 0.89]
]

Statistics per neuron (across batch):
Neuron 1: [0.56, 2.31, 1.78, 0]
  mean = 1.1625, std = 0.963

Neuron 2: [0, 0.87, 2.45, 0.12]
  mean = 0.86, std = 1.058

Neuron 3: [1.45, 0, 0.34, 0.89]
  mean = 0.67, std = 0.604

Problem: Different means and stds!
Layer 2 receives inconsistent inputs!
```

---

### With Batch Normalization:

**After computing Zâ‚, apply Batch Norm BEFORE activation:**

```
Zâ‚ = [
  [0.56, -0.23, 1.45],  # Sample 1
  [2.31, 0.87, -1.12],  # Sample 2
  [1.78, 2.45, 0.34],   # Sample 3
  [-0.45, 0.12, 0.89]   # Sample 4
]
```

**Normalize each neuron separately (across the batch):**

**For Neuron 1:**
```
Values: [0.56, 2.31, 1.78, -0.45]

Step 1: Compute batch mean
Î¼_B = (0.56 + 2.31 + 1.78 - 0.45) / 4
    = 4.2 / 4
    = 1.05

Step 2: Compute batch variance
ÏƒÂ²_B = [(0.56-1.05)Â² + (2.31-1.05)Â² + (1.78-1.05)Â² + (-0.45-1.05)Â²] / 4
     = [0.2401 + 1.5876 + 0.5329 + 2.25] / 4
     = 4.6106 / 4
     = 1.1527

Step 3: Normalize
Îµ = 0.00001

For sample 1:
xÌ‚â‚ = (0.56 - 1.05) / âˆš(1.1527 + 0.00001)
   = -0.49 / âˆš1.1528
   = -0.49 / 1.0737
   = -0.456

For sample 2:
xÌ‚â‚‚ = (2.31 - 1.05) / 1.0737
   = 1.26 / 1.0737
   = 1.174

For sample 3:
xÌ‚â‚ƒ = (1.78 - 1.05) / 1.0737
   = 0.73 / 1.0737
   = 0.680

For sample 4:
xÌ‚â‚„ = (-0.45 - 1.05) / 1.0737
   = -1.5 / 1.0737
   = -1.397

Normalized values: [-0.456, 1.174, 0.680, -1.397]

Verify: mean â‰ˆ 0, std â‰ˆ 1 âœ“
```

**Step 4: Scale and shift (learnable parameters)**
```
Initial: Î³ = 1, Î² = 0 (standard initialization)

yâ‚ = Î³Â·xÌ‚â‚ + Î² = 1Â·(-0.456) + 0 = -0.456
yâ‚‚ = Î³Â·xÌ‚â‚‚ + Î² = 1Â·(1.174) + 0 = 1.174
yâ‚ƒ = Î³Â·xÌ‚â‚ƒ + Î² = 1Â·(0.680) + 0 = 0.680
yâ‚„ = Î³Â·xÌ‚â‚„ + Î² = 1Â·(-1.397) + 0 = -1.397

(With initial Î³=1, Î²=0, output = normalized value)
```

**Apply ReLU:**
```
Aâ‚ = ReLU(y) = [0, 1.174, 0.680, 0]

Now feed to Layer 2 with STABLE inputs! âœ“
```

---

### Detailed Calculation for Batch 1:

**Complete Batch Norm for all 3 neurons:**

| Neuron | Original Values | Î¼_B | ÏƒÂ²_B | Normalized (xÌ‚) | After Î³,Î² | After ReLU |
|--------|----------------|-----|------|----------------|-----------|------------|
| **1** | [0.56, 2.31, 1.78, -0.45] | 1.05 | 1.153 | [-0.456, 1.174, 0.680, -1.397] | Same (Î³=1, Î²=0) | [0, 1.174, 0.680, 0] |
| **2** | [-0.23, 0.87, 2.45, 0.12] | 0.80 | 1.292 | [-0.908, 0.061, 1.451, -0.604] | Same | [0, 0.061, 1.451, 0] |
| **3** | [1.45, -1.12, 0.34, 0.89] | 0.39 | 0.954 | [1.084, -1.545, -0.051, 0.512] | Same | [1.084, 0, 0, 0.512] |

**Result:**
```
All neurons now have:
- Mean â‰ˆ 0
- Std â‰ˆ 1
- Consistent distribution

Layer 2 receives stable, normalized inputs! âœ“
```

---

## 4. Visual Comparison

### Activation Distributions:

**Without Batch Norm (after 100 training iterations):**

```
    Frequency
        â†‘
    25  â”‚   â•±â•²                          Layer 1
        â”‚  â•±  â•²                         (centered, narrow)
    20  â”‚ â•±    â•²
        â”‚â•±      â•²
    15  â—â”€â”€â”€â”€â”€â”€â”€â”€â•²___
        â”‚
        â”‚              â•±â”€â”€â•²             Layer 2
    10  â”‚          â•±â”€â”€â•±    â•²â”€â”€          (shifted right, wider)
        â”‚      â•±â”€â”€â•±          â•²â”€â”€
     5  â”‚  â•±â”€â”€â•±                â•²â”€â”€
        â”‚â”€â”€â•±                      â•²â”€â”€
     0  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Activation Value
       -5   0   5   10  15  20

Different distributions per layer!
Layer 3 has to adapt to Layer 2's shifted, wide distribution!
```

**With Batch Norm (after 100 iterations):**

```
    Frequency
        â†‘
    25  â”‚   â•±â•²                          All Layers!
        â”‚  â•±  â•²                         (all centered at 0)
    20  â”‚ â•±    â•²                        (all std â‰ˆ 1)
        â”‚â•±      â•²
    15  â—â”€â”€â”€â”€â”€â”€â”€â”€â•²___
        â”‚
    10  â”‚
        â”‚
     5  â”‚
        â”‚
     0  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Activation Value
       -3   -2  -1   0   1   2   3

All layers have same normalized distribution!
Stable learning environment! âœ“
```

---

### Gradient Flow:

**Without Batch Norm:**

```
    Gradient Magnitude
         â†‘
    10.0 â”‚â—                             Layer 1
         â”‚                              (vanishing!)
     1.0 â”‚  â—                           Layer 2
         â”‚    â—                         Layer 3
     0.1 â”‚      â—â—                      Layer 4-5
         â”‚        â—â—â—                   (gradients dying)
    0.01 â”‚           â—â—â—â—
         â”‚               â—â—â—â—â—
   0.001 â”‚                    â—â—â—â—â—â—â—â— Layers 6-10
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Layer Depth
          1  2  3  4  5  6  7  8  9  10

Vanishing gradient problem!
Deep layers barely learn!
```

**With Batch Norm:**

```
    Gradient Magnitude
         â†‘
    10.0 â”‚
         â”‚
     1.0 â”‚â—â—â—â—â—â—â—â—â—â—                   All Layers!
         â”‚                              (stable gradients)
     0.1 â”‚
         â”‚
    0.01 â”‚
         â”‚
   0.001 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Layer Depth
          1  2  3  4  5  6  7  8  9  10

Gradients flow smoothly through all layers!
All layers learn at similar rates! âœ“
```

---

## 5. Batch Norm: Training vs Test Time

### Training Mode:

**During training, use BATCH statistics:**

```python
def batch_norm_training(x, gamma, beta, eps=1e-5):
    """
    x: Input batch (B, D) where B=batch size, D=features
    gamma, beta: Learned parameters (D,)
    """
    # Compute batch statistics
    mu = x.mean(dim=0)        # Mean per feature
    var = x.var(dim=0)        # Variance per feature
    
    # Normalize
    x_norm = (x - mu) / torch.sqrt(var + eps)
    
    # Scale and shift
    out = gamma * x_norm + beta
    
    return out, mu, var  # Return statistics for running average
```

**Example:**
```
Batch: [[1, 2], [3, 4], [5, 6], [7, 8]]

Î¼ = [4, 5]  â† Computed from THIS batch
ÏƒÂ² = [5, 5]

Normalized using THIS batch's statistics
```

---

### Test Mode (Inference):

**During testing, use POPULATION statistics (running average):**

```python
def batch_norm_inference(x, gamma, beta, running_mean, running_var, eps=1e-5):
    """
    x: Input (can be single sample!)
    running_mean, running_var: Statistics from training
    """
    # Normalize using running statistics
    x_norm = (x - running_mean) / torch.sqrt(running_var + eps)
    
    # Scale and shift
    out = gamma * x_norm + beta
    
    return out
```

**Example:**
```
Test sample: [2, 3]

Use running_mean and running_var from training:
running_mean = [4.2, 5.1]  â† Accumulated during training
running_var = [4.8, 4.9]

Normalized using TRAINING statistics
NOT computed from test sample!

Why? Test batch might be size 1!
Can't compute meaningful statistics from 1 sample!
```

---

### Maintaining Running Statistics:

**During training, maintain exponentially weighted averages:**

$$\mu_{\text{running}} = \lambda \mu_{\text{running}} + (1-\lambda)\mu_B$$
$$\sigma^2_{\text{running}} = \lambda \sigma^2_{\text{running}} + (1-\lambda)\sigma^2_B$$

Where $\lambda$ = momentum (typically 0.9 or 0.99)

**Example:**

```
Training batch 1:
Î¼_B = 2.3, ÏƒÂ²_B = 1.5

Update running stats (Î» = 0.9):
Î¼_running = 0.9 Ã— 0 + 0.1 Ã— 2.3 = 0.23
ÏƒÂ²_running = 0.9 Ã— 1 + 0.1 Ã— 1.5 = 1.05

Training batch 2:
Î¼_B = 2.7, ÏƒÂ²_B = 1.8

Update:
Î¼_running = 0.9 Ã— 0.23 + 0.1 Ã— 2.7 = 0.207 + 0.27 = 0.477
ÏƒÂ²_running = 0.9 Ã— 1.05 + 0.1 Ã— 1.8 = 0.945 + 0.18 = 1.125

...

After 1000 batches:
Î¼_running â‰ˆ 2.5 (stable population estimate)
ÏƒÂ²_running â‰ˆ 1.6 (stable population estimate)

Use these for test time! âœ“
```

---

## 6. Complete PyTorch Implementation

### Manual Batch Normalization:

```python
import torch
import torch.nn as nn

class BatchNorm1d:
    """Manual implementation of Batch Normalization"""
    
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        """
        Args:
            num_features: Number of features (neurons) in layer
            eps: Small constant for numerical stability
            momentum: For running statistics (0.1 means 10% current batch)
        """
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        
        # Learnable parameters
        self.gamma = torch.ones(num_features)   # Scale
        self.beta = torch.zeros(num_features)   # Shift
        
        # Running statistics (for inference)
        self.running_mean = torch.zeros(num_features)
        self.running_var = torch.ones(num_features)
        
        # Track if in training or eval mode
        self.training = True
    
    def forward(self, x):
        """
        Args:
            x: Input tensor (batch_size, num_features)
        
        Returns:
            Normalized and scaled tensor
        """
        if self.training:
            # TRAINING MODE: Use batch statistics
            
            # Compute batch statistics
            batch_mean = x.mean(dim=0)  # Mean per feature
            batch_var = x.var(dim=0, unbiased=False)  # Variance per feature
            
            # Normalize
            x_norm = (x - batch_mean) / torch.sqrt(batch_var + self.eps)
            
            # Scale and shift
            out = self.gamma * x_norm + self.beta
            
            # Update running statistics (exponentially weighted average)
            with torch.no_grad():
                self.running_mean = (
                    (1 - self.momentum) * self.running_mean + 
                    self.momentum * batch_mean
                )
                self.running_var = (
                    (1 - self.momentum) * self.running_var + 
                    self.momentum * batch_var
                )
            
            return out
        
        else:
            # INFERENCE MODE: Use running statistics
            
            # Normalize using population statistics
            x_norm = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)
            
            # Scale and shift
            out = self.gamma * x_norm + self.beta
            
            return out
    
    def train(self):
        """Set to training mode"""
        self.training = True
    
    def eval(self):
        """Set to evaluation mode"""
        self.training = False


# Example usage
batch_size = 4
num_features = 3

# Create batch norm layer
bn = BatchNorm1d(num_features)

# Training
bn.train()
x_train = torch.randn(batch_size, num_features)

print("Training Mode:")
print(f"Input:\n{x_train}")
print(f"\nInput mean: {x_train.mean(dim=0)}")
print(f"Input std: {x_train.std(dim=0)}")

out_train = bn.forward(x_train)

print(f"\nOutput:\n{out_train}")
print(f"Output mean: {out_train.mean(dim=0)}")
print(f"Output std: {out_train.std(dim=0)}")

print(f"\nRunning mean: {bn.running_mean}")
print(f"Running var: {bn.running_var}")

# Inference
bn.eval()
x_test = torch.randn(1, num_features)  # Single sample!

print("\n" + "="*50)
print("Inference Mode:")
print(f"Input: {x_test}")

out_test = bn.forward(x_test)

print(f"Output: {out_test}")
print(f"\nUsed running_mean: {bn.running_mean}")
print(f"Used running_var: {bn.running_var}")
```

---

**Expected Output:**

```
Training Mode:
Input:
tensor([[ 0.3367,  0.1288,  0.2345],
        [ 0.2303, -1.1229, -0.1863],
        [ 2.1895, -0.5516,  1.5770],
        [-0.8972,  0.5638,  0.1920]])

Input mean: tensor([0.4648, -0.2455,  0.4543])
Input std: tensor([1.1910, 0.6982, 0.7249])

Output:
tensor([[-0.1076,  0.5358, -0.3032],
        [-0.1968, -1.2570, -0.8827],
        [ 1.4470, -0.4379,  1.5491],
        [-1.1426,  1.1592, -0.3632]])

Output mean: tensor([-2.9802e-08,  0.0000e+00, -2.9802e-08])  # â‰ˆ 0 âœ“
Output std: tensor([1.0000, 1.0000, 1.0000])  # = 1 âœ“

Running mean: tensor([0.0465, -0.0246,  0.0454])
Running var: tensor([1.0182, 1.0049, 1.0053])

==================================================
Inference Mode:
Input: tensor([[-0.6237,  1.2345, -0.8932]])

Output: tensor([[-0.6577,  1.2667, -0.9305]])

Used running_mean: tensor([0.0465, -0.0246,  0.0454])
Used running_var: tensor([1.0182, 1.0049, 1.0053])
```

---

### Using PyTorch Built-in:

```python
import torch
import torch.nn as nn

class NeuralNetWithBatchNorm(nn.Module):
    """Network with Batch Normalization"""
    
    def __init__(self, input_size=1000, hidden1=100, hidden2=50, output_size=2):
        super().__init__()
        
        # Layer 1
        self.fc1 = nn.Linear(input_size, hidden1)
        self.bn1 = nn.BatchNorm1d(hidden1)  # Batch norm after fc1
        
        # Layer 2
        self.fc2 = nn.Linear(hidden1, hidden2)
        self.bn2 = nn.BatchNorm1d(hidden2)  # Batch norm after fc2
        
        # Output layer (typically no batch norm on output)
        self.fc3 = nn.Linear(hidden2, output_size)
    
    def forward(self, x):
        # Layer 1
        x = self.fc1(x)
        x = self.bn1(x)      # Normalize
        x = torch.relu(x)    # Then activate
        
        # Layer 2
        x = self.fc2(x)
        x = self.bn2(x)      # Normalize
        x = torch.relu(x)    # Then activate
        
        # Output (no batch norm, no activation)
        x = self.fc3(x)
        
        return x


# Create model
model = NeuralNetWithBatchNorm()

# Check batch norm parameters
print("Batch Norm 1 parameters:")
print(f"  Gamma (scale): {model.bn1.weight[:5]}")  # First 5 values
print(f"  Beta (shift): {model.bn1.bias[:5]}")
print(f"  Running mean: {model.bn1.running_mean[:5]}")
print(f"  Running var: {model.bn1.running_var[:5]}")

# Training
model.train()  # Set to training mode
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

for epoch in range(5):
    for batch_x, batch_y in train_loader:
        # Forward (uses batch statistics)
        output = model(batch_x)
        loss = criterion(output, batch_y)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

# Inference
model.eval()  # Set to evaluation mode (uses running statistics)

with torch.no_grad():
    test_output = model(test_x)  # Can be single sample!
    pred = test_output.argmax(dim=1)
```

---

## 7. Where to Place Batch Norm?

### Before vs After Activation:

**Original Paper (2015): Batch Norm BEFORE activation**

```python
# Original placement
x = linear(x)
x = batch_norm(x)
x = relu(x)
```

**Modern Practice: Both work, but trends vary**

```python
# Also common (especially in ResNets)
x = linear(x)
x = relu(x)
x = batch_norm(x)
```

**Comparison:**

| Placement | Effect | When to Use |
|-----------|--------|-------------|
| **Before activation** | Normalizes pre-activations | Original paper, default choice |
| **After activation** | Normalizes post-activations | Some ResNet variants |

**In practice:** Before activation is more common and better studied.

---

### Batch Norm in CNNs:

**For convolutional layers:**

```python
class ConvNetWithBatchNorm(nn.Module):
    def __init__(self):
        super().__init__()
        
        # Conv layer 1
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)  # BatchNorm2d for conv!
        
        # Conv layer 2
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        
        # Fully connected
        self.fc = nn.Linear(128 * 7 * 7, 10)
    
    def forward(self, x):
        # Conv block 1
        x = self.conv1(x)        # (B, 64, H, W)
        x = self.bn1(x)          # Normalize per channel
        x = torch.relu(x)
        x = torch.max_pool2d(x, 2)
        
        # Conv block 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = torch.relu(x)
        x = torch.max_pool2d(x, 2)
        
        # Flatten and classify
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        
        return x
```

**How BatchNorm2d works:**

```
Input: (Batch, Channels, Height, Width) = (32, 64, 28, 28)

For each of 64 channels:
  Compute mean and variance across:
  - All 32 samples in batch
  - All 28Ã—28 spatial locations
  
  Total values per channel: 32 Ã— 28 Ã— 28 = 25,088
  
  Normalize all these values together
  One Î³ and one Î² per channel (64 learnable params each)

Result: Each channel normalized independently
```

---

### Batch Norm in RNNs:

**RNNs are tricky - batch norm not commonly used!**

```
Problem with RNNs:
- Variable sequence lengths
- Different timesteps have different statistics
- Batch norm across timesteps problematic

Solutions:
1. Use Layer Normalization instead (more common)
2. Apply batch norm only to input-to-hidden transformation
3. Use specialized variants like Recurrent Batch Norm
```

**Example (if using batch norm in RNN):**

```python
class RNNWithBatchNorm(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size
        
        # Input transformation with batch norm
        self.fc_input = nn.Linear(input_size, hidden_size)
        self.bn_input = nn.BatchNorm1d(hidden_size)
        
        # Recurrent transformation (no batch norm usually)
        self.fc_hidden = nn.Linear(hidden_size, hidden_size)
    
    def forward(self, x, h):
        # Input transformation with batch norm
        x = self.fc_input(x)
        x = self.bn_input(x)  # Normalize input contribution
        
        # Recurrent transformation
        h = self.fc_hidden(h)  # No batch norm here
        
        # Combine
        out = torch.tanh(x + h)
        return out

# More common: Use LayerNorm for RNNs instead!
```

---

## 8. Effects on Training

### Learning Rate with Batch Norm:

**Can use MUCH higher learning rates!**

**Without Batch Norm:**
```
Typical LR with Adam: 0.001
Typical LR with SGD: 0.01

Higher LR causes:
- Gradient explosion
- Unstable training
- Divergence
```

**With Batch Norm:**
```
Can use with Adam: 0.001 - 0.01 (10Ã— higher!)
Can use with SGD: 0.1 - 1.0 (100Ã— higher!)

Benefits:
âœ“ Faster convergence
âœ“ More stable training
âœ“ Can explore wider range of LRs
```

**Example:**

| Configuration | Max Stable LR | Epochs to 90% | Final Acc |
|--------------|---------------|---------------|-----------|
| No Batch Norm + SGD | 0.01 | 25 | 91% |
| Batch Norm + SGD | 0.5 | 8 | 94% |
| No Batch Norm + Adam | 0.003 | 15 | 92% |
| Batch Norm + Adam | 0.01 | 5 | 95% |

**Batch Norm allows 3-5Ã— faster training! âœ“**

---

### Regularization Effect:

**Batch Norm adds noise â†’ acts as regularizer!**

```
Each sample normalized using batch statistics:
xÌ‚áµ¢ = (xáµ¢ - Î¼_B) / Ïƒ_B

But Î¼_B and Ïƒ_B vary batch to batch!

Batch 1: Î¼ = 2.3, Ïƒ = 1.5
Batch 2: Î¼ = 2.7, Ïƒ = 1.8
Batch 3: Î¼ = 2.1, Ïƒ = 1.6

Same input sample gets SLIGHTLY different
normalized values depending on which batch it's in!

This noise acts like regularization!
Reduces overfitting!
```

**Evidence:**

```
Without Batch Norm:
Train Acc: 98%, Test Acc: 87% (overfitting!)

With Batch Norm:
Train Acc: 96%, Test Acc: 92% (better generalization!)

Can often reduce/remove Dropout when using Batch Norm!
```

---

### Convergence Speed:

**Numerical comparison (same network, same data):**

```
Training to 95% test accuracy:

Standard Network:
- Optimizer: SGD, LR: 0.01
- Epochs needed: 45
- Time: 67 seconds
- Best test acc: 95.2%

With Batch Norm:
- Optimizer: SGD, LR: 0.1
- Epochs needed: 12
- Time: 19 seconds
- Best test acc: 96.1%

3.7Ã— faster convergence!
Better final accuracy!
```

**Loss curve comparison:**

```
    Loss
     â†‘
  0.7â”‚â—
     â”‚ â•²___                           Without Batch Norm
  0.6â”‚     â•²___
     â”‚         â•²___
  0.5â”‚             â•²___
     â”‚                 â•²___
  0.4â”‚  â—                  â•²___       With Batch Norm
     â”‚   â•²___                  â•²__    (much faster!)
  0.3â”‚       â•²___
     â”‚           â•²___
  0.2â”‚               â•²___
     â”‚                   â•²___â—
  0.1â”‚                        
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs
      0   10   20   30   40   50

With Batch Norm: Reaches 0.2 loss in 12 epochs
Without: Takes 45 epochs!
```

---

## 9. Batch Size and Batch Norm

### Minimum Batch Size Requirements:

**Batch Norm needs reasonable batch statistics:**

```
Batch size 1:
Î¼ = xâ‚  (just that one value!)
ÏƒÂ² = 0  (can't compute variance!)
Normalization FAILS!

Batch size 2:
Î¼ = (xâ‚ + xâ‚‚) / 2
ÏƒÂ² = [(xâ‚-Î¼)Â² + (xâ‚‚-Î¼)Â²] / 2
Very noisy estimates!

Batch size 8-16:
Reasonable statistics
Works but noisy

Batch size 32+:
Good statistics âœ“
Recommended minimum!
```

---

### Problems with Small Batches:

**Numerical example:**

```
True population: Î¼ = 5, Ïƒ = 2

Batch size 2:
Sample 1: [4.2, 6.8]
Î¼_B = 5.5, Ïƒ_B = 1.3
Normalized: [-1.0, 1.0]

Sample 2: [3.1, 5.9]
Î¼_B = 4.5, Ïƒ_B = 1.4
Normalized: [-1.0, 1.0]

Same conceptual samples, different normalizations!
Very noisy!

Batch size 32:
Samples: [5.1, 4.8, 6.2, 4.9, ..., 5.3] (32 values)
Î¼_B = 4.98, Ïƒ_B = 1.97
Much closer to true Î¼=5, Ïƒ=2! âœ“
Stable normalization!
```

---

### Solutions for Small Batches:

#### 1. Layer Normalization (LayerNorm):

```python
# Normalize across features, not batch
layer_norm = nn.LayerNorm(hidden_size)

# Works with ANY batch size, even 1!
x = torch.randn(1, hidden_size)  # Batch size 1
out = layer_norm(x)  # No problem!

# Computes statistics per sample:
# Î¼ = mean across hidden_size dimension
# ÏƒÂ² = variance across hidden_size dimension
```

---

#### 2. Group Normalization:

```python
# Normalize within groups of channels
group_norm = nn.GroupNorm(
    num_groups=8,      # Divide channels into 8 groups
    num_channels=64
)

# Works with batch size 1
# Each group normalized independently
```

---

#### 3. Increase Batch Size:

```
If possible, use batch size â‰¥ 16, preferably â‰¥ 32

If memory limited:
- Use gradient accumulation
- Simulate larger batch with multiple small batches
- Use Group Norm or Layer Norm instead
```

---

## 10. Alternatives to Batch Normalization

### Layer Normalization:

**Normalize across features instead of batch:**

$$\mu_L = \frac{1}{D}\sum_{i=1}^{D}x_i$$
$$\sigma^2_L = \frac{1}{D}\sum_{i=1}^{D}(x_i - \mu_L)^2$$
$$\hat{x}_i = \frac{x_i - \mu_L}{\sqrt{\sigma^2_L + \epsilon}}$$

**Visual comparison:**

```
Batch Norm:                Layer Norm:
Normalize across batch     Normalize across features

    Features                   Features
    â†“ â†“ â†“ â†“                   â†“ â†“ â†“ â†“
â†’ [1 2 3 4] Sample 1      â†’ [1 2 3 4] â† Normalize across
â†’ [5 6 7 8] Sample 2      â†’ [5 6 7 8] â† Normalize across
â†’ [2 3 4 5] Sample 3      â†’ [2 3 4 5] â† Normalize across
â†’ [6 7 8 9] Sample 4      â†’ [6 7 8 9] â† Normalize across
  â†‘
  Normalize down each feature

Batch Norm:
- Mean/var per feature across batch
- Batch size dependent

Layer Norm:
- Mean/var per sample across features
- Batch size independent!
```

**When to use:**
```
Layer Norm:
âœ“ RNNs, LSTMs, Transformers (standard choice!)
âœ“ Small batch sizes (<16)
âœ“ Reinforcement learning (batch size 1 often)
âœ“ Variable batch sizes
```

---

### Instance Normalization:

**Normalize each sample and each channel independently:**

```
Used in: Style transfer, GANs

Input: (Batch, Channels, Height, Width)

For each sample, for each channel:
  Compute mean and var across (Height Ã— Width)
  Normalize
```

```python
instance_norm = nn.InstanceNorm2d(num_channels)

# Normalizes each sample independently
# Good for style-based tasks
```

---

### Group Normalization:

**Compromise between Layer and Instance Norm:**

```
Divide channels into groups
Normalize within each group

Input: (Batch, Channels, Height, Width)
Groups: 8

For each sample:
  Divide 64 channels into 8 groups of 8
  For each group:
    Compute mean/var across (8 channels Ã— H Ã— W)
    Normalize
```

```python
group_norm = nn.GroupNorm(
    num_groups=8,
    num_channels=64
)

# Batch size independent!
# Works well for small batches
```

---

### Comparison of Normalization Techniques:

| Technique | Normalizes Across | Batch Dependent? | Best For | Batch Size Req |
|-----------|------------------|------------------|----------|----------------|
| **Batch Norm** | (Batch, H, W) per channel | Yes | CNNs, MLPs | â‰¥16, prefer â‰¥32 |
| **Layer Norm** | Features per sample | No | RNNs, Transformers | Any (even 1) |
| **Instance Norm** | (H, W) per sample, per channel | No | Style transfer, GANs | Any |
| **Group Norm** | Channels in groups, per sample | No | Small batch CNNs | Any |

**Visual comparison:**

```
Input shape: (4 batch, 6 channels, H, W)

Batch Norm:
  For channel 1: Mean/Var across all 4 samples' (HÃ—W)
  For channel 2: Mean/Var across all 4 samples' (HÃ—W)
  ...
  Total: 6 normalizations

Layer Norm:
  For sample 1: Mean/Var across all 6 channels' (HÃ—W)
  For sample 2: Mean/Var across all 6 channels' (HÃ—W)
  ...
  Total: 4 normalizations

Instance Norm:
  For each (sample, channel) pair: Mean/Var across (HÃ—W)
  Total: 4 Ã— 6 = 24 normalizations

Group Norm (3 groups of 2 channels):
  For sample 1, group 1: Mean/Var across channels 1-2's (HÃ—W)
  For sample 1, group 2: Mean/Var across channels 3-4's (HÃ—W)
  ...
  Total: 4 Ã— 3 = 12 normalizations
```

---

## 11. Advanced Topics

### Batch Renormalization:

**Problem with standard Batch Norm:**
```
Small batches â†’ Noisy batch statistics
Test time uses different statistics (running avg)
Can cause train/test mismatch
```

**Batch Renormalization solution:**

$$y = \frac{\gamma}{\sigma_B}(x - \mu_B) + \beta$$

But constrains ratio $\frac{\sigma_B}{\sigma_{running}}$ and difference $\mu_B - \mu_{running}$

```python
# Constrains batch stats to be close to running stats
# More stable with small batches
# Rarely used in practice
```

---

### Weight Normalization:

**Instead of normalizing activations, normalize weights!**

$$\hat{W} = \frac{g}{\|v\|}v$$

Where:
- $v$ = Weight vector
- $g$ = Learnable scalar
- $\hat{W}$ = Normalized weight

**Benefits:**
- Batch size independent
- Faster than batch norm (no mean/var computation)

**Drawbacks:**
- Doesn't address covariate shift
- Less effective than batch norm for most tasks

---

### Spectral Normalization:

**Normalize weights by largest singular value:**

$$\hat{W} = \frac{W}{\sigma(W)}$$

Where $\sigma(W)$ = largest singular value of $W$

**Use case:**
- GANs (prevents discriminator from being too strong)
- Ensures Lipschitz continuity

```python
# Spectral norm for GANs
discriminator = nn.Sequential(
    nn.utils.spectral_norm(nn.Linear(100, 128)),
    nn.LeakyReLU(),
    nn.utils.spectral_norm(nn.Linear(128, 1))
)
```

---

## 12. Practical Guidelines

### When to Use Batch Norm:

```
âœ“ ALWAYS try Batch Norm for:
  - CNNs (image classification, object detection)
  - MLPs (fully connected networks)
  - Deep networks (>5 layers)

âœ“ Provides benefits:
  - Faster training (2-5Ã— speedup)
  - Higher learning rates possible
  - Less sensitive to initialization
  - Regularization effect
  - Better gradient flow

âœ— DON'T use Batch Norm for:
  - RNNs (use Layer Norm instead)
  - Small batches (<8, use Group/Layer Norm)
  - Online learning (batch size 1)
  - When batch statistics unreliable
```

---

### Hyperparameter Guidelines:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Batch Norm Hyperparameters         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Momentum (for running stats):
â”œâ”€ Default: 0.1
â”œâ”€ Large datasets: 0.01 (more history)
â”œâ”€ Small datasets: 0.1-0.2 (faster adaptation)
â””â”€ Almost always use default!

Epsilon (Îµ):
â”œâ”€ Default: 1e-5
â”œâ”€ FP16 training: 1e-3 (larger for stability)
â”œâ”€ Almost never change for FP32
â””â”€ Use default!

Affine parameters (Î³, Î²):
â”œâ”€ Default: True (learnable)
â”œâ”€ Can set to False (no scale/shift)
â”œâ”€ Almost always keep True!
â””â”€ Let network learn optimal distribution

Placement:
â”œâ”€ After linear/conv, before activation (standard)
â”œâ”€ After activation (alternative, less common)
â””â”€ Never on output layer
```

---

### Common Mistakes:

#### âŒ Mistake 1: Using with Very Small Batches

```python
# BAD: Batch norm with batch size 2
train_loader = DataLoader(dataset, batch_size=2)
model = nn.Sequential(
    nn.Linear(100, 50),
    nn.BatchNorm1d(50),  # Will be very noisy!
    nn.ReLU()
)

# GOOD: Use larger batch size OR different normalization
train_loader = DataLoader(dataset, batch_size=32)
# OR
model = nn.Sequential(
    nn.Linear(100, 50),
    nn.LayerNorm(50),  # Works with any batch size!
    nn.ReLU()
)
```

---

#### âŒ Mistake 2: Forgetting to Switch to Eval Mode

```python
# BAD: Using batch statistics at test time
model.train()  # Still in training mode!
with torch.no_grad():
    predictions = model(test_data)  # Using batch stats from test data!
    # If test batch size differs from train, results will be inconsistent!

# GOOD: Switch to eval mode
model.eval()  # Use running statistics
with torch.no_grad():
    predictions = model(test_data)  # Consistent results!
```

---

#### âŒ Mistake 3: Applying Batch Norm to Output Layer

```python
# BAD: Batch norm on output
model = nn.Sequential(
    nn.Linear(100, 50),
    nn.BatchNorm1d(50),
    nn.ReLU(),
    nn.Linear(50, 10),
    nn.BatchNorm1d(10),  # NO! Don't normalize outputs!
    nn.Softmax(dim=1)
)

# GOOD: No batch norm on output
model = nn.Sequential(
    nn.Linear(100, 50),
    nn.BatchNorm1d(50),
    nn.ReLU(),
    nn.Linear(50, 10),  # No batch norm here
    nn.Softmax(dim=1)
)
```

---

#### âŒ Mistake 4: Wrong Batch Norm for Layer Type

```python
# BAD: BatchNorm1d for Conv2d
conv = nn.Conv2d(3, 64, 3)
bn = nn.BatchNorm1d(64)  # WRONG!

# GOOD: Use BatchNorm2d for convolutions
conv = nn.Conv2d(3, 64, 3)
bn = nn.BatchNorm2d(64)  # Correct!

# Rule:
# - Linear/MLP: BatchNorm1d
# - Conv2d: BatchNorm2d
# - Conv3d: BatchNorm3d
```

---

#### âŒ Mistake 5: Not Tracking Running Stats

```python
# BAD: Creating batch norm without allowing state updates
model = nn.Sequential(
    nn.Linear(100, 50),
    nn.BatchNorm1d(50, track_running_stats=False)  # Bad idea!
)

# GOOD: Default behavior (track running stats)
model = nn.Sequential(
    nn.Linear(100, 50),
    nn.BatchNorm1d(50)  # track_running_stats=True by default
)
```

---

## 13. Summary: Batch Normalization

### What Batch Norm Does:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Batch Normalization              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FORMULA (Training):
  Î¼_B = (1/B)âˆ‘xáµ¢
  ÏƒÂ²_B = (1/B)âˆ‘(xáµ¢ - Î¼_B)Â²
  xÌ‚áµ¢ = (xáµ¢ - Î¼_B) / âˆš(ÏƒÂ²_B + Îµ)
  yáµ¢ = Î³Â·xÌ‚áµ¢ + Î²

FORMULA (Inference):
  xÌ‚áµ¢ = (xáµ¢ - Î¼_running) / âˆš(ÏƒÂ²_running + Îµ)
  yáµ¢ = Î³Â·xÌ‚áµ¢ + Î²

EFFECT:
- Normalizes layer inputs to mean=0, std=1
- Reduces internal covariate shift
- Stabilizes training
- Allows higher learning rates
- Regularization effect

PARAMETERS:
- Î³ (scale): Learned, init to 1
- Î² (shift): Learned, init to 0  
- Î¼_running: EWA of batch means
- ÏƒÂ²_running: EWA of batch variances
- Îµ = 10â»âµ (stability constant)

ADVANTAGES:
âœ“ 2-5Ã— faster training
âœ“ Higher learning rates possible
âœ“ Less sensitive to initialization
âœ“ Regularization (reduces overfitting)
âœ“ Better gradient flow
âœ“ Enables very deep networks

DISADVANTAGES:
âœ— Batch size dependent (â‰¥16 recommended)
âœ— Different behavior train vs test
âœ— Extra memory for running statistics
âœ— Slower per iteration (minimal)
âœ— Can be tricky with RNNs
```

---

### Key Formulas:

**Training Mode:**
$$\mu_B = \frac{1}{B}\sum_{i=1}^{B}x_i$$
$$\sigma^2_B = \frac{1}{B}\sum_{i=1}^{B}(x_i - \mu_B)^2$$
$$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma^2_B + \epsilon}}$$
$$y_i = \gamma\hat{x}_i + \beta$$

**Running Statistics Update:**
$$\mu_{\text{running}} = \lambda \mu_{\text{running}} + (1-\lambda)\mu_B$$
$$\sigma^2_{\text{running}} = \lambda \sigma^2_{\text{running}} + (1-\lambda)\sigma^2_B$$

**Inference Mode:**
$$\hat{x}_i = \frac{x_i - \mu_{\text{running}}}{\sqrt{\sigma^2_{\text{running}} + \epsilon}}$$
$$y_i = \gamma\hat{x}_i + \beta$$

---

### Practical Recommendations:

```
âœ“ Use Batch Norm for CNNs and MLPs
âœ“ Use Layer Norm for RNNs and Transformers
âœ“ Use batch size â‰¥ 32 with Batch Norm
âœ“ Place after linear/conv, before activation
âœ“ Don't apply to output layer
âœ“ Use running stats for inference (model.eval())
âœ“ Can use higher learning rates (5-10Ã— increase)
âœ“ Reduces need for Dropout
âœ“ Initialize Î³=1, Î²=0 (defaults)

âœ— Don't use with batch size <8
âœ— Don't forget to call model.eval() for testing
âœ— Don't use wrong BatchNorm type (1d vs 2d vs 3d)
âœ— Don't disable running stats tracking
âœ— Don't apply to recurrent connections in RNNs
```

---

### Complete Example: CNN with Batch Norm

```python
import torch
import torch.nn as nn
import torch.optim as optim

class CNN_WithBatchNorm(nn.Module):
    """CNN for image classification with Batch Normalization"""
    
    def __init__(self, num_classes=10):
        super().__init__()
        
        # Convolutional layers with Batch Norm
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(256)
        
        # Fully connected with Batch Norm
        self.fc1 = nn.Linear(256 * 4 * 4, 512)
        self.bn4 = nn.BatchNorm1d(512)
        
        self.fc2 = nn.Linear(512, num_classes)
        # No batch norm on output!
    
    def forward(self, x):
        # Conv block 1
        x = self.conv1(x)
        x = self.bn1(x)      # Normalize
        x = torch.relu(x)
        x = torch.max_pool2d(x, 2)
        
        # Conv block 2
        x = self.conv2(x)
        x = self.bn2(x)      # Normalize
        x = torch.relu(x)
        x = torch.max_pool2d(x, 2)
        
        # Conv block 3
        x = self.conv3(x)
        x = self.bn3(x)      # Normalize
        x = torch.relu(x)
        x = torch.max_pool2d(x, 2)
        
        # Flatten
        x = x.view(x.size(0), -1)
        
        # FC block
        x = self.fc1(x)
        x = self.bn4(x)      # Normalize
        x = torch.relu(x)
        
        # Output (no batch norm!)
        x = self.fc2(x)
        
        return x


# Create model
model = CNN_WithBatchNorm(num_classes=10)

# Optimizer (can use higher LR with batch norm!)
optimizer = optim.SGD(
    model.parameters(),
    lr=0.1,  # 10Ã— higher than without batch norm!
    momentum=0.9
)

criterion = nn.CrossEntropyLoss()

# Training
model.train()  # Enable batch norm training mode

for epoch in range(20):
    for batch_x, batch_y in train_loader:
        # Forward (uses batch statistics)
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    # Validation
    model.eval()  # Switch to inference mode
    with torch.no_grad():
        val_outputs = model(val_x)
        val_acc = (val_outputs.argmax(1) == val_y).float().mean()
    
    print(f"Epoch {epoch}: Val Acc = {val_acc:.2%}")
    model.train()  # Back to training mode

# Final testing
model.eval()  # Inference mode
with torch.no_grad():
    test_outputs = model(test_x)
    test_acc = (test_outputs.argmax(1) == test_y).float().mean()

print(f"\nFinal Test Accuracy: {test_acc:.2%}")
```

---

**You now understand Batch Normalization completely! ğŸ‰**

The key insights:
- **Batch Norm stabilizes layer inputs during training**
- **Reduces internal covariate shift** (changing distributions)
- **Allows 5-10Ã— higher learning rates**
- **Acts as regularizer** (reduces overfitting)
- **Enables training of very deep networks**
- **Different behavior: training vs inference**
- **Requires batch size â‰¥16, preferably â‰¥32**
- **Use LayerNorm for RNNs/Transformers instead**

