# Foundations of Convolutional Neural Networks
## Understanding CNNs from First Principles
### (Detailed Step-by-Step with Visual Examples)

## Table of Contents

### Part 1: Edge Detection and Padding
- [Overview & Connection to Previous Topics](#-connection-to-previous-topics)
- [1. Understanding Edge Detection](#part-1-understanding-edge-detection)
  - [Plain English Explanation](#1-plain-english-explanation)
  - [The Core Idea](#the-core-idea-convolution)
  - [Photography Analogy](#real-world-analogy-photography-filters)
  - [Computer Vision Context](#computer-vision-context)
- [2. Convolution Operation](#2-the-convolution-operation)
  - [Mathematical Definition](#mathematical-definition)
  - [Step-by-Step Process](#step-by-step-process)
  - [Key Components](#key-components)
- [3. Complete Numerical Example](#3-complete-numerical-example-vertical-edge)
  - [Vertical Edge Detection](#vertical-edge-detection)
  - [Horizontal Edge Detection](#horizontal-edge-detection)
  - [Different Filter Types](#different-edge-detection-filters)
- [4. Visual Examples](#4-visual-examples-with-images)
  - [Simple Patterns](#detecting-edges-in-simple-patterns)
  - [Detailed Calculations](#detailed-step-by-step-calculation)
- [5. The Padding Problem](#5-the-padding-problem)
  - [Output Size Reduction](#problem-1-shrinking-output)
  - [Edge Information Loss](#problem-2-losing-edge-information)
  - [Numerical Impact](#numerical-example-of-shrinkage)
- [6. Padding Solutions](#6-padding-solutions)
  - [Valid Padding](#valid-padding-no-padding)
  - [Same Padding](#same-padding)
  - [Full Padding](#full-padding)
  - [Comparison](#padding-comparison)
- [7. Padding Calculations](#7-padding-calculations)
  - [Output Size Formula](#output-size-formula)
  - [Same Padding Formula](#calculating-same-padding)
  - [Examples](#numerical-examples)
- [8. Complete Implementation](#8-complete-pytorch-implementation)
  - [Manual Convolution](#manual-convolution-implementation)
  - [Built-in Conv2d](#using-pytorch-conv2d)
  - [Custom Filters](#implementing-custom-edge-detectors)
- [9. Types of Padding](#9-types-of-padding-in-detail)
  - [Zero Padding](#zero-padding-most-common)
  - [Reflection Padding](#reflection-padding)
  - [Replication Padding](#replication-padding)
  - [Circular Padding](#circular-padding)
- [10. Learned Filters](#10-learning-filters-automatically)
  - [From Fixed to Learned](#from-hand-crafted-to-learned-filters)
  - [Training Process](#how-cnns-learn-filters)
- [11. Multiple Channels](#11-convolution-with-multiple-channels)
  - [RGB Images](#rgb-images-3-channels)
- [12. Practical Guidelines](#12-practical-guidelines)
- [13. Edge Detection Summary](#13-summary-edge-detection-and-padding)

---

---

## ğŸ”— **Connection to Previous Topics**

### **What We Know So Far:**

**From Fully Connected Networks:**
```
For image classification:
- Flatten 28Ã—28 image â†’ 784-dimensional vector
- Feed to fully connected layers
- Learn weights for each pixel connection

Example: MNIST digits
Input: 28Ã—28 = 784 pixels
Hidden: 128 neurons
Weights: 784Ã—128 = 100,352 parameters!

For ImageNet (224Ã—224 RGB):
Input: 224Ã—224Ã—3 = 150,528 pixels
Hidden: 1000 neurons
Weights: 150,528Ã—1000 = 150,528,000 parameters!

Too many! âœ—
```

**The Problems:**

```
1. Too many parameters
   - Hard to train
   - Requires huge amounts of data
   - Prone to overfitting
   - Computationally expensive

2. No spatial awareness
   - Treats pixel at (0,0) and (27,27) independently
   - Doesn't understand that nearby pixels form patterns
   - Can't detect edges, shapes, textures
   - Position-dependent (can't recognize shifted objects)

3. Not translation invariant
   - Cat in top-left corner â‰  Cat in bottom-right
   - Must learn same feature in every position
   - Inefficient use of parameters
```

**The Solution: Convolutional Neural Networks**

```
Key innovations:
âœ“ Local connectivity (small filters)
âœ“ Weight sharing (same filter everywhere)
âœ“ Spatial awareness (preserve 2D structure)
âœ“ Translation invariance (detect patterns anywhere)
âœ“ Dramatically fewer parameters

Foundation: CONVOLUTION operation
First application: EDGE DETECTION
```

---

# Part 1: Understanding Edge Detection

## 1. Plain English Explanation

### The Core Idea: Convolution

**Convolution:** "Slide a small pattern detector (filter/kernel) across an image to find where that pattern exists"

Think of it as a small "window" that looks at a tiny region of the image at a time, performs a mathematical operation, then slides to the next region.

---

### Real-World Analogy: Photography Filters

Imagine a photographer using different lens filters:

**Scenario 1: Edge-Detection Lens**
```
Photographer's process:
1. Point lens at small 3Ã—3 area of scene
2. Lens highlights where brightness changes rapidly
3. Move lens slightly (by 1 pixel)
4. Repeat for entire scene
5. Result: Map showing all edges!

Applied to building photo:
- Vertical edges (walls): Highlighted!
- Horizontal edges (roof, windows): Highlighted!
- Smooth areas (sky, walls): Suppressed
- Result: Outline of building! âœ“
```

**Scenario 2: Blur Lens**
```
Different lens, different effect:
1. Point at 3Ã—3 area
2. Lens averages all pixels in area
3. Move to next area
4. Result: Smooth, blurred image
```

**Key concept:** Different filters detect different patterns!

---

### Computer Vision Context

**What is an edge?**

```
An edge is a rapid change in brightness/color

Vertical edge:          Horizontal edge:
Dark â†’ Light           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â† Dark
                       _______ â† Light

Real example:
Photo of a door:
- Wall (dark gray): 50, 50, 50, 50
- Door frame (bright): 200, 200, 200, 200

Transition 50 â†’ 200 = EDGE!
```

**Why edges matter:**

```
Edges define:
âœ“ Object boundaries (where one object ends, another begins)
âœ“ Shapes (circle, square, triangle)
âœ“ Textures (rough vs smooth surfaces)
âœ“ Depth cues (foreground vs background)

Edges are fundamental building blocks!
Low-level features that combine to form high-level objects!
```

---

## 2. The Convolution Operation

### Mathematical Definition:

For a 2D image $I$ and filter (kernel) $K$:

$$(I * K)_{ij} = \sum_{m}\sum_{n}I_{i+m, j+n} \cdot K_{m,n}$$

Where:
- $I$ = Input image (e.g., 6Ã—6)
- $K$ = Filter/kernel (e.g., 3Ã—3)
- $*$ = Convolution operator (not multiplication!)
- $(i,j)$ = Position in output
- $(m,n)$ = Position within filter

**In plain terms:** Element-wise multiply and sum!

---

### Step-by-Step Process:

```
Input image (6Ã—6):          Filter (3Ã—3):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3 3 2 1 0 0â”‚            â”‚ 1  0 -1â”‚
â”‚ 0 0 1 3 1 0â”‚            â”‚ 1  0 -1â”‚
â”‚ 3 1 2 2 3 0â”‚            â”‚ 1  0 -1â”‚
â”‚ 2 0 0 2 2 0â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ 2 0 0 0 1 0â”‚
â”‚ 0 1 1 0 0 2â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Position filter at top-left (0,0)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3 3 2â”‚ 1 0 0
â”‚ 0 0 1â”‚ 3 1 0
â”‚ 3 1 2â”‚ 2 3 0
â””â”€â”€â”€â”€â”€â”€â”€â”˜
  2 0 0 2 2 0
  2 0 0 0 1 0
  0 1 1 0 0 2

Compute: (3Ã—1 + 3Ã—0 + 2Ã—(-1)) +
         (0Ã—1 + 0Ã—0 + 1Ã—(-1)) +
         (3Ã—1 + 1Ã—0 + 2Ã—(-1))
       = (3 + 0 - 2) + (0 + 0 - 1) + (3 + 0 - 2)
       = 1 + (-1) + 1
       = 1

Output[0,0] = 1

Step 2: Slide filter one position right (0,1)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”
3 â”‚ 3 2 1â”‚ 0 0
0 â”‚ 0 1 3â”‚ 1 0
3 â”‚ 1 2 2â”‚ 3 0
  â””â”€â”€â”€â”€â”€â”€â”€â”˜
  2 0 0 2 2 0
  2 0 0 0 1 0
  0 1 1 0 0 2

Compute: (3Ã—1 + 2Ã—0 + 1Ã—(-1)) +
         (0Ã—1 + 1Ã—0 + 3Ã—(-1)) +
         (1Ã—1 + 2Ã—0 + 2Ã—(-1))
       = (3 + 0 - 1) + (0 + 0 - 3) + (1 + 0 - 2)
       = 2 + (-3) + (-1)
       = -2

Output[0,1] = -2

Step 3: Continue sliding...
Slide right until can't fit (4Ã—4 output)
Then move down one row and repeat

Final output: 4Ã—4 matrix
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1 -2  3 4â”‚
â”‚ 5  6 -7 8â”‚
â”‚ 9 10 11 2â”‚
â”‚ 3  4  5 6â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Components:

| Component | Dimension | Purpose |
|-----------|-----------|---------|
| **Input Image** | n Ã— n (e.g., 6Ã—6) | Original image |
| **Filter/Kernel** | f Ã— f (e.g., 3Ã—3) | Pattern to detect |
| **Stride** | s (e.g., 1) | How many pixels to slide |
| **Output** | (n-f+1) Ã— (n-f+1) | Convolved feature map |

**For 6Ã—6 input with 3Ã—3 filter:**
- Output size: (6-3+1) Ã— (6-3+1) = 4Ã—4

---

## 3. Complete Numerical Example: Vertical Edge

### Vertical Edge Detection

**Input image (6Ã—6): Dark left, bright right**

```
Image represents a vertical edge in middle:
Dark side | Bright side

Pixel values (0=black, 10=white):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3  3  2â”‚ 0  0  0â”‚
â”‚ 3  3  2â”‚ 0  0  0â”‚
â”‚ 3  3  2â”‚ 0  0  0â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3  3  2â”‚ 0  0  0â”‚
â”‚ 3  3  2â”‚ 0  0  0â”‚
â”‚ 3  3  2â”‚ 0  0  0â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†‘
Vertical edge here (darkâ†’bright transition)
```

**Vertical edge detector filter (3Ã—3):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1   0  -1 â”‚  â† Left column: +1 (detect dark)
â”‚ 1   0  -1 â”‚  â† Middle: 0 (ignore)
â”‚ 1   0  -1 â”‚  â† Right column: -1 (detect bright)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

This filter detects: dark (left) â†’ bright (right)
```

---

### Position 1: Top-left (0,0)

```
Image region:        Filter:           Element-wise multiply:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3 3 2â”‚           â”‚ 1 0 -1â”‚          â”‚ 3  0 -2â”‚
â”‚ 3 3 2â”‚     Ã—     â”‚ 1 0 -1â”‚    =     â”‚ 3  0 -2â”‚
â”‚ 3 3 2â”‚           â”‚ 1 0 -1â”‚          â”‚ 3  0 -2â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”˜

Sum all elements:
3 + 0 + (-2) + 3 + 0 + (-2) + 3 + 0 + (-2)
= 3 - 2 + 3 - 2 + 3 - 2
= 3

Output[0,0] = 3

Interpretation: Uniform dark region (no edge here)
```

---

### Position 2: One column right (0,1)

```
Image region:        Filter:           Result:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3 2 0â”‚           â”‚ 1 0 -1â”‚          â”‚ 3  0  0â”‚
â”‚ 3 2 0â”‚     Ã—     â”‚ 1 0 -1â”‚    =     â”‚ 3  0  0â”‚
â”‚ 3 2 0â”‚           â”‚ 1 0 -1â”‚          â”‚ 3  0  0â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”˜

Sum: 3 + 0 + 0 + 3 + 0 + 0 + 3 + 0 + 0 = 9

Output[0,1] = 9

Interpretation: Medium transition (getting closer to edge!)
```

---

### Position 3: Center (0,2) - RIGHT AT THE EDGE!

```
Image region:        Filter:           Result:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2 0 0â”‚           â”‚ 1 0 -1â”‚          â”‚ 2  0  0â”‚
â”‚ 2 0 0â”‚     Ã—     â”‚ 1 0 -1â”‚    =     â”‚ 2  0  0â”‚
â”‚ 2 0 0â”‚           â”‚ 1 0 -1â”‚          â”‚ 2  0  0â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”˜

Sum: 2 + 0 + 0 + 2 + 0 + 0 + 2 + 0 + 0 = 6

Output[0,2] = 6

Still detecting transition!
```

---

### Position 4: Right side (0,3)

```
Image region:        Filter:           Result:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0 0 0â”‚           â”‚ 1 0 -1â”‚          â”‚ 0  0  0â”‚
â”‚ 0 0 0â”‚     Ã—     â”‚ 1 0 -1â”‚    =     â”‚ 0  0  0â”‚
â”‚ 0 0 0â”‚           â”‚ 1 0 -1â”‚          â”‚ 0  0  0â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”˜

Sum: 0 + 0 + 0 + ... = 0

Output[0,3] = 0

Interpretation: Uniform bright region (no edge)
```

---

### Complete Output (4Ã—4):

```
Convolving entire image:

Output = [
  [ 3,  9,  6,  0],  â† Top row
  [ 3,  9,  6,  0],
  [ 3,  9,  6,  0],
  [ 3,  9,  6,  0]   â† Bottom row
]

Visualization:
    0   1   2   3
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
0 â”‚ 3   9   6   0 â”‚ â† Weak  Strong Edge Weak None
1 â”‚ 3   9   6   0 â”‚
2 â”‚ 3   9   6   0 â”‚
3 â”‚ 3   9   6   0 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†‘
    Vertical edge detected!
    Largest values (9, 6) indicate edge location!
```

---

### Interpretation:

```
Original image:       Edge detection output:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dark â”‚Bright â”‚     â”‚ Low â”‚Highâ”‚Medâ”‚
â”‚ Side â”‚ Side  â”‚     â”‚     â”‚    â”‚   â”‚
â”‚      â”‚       â”‚  â†’  â”‚  3  â”‚ 9  â”‚ 6 â”‚
â”‚      â”‚       â”‚     â”‚     â”‚    â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†‘                    â†‘
  Vertical edge     Detected here! (peak at 9)

Filter successfully found the vertical edge! âœ“
```

---

## 4. Visual Examples with Images

### Detecting Edges in Simple Patterns

**Example 1: Clear vertical edge**

```
Input (6Ã—6):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚10 10â”‚ 0  0â”‚  10 = bright
â”‚10 10â”‚ 0  0â”‚   0 = dark
â”‚10 10â”‚ 0  0â”‚
â”‚10 10â”‚ 0  0â”‚
â”‚10 10â”‚ 0  0â”‚
â”‚10 10â”‚ 0  0â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†‘
Sharp vertical edge

Vertical edge filter:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  0 -1 â”‚
â”‚ 1  0 -1 â”‚
â”‚ 1  0 -1 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Output (4Ã—4):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  0  30 -30  0  â”‚
â”‚  0  30 -30  0  â”‚
â”‚  0  30 -30  0  â”‚
â”‚  0  30 -30  0  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†‘   â†‘
    Edge detected!
```

---

### Detailed Step-by-Step Calculation:

**Position (0,0) - Far left:**

```
Region:             Filter:           Multiply:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚10 10 10 â”‚        â”‚ 1  0 -1 â”‚     â”‚10  0 -10â”‚
â”‚10 10 10 â”‚   Ã—    â”‚ 1  0 -1 â”‚  =  â”‚10  0 -10â”‚
â”‚10 10 10 â”‚        â”‚ 1  0 -1 â”‚     â”‚10  0 -10â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sum: 10 + 0 - 10 + 10 + 0 - 10 + 10 + 0 - 10 = 0

No edge (all bright pixels)
```

**Position (0,1) - AT THE EDGE:**

```
Region:             Filter:           Multiply:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚10 10  0 â”‚        â”‚ 1  0 -1 â”‚     â”‚10  0   0â”‚
â”‚10 10  0 â”‚   Ã—    â”‚ 1  0 -1 â”‚  =  â”‚10  0   0â”‚
â”‚10 10  0 â”‚        â”‚ 1  0 -1 â”‚     â”‚10  0   0â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sum: 10 + 0 + 0 + 10 + 0 + 0 + 10 + 0 + 0 = 30

Strong positive response! Edge detected! âœ“
```

**Position (0,2) - Just past edge:**

```
Region:             Filter:           Multiply:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚10  0  0 â”‚        â”‚ 1  0 -1 â”‚     â”‚10  0   0â”‚
â”‚10  0  0 â”‚   Ã—    â”‚ 1  0 -1 â”‚  =  â”‚10  0   0â”‚
â”‚10  0  0 â”‚        â”‚ 1  0 -1 â”‚     â”‚10  0   0â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sum: 10 + 0 + 0 + 10 + 0 + 0 + 10 + 0 + 0 = 30

Wait, this is wrong! Let me recalculate:

Region:             Filter:           Multiply:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚10  0  0 â”‚        â”‚ 1  0 -1 â”‚     â”‚10  0   0â”‚
â”‚10  0  0 â”‚   Ã—    â”‚ 1  0 -1 â”‚  =  â”‚10  0   0â”‚
â”‚10  0  0 â”‚        â”‚ 1  0 -1 â”‚     â”‚10  0   0â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Multiply element-wise:
(10Ã—1) + (0Ã—0) + (0Ã—(-1)) +
(10Ã—1) + (0Ã—0) + (0Ã—(-1)) +
(10Ã—1) + (0Ã—0) + (0Ã—(-1))
= 10 + 0 + 0 + 10 + 0 + 0 + 10 + 0 + 0 = 30

Hmm, still 30. Let me reconsider the image:

Actually, for position (0,2), the region would be:
Columns 2, 3, 4 of the image

Region:             Filter:           Multiply:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0  0  0 â”‚        â”‚ 1  0 -1 â”‚     â”‚ 0  0   0â”‚
â”‚ 0  0  0 â”‚   Ã—    â”‚ 1  0 -1 â”‚  =  â”‚ 0  0   0â”‚
â”‚ 0  0  0 â”‚        â”‚ 1  0 -1 â”‚     â”‚ 0  0   0â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sum: 0

No edge (all dark pixels)

Better! The edge is between positions 1 and 2.
```

Let me recalculate with a clearer image:

```
Clearer vertical edge image:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3  3  3â”‚ 0  0  0â”‚  Position of |
â”‚ 3  3  3â”‚ 0  0  0â”‚  vertical edge
â”‚ 3  3  3â”‚ 0  0  0â”‚
â”‚ 3  3  3â”‚ 0  0  0â”‚
â”‚ 3  3  3â”‚ 0  0  0â”‚
â”‚ 3  3  3â”‚ 0  0  0â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Position (0,0):
Region: [3,3,3], [3,3,3], [3,3,3]
Filter: [1,0,-1], [1,0,-1], [1,0,-1]
Result: 3+0-3+3+0-3+3+0-3 = 0
(Uniform bright, no edge)

Position (0,1):
Region: [3,3,0], [3,3,0], [3,3,0]
Filter: [1,0,-1], [1,0,-1], [1,0,-1]
Result: 3+0-0+3+0-0+3+0-0 = 9
(At the edge! Strong response!)

Position (0,2):
Region: [3,0,0], [3,0,0], [3,0,0]
Filter: [1,0,-1], [1,0,-1], [1,0,-1]
Result: 3+0-0+3+0-0+3+0-0 = 9
(Still strong)

Position (0,3):
Region: [0,0,0], [0,0,0], [0,0,0]
Filter: [1,0,-1], [1,0,-1], [1,0,-1]
Result: 0+0-0+0+0-0+0+0-0 = 0
(Uniform dark, no edge)

Complete output (4Ã—4):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0  9  9  0 â”‚
â”‚ 0  9  9  0 â”‚
â”‚ 0  9  9  0 â”‚
â”‚ 0  9  9  0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†‘   â†‘
   Edge detected!
```

---

### Horizontal Edge Detection

**Input image: Dark top, bright bottom**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3  3  3  3  3  3â”‚ â† Dark
â”‚ 3  3  3  3  3  3â”‚
â”‚ 3  3  3  3  3  3â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â† Horizontal edge
â”‚ 0  0  0  0  0  0â”‚ â† Bright
â”‚ 0  0  0  0  0  0â”‚
â”‚ 0  0  0  0  0  0â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Horizontal edge detector filter:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  1  1 â”‚ â† Top row: +1 (detect dark)
â”‚ 0  0  0 â”‚ â† Middle: 0 (ignore)
â”‚-1 -1 -1 â”‚ â† Bottom row: -1 (detect bright)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Apply convolution:**

```
Position (0,0) - Top rows (all dark):
Region: [3,3,3], [3,3,3], [3,3,3]
Filter: [1,1,1], [0,0,0], [-1,-1,-1]
Result: 3+3+3+0+0+0-3-3-3 = 0
(No edge)

Position (1,0) - At transition:
Region: [3,3,3], [3,3,3], [0,0,0]
Filter: [1,1,1], [0,0,0], [-1,-1,-1]
Result: 3+3+3+0+0+0-0-0-0 = 9
(Edge detected!)

Position (2,0) - Just past edge:
Region: [3,3,3], [0,0,0], [0,0,0]
Filter: [1,1,1], [0,0,0], [-1,-1,-1]
Result: 3+3+3+0+0+0-0-0-0 = 9
(Still strong)

Position (3,0) - Bottom (all bright):
Region: [0,0,0], [0,0,0], [0,0,0]
Filter: [1,1,1], [0,0,0], [-1,-1,-1]
Result: 0+0+0+0+0+0-0-0-0 = 0
(No edge)

Output (4Ã—4):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0  0  0  0 â”‚
â”‚ 9  9  9  9 â”‚ â† Horizontal edge detected!
â”‚ 9  9  9  9 â”‚
â”‚ 0  0  0  0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Different Edge Detection Filters:

**1. Sobel Filter (more weight on center):**

```
Vertical Sobel:         Horizontal Sobel:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  0 -1 â”‚           â”‚ 1  2  1 â”‚
â”‚ 2  0 -2 â”‚           â”‚ 0  0  0 â”‚
â”‚ 1  0 -1 â”‚           â”‚-1 -2 -1 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Emphasizes center row/column (Ã—2)
Less sensitive to noise
```

**2. Scharr Filter (even more emphasis):**

```
Vertical Scharr:        Horizontal Scharr:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3  0 -3 â”‚           â”‚ 3 10  3 â”‚
â”‚10  0-10 â”‚           â”‚ 0  0  0 â”‚
â”‚ 3  0 -3 â”‚           â”‚-3-10 -3 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Even stronger center emphasis
Better edge localization
```

**3. Prewitt Filter:**

```
Vertical Prewitt:       Horizontal Prewitt:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  0 -1 â”‚           â”‚ 1  1  1 â”‚
â”‚ 1  0 -1 â”‚           â”‚ 0  0  0 â”‚
â”‚ 1  0 -1 â”‚           â”‚-1 -1 -1 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Simpler than Sobel
Equal weighting
```

---

## 5. The Padding Problem

### Problem 1: Shrinking Output

**Every convolution reduces image size:**

```
Input: 6Ã—6
Filter: 3Ã—3
Output: 4Ã—4 (lost 2 pixels per dimension!)

After 5 layers of convolution:
6Ã—6 â†’ 4Ã—4 â†’ 2Ã—2 â†’ 0Ã—0 (vanishes!)

Can only apply convolution a few times
before image shrinks to nothing!
```

---

### Problem 2: Losing Edge Information

**Corner and edge pixels used less frequently:**

```
6Ã—6 image, 3Ã—3 filter:

Center pixel (3,3):
Used in 9 different filter positions!
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ... ... ... ..â”‚
â”‚ ... â—â”€â”€ â—â”€â”€ â—â”€â”‚
â”‚ ... â”‚   â”‚   â”‚ â”‚
â”‚ ... â—â”€â”€ â—â”€â”€ â—â”€â”‚ â† Pixel (3,3) participates
â”‚ ... â”‚   â”‚   â”‚ â”‚    in 9 convolutions
â”‚ ... â—â”€â”€ â—â”€â”€ â—â”€â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Corner pixel (0,0):
Used in ONLY 1 filter position!
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â—â”€â”€ ... ... ..â”‚ â† Pixel (0,0) only
â”‚ â”‚   ... ... ..â”‚    participates once!
â”‚ â”‚   ... ... ..â”‚
â”‚ ... ... ... ..â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Edge pixels get "thrown away"!
Lose information about image boundaries!
```

---

### Numerical Example of Shrinkage:

**Without padding:**

```
Layer 1:
Input: 32Ã—32
Filter: 3Ã—3
Output: 30Ã—30 (lost 2 pixels)

Layer 2:
Input: 30Ã—30
Filter: 3Ã—3
Output: 28Ã—28 (lost 2 more)

Layer 3:
Input: 28Ã—28
Filter: 3Ã—3
Output: 26Ã—26

Layer 4:
Input: 26Ã—26
Filter: 3Ã—3
Output: 24Ã—24

Layer 5:
Input: 24Ã—24
Filter: 3Ã—3
Output: 22Ã—22

After 5 conv layers: 32Ã—32 â†’ 22Ã—22
Lost 10 pixels per dimension (31% of image!)

After 10 layers: 32Ã—32 â†’ 12Ã—12
After 15 layers: 32Ã—32 â†’ 2Ã—2 (almost gone!)
```

**This limits how deep CNNs can be! âœ—**

---

## 6. Padding Solutions

### Valid Padding (No Padding):

**No padding added, accept the shrinkage:**

```
Input: 6Ã—6
Filter: 3Ã—3
Padding: 0
Output: 4Ã—4

Formula: nÃ—n input â†’ (n-f+1)Ã—(n-f+1) output
```

**Use when:**
- Don't care about preserving size
- Shallow networks (few layers)
- Want to reduce spatial dimensions quickly

---

### Same Padding:

**Add padding so output size = input size:**

```
Input: 6Ã—6
Add padding: 1 pixel border of zeros
Padded input: 8Ã—8
Filter: 3Ã—3
Output: 6Ã—6 (same as input!)

Visual:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0  0  0  0  0  0â”‚ â† Added padding
â”‚ 0â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”0â”‚
â”‚ 0â”‚ 3  3  3  0â”‚0â”‚
â”‚ 0â”‚ 3  3  3  0â”‚0â”‚ â† Original 6Ã—6
â”‚ 0â”‚ 3  3  3  0â”‚0â”‚
â”‚ 0â”‚ 3  3  3  0â”‚0â”‚
â”‚ 0â”‚ 3  3  3  0â”‚0â”‚
â”‚ 0â”‚ 3  3  3  0â”‚0â”‚
â”‚ 0â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜0â”‚
â”‚ 0  0  0  0  0  0â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  8Ã—8 padded input

After convolution with 3Ã—3 filter:
8Ã—8 â†’ 6Ã—6 (preserved size!)
```

**Use when:**
- Want to preserve spatial dimensions
- Deep networks (many layers)
- Don't want information loss

---

### Full Padding:

**Maximum padding:**

```
Input: 6Ã—6
Filter: 3Ã—3
Full padding: 2 pixels (f-1)
Padded input: 10Ã—10
Output: 8Ã—8 (larger than input!)

Rarely used in practice
```

---

### Padding Comparison:

| Padding Type | Padding Amount | Output Size (n=6, f=3) | Use Case |
|-------------|---------------|----------------------|----------|
| **Valid** | 0 | 4Ã—4 | Shallow networks |
| **Same** | (f-1)/2 = 1 | 6Ã—6 | Deep networks âœ“ |
| **Full** | f-1 = 2 | 8Ã—8 | Rarely used |

---

## 7. Padding Calculations

### Output Size Formula:

**General formula for convolution output size:**

$$\text{Output size} = \left\lfloor\frac{n + 2p - f}{s}\right\rfloor + 1$$

Where:
- $n$ = Input size (height or width)
- $p$ = Padding
- $f$ = Filter size
- $s$ = Stride (we'll use 1 for now)
- $\lfloor \cdot \rfloor$ = Floor function

**For stride s=1:**

$$\text{Output size} = n + 2p - f + 1$$

---

### Calculating Same Padding:

**To make output size = input size:**

$$n + 2p - f + 1 = n$$
$$2p = f - 1$$
$$p = \frac{f - 1}{2}$$

**Examples:**

```
Filter 3Ã—3: p = (3-1)/2 = 1
Filter 5Ã—5: p = (5-1)/2 = 2
Filter 7Ã—7: p = (7-1)/2 = 3

Note: This only works for ODD filter sizes!
Even filter sizes (2Ã—2, 4Ã—4) need asymmetric padding
(That's why CNNs almost always use odd filter sizes: 1, 3, 5, 7)
```

---

### Numerical Examples:

**Example 1: Valid padding (p=0)**

```
Input: 32Ã—32
Filter: 5Ã—5
Padding: 0
Stride: 1

Output = (32 + 2Ã—0 - 5)/1 + 1
       = (32 - 5) + 1
       = 28

Output: 28Ã—28
Lost 4 pixels per dimension
```

**Example 2: Same padding**

```
Input: 32Ã—32
Filter: 5Ã—5
Padding: (5-1)/2 = 2
Stride: 1

Output = (32 + 2Ã—2 - 5)/1 + 1
       = (32 + 4 - 5) + 1
       = 32

Output: 32Ã—32 (preserved!)
```

**Example 3: Different input sizes**

| Input | Filter | Padding | Stride | Output | Calculation |
|-------|--------|---------|--------|--------|-------------|
| 64Ã—64 | 3Ã—3 | 0 | 1 | 62Ã—62 | (64+0-3)+1 |
| 64Ã—64 | 3Ã—3 | 1 | 1 | 64Ã—64 | (64+2-3)+1 |
| 28Ã—28 | 5Ã—5 | 0 | 1 | 24Ã—24 | (28+0-5)+1 |
| 28Ã—28 | 5Ã—5 | 2 | 1 | 28Ã—28 | (28+4-5)+1 |
| 224Ã—224 | 7Ã—7 | 3 | 1 | 224Ã—224 | (224+6-7)+1 |

---

## 8. Complete PyTorch Implementation

### Manual Convolution Implementation:

```python
import torch
import torch.nn.functional as F

def convolve2d_manual(image, kernel):
    """
    Manual 2D convolution (for understanding)
    
    Args:
        image: (H, W) input image
        kernel: (f, f) filter
    
    Returns:
        (H-f+1, W-f+1) convolved output
    """
    h, w = image.shape
    f = kernel.shape[0]
    
    # Output size (valid padding)
    out_h = h - f + 1
    out_w = w - f + 1
    
    output = torch.zeros(out_h, out_w)
    
    # Slide filter across image
    for i in range(out_h):
        for j in range(out_w):
            # Extract region
            region = image[i:i+f, j:j+f]
            
            # Element-wise multiply and sum
            output[i, j] = (region * kernel).sum()
    
    return output


# Example: Detect vertical edges
image = torch.tensor([
    [3., 3., 2., 0., 0., 0.],
    [3., 3., 2., 0., 0., 0.],
    [3., 3., 2., 0., 0., 0.],
    [3., 3., 2., 0., 0., 0.],
    [3., 3., 2., 0., 0., 0.],
    [3., 3., 2., 0., 0., 0.]
])

vertical_filter = torch.tensor([
    [ 1.,  0., -1.],
    [ 1.,  0., -1.],
    [ 1.,  0., -1.]
])

result = convolve2d_manual(image, vertical_filter)

print("Input image (6Ã—6):")
print(image)
print(f"\nVertical edge filter (3Ã—3):")
print(vertical_filter)
print(f"\nOutput (4Ã—4):")
print(result)
```

---

**Output:**

```
Input image (6Ã—6):
tensor([[3., 3., 2., 0., 0., 0.],
        [3., 3., 2., 0., 0., 0.],
        [3., 3., 2., 0., 0., 0.],
        [3., 3., 2., 0., 0., 0.],
        [3., 3., 2., 0., 0., 0.],
        [3., 3., 2., 0., 0., 0.]])

Vertical edge filter (3Ã—3):
tensor([[ 1.,  0., -1.],
        [ 1.,  0., -1.],
        [ 1.,  0., -1.]])

Output (4Ã—4):
tensor([[ 3.,  9.,  6.,  0.],
        [ 3.,  9.,  6.,  0.],
        [ 3.,  9.,  6.,  0.],
        [ 3.,  9.,  6.,  0.]])

Edge detected in column 1-2! âœ“
```

---

### Using PyTorch Conv2d:

```python
import torch
import torch.nn as nn

# Create a simple edge detector using Conv2d
class EdgeDetector(nn.Module):
    def __init__(self):
        super().__init__()
        
        # Conv2d: (in_channels, out_channels, kernel_size)
        self.conv = nn.Conv2d(
            in_channels=1,      # Grayscale image
            out_channels=1,     # One output channel
            kernel_size=3,      # 3Ã—3 filter
            stride=1,           # Slide by 1 pixel
            padding=0,          # No padding (valid)
            bias=False          # No bias for simple edge detection
        )
        
        # Set filter weights manually (vertical edge detector)
        with torch.no_grad():
            self.conv.weight = nn.Parameter(torch.tensor([
                [[[1.,  0., -1.],
                  [1.,  0., -1.],
                  [1.,  0., -1.]]]
            ]))
    
    def forward(self, x):
        return self.conv(x)


# Example usage
edge_detector = EdgeDetector()

# Input: (batch, channels, height, width)
image = torch.tensor([
    [3., 3., 2., 0., 0., 0.],
    [3., 3., 2., 0., 0., 0.],
    [3., 3., 2., 0., 0., 0.],
    [3., 3., 2., 0., 0., 0.],
    [3., 3., 2., 0., 0., 0.],
    [3., 3., 2., 0., 0., 0.]
]).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims

print(f"Input shape: {image.shape}")  # (1, 1, 6, 6)

output = edge_detector(image)

print(f"Output shape: {output.shape}")  # (1, 1, 4, 4)
print(f"\nEdge detection result:")
print(output.squeeze())
```

---

### Implementing Custom Edge Detectors:

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

class MultiEdgeDetector(nn.Module):
    """Detect both vertical and horizontal edges"""
    
    def __init__(self):
        super().__init__()
        
        # 2 output channels: one for vertical, one for horizontal
        self.conv = nn.Conv2d(1, 2, kernel_size=3, padding=1, bias=False)
        
        # Set weights manually
        with torch.no_grad():
            # Channel 0: Vertical edge detector
            self.conv.weight[0] = torch.tensor([
                [[ 1.,  0., -1.],
                 [ 1.,  0., -1.],
                 [ 1.,  0., -1.]]
            ])
            
            # Channel 1: Horizontal edge detector
            self.conv.weight[1] = torch.tensor([
                [[ 1.,  1.,  1.],
                 [ 0.,  0.,  0.],
                 [-1., -1., -1.]]
            ])
    
    def forward(self, x):
        return self.conv(x)


# Test on image with both edge types
test_image = torch.tensor([
    [0., 0., 0., 0., 0., 0.],
    [0., 5., 5., 5., 5., 0.],
    [0., 5., 5., 5., 5., 0.],
    [0., 5., 5., 5., 5., 0.],
    [0., 5., 5., 5., 5., 0.],
    [0., 0., 0., 0., 0., 0.]
]).unsqueeze(0).unsqueeze(0)

detector = MultiEdgeDetector()
edges = detector(test_image)

print("Input (rectangle):")
print(test_image.squeeze())
print("\nVertical edges detected:")
print(edges[0, 0])  # First output channel
print("\nHorizontal edges detected:")
print(edges[0, 1])  # Second output channel
```

---

**Output:**

```
Input (rectangle):
tensor([[0., 0., 0., 0., 0., 0.],
        [0., 5., 5., 5., 5., 0.],
        [0., 5., 5., 5., 5., 0.],
        [0., 5., 5., 5., 5., 0.],
        [0., 5., 5., 5., 5., 0.],
        [0., 0., 0., 0., 0., 0.]])

Vertical edges detected:
tensor([[ 0.,  5.,  0.,  0., -5.,  0.],
        [ 0.,  5.,  0.,  0., -5.,  0.],
        [ 0.,  5.,  0.,  0., -5.,  0.],
        [ 0.,  5.,  0.,  0., -5.,  0.],
        [ 0.,  5.,  0.,  0., -5.,  0.],
        [ 0.,  0.,  0.,  0.,  0.,  0.]])
         â†‘                 â†‘
    Left edge        Right edge

Horizontal edges detected:
tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],
        [ 0.,  5.,  5.,  5.,  5.,  0.],
        [ 0.,  0.,  0.,  0.,  0.,  0.],
        [ 0.,  0.,  0.,  0.,  0.,  0.],
        [ 0., -5., -5., -5., -5.,  0.],
        [ 0.,  0.,  0.,  0.,  0.,  0.]])
              â†‘
         Top edge (positive)
              â†“
        Bottom edge (negative)

Both edge types detected simultaneously! âœ“
```

---

## 9. Types of Padding in Detail

### Zero Padding (Most Common):

**Add zeros around border:**

```
Original (4Ã—4):        Zero padded with p=1 (6Ã—6):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1 2 3 4â”‚            â”‚ 0 0 0 0 0 0 â”‚
â”‚ 5 6 7 8â”‚            â”‚ 0 1 2 3 4 0 â”‚
â”‚ 9 0 1 2â”‚            â”‚ 0 5 6 7 8 0 â”‚
â”‚ 3 4 5 6â”‚            â”‚ 0 9 0 1 2 0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚ 0 3 4 5 6 0 â”‚
                      â”‚ 0 0 0 0 0 0 â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PyTorch: padding='same' or padding=1
```

**Pros:**
- âœ“ Simple and fast
- âœ“ Most common (default)
- âœ“ Works well in practice

**Cons:**
- âœ— Introduces artificial zeros
- âœ— Can affect edge detection near boundaries

---

### Reflection Padding:

**Mirror image at borders:**

```
Original (4Ã—4):        Reflection padded (6Ã—6):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1 2 3 4â”‚            â”‚ 6 5 6 7 8 7 â”‚ â† Mirrored
â”‚ 5 6 7 8â”‚            â”‚ 2 1 2 3 4 3 â”‚
â”‚ 9 0 1 2â”‚            â”‚ 6 5 6 7 8 7 â”‚ â† Original
â”‚ 3 4 5 6â”‚            â”‚ 0 9 0 1 2 1 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚ 4 3 4 5 6 5 â”‚
                      â”‚ 0 9 0 1 2 1 â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘
                      Mirrored columns

PyTorch: F.pad(x, pad=(1,1,1,1), mode='reflect')
```

**Pros:**
- âœ“ More natural than zeros
- âœ“ Better for image processing tasks
- âœ“ Preserves continuity

**Cons:**
- âœ— Slightly more computation
- âœ— Not always better than zero padding

---

### Replication Padding:

**Repeat edge pixels:**

```
Original (4Ã—4):        Replication padded (6Ã—6):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1 2 3 4â”‚            â”‚ 1 1 2 3 4 4 â”‚ â† Repeated
â”‚ 5 6 7 8â”‚            â”‚ 1 1 2 3 4 4 â”‚
â”‚ 9 0 1 2â”‚            â”‚ 5 5 6 7 8 8 â”‚ â† Original
â”‚ 3 4 5 6â”‚            â”‚ 9 9 0 1 2 2 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚ 3 3 4 5 6 6 â”‚
                      â”‚ 3 3 4 5 6 6 â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†‘       â†‘
                    Repeated columns

PyTorch: F.pad(x, pad=(1,1,1,1), mode='replicate')
```

**Pros:**
- âœ“ Natural extension
- âœ“ No artificial discontinuities

**Cons:**
- âœ— Can emphasize edges that don't exist

---

### Circular Padding:

**Wrap around (treat image as torus):**

```
Original (4Ã—4):        Circular padded (6Ã—6):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1 2 3 4â”‚            â”‚ 6 3 4 5 6 3 â”‚
â”‚ 5 6 7 8â”‚            â”‚ 2 1 2 3 4 1 â”‚
â”‚ 9 0 1 2â”‚            â”‚ 6 5 6 7 8 5 â”‚
â”‚ 3 4 5 6â”‚            â”‚ 0 9 0 1 2 9 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚ 4 3 4 5 6 3 â”‚
                      â”‚ 2 1 2 3 4 1 â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PyTorch: F.pad(x, pad=(1,1,1,1), mode='circular')
```

**Use case:** Periodic patterns (rarely used for images)

---

### Visual Comparison of Padding Types:

```
Original 4Ã—4 image:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  2  3  4â”‚
â”‚ 5  6  7  8â”‚
â”‚ 9 10 11 12â”‚
â”‚13 14 15 16â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After different padding (p=1):

Zero:                  Reflection:            Replication:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0  0  0  0  0  0â”‚    â”‚ 6  5  6  7  8  7â”‚    â”‚ 1  1  2  3  4  4â”‚
â”‚ 0  1  2  3  4  0â”‚    â”‚ 2  1  2  3  4  3â”‚    â”‚ 1  1  2  3  4  4â”‚
â”‚ 0  5  6  7  8  0â”‚    â”‚ 6  5  6  7  8  7â”‚    â”‚ 5  5  6  7  8  8â”‚
â”‚ 0  9 10 11 12  0â”‚    â”‚10  9 10 11 12 11â”‚    â”‚ 9  9 10 11 12 12â”‚
â”‚ 0 13 14 15 16  0â”‚    â”‚14 13 14 15 16 15â”‚    â”‚13 13 14 15 16 16â”‚
â”‚ 0  0  0  0  0  0â”‚    â”‚10  9 10 11 12 11â”‚    â”‚13 13 14 15 16 16â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Zero: Simple, works well
Reflection: Natural, preserves patterns
Replication: Extends edges
```

---

## 10. Learning Filters Automatically

### From Hand-Crafted to Learned Filters:

**Traditional computer vision:**
```
Design filters manually:
- Vertical edge detector: [1, 0, -1]
- Horizontal edge detector: [1, 1, 1; 0, 0, 0; -1, -1, -1]
- Gaussian blur: [[1,2,1], [2,4,2], [1,2,1]]/16
- Sobel, Prewitt, Laplacian, etc.

Problems:
âœ— Requires expertise
âœ— Hand-tuned for specific tasks
âœ— May not be optimal
âœ— Doesn't adapt to data
```

**Convolutional Neural Networks:**
```
Learn filters from data!

Initialize: Random filter weights
Train: Update filters using backpropagation
Result: Filters optimized for the specific task!

Example:
Layer 1 might learn:
- Vertical edges
- Horizontal edges
- Diagonal edges
- Curves
- Color gradients

Layer 2 might learn:
- Corners (combining multiple edges)
- Simple textures
- Basic shapes

Layer 3 might learn:
- Object parts (eyes, wheels, etc.)
- Complex textures

Automatically discovers optimal features! âœ“
```

---

### How CNNs Learn Filters:

```python
import torch
import torch.nn as nn
import torch.optim as optim

# Simple CNN that learns edge detection
class LearnableEdgeDetector(nn.Module):
    def __init__(self):
        super().__init__()
        
        # Convolutional layer with LEARNABLE weights
        self.conv1 = nn.Conv2d(
            in_channels=1,
            out_channels=2,     # Learn 2 different filters
            kernel_size=3,
            padding=1
        )
        
        # Fully connected for classification
        self.fc = nn.Linear(2 * 6 * 6, 2)  # Binary: edge vs no-edge
    
    def forward(self, x):
        # Apply learned filters
        x = torch.relu(self.conv1(x))  # (batch, 2, 6, 6)
        
        # Flatten and classify
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        
        return x


model = LearnableEdgeDetector()

# Check initial random filters
print("Initial random filters:")
print("Filter 1:")
print(model.conv1.weight[0, 0])
print("\nFilter 2:")
print(model.conv1.weight[1, 0])

# After training on edge detection task...
# (Simplified - in practice, train on labeled data)

optimizer = optim.Adam(model.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()

# Training loop (simplified)
for epoch in range(100):
    # Generate synthetic training data
    # Images with edges â†’ label 1
    # Images without edges â†’ label 0
    
    # ... training code ...
    pass

print("\n" + "="*50)
print("After training, learned filters:")
print("Filter 1 (learned to detect vertical edges):")
print(model.conv1.weight[0, 0])
print("\nFilter 2 (learned to detect horizontal edges):")
print(model.conv1.weight[1, 0])

# Filters will have learned patterns similar to hand-crafted ones!
# But optimized for the specific task!
```

---

### What CNNs Learn:

**Layer 1 (First conv layer):**
```
Learns low-level features:
Filter 1: [1, 0, -1; 1, 0, -1; 1, 0, -1]  (vertical edges)
Filter 2: [1, 1, 1; 0, 0, 0; -1, -1, -1]  (horizontal edges)
Filter 3: [1, 1, 0; 1, 0, -1; 0, -1, -1]  (diagonal edges)
...

Similar to hand-crafted edge detectors!
```

**Layer 2 (Second conv layer):**
```
Learns mid-level features (combinations of edges):
Filter 1: Detects corners (vertical + horizontal edge)
Filter 2: Detects curves
Filter 3: Detects textures
...

More complex than single edges!
```

**Layer 3+ (Deeper layers):**
```
Learns high-level features:
Filter 1: Detects eyes
Filter 2: Detects wheels
Filter 3: Detects fur patterns
...

Object parts and semantic features!
```

**The hierarchy:**

```
Input Image
    â†“
Layer 1: Edges, colors
    â†“
Layer 2: Textures, corners
    â†“
Layer 3: Object parts
    â†“
Layer 4: Objects
    â†“
Classification

Each layer builds on previous!
Automatic feature hierarchy! âœ“
```

---

## 11. Convolution with Multiple Channels

### RGB Images (3 Channels):

**RGB image = 3 separate channels:**

```
Red channel (6Ã—6):     Green channel (6Ã—6):    Blue channel (6Ã—6):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 255 255 0  0â”‚      â”‚ 0  0 255 255â”‚        â”‚ 0  0  0  0  â”‚
â”‚ 255 255 0  0â”‚      â”‚ 0  0 255 255â”‚        â”‚ 0  0  0  0  â”‚
â”‚ 255 255 0  0â”‚      â”‚ 0  0 255 255â”‚        â”‚ 0  0  0  0  â”‚
â”‚ 255 255 0  0â”‚      â”‚ 0  0 255 255â”‚        â”‚ 0  0  0  0  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Combined RGB image: Red+Green on left, Green on right
```

**Filter for RGB (3Ã—3Ã—3):**

```
Filter has 3 channels too!
One 3Ã—3 filter per input channel

Red filter:          Green filter:        Blue filter:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  0 -1â”‚        â”‚ 1  0 -1â”‚        â”‚ 1  0 -1â”‚
â”‚ 1  0 -1â”‚        â”‚ 1  0 -1â”‚        â”‚ 1  0 -1â”‚
â”‚ 1  0 -1â”‚        â”‚ 1  0 -1â”‚        â”‚ 1  0 -1â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total filter size: 3Ã—3Ã—3 = 27 weights
```

---

### Complete Calculation:

**Convolving RGB image:**

```
For each output position (i, j):

1. Extract 3Ã—3 region from RED channel
2. Multiply by RED filter
3. Sum â†’ result_red

4. Extract 3Ã—3 region from GREEN channel
5. Multiply by GREEN filter
6. Sum â†’ result_green

7. Extract 3Ã—3 region from BLUE channel
8. Multiply by BLUE filter
9. Sum â†’ result_blue

10. Add all channel results:
    Output[i,j] = result_red + result_green + result_blue

One output value per position!
```

**Example at position (0,0):**

```
Red region:          Red filter:        Result:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        
â”‚255 255 0â”‚  Ã—     â”‚ 1  0 -1â”‚  = 255 + 0 + 0 + ... = 510
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Green region:        Green filter:      Result:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  0   0 255â”‚ Ã—     â”‚ 1  0 -1â”‚  = 0 + 0 - 255 + ... = -510
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Blue region:         Blue filter:       Result:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  0   0  0â”‚  Ã—     â”‚ 1  0 -1â”‚  = 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: 510 + (-510) + 0 = 0

Output[0,0] = 0
```

**PyTorch handles this automatically:**

```python
# RGB image: (batch, 3, height, width)
rgb_image = torch.randn(1, 3, 224, 224)

# Conv2d for RGB
conv = nn.Conv2d(
    in_channels=3,    # RGB input
    out_channels=64,  # 64 different filters
    kernel_size=3,
    padding=1
)

# Each of 64 filters has shape (3, 3, 3)
# Total weights: 64 Ã— 3 Ã— 3 Ã— 3 = 1,728

output = conv(rgb_image)
# Output shape: (1, 64, 224, 224)
# 64 feature maps, one per filter
```

---

## 12. Practical Guidelines

### Choosing Padding Type:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Padding Selection Guide          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

For most CNNs:
â””â”€ Use 'same' padding (zero padding) âœ“
   Preserves spatial dimensions
   Standard practice

For image segmentation:
â””â”€ Use 'same' padding
   Need to maintain resolution

For shallow networks (<5 layers):
â””â”€ Can use 'valid' (no padding)
   Size reduction acceptable

For image generation/restoration:
â””â”€ Consider reflection padding
   More natural than zeros

For feature extraction only:
â””â”€ 'valid' padding okay
   Don't need to preserve size
```

---

### Choosing Filter Size:

| Filter Size | Use Case | Receptive Field | Computation |
|------------|----------|----------------|-------------|
| **1Ã—1** | Channel mixing, dimensionality reduction | Very local | Very fast |
| **3Ã—3** | **Default choice** âœ“ Most common | Small | Fast |
| **5Ã—5** | Larger patterns | Medium | Moderate |
| **7Ã—7** | First layer of network (e.g., ResNet) | Large | Slow |
| **11Ã—11** | Very first layer (e.g., AlexNet) | Very large | Very slow |

**Modern trend: Stack multiple 3Ã—3 instead of one large filter**

```
Why 3Ã—3 is preferred:

Two 3Ã—3 filters:
- Receptive field: 5Ã—5
- Parameters: 2 Ã— (3Ã—3) = 18

One 5Ã—5 filter:
- Receptive field: 5Ã—5
- Parameters: 25

Same receptive field, fewer parameters with stacking! âœ“
Plus: More non-linearities (ReLU between layers)
```

---

### Common Patterns in Practice:

**Pattern 1: VGG-style (stack 3Ã—3)**

```python
# Modern standard
model = nn.Sequential(
    nn.Conv2d(3, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.Conv2d(64, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2),  # Downsample
    
    nn.Conv2d(64, 128, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.Conv2d(128, 128, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2),
    # ... more layers
)

All 3Ã—3 filters with same padding âœ“
```

---

**Pattern 2: ResNet-style (7Ã—7 first, then 3Ã—3)**

```python
# First layer: larger receptive field
model = nn.Sequential(
    nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
    nn.BatchNorm2d(64),
    nn.ReLU(),
    nn.MaxPool2d(3, stride=2, padding=1),
    
    # All subsequent layers: 3Ã—3
    nn.Conv2d(64, 64, kernel_size=3, padding=1),
    nn.BatchNorm2d(64),
    nn.ReLU(),
    # ... more 3Ã—3 layers
)
```

---

## 13. Summary: Edge Detection and Padding

### What We Learned:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Edge Detection via Convolution       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CONVOLUTION:
- Slide small filter across image
- Element-wise multiply and sum
- Detect local patterns

EDGE DETECTION:
- Vertical edges: [1, 0, -1] pattern
- Horizontal edges: [1, 1, 1; 0, 0, 0; -1, -1, -1]
- Combines to detect any edge orientation

OUTPUT SIZE (no padding):
- Input: nÃ—n
- Filter: fÃ—f
- Output: (n-f+1) Ã— (n-f+1)
- Shrinks by (f-1) per dimension

PADDING SOLUTIONS:
- Valid (p=0): No padding, accept shrinkage
- Same (p=(f-1)/2): Preserve size
- Custom: Any padding amount

PADDING TYPES:
- Zero: Add zeros (most common)
- Reflect: Mirror borders
- Replicate: Repeat edges
- Circular: Wrap around

LEARNED FILTERS:
- CNNs learn optimal filters automatically
- Layer 1: Edges, colors
- Layer 2: Textures, patterns
- Layer 3+: Object parts
- Hierarchical feature learning
```

---

### Key Formulas:

**Convolution:**
$$(I * K)_{ij} = \sum_m\sum_n I_{i+m,j+n} \cdot K_{m,n}$$

**Output Size:**
$$\text{Output} = \frac{n + 2p - f}{s} + 1$$

**Same Padding:**
$$p = \frac{f - 1}{2}$$

**Cross-Entropy Loss (for classification):**
$$\mathcal{L} = -\sum_k y_k \log(\hat{y}_k)$$

---

### Practical Recommendations:

```
âœ“ Use 3Ã—3 filters (most common, efficient)
âœ“ Use 'same' padding (preserves size)
âœ“ Use zero padding (default, works well)
âœ“ Let network learn filters (don't hand-craft)
âœ“ Stack multiple small filters > one large filter
âœ“ Use batch normalization after convolution
âœ“ Odd filter sizes (1, 3, 5, 7) for symmetric padding

âœ— Don't use even filter sizes (2Ã—2, 4Ã—4)
âœ— Don't manually design filters (let network learn)
âœ— Don't forget padding for deep networks
âœ— Don't use padding on final classification layer
âœ— Don't use very large filters (>7Ã—7) except first layer
```

---

### Complete Example: Edge Detection CNN

```python
import torch
import torch.nn as nn

class EdgeDetectionCNN(nn.Module):
    """CNN for detecting and classifying edges"""
    
    def __init__(self):
        super().__init__()
        
        # Conv layer 1: Learn multiple edge detectors
        self.conv1 = nn.Conv2d(
            in_channels=1,      # Grayscale
            out_channels=16,    # Learn 16 different filters
            kernel_size=3,
            padding=1,          # Same padding
            stride=1
        )
        self.bn1 = nn.BatchNorm2d(16)
        
        # Conv layer 2: Combine edges into patterns
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(32)
        
        # Conv layer 3: Higher-level features
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(64)
        
        # Global average pooling + classification
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(64, 10)  # 10 classes
    
    def forward(self, x):
        # Conv block 1
        x = self.conv1(x)       # (B, 16, H, W) - same size!
        x = self.bn1(x)
        x = torch.relu(x)
        x = torch.max_pool2d(x, 2)  # (B, 16, H/2, W/2)
        
        # Conv block 2
        x = self.conv2(x)       # (B, 32, H/2, W/2) - same!
        x = self.bn2(x)
        x = torch.relu(x)
        x = torch.max_pool2d(x, 2)  # (B, 32, H/4, W/4)
        
        # Conv block 3
        x = self.conv3(x)       # (B, 64, H/4, W/4) - same!
        x = self.bn3(x)
        x = torch.relu(x)
        
        # Classification
        x = self.gap(x)         # (B, 64, 1, 1)
        x = x.view(x.size(0), -1)  # (B, 64)
        x = self.fc(x)          # (B, 10)
        
        return x


# Create and inspect model
model = EdgeDetectionCNN()

# Input: 28Ã—28 grayscale image (e.g., MNIST)
x = torch.randn(1, 1, 28, 28)

print("Input shape:", x.shape)

# Forward pass
output = model(x)

print("Output shape:", output.shape)
print("\nArchitecture:")
print(model)

# Check learned filters after training
print("\nFirst layer filters (shape):", model.conv1.weight.shape)
# Shape: (16, 1, 3, 3) = 16 filters, each 3Ã—3Ã—1
```

---

**You now understand Edge Detection and Padding completely! ğŸ‰**

The key insights:
- **Convolution slides filters across images** to detect patterns
- **Edge detection is the foundation** of computer vision
- **Filters are small matrices** (typically 3Ã—3) that detect specific patterns
- **Padding solves the shrinking problem** and preserves edge information
- **'Same' padding preserves spatial dimensions** (most common in practice)
- **CNNs learn filters automatically** from data via backpropagation
- **Hierarchical learning:** edges â†’ textures â†’ parts â†’ objects
- **Multiple channels** (RGB) use 3D filters
- **Modern practice:** 3Ã—3 filters with 'same' padding

Edge detection through convolution is the fundamental operation that makes CNNs work - it allows networks to detect local patterns efficiently while preserving spatial structure and using far fewer parameters than fully connected networks!
