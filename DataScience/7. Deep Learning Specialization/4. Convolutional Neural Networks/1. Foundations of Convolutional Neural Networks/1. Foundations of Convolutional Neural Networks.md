# Foundations of Convolutional Neural Networks

## Table of Contents

1. [Introduction to Convolutional Neural Networks](#introduction-to-convolutional-neural-networks)
   - [What Are CNNs?](#what-are-cnns)
   - [Why Do CNNs Exist?](#why-do-cnns-exist)
   - [Key Advantages Over Fully Connected Networks](#key-advantages-over-fully-connected-networks)

2. [The Convolution Operation](#the-convolution-operation)
   - [Plain English Overview](#plain-english-overview)
   - [How Convolution Works](#how-convolution-works)
   - [Mathematical Foundation](#mathematical-foundation)
   - [Step-by-Step Calculation Example](#step-by-step-calculation-example)
   - [Forward Propagation in Convolution](#forward-propagation-in-convolution)
   - [Backward Propagation in Convolution](#backward-propagation-in-convolution)
   - [When to Use Convolution](#when-to-use-convolution)

3. [Basic Filters (Kernels)](#basic-filters-kernels)
   - [Plain English Overview](#filters-plain-english-overview)
   - [How Filters Work](#how-filters-work)
   - [Types of Basic Filters](#types-of-basic-filters)
   - [Mathematical Representation](#filters-mathematical-representation)
   - [Filter Examples with Calculations](#filter-examples-with-calculations)
   - [Forward Propagation with Filters](#forward-propagation-with-filters)
   - [Backward Propagation with Filters](#backward-propagation-with-filters)
   - [Practical Guidelines for Filters](#practical-guidelines-for-filters)

---

## Introduction to Convolutional Neural Networks

### What Are CNNs?

**Plain English Overview:**

Convolutional Neural Networks (CNNs) are a specialized type of neural network designed primarily for processing grid-like data such as images, videos, and time-series data. Think of CNNs as neural networks with a built-in "scanner" that moves across the input data, examining small regions at a time rather than looking at everything all at once.

Imagine you're looking at a photograph of a cat. A regular fully connected neural network would try to understand the entire image by examining every single pixel independently and connecting each one to every neuron in the next layer. A CNN, however, works more like how your eyes scan an image - it looks at small patches of the image (maybe a 3×3 or 5×5 pixel region), recognizes patterns like edges or textures in those patches, and gradually builds up an understanding of what's in the image.

**Key Concept:** CNNs use small "windows" that slide across the input data, applying the same learned pattern detector repeatedly at different positions. This sliding window approach is called **convolution**.

### Why Do CNNs Exist?

**The Problem with Fully Connected Networks:**

Consider a small color image of size 32×32 pixels with 3 color channels (RGB). This gives us:
- Total input size = 32 × 32 × 3 = 3,072 values

In a fully connected (dense) network with just 1,000 neurons in the first hidden layer:
- Number of parameters = 3,072 × 1,000 = 3,072,000 parameters
- Just for one layer!

For a more realistic image of 224×224×3 (standard ImageNet size):
- Total input size = 224 × 224 × 3 = 150,528 values
- With 1,000 neurons: 150,528 × 1,000 = **150,528,000 parameters**
- This is computationally expensive and prone to overfitting

**How CNNs Solve This:**

CNNs make three key assumptions about images (and similar data):
1. **Spatial locality**: Nearby pixels are more related than distant pixels
2. **Translation invariance**: A cat is still a cat whether it's in the top-left or bottom-right of the image
3. **Hierarchical patterns**: Simple patterns (edges) combine to form complex patterns (shapes), which combine to form objects

By exploiting these properties, CNNs can:
- Use far fewer parameters
- Detect patterns regardless of their position in the image
- Build complex understanding from simple building blocks

### Key Advantages Over Fully Connected Networks

1. **Parameter Sharing**: The same filter (pattern detector) is used across the entire image, drastically reducing the number of parameters
   - Instead of 150 million parameters, a CNN might use only a few million

2. **Sparse Connectivity**: Each neuron only connects to a small region of the input (called the receptive field)
   - Reduces computation and captures local patterns effectively

3. **Translation Equivariance**: If the input shifts, the output shifts accordingly
   - A face detector works regardless of where the face appears in the image

4. **Hierarchical Feature Learning**: Lower layers detect simple features (edges, colors), middle layers detect parts (eyes, wheels), upper layers detect objects (faces, cars)

---

## The Convolution Operation

### Plain English Overview

The convolution operation is the fundamental building block of Convolutional Neural Networks. It's a mathematical operation that takes two inputs: an image (or feature map) and a filter (also called a kernel), and produces an output feature map.

**Analogy:** Think of convolution like using a rubber stamp. You have a small rubber stamp (the filter) and a large piece of paper (the input image). You press the stamp at one corner of the paper, then slide it over a bit and press again, slide and press, slide and press, covering the entire paper systematically. Each time you press, you're checking how well the stamp pattern matches that part of the paper. The result is a new sheet showing where the stamp pattern appeared strongest.

**Connection to Previous Knowledge:** If you're familiar with fully connected neural networks, you know that each layer performs matrix multiplication (Y = WX + b). Convolution is similar, but instead of a full matrix multiplication, we perform many small local multiplications using a sliding window approach.

### How Convolution Works

**Step-by-Step Process:**

1. **Start with two inputs:**
   - Input image/feature map: A 2D grid of values (for simplicity, we'll use 2D; in practice, images have 3D with color channels)
   - Filter/Kernel: A small 2D grid of learnable weights

2. **Place the filter on the top-left corner of the input**

3. **Perform element-wise multiplication:**
   - Multiply each filter value with the corresponding input value it's covering
   
4. **Sum all the products:**
   - Add up all the multiplication results to get a single output value

5. **Slide the filter to the right:**
   - Move the filter by a certain number of positions (called stride, usually 1)
   
6. **Repeat steps 3-5:**
   - Continue until you've covered the entire width
   
7. **Move to the next row:**
   - Shift the filter down and repeat the process

8. **Complete the output:**
   - The result is a new 2D grid (output feature map) where each value represents how strongly the filter pattern matched at that location

### Mathematical Foundation

**Basic Convolution Formula:**

Before diving into the formula, let's understand what we're computing. The convolution operation takes a weighted sum of input values in a local region, producing a single output value. We repeat this process across the entire input.

**Discrete 2D Convolution Formula:**

```
S(i,j) = (I * K)(i,j) = ΣΣ I(m,n) · K(i-m, j-n)
                        m n
```

**Symbol Legend:**
- `S(i,j)`: Output feature map value at position (i,j)
- `I`: Input image/feature map
- `K`: Kernel/filter
- `*`: Convolution operator (not regular multiplication)
- `I(m,n)`: Input value at position (m,n)
- `K(i-m, j-n)`: Kernel weight at relative position
- `ΣΣ`: Double summation over all kernel positions
- `i, j`: Output position indices
- `m, n`: Input position indices within the kernel region

**Simplified Practical Formula (for implementation):**

For a filter of size f×f applied to an input of size n×n, the output at position (i,j) is:

```
Output[i,j] = Σ   Σ   Input[i+m, j+n] × Filter[m,n]
             m=0 n=0
             
where m,n go from 0 to (f-1)
```

**Output Size Formula:**

When applying a filter to an input, the output size is calculated as:

```
Output_height = (Input_height - Filter_height) / Stride + 1
Output_width = (Input_width - Filter_width) / Stride + 1
```

**Symbol Legend for Output Size:**
- `Input_height`: Height of input feature map
- `Input_width`: Width of input feature map
- `Filter_height`: Height of filter (usually equals filter_width)
- `Filter_width`: Width of filter
- `Stride`: Step size for sliding the filter (typically 1 or 2)

### Step-by-Step Calculation Example

**Example Setup:**

Let's perform a complete convolution operation with concrete numbers.

**Input Image (6×6):**
```
I = [
  [3, 0, 1, 2, 7, 4],
  [1, 5, 8, 9, 3, 1],
  [2, 7, 2, 5, 1, 3],
  [0, 1, 3, 1, 7, 8],
  [4, 2, 1, 6, 2, 8],
  [2, 4, 5, 2, 3, 9]
]
```

**Filter/Kernel (3×3):**
```
K = [
  [1,  0, -1],
  [1,  0, -1],
  [1,  0, -1]
]
```

This is an edge detection filter (vertical edge detector).

**Stride = 1** (move filter one position at a time)

**Step 1: Calculate Output Size**

Using the formula:
```
Output_height = (6 - 3) / 1 + 1 = 4
Output_width = (6 - 3) / 1 + 1 = 4
```

So our output will be a 4×4 matrix.

**Step 2: Calculate First Output Value (Position 0,0)**

Place the 3×3 filter on the top-left corner of the input:

```
Input region covered:        Filter:
[3, 0, 1]                   [1,  0, -1]
[1, 5, 8]                   [1,  0, -1]
[2, 7, 2]                   [1,  0, -1]
```

Perform element-wise multiplication:
```
(3×1) + (0×0) + (1×-1) +
(1×1) + (5×0) + (8×-1) +
(2×1) + (7×0) + (2×-1)

= 3 + 0 + (-1) + 1 + 0 + (-8) + 2 + 0 + (-2)
= 3 - 1 + 1 - 8 + 2 - 2
= -5
```

Output[0,0] = -5

**Step 3: Calculate Next Output Value (Position 0,1)**

Slide the filter one position to the right:

```
Input region covered:        Filter:
[0, 1, 2]                   [1,  0, -1]
[5, 8, 9]                   [1,  0, -1]
[7, 2, 5]                   [1,  0, -1]
```

Element-wise multiplication and sum:
```
(0×1) + (1×0) + (2×-1) +
(5×1) + (8×0) + (9×-1) +
(7×1) + (2×0) + (5×-1)

= 0 + 0 + (-2) + 5 + 0 + (-9) + 7 + 0 + (-5)
= -2 + 5 - 9 + 7 - 5
= -4
```

Output[0,1] = -4

**Step 4: Continue Sliding Across the First Row**

Position (0,2):
```
Input region:                Filter:
[1, 2, 7]                   [1,  0, -1]
[8, 9, 3]                   [1,  0, -1]
[2, 5, 1]                   [1,  0, -1]

Calculation:
(1×1) + (2×0) + (7×-1) +
(8×1) + (9×0) + (3×-1) +
(2×1) + (5×0) + (1×-1)

= 1 + 0 + (-7) + 8 + 0 + (-3) + 2 + 0 + (-1)
= 1 - 7 + 8 - 3 + 2 - 1
= 0
```

Output[0,2] = 0

Position (0,3):
```
Input region:                Filter:
[2, 7, 4]                   [1,  0, -1]
[9, 3, 1]                   [1,  0, -1]
[5, 1, 3]                   [1,  0, -1]

Calculation:
(2×1) + (7×0) + (4×-1) +
(9×1) + (3×0) + (1×-1) +
(5×1) + (1×0) + (3×-1)

= 2 + 0 + (-4) + 9 + 0 + (-1) + 5 + 0 + (-3)
= 2 - 4 + 9 - 1 + 5 - 3
= 8
```

Output[0,3] = 8

**Step 5: Move Down and Continue**

Now we've completed the first row. We move the filter down one row and repeat the process for rows 1, 2, and 3.

For brevity, I'll show the calculations for position (1,0):

```
Input region:                Filter:
[1, 5, 8]                   [1,  0, -1]
[2, 7, 2]                   [1,  0, -1]
[0, 1, 3]                   [1,  0, -1]

Calculation:
(1×1) + (5×0) + (8×-1) +
(2×1) + (7×0) + (2×-1) +
(0×1) + (1×0) + (3×-1)

= 1 + 0 + (-8) + 2 + 0 + (-2) + 0 + 0 + (-3)
= 1 - 8 + 2 - 2 + 0 - 3
= -10
```

Output[1,0] = -10

**Complete Output Feature Map (4×4):**

After performing all calculations across the entire input:

```
Output = [
  [-5,  -4,   0,   8],
  [-10, -2,   2,   3],
  [0,   -2,  -2,  -7],
  [4,    0,   0,  -6]
]
```

**Interpretation:**
- Negative values indicate edges where brightness decreases from left to right
- Positive values indicate edges where brightness increases from left to right
- Values near zero indicate no strong vertical edges

### Forward Propagation in Convolution

**Plain English Explanation:**

Forward propagation in a convolutional layer means taking an input (like an image or previous layer's output) and applying convolution operations with learned filters to produce output feature maps. This process transforms the input through pattern detection.

Think of it like a factory assembly line: the raw material (input) enters, passes through various specialized machines (filters), and each machine produces a specific product (feature map) that highlights certain characteristics.

**Complete Forward Propagation Process:**

**Step 1: Initialize Parameters**

Before forward propagation begins, we need:
- Input: `I` with shape (H_in, W_in, C_in)
  - H_in: input height
  - W_in: input width
  - C_in: number of input channels
  
- Filters: `F` with shape (f, f, C_in, C_out)
  - f: filter size (height and width)
  - C_in: must match input channels
  - C_out: number of filters (number of output channels)
  
- Biases: `b` with shape (C_out,) - one bias per filter

**Step 2: Calculate Output Dimensions**

```
H_out = floor((H_in - f + 2×pad) / stride) + 1
W_out = floor((W_in - f + 2×pad) / stride) + 1
```

Where:
- pad: padding size (we'll cover this later, assume 0 for now)
- stride: step size
- floor: round down to nearest integer

**Step 3: Convolve Each Filter Across the Input**

For each filter k from 1 to C_out:
  For each spatial position (i,j) in the output:
    1. Extract the input patch at position (i,j)
    2. Multiply filter weights with input patch values
    3. Sum all products
    4. Add bias term
    5. Store result in Output[i,j,k]

**Mathematical Formulation:**

```
Output[i,j,k] = Σ   Σ   Σ   Input[i×s + m, j×s + n, c] × Filter[m,n,c,k] + bias[k]
               m=0 n=0 c=0
```

**Symbol Legend:**
- `i, j`: Output spatial position
- `k`: Output channel (filter) index
- `m, n`: Position within filter
- `c`: Input channel index
- `s`: Stride value
- `f`: Filter size
- `C_in`: Number of input channels

**Step 4: Apply Activation Function (typically)**

After convolution, an activation function (like ReLU) is usually applied:

```
Activated_Output[i,j,k] = ReLU(Output[i,j,k]) = max(0, Output[i,j,k])
```

**Complete Example with Multiple Channels:**

**Given:**
- Input: 6×6×2 (height × width × 2 channels)
- Filter: 3×3×2 (3×3 size × 2 input channels)
- Number of filters: 2
- Stride: 1
- No padding

**Input Channel 1:**
```
I[:,:,0] = [
  [1, 2, 3, 0, 1, 2],
  [0, 1, 4, 3, 2, 1],
  [2, 3, 1, 2, 0, 3],
  [1, 0, 2, 1, 3, 2],
  [3, 2, 1, 0, 1, 4],
  [0, 1, 2, 3, 2, 1]
]
```

**Input Channel 2:**
```
I[:,:,1] = [
  [0, 1, 2, 1, 0, 1],
  [1, 2, 0, 3, 1, 2],
  [2, 1, 3, 2, 1, 0],
  [0, 3, 1, 2, 0, 3],
  [1, 0, 2, 1, 3, 2],
  [2, 1, 0, 3, 1, 0]
]
```

**Filter 1 (both channels):**
```
F[:,:,0,0] = [        F[:,:,1,0] = [
  [1,  0, -1],           [0, 1, 0],
  [1,  0, -1],           [0, 1, 0],
  [1,  0, -1]            [0, 1, 0]
]                       ]
```

**Filter 2 (both channels):**
```
F[:,:,0,1] = [        F[:,:,1,1] = [
  [1,  1,  1],           [0, 0, 0],
  [0,  0,  0],           [1, 1, 1],
  [-1,-1, -1]            [0, 0, 0]
]                       ]
```

**Biases:**
```
b[0] = 0.5
b[1] = -0.3
```

**Calculate First Output Value for Filter 1:**

Position (0,0,0) - first output channel:

From Input Channel 1:
```
Region:                  Filter 1, Channel 1:
[1, 2, 3]               [1,  0, -1]
[0, 1, 4]               [1,  0, -1]
[2, 3, 1]               [1,  0, -1]

Sum_channel_1 = (1×1) + (2×0) + (3×-1) + (0×1) + (1×0) + (4×-1) + (2×1) + (3×0) + (1×-1)
              = 1 + 0 - 3 + 0 + 0 - 4 + 2 + 0 - 1
              = -5
```

From Input Channel 2:
```
Region:                  Filter 1, Channel 2:
[0, 1, 2]               [0, 1, 0]
[1, 2, 0]               [0, 1, 0]
[2, 1, 3]               [0, 1, 0]

Sum_channel_2 = (0×0) + (1×1) + (2×0) + (1×0) + (2×1) + (0×0) + (2×0) + (1×1) + (3×0)
              = 0 + 1 + 0 + 0 + 2 + 0 + 0 + 1 + 0
              = 4
```

Total before bias:
```
Sum_total = Sum_channel_1 + Sum_channel_2
          = -5 + 4
          = -1
```

Add bias:
```
Output[0,0,0] = Sum_total + bias[0]
              = -1 + 0.5
              = -0.5
```

**Key Points for Forward Propagation:**

1. **Channel summation**: For multi-channel inputs, we sum contributions from all input channels
2. **One bias per filter**: Each output channel has its own bias
3. **Spatial sliding**: The same filter weights slide across all spatial positions
4. **Multiple filters**: Each filter produces one output channel
5. **Order matters**: Process filters one by one, positions systematically

### Backward Propagation in Convolution

**Plain English Explanation:**

Backward propagation (backprop) in convolutional layers calculates gradients - the rates of change of the loss function with respect to the inputs, weights, and biases. These gradients tell us how to adjust the filter weights to minimize our error.

Think of it like tracing responsibility backwards: if the final answer was wrong, we need to figure out which filter values contributed most to that error. It's like a teacher grading a test - they look at the wrong answer, trace back through the student's work, and identify exactly where the mistakes were made.

**Three Gradients to Calculate:**

1. **∂L/∂F**: Gradient with respect to filters (how to update filter weights)
2. **∂L/∂b**: Gradient with respect to biases (how to update biases)
3. **∂L/∂I**: Gradient with respect to input (needed for previous layer's backprop)

Where:
- L: Loss function
- F: Filter weights
- b: Biases
- I: Input

**Complete Backward Propagation Process:**

**Given:**
- `∂L/∂Output`: Gradient from the next layer (shape: H_out × W_out × C_out)
- `Input`: The input that was used in forward prop (shape: H_in × W_in × C_in)
- `Filters`: The filters used in forward prop (shape: f × f × C_in × C_out)

**Step 1: Compute Gradient with Respect to Filters (∂L/∂F)**

This gradient tells us how much each filter weight contributed to the loss. We calculate it by looking at where each weight was used and how it affected the output.

**Formula:**
```
∂L/∂F[m,n,c,k] = Σ   Σ   Input[i×s + m, j×s + n, c] × ∂L/∂Output[i,j,k]
                 i=0 j=0
```

**Symbol Legend:**
- `m, n`: Position within filter
- `c`: Input channel
- `k`: Output channel (filter index)
- `i, j`: Spatial positions in output
- `s`: Stride

**Plain English:** For each filter weight, multiply the input values it "saw" during forward prop with the output gradients at those positions, then sum everything.

**Detailed Example:**

Let's continue with our previous example. Suppose after forward propagation and the loss calculation, we have:

```
∂L/∂Output = [
  [0.1,  0.2],    // Shape: 4×4×2, showing only 2×2 for space
  [0.3, -0.1]     // (in reality, this would be 4×4 for each channel)
]
```

We want to calculate ∂L/∂F for Filter 1, position (0,0), channel 1:

**Filter position F[0,0,0,0]:**

This weight was used at multiple output positions. Let's trace them:

At output position (0,0):
- Input value: Input[0×1 + 0, 0×1 + 0, 0] = Input[0,0,0] = 1
- Output gradient: ∂L/∂Output[0,0,0] = 0.1
- Contribution: 1 × 0.1 = 0.1

At output position (0,1):
- Input value: Input[0×1 + 0, 1×1 + 0, 0] = Input[0,1,0] = 2
- Output gradient: ∂L/∂Output[0,1,0] = 0.2
- Contribution: 2 × 0.2 = 0.4

At output position (1,0):
- Input value: Input[1×1 + 0, 0×1 + 0, 0] = Input[1,0,0] = 0
- Output gradient: ∂L/∂Output[1,0,0] = 0.3
- Contribution: 0 × 0.3 = 0

At output position (1,1):
- Input value: Input[1×1 + 0, 1×1 + 0, 0] = Input[1,1,0] = 1
- Output gradient: ∂L/∂Output[1,1,0] = -0.1
- Contribution: 1 × (-0.1) = -0.1

(Continue for all other positions...)

Sum all contributions:
```
∂L/∂F[0,0,0,0] = 0.1 + 0.4 + 0 + (-0.1) + ... (all positions)
```

**Step 2: Compute Gradient with Respect to Biases (∂L/∂b)**

The bias gradient is simpler - each bias is added to every output position in its channel, so we just sum all the gradients for that channel.

**Formula:**
```
∂L/∂b[k] = Σ   Σ   ∂L/∂Output[i,j,k]
           i=0 j=0
```

**Plain English:** Sum all the output gradients for each filter's output channel.

**Example:**

For bias[0] (first filter):
```
∂L/∂b[0] = sum of all ∂L/∂Output[:,:,0]
         = 0.1 + 0.2 + 0.3 + (-0.1) + ... (all 16 positions in 4×4)
```

**Step 3: Compute Gradient with Respect to Input (∂L/∂I)**

This is needed to pass gradients to the previous layer. It's like a "reverse convolution" - we use the same filters but flip them and slide them in a different pattern.

**Formula (full convolution):**
```
∂L/∂I[i,j,c] = Σ   Σ   Σ   F[m,n,c,k] × ∂L/∂Output[i-m, j-n, k]
               m=0 n=0 k=0
```

Where we need to be careful about boundaries.

**More Practical Formula (with proper indexing):**
```
∂L/∂I[i,j,c] = Σ   (∂L/∂Output[⌊(i-m)/s⌋, ⌊(j-n)/s⌋, k] × F[m,n,c,k])
              m,n,k
```

Only summing over positions where the filter overlapped with input position (i,j).

**Plain English:** For each input position, we find all the times it was used (multiplied by a filter weight), multiply those filter weights by the corresponding output gradients, and sum them up.

**Detailed Example:**

Calculate ∂L/∂I[0,0,0] (gradient for input position (0,0) in channel 1):

Input[0,0,0] was used in multiple convolutions:

1. **When computing Output[0,0,0]:**
   - Filter weight used: F[0,0,0,0] = 1
   - Output gradient: ∂L/∂Output[0,0,0] = 0.1
   - Contribution: 1 × 0.1 = 0.1

2. **When computing Output[0,0,1]:**
   - Filter weight used: F[0,0,0,1] = 1
   - Output gradient: ∂L/∂Output[0,0,1] (assume 0.05)
   - Contribution: 1 × 0.05 = 0.05

Sum all contributions:
```
∂L/∂I[0,0,0] = 0.1 + 0.05 + ... (other filters/positions that used this input)
```

**Step 4: Update Parameters**

After computing all gradients, update filters and biases using gradient descent:

```
F_new = F_old - learning_rate × ∂L/∂F
b_new = b_old - learning_rate × ∂L/∂b
```

**Symbol Legend:**
- `learning_rate`: Step size for updates (hyperparameter, typically 0.001 to 0.1)
- `F_old`: Current filter weights
- `F_new`: Updated filter weights
- `b_old`: Current biases
- `b_new`: Updated biases

**Complete Backpropagation Algorithm:**

```
Algorithm: Convolutional Layer Backpropagation

Input: 
  - ∂L/∂Output (gradient from next layer)
  - Input (saved from forward pass)
  - Filters (saved from forward pass)
  - Stride, padding parameters

Output:
  - ∂L/∂F (filter gradients)
  - ∂L/∂b (bias gradients)
  - ∂L/∂I (input gradients for previous layer)

Steps:
1. Initialize gradient arrays:
   - ∂L/∂F = zeros(filter shape)
   - ∂L/∂b = zeros(bias shape)
   - ∂L/∂I = zeros(input shape)

2. For each output position (i,j,k):
   a. Locate the input region used in forward pass
   b. For each position (m,n,c) in that region:
      - Add Input[i×s+m, j×s+n, c] × ∂L/∂Output[i,j,k] to ∂L/∂F[m,n,c,k]
      - Add F[m,n,c,k] × ∂L/∂Output[i,j,k] to ∂L/∂I[i×s+m, j×s+n, c]
   c. Add ∂L/∂Output[i,j,k] to ∂L/∂b[k]

3. Return ∂L/∂F, ∂L/∂b, ∂L/∂I
```

**Key Insights:**

1. **Gradient accumulation**: Each weight and input is used multiple times, so gradients accumulate
2. **Shape matching**: Filter gradients have the same shape as filters, input gradients match input shape
3. **Bias simplicity**: Bias gradients are just sums of output gradients
4. **Computational cost**: Backprop through convolution is roughly the same cost as forward prop
5. **Numerical stability**: Gradients can explode or vanish with very deep networks

### When to Use Convolution

**Use Convolutional Layers When:**

1. **Data has grid-like topology**
   - Images (2D grids of pixels)
   - Videos (3D: time + 2D spatial)
   - Audio spectrograms (time × frequency)
   - Time series with multiple sensors

2. **Spatial relationships matter**
   - Local patterns are important (edges in images, notes in music)
   - Nearby points are correlated
   - You want to detect patterns regardless of position

3. **You need parameter efficiency**
   - Input is high-dimensional
   - Limited training data
   - Need to reduce overfitting

4. **Translation invariance is desired**
   - A cat should be recognized whether it's left or right
   - A spoken word should be detected at any time point

**Don't Use Convolutional Layers When:**

1. **No spatial structure**
   - Tabular data (age, income, etc.)
   - Text after embedding (use in embedding layer is different)
   - General feature vectors

2. **Global context is more important**
   - Small inputs where every position relates to every other
   - Tasks requiring long-range dependencies

3. **Position matters absolutely**
   - When the meaning depends on exact position
   - Structured data with positional significance

**How to Set Up Correctly:**

1. **Filter Size:**
   - Start with 3×3 filters (most common)
   - Use 5×5 or 7×7 for first layer if input is large
   - Avoid filters larger than 7×7 (use multiple 3×3 instead)
   - Rationale: Smaller filters are more efficient and can learn complex patterns when stacked

2. **Number of Filters:**
   - Start small: 16-32 filters in first layer
   - Increase in deeper layers: 64, 128, 256, 512
   - Double the filters when spatial dimensions are halved
   - Rule of thumb: More filters = more different patterns to detect

3. **Stride:**
   - Use stride=1 for most layers
   - Use stride=2 to downsample (alternative to pooling)
   - Avoid stride > 2 (loses too much information)

4. **Padding:**
   - Use 'same' padding to maintain dimensions
   - Use 'valid' (no padding) to reduce dimensions
   - For odd filter sizes (3×3, 5×5), 'same' padding maintains exact size

5. **Architecture Pattern:**
   - Common pattern: CONV → RELU → CONV → RELU → POOL → repeat
   - Gradually increase filters while decreasing spatial dimensions
   - Typical progression: 32 → 64 → 128 → 256 → 512

6. **Hyperparameter Tips:**
   - Learning rate: Start with 0.001, use learning rate scheduling
   - Batch size: 32-256 depending on memory
   - Weight initialization: He initialization for ReLU, Xavier for tanh/sigmoid
   - Regularization: Dropout (0.5), L2 regularization (0.0001-0.001)

---

## Basic Filters (Kernels)

### Filters: Plain English Overview

Filters, also called kernels, are the small matrices of learnable weights that slide across the input in a convolutional layer. They are the "pattern detectors" of the network. Each filter is designed (or learns) to detect a specific type of pattern in the data.

**Analogy:** Think of filters like specialized inspectors at a factory. You have one inspector who only looks for vertical cracks (vertical edge detector), another who only looks for horizontal cracks (horizontal edge detector), and another who checks for color changes (color detector). Each inspector has their own checklist (filter weights) and produces a report (feature map) showing where they found their specific issue.

**Why Multiple Filters?** 
- One filter can only detect one type of pattern
- Images contain many types of patterns (edges, textures, colors, shapes)
- Multiple filters allow the network to detect multiple patterns simultaneously
- Each filter produces one "channel" in the output

**Connection to Mathematics:**
Remember the convolution formula we learned:
```
Output[i,j,k] = Σ Σ Σ Input[i+m,j+n,c] × Filter[m,n,c,k] + bias[k]
```
The filter values `Filter[m,n,c,k]` are what make each filter unique and determine what pattern it detects.

### How Filters Work

**The Learning Process:**

1. **Initialization:** Filters start with random small values (typically using He or Xavier initialization)

2. **Forward Pass:** 
   - Each filter slides across the input
   - Computes dot product at each position
   - High dot product = strong pattern match
   - Low dot product = weak or no match

3. **Pattern Matching Intuition:**
   - If input pixels match filter weights, dot product is large and positive
   - If input is opposite of filter, dot product is large and negative
   - If no relationship, dot product is near zero

4. **Training:**
   - Through backpropagation, filter values are updated
   - Filters learn to detect patterns that help minimize the loss
   - Early layers learn simple patterns (edges, colors)
   - Deeper layers learn complex patterns (shapes, objects)

**Hierarchical Feature Detection:**

```
Layer 1 (Early): Simple patterns
├── Vertical edges
├── Horizontal edges  
├── Diagonal edges
└── Color blobs

Layer 2 (Middle): Combinations
├── Corners (combining edge filters)
├── Simple textures
└── Small shapes

Layer 3 (Deep): Complex patterns
├── Object parts (eyes, wheels)
├── Complex textures
└── Partial objects

Layer 4 (Very Deep): High-level concepts
├── Complete objects
├── Scenes
└── Semantic concepts
```

### Types of Basic Filters

In practice, filters are learned automatically, but understanding classic hand-crafted filters helps build intuition about what filters do.

**1. Edge Detection Filters**

These detect changes in intensity, which correspond to edges in images.

**Vertical Edge Detector:**
```
K_vertical = [
  [1,  0, -1],
  [1,  0, -1],
  [1,  0, -1]
]
```

**How it works:**
- Left column: weights of +1 (looks for bright pixels on left)
- Middle column: weights of 0 (ignores middle)
- Right column: weights of -1 (looks for dark pixels on right)
- Result: Detects transitions from bright→dark (left to right)

**Horizontal Edge Detector:**
```
K_horizontal = [
  [ 1,  1,  1],
  [ 0,  0,  0],
  [-1, -1, -1]
]
```

**How it works:**
- Top row: +1 (bright on top)
- Middle row: 0 (ignore)
- Bottom row: -1 (dark on bottom)
- Result: Detects transitions from bright→dark (top to bottom)

**Sobel Filters (Smoother Edge Detection):**

Vertical Sobel:
```
K_sobel_x = [
  [1,  0, -1],
  [2,  0, -2],
  [1,  0, -1]
]
```

Horizontal Sobel:
```
K_sobel_y = [
  [ 1,  2,  1],
  [ 0,  0,  0],
  [-1, -2, -1]
]
```

The middle row/column has weight 2, giving more importance to the center pixels.

**2. Blur/Smoothing Filters**

These average nearby pixels, reducing noise and detail.

**Box Blur (Average Filter):**
```
K_blur = (1/9) × [
  [1, 1, 1],
  [1, 1, 1],
  [1, 1, 1]
]
```

All weights are equal, so each output is the average of the 3×3 region.

**Gaussian Blur:**
```
K_gaussian = (1/16) × [
  [1, 2, 1],
  [2, 4, 2],
  [1, 2, 1]
]
```

Weights follow a Gaussian distribution, giving more weight to center pixels.

**3. Sharpening Filters**

These enhance edges and details.

```
K_sharpen = [
  [ 0, -1,  0],
  [-1,  5, -1],
  [ 0, -1,  0]
]
```

**How it works:**
- Center: +5 (strongly emphasize current pixel)
- Neighbors: -1 (subtract neighbors)
- Result: Enhances differences = sharpens edges

**4. Identity Filter**

```
K_identity = [
  [0, 0, 0],
  [0, 1, 0],
  [0, 0, 0]
]
```

Outputs the input unchanged (useful for skip connections and residual networks).

### Filters: Mathematical Representation

**General Filter Structure:**

A filter F for a convolutional layer has shape:
```
F.shape = (f_h, f_w, C_in, C_out)
```

**Symbol Legend:**
- `f_h`: Filter height (typically 3, 5, or 7)
- `f_w`: Filter width (typically equals f_h)
- `C_in`: Number of input channels (e.g., 3 for RGB images)
- `C_out`: Number of filters (number of output channels)

**For a Single Filter:**

A single filter detecting one pattern:
```
F_k[:,:,:,k] represents filter k
```

For an RGB image (3 channels), filter k consists of:
```
F_k[:,:,0,k] - weights for red channel
F_k[:,:,1,k] - weights for green channel  
F_k[:,:,2,k] - weights for blue channel
```

**Weight Initialization:**

Filters cannot start at zero (would never learn), so we initialize randomly:

**He Initialization (for ReLU):**
```
W ~ Normal(0, √(2/n_in))
```
Where n_in = f_h × f_w × C_in

**Xavier Initialization (for tanh/sigmoid):**
```
W ~ Normal(0, √(1/n_in))
```

**Output Feature Map Formula:**

For input I and filter F_k:
```
Output[:,:,k] = Σ   (I[:,:,c] * F_k[:,:,c,k]) + b_k
                c=0
                to C_in-1
```

Where:
- `*` denotes 2D convolution
- `c` indexes input channels
- `b_k` is bias for filter k

**Filter Parameters Count:**

For a convolutional layer:
```
Number of parameters = (f_h × f_w × C_in × C_out) + C_out
                       \_______filter weights______/   \bias/
```

**Example:**
- Input: 28×28×3
- Filter size: 3×3
- Number of filters: 64
- Parameters: (3 × 3 × 3 × 64) + 64 = 1,728 + 64 = 1,792 parameters

Compare to fully connected layer:
- Would need: (28 × 28 × 3) × 64 = 150,528 parameters
- Reduction: 1,792 / 150,528 ≈ 1.2% (85× fewer parameters!)

### Filter Examples with Calculations

Let's work through complete examples of applying different filter types.

**Example 1: Vertical Edge Detection**

**Input Image (6×6, grayscale):**
```
I = [
  [10, 10, 10, 100, 100, 100],
  [10, 10, 10, 100, 100, 100],
  [10, 10, 10, 100, 100, 100],
  [10, 10, 10, 100, 100, 100],
  [10, 10, 10, 100, 100, 100],
  [10, 10, 10, 100, 100, 100]
]
```

This image has a clear vertical edge in the middle (transition from 10 to 100).

**Vertical Edge Detection Filter:**
```
F = [
  [1,  0, -1],
  [1,  0, -1],
  [1,  0, -1]
]
```

**Calculate Output[1,1]:**

Extract the 3×3 region at position (1,1):
```
Region = [
  [10, 10, 10],
  [10, 10, 10],
  [10, 10, 10]
]
```

Apply filter:
```
Output[1,1] = (10×1) + (10×0) + (10×-1) +
              (10×1) + (10×0) + (10×-1) +
              (10×1) + (10×0) + (10×-1)
            
            = 10 + 0 - 10 + 10 + 0 - 10 + 10 + 0 - 10
            = 0
```

**Result:** 0 indicates no vertical edge (all pixels same intensity)

**Calculate Output[1,2]:**

Region at position (1,2):
```
Region = [
  [10, 10, 100],
  [10, 10, 100],
  [10, 10, 100]
]
```

Apply filter:
```
Output[1,2] = (10×1) + (10×0) + (100×-1) +
              (10×1) + (10×0) + (100×-1) +
              (10×1) + (10×0) + (100×-1)
            
            = 10 + 0 - 100 + 10 + 0 - 100 + 10 + 0 - 100
            = (10 - 100) × 3
            = -90 × 3
            = -270
```

**Result:** Large negative value indicates strong vertical edge (dark→bright from left→right)

**Complete Output (4×4):**
```
Output = [
  [0,    0,  -270, 0],
  [0,    0,  -270, 0],
  [0,    0,  -270, 0],
  [0,    0,  -270, 0]
]
```

**Interpretation:**
- Column 2 has strong negative values: vertical edge detected
- Other positions: no edge
- The filter successfully found the vertical boundary

**Example 2: Horizontal Edge Detection**

**Input Image (6×6):**
```
I = [
  [10,  10,  10,  10,  10,  10],
  [10,  10,  10,  10,  10,  10],
  [10,  10,  10,  10,  10,  10],
  [100, 100, 100, 100, 100, 100],
  [100, 100, 100, 100, 100, 100],
  [100, 100, 100, 100, 100, 100]
]
```

Horizontal edge between rows 2 and 3.

**Horizontal Edge Filter:**
```
F = [
  [ 1,  1,  1],
  [ 0,  0,  0],
  [-1, -1, -1]
]
```

**Calculate Output[1,1]:**

Region:
```
[10,  10,  10]
[10,  10,  10]
[100, 100, 100]
```

Apply filter:
```
Output[1,1] = (10×1) + (10×1) + (10×1) +
              (10×0) + (10×0) + (10×0) +
              (100×-1) + (100×-1) + (100×-1)
            
            = 30 + 0 - 300
            = -270
```

**Result:** Strong negative value indicates horizontal edge (dark→bright from top→bottom)

**Complete Output:**
```
Output = [
  [0,    0,    0,   0],
  [-270, -270, -270, -270],
  [0,    0,    0,   0],
  [0,    0,    0,   0]
]
```

**Example 3: Multi-Channel Filter (RGB)**

**Input Image (4×4×3):**

Red Channel:
```
I[:,:,0] = [
  [100, 100, 50, 50],
  [100, 100, 50, 50],
  [20,  20,  80, 80],
  [20,  20,  80, 80]
]
```

Green Channel:
```
I[:,:,1] = [
  [50, 50, 100, 100],
  [50, 50, 100, 100],
  [80, 80, 20,  20],
  [80, 80, 20,  20]
]
```

Blue Channel:
```
I[:,:,2] = [
  [30, 30, 70, 70],
  [30, 30, 70, 70],
  [70, 70, 30, 30],
  [70, 70, 30, 30]
]
```

**Filter (3×3×3):**

For Red Channel:
```
F[:,:,0] = [
  [1,  0, -1],
  [1,  0, -1],
  [1,  0, -1]
]
```

For Green Channel:
```
F[:,:,1] = [
  [0.5,  0, -0.5],
  [0.5,  0, -0.5],
  [0.5,  0, -0.5]
]
```

For Blue Channel:
```
F[:,:,2] = [
  [0.5,  0, -0.5],
  [0.5,  0, -0.5],
  [0.5,  0, -0.5]
]
```

Bias: b = 0

**Calculate Output[0,0]:**

From Red Channel:
```
Region = [
  [100, 100, 50],
  [100, 100, 50],
  [20,  20,  80]
]

Sum_red = (100×1) + (100×0) + (50×-1) +
          (100×1) + (100×0) + (50×-1) +
          (20×1)  + (20×0)  + (80×-1)
        
        = 100 + 0 - 50 + 100 + 0 - 50 + 20 + 0 - 80
        = 40
```

From Green Channel:
```
Region = [
  [50, 50, 100],
  [50, 50, 100],
  [80, 80, 20]
]

Sum_green = (50×0.5) + (50×0) + (100×-0.5) +
            (50×0.5) + (50×0) + (100×-0.5) +
            (80×0.5) + (80×0) + (20×-0.5)
          
          = 25 + 0 - 50 + 25 + 0 - 50 + 40 + 0 - 10
          = -20
```

From Blue Channel:
```
Region = [
  [30, 30, 70],
  [30, 30, 70],
  [70, 70, 30]
]

Sum_blue = (30×0.5) + (30×0) + (70×-0.5) +
           (30×0.5) + (30×0) + (70×-0.5) +
           (70×0.5) + (70×0) + (30×-0.5)
         
         = 15 + 0 - 35 + 15 + 0 - 35 + 35 + 0 - 15
         = -20
```

Total output:
```
Output[0,0] = Sum_red + Sum_green + Sum_blue + bias
            = 40 + (-20) + (-20) + 0
            = 0
```

**Key Point:** The filter combines information from all three color channels, allowing it to detect patterns that depend on color relationships.

### Forward Propagation with Filters

**Plain English Explanation:**

Forward propagation with filters means sliding multiple filters across the input, with each filter detecting a different pattern and producing its own output channel. The complete forward pass applies all filters and stacks their outputs.

**Complete Process for Multiple Filters:**

**Given:**
- Input: I with shape (H, W, C_in)
- Filters: F with shape (f, f, C_in, C_out)
- Biases: b with shape (C_out,)
- Stride: s
- Padding: p

**Step-by-Step Algorithm:**

```
Algorithm: Multi-Filter Forward Propagation

1. Add padding to input if p > 0:
   I_padded = add_padding(I, p)

2. Calculate output dimensions:
   H_out = floor((H + 2p - f) / s) + 1
   W_out = floor((W + 2p - f) / s) + 1

3. Initialize output:
   Output = zeros(H_out, W_out, C_out)

4. For each filter k from 0 to (C_out - 1):
   
   For each output row i from 0 to (H_out - 1):
      
      For each output column j from 0 to (W_out - 1):
         
         # Calculate input region coordinates
         row_start = i × s
         row_end = row_start + f
         col_start = j × s
         col_end = col_start + f
         
         # Extract input region
         region = I_padded[row_start:row_end, col_start:col_end, :]
         
         # Convolve with filter k
         sum = 0
         For each channel c from 0 to (C_in - 1):
            For each filter row m from 0 to (f - 1):
               For each filter col n from 0 to (f - 1):
                  sum += region[m,n,c] × F[m,n,c,k]
         
         # Add bias and store
         Output[i,j,k] = sum + b[k]

5. Apply activation (typically ReLU):
   Output = activation(Output)

6. Return Output
```

**Detailed Example with 2 Filters:**

**Input (4×4×2):**

Channel 0:
```
I[:,:,0] = [
  [1, 2, 3, 4],
  [5, 6, 7, 8],
  [9, 10, 11, 12],
  [13, 14, 15, 16]
]
```

Channel 1:
```
I[:,:,1] = [
  [1, 1, 2, 2],
  [1, 1, 2, 2],
  [3, 3, 4, 4],
  [3, 3, 4, 4]
]
```

**Filter 1 (2×2×2):**
```
F[:,:,0,0] = [      F[:,:,1,0] = [
  [1, 0],              [0.5, 0.5],
  [0, 1]               [0.5, 0.5]
]                    ]

bias[0] = 0
```

**Filter 2 (2×2×2):**
```
F[:,:,0,1] = [      F[:,:,1,1] = [
  [1, 1],              [1, 1],
  [1, 1]               [1, 1]
]                    ]

bias[1] = -1
```

Stride = 1, No padding

**Output size:**
```
H_out = (4 - 2)/1 + 1 = 3
W_out = (4 - 2)/1 + 1 = 3
C_out = 2 (two filters)
```

Output shape: (3, 3, 2)

**Calculate Output[0,0,0] (Filter 1 at position 0,0):**

Region from channel 0:
```
[1, 2]
[5, 6]
```

Region from channel 1:
```
[1, 1]
[1, 1]
```

Computation:
```
From channel 0:
sum_c0 = (1×1) + (2×0) + (5×0) + (6×1)
       = 1 + 0 + 0 + 6
       = 7

From channel 1:
sum_c1 = (1×0.5) + (1×0.5) + (1×0.5) + (1×0.5)
       = 0.5 + 0.5 + 0.5 + 0.5
       = 2

Total:
Output[0,0,0] = sum_c0 + sum_c1 + bias[0]
              = 7 + 2 + 0
              = 9
```

**Calculate Output[0,0,1] (Filter 2 at position 0,0):**

Same regions as above.

Computation:
```
From channel 0:
sum_c0 = (1×1) + (2×1) + (5×1) + (6×1)
       = 1 + 2 + 5 + 6
       = 14

From channel 1:
sum_c1 = (1×1) + (1×1) + (1×1) + (1×1)
       = 1 + 1 + 1 + 1
       = 4

Total:
Output[0,0,1] = sum_c0 + sum_c1 + bias[1]
              = 14 + 4 + (-1)
              = 17
```

**Continuing this process for all positions gives:**

Output channel 0:
```
Output[:,:,0] = [
  [9,   11,  13],
  [17,  19,  21],
  [25,  27,  29]
]
```

Output channel 1:
```
Output[:,:,1] = [
  [17,  21,  25],
  [33,  37,  41],
  [49,  53,  57]
]
```

After ReLU (no change since all positive):
```
Final Output = same as above
```

### Backward Propagation with Filters

**Plain English Explanation:**

During backward propagation with multiple filters, we compute gradients for each filter independently, but we accumulate the input gradients from all filters since the same input was used by all of them.

**Three Key Gradients:**

1. **∂L/∂F[k]**: Gradient for each filter k
2. **∂L/∂b[k]**: Gradient for each bias k
3. **∂L/∂I**: Gradient for input (accumulated from all filters)

**Complete Backpropagation Algorithm:**

```
Algorithm: Multi-Filter Backward Propagation

Input:
  - ∂L/∂Output: shape (H_out, W_out, C_out)
  - I: input saved from forward pass, shape (H, W, C_in)
  - F: filters saved from forward pass, shape (f, f, C_in, C_out)
  - s: stride
  - p: padding

Output:
  - ∂L/∂F: filter gradients, shape (f, f, C_in, C_out)
  - ∂L/∂b: bias gradients, shape (C_out,)
  - ∂L/∂I: input gradients, shape (H, W, C_in)

Steps:

1. Initialize gradients:
   ∂L/∂F = zeros(f, f, C_in, C_out)
   ∂L/∂b = zeros(C_out)
   ∂L/∂I = zeros(H, W, C_in)

2. For each filter k from 0 to (C_out - 1):
   
   # Compute bias gradient (sum all output gradients for this filter)
   ∂L/∂b[k] = sum(∂L/∂Output[:,:,k])
   
   # Compute filter and input gradients
   For each output position (i,j):
      
      # Calculate corresponding input region
      row_start = i × s
      row_end = row_start + f
      col_start = j × s
      col_end = col_start + f
      
      # Get gradient from output
      grad_out = ∂L/∂Output[i,j,k]
      
      # Update filter gradients
      For each channel c:
         For each filter position (m,n):
            ∂L/∂F[m,n,c,k] += I[row_start+m, col_start+n, c] × grad_out
      
      # Update input gradients
      For each channel c:
         For each filter position (m,n):
            ∂L/∂I[row_start+m, col_start+n, c] += F[m,n,c,k] × grad_out

3. Return ∂L/∂F, ∂L/∂b, ∂L/∂I
```

**Detailed Example:**

Let's continue with our previous example and compute gradients.

**Given from forward pass:**
- Input I (4×4×2)
- Filters F (2×2×2×2) - two filters
- Output (3×3×2)

**Suppose we have output gradients:**

```
∂L/∂Output[:,:,0] = [    ∂L/∂Output[:,:,1] = [
  [0.1, 0.2, 0.1],          [0.3, 0.1, 0.2],
  [0.2, 0.3, 0.2],          [0.2, 0.4, 0.1],
  [0.1, 0.2, 0.1]           [0.3, 0.2, 0.3]
]                         ]
```

**Step 1: Compute Bias Gradients**

For bias[0]:
```
∂L/∂b[0] = sum(∂L/∂Output[:,:,0])
         = 0.1 + 0.2 + 0.1 + 0.2 + 0.3 + 0.2 + 0.1 + 0.2 + 0.1
         = 1.5
```

For bias[1]:
```
∂L/∂b[1] = sum(∂L/∂Output[:,:,1])
         = 0.3 + 0.1 + 0.2 + 0.2 + 0.4 + 0.1 + 0.3 + 0.2 + 0.3
         = 2.1
```

**Step 2: Compute Filter Gradients**

For ∂L/∂F[0,0,0,0] (Filter 1, position (0,0), channel 0):

This weight was used at multiple output positions:

At output (0,0):
- Input value: I[0,0,0] = 1
- Output gradient: ∂L/∂Output[0,0,0] = 0.1
- Contribution: 1 × 0.1 = 0.1

At output (0,1):
- Input value: I[0,1,0] = 2
- Output gradient: ∂L/∂Output[0,1,0] = 0.2
- Contribution: 2 × 0.2 = 0.4

At output (0,2):
- Input value: I[0,2,0] = 3
- Output gradient: ∂L/∂Output[0,2,0] = 0.1
- Contribution: 3 × 0.1 = 0.3

At output (1,0):
- Input value: I[1,0,0] = 5
- Output gradient: ∂L/∂Output[1,0,0] = 0.2
- Contribution: 5 × 0.2 = 1.0

...and so on for positions (1,1), (1,2), (2,0), (2,1), (2,2)

```
∂L/∂F[0,0,0,0] = 0.1 + 0.4 + 0.3 + 1.0 + ... (all 9 contributions)
```

For ∂L/∂F[1,1,0,0] (Filter 1, position (1,1), channel 0):

This weight multiplies the bottom-right element of each region:

At output (0,0):
- Input value: I[1,1,0] = 6
- Output gradient: ∂L/∂Output[0,0,0] = 0.1
- Contribution: 6 × 0.1 = 0.6

At output (0,1):
- Input value: I[1,2,0] = 7
- Output gradient: ∂L/∂Output[0,1,0] = 0.2
- Contribution: 7 × 0.2 = 1.4

...continue for all positions

```
∂L/∂F[1,1,0,0] = 0.6 + 1.4 + ... (all contributions)
```

**Step 3: Compute Input Gradients**

For ∂L/∂I[0,0,0] (Input position (0,0), channel 0):

This input was used by multiple filters at multiple positions:

By Filter 1 at output (0,0):
- Filter weight: F[0,0,0,0] = 1
- Output gradient: ∂L/∂Output[0,0,0] = 0.1
- Contribution: 1 × 0.1 = 0.1

By Filter 2 at output (0,0):
- Filter weight: F[0,0,0,1] = 1
- Output gradient: ∂L/∂Output[0,0,1] = 0.3
- Contribution: 1 × 0.3 = 0.3

```
∂L/∂I[0,0,0] = 0.1 + 0.3
             = 0.4
```

For ∂L/∂I[1,1,0] (Input position (1,1), channel 0):

This input is in the overlapping region of four output positions:

At output (0,0): Used by position (1,1) of filter
- Filter 1 weight: F[1,1,0,0] = 1
- Output gradient: ∂L/∂Output[0,0,0] = 0.1
- Contribution: 1 × 0.1 = 0.1
- Filter 2 weight: F[1,1,0,1] = 1
- Output gradient: ∂L/∂Output[0,0,1] = 0.3
- Contribution: 1 × 0.3 = 0.3

At output (0,1): Used by position (1,0) of filter
- Filter 1 weight: F[1,0,0,0] = 0
- Contribution: 0 × 0.2 = 0
- Filter 2 weight: F[1,0,0,1] = 1
- Contribution: 1 × 0.1 = 0.1

At output (1,0): Used by position (0,1) of filter
- Filter 1 weight: F[0,1,0,0] = 0
- Contribution: 0 × 0.2 = 0
- Filter 2 weight: F[0,1,0,1] = 1
- Contribution: 1 × 0.2 = 0.2

At output (1,1): Used by position (0,0) of filter
- Filter 1 weight: F[0,0,0,0] = 1
- Contribution: 1 × 0.3 = 0.3
- Filter 2 weight: F[0,0,0,1] = 1
- Contribution: 1 × 0.4 = 0.4

```
∂L/∂I[1,1,0] = 0.1 + 0.3 + 0 + 0.1 + 0 + 0.2 + 0.3 + 0.4
             = 1.4
```

**Key Observations:**

1. **Gradient Accumulation**: Input gradients accumulate from all filters and all positions where that input was used
2. **Filter Gradients**: Each filter gradient is computed independently by looking at where it was applied
3. **Bias Simplicity**: Bias gradients are just sums
4. **Computational Cost**: Backprop is roughly 2× the cost of forward prop (computing two sets of gradients)

### Practical Guidelines for Filters

**Filter Size Selection:**

**Use 3×3 filters (most common):**
- Pro: Efficient, captures local patterns well
- Pro: Can be stacked to achieve larger receptive fields
- Pro: Fewer parameters than larger filters
- Con: Smaller receptive field per layer
- **Recommendation**: Default choice for most layers

**Use 1×1 filters for:**
- Changing the number of channels without spatial convolution
- Reducing dimensions (fewer output channels than input)
- Adding non-linearity without changing spatial size
- Example: Inception networks, MobileNets

**Use 5×5 or 7×7 filters for:**
- First layer on large images (to capture larger patterns)
- When you need larger receptive fields quickly
- Downside: More parameters, slower computation
- **Alternative**: Use two 3×3 layers instead (fewer parameters, same receptive field)

**Receptive Field Calculation:**

Two 3×3 filters give same receptive field as one 5×5:
```
Receptive field of 3×3 → 3×3 = 5×5
Parameters: 2 × (3×3) = 18 weights
vs.
Parameters: 1 × (5×5) = 25 weights
Savings: 28% fewer parameters with same receptive field
```

**Number of Filters per Layer:**

**General Pattern:**
```
Early layers: 32-64 filters
Middle layers: 128-256 filters
Deep layers: 512-1024 filters
```

**Reasoning:**
- Early layers: Detect simple patterns (edges, colors) - need fewer filters
- Deep layers: Detect complex patterns (object parts) - need more filters
- Double filters when halving spatial dimensions (maintains computational balance)

**Example Architecture:**
```
Input: 224×224×3

Conv1: 64 filters, 7×7    → 112×112×64 (stride 2)
Conv2: 128 filters, 3×3   → 56×56×128 (stride 2)
Conv3: 256 filters, 3×3   → 28×28×256 (stride 2)
Conv4: 512 filters, 3×3   → 14×14×512 (stride 2)
Conv5: 512 filters, 3×3   → 7×7×512 (stride 2)
```

**Filter Initialization:**

**He Initialization (ReLU):**
```python
std = sqrt(2.0 / (filter_height × filter_width × input_channels))
weights ~ Normal(mean=0, std=std)
```

**Xavier Initialization (Tanh/Sigmoid):**
```python
std = sqrt(1.0 / (filter_height × filter_width × input_channels))
weights ~ Normal(mean=0, std=std)
```

**Why proper initialization matters:**
- Too small: Gradients vanish, slow learning
- Too large: Gradients explode, unstable training
- Just right: Fast, stable convergence

**When to Use Different Filter Configurations:**

1. **Dense Prediction (Segmentation):**
   - Use smaller strides (1)
   - Use padding to maintain resolution
   - Use dilated convolutions for larger receptive fields without downsampling

2. **Classification:**
   - Progressive downsampling (stride 2)
   - Larger filters early, smaller later
   - Heavy filtering at end before classification layer

3. **Object Detection:**
   - Multi-scale filters
   - Feature Pyramid Networks (FPN)
   - Mix of large and small receptive fields

4. **Efficient Networks (Mobile/Edge):**
   - Depthwise separable convolutions
   - 1×1 filters for channel reduction
   - Bottleneck architectures

**Common Mistakes to Avoid:**

1. **Using too many filters too early**
   - Wastes computation
   - Risk overfitting
   - Start small, grow gradually

2. **Not increasing filters when downsampling**
   - Loses information
   - Pattern: halve spatial size → double filters

3. **Forgetting to initialize properly**
   - Use He for ReLU, Xavier for tanh/sigmoid
   - Don't initialize to zero (all neurons would learn the same thing)

4. **Using large filters throughout**
   - More parameters = more overfitting risk
   - Prefer stacking 3×3 filters

5. **Not considering computational cost**
   - Filter size, number of filters, and input size multiply
   - A 7×7 filter has 5.4× the parameters of a 3×3
   - Monitor FLOPs (floating point operations) not just parameters

**Hyperparameter Guidelines:**

```
Typical Layer Configuration:

Input: H×W×C_in
Filter size: 3×3 (most common)
Number of filters: C_out (powers of 2: 32, 64, 128, 256, 512)
Stride: 1 (for same resolution) or 2 (for downsampling)
Padding: 'same' (maintain size) or 'valid' (no padding)
Activation: ReLU (after convolution)
Batch Normalization: After convolution, before activation

Output: H'×W'×C_out
where H' = floor((H + 2×pad - filter_size) / stride) + 1
```

**Performance Considerations:**

1. **Memory**: 
   - Larger images/channels consume more GPU memory
   - Batch size limited by memory

2. **Speed**:
   - Smaller filters (3×3) are fastest
   - More filters = slower (but more expressive)

3. **Accuracy**:
   - More filters generally improve accuracy
   - Diminishing returns after certain point
   - Must balance with regularization

**Rule of Thumb:**
```
Receptive field needed ≈ Size of smallest object you want to detect
Number of filters ≈ Complexity of patterns at that scale
Filter size: Use 3×3 unless you have a specific reason not to
```

---

**End of Initial Topics**

This completes the foundational topics: Table of Contents, Introduction to CNNs, Convolution Operation, and Basic Filters. Each section includes plain English explanations, detailed mathematics with legends, step-by-step examples, and comprehensive coverage of both forward and backward propagation.

The document is structured to be read like a textbook, progressing from basic concepts to detailed mathematical formulations, always connecting new material to previously explained concepts.