# Strided Convolution in Convolutional Neural Networks

## Table of Contents

1. [Introduction to Strided Convolution](#introduction-to-strided-convolution)
   - [What is Stride?](#what-is-stride)
   - [Why Use Strided Convolution?](#why-use-strided-convolution)
   - [Connection to Previous Topics](#connection-to-previous-topics)

2. [How Stride Works](#how-stride-works)
   - [Plain English Explanation](#plain-english-explanation)
   - [Visual Comparison: Stride 1 vs Stride 2](#visual-comparison-stride-1-vs-stride-2)
   - [Common Stride Values](#common-stride-values)

3. [Mathematical Foundation](#mathematical-foundation)
   - [Output Size with Stride](#output-size-with-stride)
   - [General Formula](#general-formula)
   - [Stride Interaction with Padding](#stride-interaction-with-padding)

4. [Detailed Examples](#detailed-examples)
   - [Example 1: Stride 2 (No Padding)](#example-1-stride-2-no-padding)
   - [Example 2: Stride 2 with Padding](#example-2-stride-2-with-padding)
   - [Example 3: Stride 3](#example-3-stride-3)
   - [Example 4: Multi-Channel with Stride](#example-4-multi-channel-with-stride)

5. [Forward Propagation with Stride](#forward-propagation-with-stride)
   - [Complete Process](#forward-complete-process)
   - [Step-by-Step Algorithm](#forward-algorithm)
   - [Numerical Example](#forward-numerical-example)

6. [Backward Propagation with Stride](#backward-propagation-with-stride)
   - [Plain English Explanation](#backward-plain-english)
   - [Gradient Computation](#backward-gradient-computation)
   - [Detailed Example](#backward-detailed-example)

7. [Stride vs Pooling](#stride-vs-pooling)
   - [Comparison](#comparison)
   - [When to Use Each](#when-to-use-each)
   - [Combining Both](#combining-both)

8. [Practical Guidelines](#practical-guidelines)
   - [Choosing Stride Values](#choosing-stride-values)
   - [Common Architectures](#common-architectures)
   - [Best Practices](#best-practices)

---

## Introduction to Strided Convolution

### What is Stride?

**Plain English Overview:**

Stride is the step size by which we move the filter across the input during convolution. Instead of sliding the filter one position at a time (stride=1), we can skip positions by using a larger stride value. Stride=2 means we move the filter 2 positions at a time, stride=3 means 3 positions, and so on.

**Analogy:** Imagine you're mowing a lawn with a lawn mower that's 3 feet wide:
- **Stride=1**: You move forward 1 foot each pass, creating lots of overlap (inefficient but thorough)
- **Stride=2**: You move forward 2 feet each pass, moderate overlap
- **Stride=3**: You move forward 3 feet each pass, no overlap (efficient but might miss spots)

**Key Concept:** Stride controls:
1. **How much we downsample**: Larger stride = smaller output
2. **Computational cost**: Larger stride = fewer computations
3. **Information preservation**: Larger stride = more information discarded

### Why Use Strided Convolution?

**Purpose 1: Downsampling (Spatial Reduction)**

CNNs typically reduce spatial dimensions progressively while increasing channels:
- Input: 224×224×3
- After processing: 7×7×512

Stride provides a learnable way to downsample (as opposed to fixed pooling operations).

**Visual Illustration:**

```
Without Stride (stride=1):
Input 8×8  →  Conv 3×3  →  Output 6×6  (mild reduction)
│████████│     ┏━━┓        │██████│
│████████│     ┗━━┛        │██████│
│████████│                 │██████│

With Stride=2:
Input 8×8  →  Conv 3×3  →  Output 3×3  (strong reduction)
│████████│     ┏━━┓        │███│
│████████│     ┗━━┛        │███│
│████████│    (skip)       │███│
```

**Purpose 2: Computational Efficiency**

Larger stride means:
- Fewer positions to compute
- Faster forward and backward passes
- Less memory usage

**Calculation:**
```
Input: 100×100
Filter: 3×3

Stride=1: 98×98 = 9,604 output positions
Stride=2: 49×49 = 2,401 output positions (4× faster!)
Stride=5: 20×20 = 400 output positions (24× faster!)
```

**Purpose 3: Learnable Downsampling**

Unlike pooling (which uses fixed operations like max or average), strided convolution has learnable parameters. The network can learn the best way to downsample for the specific task.

**Comparison:**

```
Max Pooling (Fixed):          Strided Conv (Learnable):
┌────────┐                    ┌────────┐
│ 1  5  │ → max = 9          │ 1  5  │ → w₁×1 + w₂×5
│ 3  9  │                    │ 3  9  │   + w₃×3 + w₄×9
└────────┘                    └────────┘   (learned weights!)
Always takes                  Network learns
maximum                       optimal combination
```

### Connection to Previous Topics

**Recall Basic Convolution (stride=1):**

From our first document, convolution with stride=1:
```
Output[i,j] = Σ Σ Input[i+m, j+n] × Filter[m,n]
             m n
```

**With Arbitrary Stride s:**
```
Output[i,j] = Σ Σ Input[i×s+m, j×s+n] × Filter[m,n]
             m n
```

The only change: multiply output indices (i,j) by stride s before indexing input.

**Recall Padding:**

In the padding document, we learned:
```
Output_size = (Input_size + 2×Padding - Filter_size) / Stride + 1
```

Stride is the denominator - larger stride means smaller output!

**Integration of All Concepts:**

```
Complete Formula:
H_out = floor((H_in + 2p - f) / s) + 1
              \_________/   \_/   \_/
                  |          |     |
              Padding    Filter  Stride
              effect     size   (key parameter)
```

---

## How Stride Works

### Plain English Explanation

When we apply convolution with stride s, we:
1. Place the filter at the starting position
2. Compute the convolution (element-wise multiply and sum)
3. **Move the filter s positions** instead of 1
4. Repeat until we've covered the input

**The key difference:** We skip positions. With stride=2, we skip every other position. With stride=3, we skip two positions each time.

**Visual Example of Movement:**

```
Stride = 1 (Visit Every Position):
┌─────────────┐
│ ┏━┓→┏━┓→┏━┓ │  Positions: (0,0), (0,1), (0,2), ...
│ ┗━┛ ┗━┛ ┗━┛ │  Dense coverage
│  ↓   ↓   ↓  │
│ ┏━┓→┏━┓→┏━┓ │
│ ┗━┛ ┗━┛ ┗━┛ │
└─────────────┘

Stride = 2 (Skip Every Other):
┌─────────────┐
│ ┏━┓ → → ┏━┓ │  Positions: (0,0), (0,2), (0,4), ...
│ ┗━┛     ┗━┛ │  Sparse coverage
│  ↓       ↓  │  
│  ↓       ↓  │  (Skip intermediate)
│ ┏━┓ → → ┏━┓ │
│ ┗━┛     ┗━┛ │
└─────────────┘

Stride = 3 (Skip Two Each Time):
┌─────────────┐
│ ┏━┓ → → → ┏━┓  Positions: (0,0), (0,3), ...
│ ┗━┛       ┗━┛  Very sparse
│  ↓         ↓
│  ↓         ↓
│  ↓         ↓
│ ┏━┓ → → → ┏━┓
│ ┗━┛       ┗━┛
└─────────────┘
```

### Visual Comparison: Stride 1 vs Stride 2

**Complete Side-by-Side Comparison:**

**Input (7×7):**
```
I = [
  [1,  2,  3,  4,  5,  6,  7],
  [8,  9, 10, 11, 12, 13, 14],
  [15,16, 17, 18, 19, 20, 21],
  [22,23, 24, 25, 26, 27, 28],
  [29,30, 31, 32, 33, 34, 35],
  [36,37, 38, 39, 40, 41, 42],
  [43,44, 45, 46, 47, 48, 49]
]
```

**Filter (3×3):**
```
F = [
  [1, 0, 0],
  [0, 1, 0],
  [0, 0, 1]
]
```

**Stride = 1:**

```
Filter positions (5×5 output):
┌─────────────────────────────────┐
│ ┏━━━┓                          │
│ ┃1 2┃3  4  5  6  7            │ Pos (0,0)
│ ┃8 9┃10 11 12 13 14           │
│ ┃15┃16 17 18 19 20 21         │
│ ┗━━━┛                          │
├─────────────────────────────────┤
│  1 ┏━━━┓                       │
│  8 ┃9 10┃11 12 13 14          │ Pos (0,1) - Move 1 right
│ 15 ┃16┃17 18 19 20 21         │
│ 22 ┗━━━┛                       │
├─────────────────────────────────┤
        ... (25 total positions)
        
Output: 5×5 matrix
```

**Stride = 2:**

```
Filter positions (3×3 output):
┌─────────────────────────────────┐
│ ┏━━━┓                          │
│ ┃1 2┃3  4  5  6  7            │ Pos (0,0)
│ ┃8 9┃10 11 12 13 14           │
│ ┃15┃16 17 18 19 20 21         │
│ ┗━━━┛                          │
├─────────────────────────────────┤
│  1  2 ┏━━━┓                    │
│  8  9 ┃10┃11 12 13 14         │ Pos (0,1) - Move 2 right
│ 15 16 ┃17┃18 19 20 21         │
│ 22 23 ┗━━━┛                    │
├─────────────────────────────────┤
        ... (9 total positions - skip many!)
        
Output: 3×3 matrix
```

**Output Size Comparison:**

```
Same input (7×7), same filter (3×3):

Stride=1: Output = (7-3)/1 + 1 = 5×5 = 25 values
Stride=2: Output = (7-3)/2 + 1 = 3×3 = 9 values
Stride=3: Output = (7-3)/3 + 1 = 2×2 = 4 values

Reduction factor = (stride)²
Stride=2 reduces output by 4× in total pixels
```

### Common Stride Values

**Stride = 1 (Standard Convolution):**
- Most common for intermediate layers
- Preserves spatial resolution
- Maximum information retention
- Higher computational cost

**Visual:**
```
Input   Filter Movement
┏━━━┓
┃###┃ → → → → →    Dense sampling
┗━━━┛
  ↓ ↓ ↓ ↓ ↓        Every position
```

**Stride = 2 (Most Common Downsampling):**
- Standard choice for spatial reduction
- Balances efficiency and information retention
- Often replaces pooling layers
- Reduces output by 2× in each dimension

**Visual:**
```
Input   Filter Movement
┏━━━┓
┃###┃ → → ┏━━━┓    Skip every other
┗━━━┛     ┃###┃    
  ↓         ↓        Sample alternating positions
```

**Stride = 3 or Higher (Rare):**
- Aggressive downsampling
- Might lose important information
- Used in specific architectures
- Generally avoided

**Visual:**
```
Input   Filter Movement
┏━━━┓
┃###┃ → → → ┏━━━┓  Large skips
┗━━━┛       ┃###┃
  ↓           ↓      Sparse sampling
```

---

## Mathematical Foundation

### Output Size with Stride

**Core Formula:**

For input of size n, filter of size f, padding p, and stride s:

```
Output_size = floor((n + 2p - f) / s) + 1
```

**Symbol Legend:**
- `n`: Input dimension (height or width)
- `f`: Filter dimension
- `p`: Padding size
- `s`: Stride
- `floor()`: Round down to nearest integer

**Why Floor Function?**

The filter must fit completely within the (padded) input. If the calculation doesn't result in a whole number, we can't place the filter at a fractional position, so we round down.

**Example:**
```
Input: 10×10
Filter: 3×3
Padding: 0
Stride: 3

Output_size = floor((10 - 3) / 3) + 1
            = floor(7 / 3) + 1
            = floor(2.333...) + 1
            = 2 + 1
            = 3

We can only fit the filter at 3 positions per dimension
```

### General Formula

**Complete Mathematical Formulation:**

For a convolutional layer:

**Input Dimensions:**
- Height: H_in
- Width: W_in
- Channels: C_in

**Layer Parameters:**
- Filter size: f × f
- Number of filters: C_out
- Padding: p
- Stride: s

**Output Dimensions:**

```
H_out = floor((H_in + 2p - f) / s) + 1
W_out = floor((W_in + 2p - f) / s) + 1
C_out = Number of filters
```

**Convolution Operation:**

```
Output[i,j,k] = Σ   Σ   Σ   Input[i×s + m, j×s + n, c] × Filter[m,n,c,k] + bias[k]
               m=0 n=0 c=0
```

**Symbol Legend:**
- `i, j`: Output position indices (0 to H_out-1, 0 to W_out-1)
- `k`: Output channel index (filter index)
- `m, n`: Position within filter (0 to f-1)
- `c`: Input channel index
- `s`: **Stride** - controls spacing between sampled positions
- `i×s`: Starting row in input for output position i
- `j×s`: Starting column in input for output position j

**Key Observation:** The term `i×s` and `j×s` show how stride affects which input regions we sample. Each increment in output position i corresponds to s positions in the input.

### Stride Interaction with Padding

**Combined Effect:**

Padding and stride work together to control output size:

```
Output_size = floor((Input + 2×Padding - Filter) / Stride) + 1
```

**Relationship Table:**

```
Input: 32×32, Filter: 3×3

┌─────────┬─────────┬────────────┬────────────┐
│ Padding │ Stride  │ Output     │ Effect     │
├─────────┼─────────┼────────────┼────────────┤
│    0    │    1    │ 30×30      │ Slight ↓   │
│    1    │    1    │ 32×32      │ Maintain   │
│    0    │    2    │ 15×15      │ Halve      │
│    1    │    2    │ 16×16      │ Halve      │
│    2    │    2    │ 17×17      │ Halve+     │
└─────────┴─────────┴────────────┴────────────┘
```

**Visual Examples:**

**Case 1: Padding=0, Stride=1**
```
Input (5×5) → Output (3×3)
┌───────────┐    ┌─────┐
│ X X X X X │    │ O O │
│ X X X X X │ →  │ O O │  Small reduction
│ X X X X X │    │ O O │
│ X X X X X │    └─────┘
│ X X X X X │
└───────────┘
```

**Case 2: Padding=1, Stride=1**
```
Padded (7×7) → Output (5×5)
┌─────────────┐  ┌───────────┐
│ 0 0 0 0 0 0 │  │ O O O O O │
│ 0 X X X X 0 │  │ O O O O O │
│ 0 X X X X 0 │→ │ O O O O O │  Size preserved
│ 0 X X X X 0 │  │ O O O O O │
│ 0 X X X X 0 │  │ O O O O O │
│ 0 0 0 0 0 0 │  └───────────┘
└─────────────┘
```

**Case 3: Padding=0, Stride=2**
```
Input (5×5) → Output (2×2)
┌───────────┐    ┌───┐
│ X X X X X │    │ O │    Strong reduction
│ X X X X X │ →  │ O │    (downsampling)
│ X X X X X │    └───┘
│ X X X X X │
│ X X X X X │
└───────────┘
```

**Case 4: Padding=1, Stride=2**
```
Padded (7×7) → Output (3×3)
┌─────────────┐  ┌─────┐
│ 0 0 0 0 0 0 │  │ O O │
│ 0 X X X X 0 │  │ O O │  Moderate reduction
│ 0 X X X X 0 │→ │ O O │  with edge preservation
│ 0 X X X X 0 │  └─────┘
│ 0 X X X X 0 │
│ 0 0 0 0 0 0 │
└─────────────┘
```

---

## Detailed Examples

### Example 1: Stride 2 (No Padding)

**Given:**
- Input: 6×6 (grayscale, single channel)
- Filter: 3×3
- Stride: 2
- Padding: 0

**Input:**
```
I = [
  [1,  2,  3,  4,  5,  6],
  [7,  8,  9, 10, 11, 12],
  [13,14, 15, 16, 17, 18],
  [19,20, 21, 22, 23, 24],
  [25,26, 27, 28, 29, 30],
  [31,32, 33, 34, 35, 36]
]
```

**Filter:**
```
F = [
  [1, 0, 0],
  [0, 1, 0],
  [0, 0, 1]
]
```
(Diagonal pattern detector)

**Step 1: Calculate Output Size**

```
H_out = floor((6 + 2×0 - 3) / 2) + 1
      = floor(3 / 2) + 1
      = floor(1.5) + 1
      = 1 + 1
      = 2

Output: 2×2
```

**Visual Illustration of Filter Positions:**

```
Position (0,0): Start at (0,0), stride=2
┌─────────────────────┐
│ ┏━━━━━━━┓           │
│ ┃ 1  2  3┃ 4  5  6  │  Row 0-2
│ ┃ 7  8  9┃10 11 12  │  Col 0-2
│ ┃13 14 15┃16 17 18  │
│ ┗━━━━━━━┛           │
│  19 20 21 22 23 24  │
│  25 26 27 28 29 30  │
│  31 32 33 34 35 36  │
└─────────────────────┘

Position (0,1): Move 2 columns right
┌─────────────────────┐
│  1  2  3 ┏━━━━━━━┓ │
│  7  8  9 ┃10 11 12┃ │  Row 0-2
│ 13 14 15 ┃16 17 18┃ │  Col 2-4
│          ┗━━━━━━━┛ │
│  19 20 21 22 23 24  │
│  25 26 27 28 29 30  │
│  31 32 33 34 35 36  │
└─────────────────────┘

Position (1,0): Move 2 rows down from (0,0)
┌─────────────────────┐
│  1  2  3  4  5  6   │
│  7  8  9 10 11 12   │
│ ┏━━━━━━━┓           │
│ ┃19 20 21┃22 23 24  │  Row 2-4
│ ┃25 26 27┃28 29 30  │  Col 0-2
│ ┃31 32 33┃34 35 36  │
│ ┗━━━━━━━┛           │
└─────────────────────┘

Position (1,1): Move 2 columns right, 2 rows down
┌─────────────────────┐
│  1  2  3  4  5  6   │
│  7  8  9 10 11 12   │
│ 13 14 15 ┏━━━━━━━┓ │
│ 19 20 21 ┃22 23 24┃ │  Row 2-4
│ 25 26 27 ┃28 29 30┃ │  Col 2-4
│ 31 32 33 ┃34 35 36┃ │
│          ┗━━━━━━━┛ │
└─────────────────────┘

Only 4 positions possible with stride=2!
```

**Step 2: Calculate Output Values**

**Output[0,0]:**
```
Region:
[1,  2,  3]
[7,  8,  9]
[13,14, 15]

Calculation:
(1×1) + (2×0) + (3×0) +
(7×0) + (8×1) + (9×0) +
(13×0) + (14×0) + (15×1)

= 1 + 0 + 0 + 0 + 8 + 0 + 0 + 0 + 15
= 24

Output[0,0] = 24
```

**Output[0,1]:** (column index 1 → start at column 1×2=2)
```
Region:
[3,  4,  5]
[9, 10, 11]
[15,16, 17]

Calculation:
(3×1) + (4×0) + (5×0) +
(9×0) + (10×1) + (11×0) +
(15×0) + (16×0) + (17×1)

= 3 + 0 + 0 + 0 + 10 + 0 + 0 + 0 + 17
= 30

Output[0,1] = 30
```

**Output[1,0]:** (row index 1 → start at row 1×2=2)
```
Region:
[13,14, 15]
[19,20, 21]
[25,26, 27]

Calculation:
(13×1) + (14×0) + (15×0) +
(19×0) + (20×1) + (21×0) +
(25×0) + (26×0) + (27×1)

= 13 + 0 + 0 + 0 + 20 + 0 + 0 + 0 + 27
= 60

Output[1,0] = 60
```

**Output[1,1]:**
```
Region:
[15,16, 17]
[21,22, 23]
[27,28, 29]

Calculation:
(15×1) + (16×0) + (17×0) +
(21×0) + (22×1) + (23×0) +
(27×0) + (28×0) + (29×1)

= 15 + 0 + 0 + 0 + 22 + 0 + 0 + 0 + 29
= 66

Output[1,1] = 66
```

**Complete Output (2×2):**
```
Output = [
  [24, 30],
  [60, 66]
]
```

**Visualization of Sampled Regions:**

```
Input showing which pixels contribute to output:

Output[0,0]  Output[0,1]
    ↓            ↓
┌───────────────────┐
│ A  A  A  B  B  B │  A = Region for Output[0,0]
│ A  A  A  B  B  B │  B = Region for Output[0,1]
│ A  A  A  B  B  B │  C = Region for Output[1,0]
│ C  C  C  D  D  D │  D = Region for Output[1,1]
│ C  C  C  D  D  D │
│ C  C  C  D  D  D │
└───────────────────┘
    ↑            ↑
Output[1,0]  Output[1,1]

Notice: No overlap between regions (stride=3, non-overlapping)
```

### Example 2: Stride 2 with Padding

**Given:**
- Input: 5×5
- Filter: 3×3
- Stride: 2
- Padding: 1

**Input:**
```
I = [
  [1, 2, 3, 4, 5],
  [6, 7, 8, 9, 10],
  [11,12,13,14,15],
  [16,17,18,19,20],
  [21,22,23,24,25]
]
```

**Step 1: Add Padding**

```
Padded Input (7×7):
┌─────────────────────┐
│ 0  0  0  0  0  0  0 │
│ 0  1  2  3  4  5  0 │
│ 0  6  7  8  9 10  0 │
│ 0 11 12 13 14 15  0 │
│ 0 16 17 18 19 20  0 │
│ 0 21 22 23 24 25  0 │
│ 0  0  0  0  0  0  0 │
└─────────────────────┘
```

**Step 2: Calculate Output Size**

```
H_out = floor((5 + 2×1 - 3) / 2) + 1
      = floor((7 - 3) / 2) + 1
      = floor(4 / 2) + 1
      = 2 + 1
      = 3

Output: 3×3
```

**Step 3: Filter Positions with Stride=2**

```
Visual representation:

Position (0,0) - rows 0-2, cols 0-2:
┌─────────────────────┐
│ ┏━━━━━━━┓           │
│ ┃ 0  0  0┃ 0  0  0  │
│ ┃ 0  1  2┃ 3  4  5  │
│ ┃ 0  6  7┃ 8  9 10  │
│ ┗━━━━━━━┛           │
└─────────────────────┘

Position (0,1) - rows 0-2, cols 2-4 (skip 1 column):
┌─────────────────────┐
│  0  0 ┏━━━━━━━┓     │
│  0  1 ┃ 2  3  4┃ 5  │
│  0  6 ┃ 7  8  9┃10  │
│       ┗━━━━━━━┛     │
└─────────────────────┘

Position (0,2) - rows 0-2, cols 4-6:
┌─────────────────────┐
│  0  0  0  0 ┏━━━━━━━┓
│  0  1  2  3 ┃ 4  5  0┃
│  0  6  7  8 ┃ 9 10  0┃
│             ┗━━━━━━━┛
└─────────────────────┘

Position (1,0) - rows 2-4, cols 0-2 (skip 1 row):
┌─────────────────────┐
│  0  0  0  0  0  0  0│
│  0  1  2  3  4  5  0│
│ ┏━━━━━━━┓           │
│ ┃ 0  6  7┃ 8  9 10  │
│ ┃ 0 11 12┃13 14 15  │
│ ┃ 0 16 17┃18 19 20  │
│ ┗━━━━━━━┛           │
└─────────────────────┘

... and so on for all 9 positions
```

**Step 4: Compute Outputs**

**Filter:**
```
F = [
  [1, 1, 1],
  [1, 1, 1],
  [1, 1, 1]
]
```
(Sum/average filter)

**Output[0,0]:**
```
Region (from padded):
[0, 0, 0]
[0, 1, 2]
[0, 6, 7]

Sum = 0+0+0+0+1+2+0+6+7 = 16
Output[0,0] = 16
```

**Output[0,1]:**
```
Region:
[0, 0, 0]
[2, 3, 4]
[7, 8, 9]

Sum = 0+0+0+2+3+4+7+8+9 = 33
Output[0,1] = 33
```

**Output[0,2]:**
```
Region:
[0, 0, 0]
[4, 5, 0]
[9,10, 0]

Sum = 0+0+0+4+5+0+9+10+0 = 28
Output[0,2] = 28
```

**Output[1,0]:**
```
Region:
[0, 6,  7]
[0,11, 12]
[0,16, 17]

Sum = 0+6+7+0+11+12+0+16+17 = 69
Output[1,0] = 69
```

**Output[1,1]:** (Center - no padding involved)
```
Region:
[ 7,  8,  9]
[12, 13, 14]
[17, 18, 19]

Sum = 7+8+9+12+13+14+17+18+19 = 117
Output[1,1] = 117
```

**Output[1,2]:**
```
Region:
[ 9, 10,  0]
[14, 15,  0]
[19, 20,  0]

Sum = 9+10+0+14+15+0+19+20+0 = 87
Output[1,2] = 87
```

**Output[2,0]:**
```
Region:
[ 0, 16, 17]
[ 0, 21, 22]
[ 0,  0,  0]

Sum = 0+16+17+0+21+22+0+0+0 = 76
Output[2,0] = 76
```

**Output[2,1]:**
```
Region:
[17, 18, 19]
[22, 23, 24]
[ 0,  0,  0]

Sum = 17+18+19+22+23+24+0+0+0 = 123
Output[2,1] = 123
```

**Output[2,2]:**
```
Region:
[19, 20,  0]
[24, 25,  0]
[ 0,  0,  0]

Sum = 19+20+0+24+25+0+0+0+0 = 88
Output[2,2] = 88
```

**Complete Output (3×3):**
```
Output = [
  [16,  33,  28],
  [69, 117,  87],
  [76, 123,  88]
]
```

**Analysis:**
- Center value (117) is largest - no padding involved
- Corner values (16, 28, 76, 88) are smaller - include padding zeros
- Edge values (33, 69, 87, 123) are intermediate

### Example 3: Stride 3

**Given:**
- Input: 9×9
- Filter: 3×3
- Stride: 3
- Padding: 0

**Visual Process:**

```
Input (9×9):
┌─────────────────────────────┐
│ 1  2  3│ 4  5  6│ 7  8  9  │  Divide into
│10 11 12│13 14 15│16 17 18  │  non-overlapping
│19 20 21│22 23 24│25 26 27  │  3×3 blocks
├────────┼────────┼──────────┤
│28 29 30│31 32 33│34 35 36  │
│37 38 39│40 41 42│43 44 45  │
│46 47 48│49 50 51│52 53 54  │
├────────┼────────┼──────────┤
│55 56 57│58 59 60│61 62 63  │
│64 65 66│67 68 69│70 71 72  │
│73 74 75│76 77 78│79 80 81  │
└─────────────────────────────┘

Output size:
H_out = floor((9 - 3) / 3) + 1 = 3
Output: 3×3

Filter positions:
(0,0): rows 0-2, cols 0-2  ┏━━━┓
(0,1): rows 0-2, cols 3-5      ┏━━━┓
(0,2): rows 0-2, cols 6-8          ┏━━━┓

(1,0): rows 3-5, cols 0-2
(1,1): rows 3-5, cols 3-5
(1,2): rows 3-5, cols 6-8

(2,0): rows 6-8, cols 0-2
(2,1): rows 6-8, cols 3-5
(2,2): rows 6-8, cols 6-8

No overlap between regions!
```

**With identity-like filter:**
```
F = [
  [0, 0, 0],
  [0, 1, 0],
  [0, 0, 0]
]

This extracts center values:

Output = [
  [11, 14, 17],  (centers of top 3 blocks)
  [38, 41, 44],  (centers of middle 3 blocks)
  [65, 68, 71]   (centers of bottom 3 blocks)
]
```

**Key Observation:** Stride=3 with filter=3×3 creates **non-overlapping** blocks, similar to pooling but with learnable parameters.

### Example 4: Multi-Channel with Stride

**Given:**
- Input: 6×6×2 (two channels)
- Filter: 3×3×2
- Stride: 2
- Padding: 0
- Number of filters: 2

**Input Channel 0:**
```
I[:,:,0] = [
  [1,  2,  3,  4,  5,  6],
  [7,  8,  9, 10, 11, 12],
  [13,14, 15, 16, 17, 18],
  [19,20, 21, 22, 23, 24],
  [25,26, 27, 28, 29, 30],
  [31,32, 33, 34, 35, 36]
]
```

**Input Channel 1:**
```
I[:,:,1] = [
  [2,  3,  4,  5,  6,  7],
  [8,  9, 10, 11, 12, 13],
  [14,15, 16, 17, 18, 19],
  [20,21, 22, 23, 24, 25],
  [26,27, 28, 29, 30, 31],
  [32,33, 34, 35, 36, 37]
]
```

**Filter 1:**
```
F[:,:,0,0] = [        F[:,:,1,0] = [
  [1,  0, -1],          [0.5, 0, -0.5],
  [1,  0, -1],          [0.5, 0, -0.5],
  [1,  0, -1]           [0.5, 0, -0.5]
]                     ]

bias[0] = 0
```

**Filter 2:**
```
F[:,:,0,1] = [        F[:,:,1,1] = [
  [1, 1, 1],            [1, 1, 1],
  [0, 0, 0],            [0, 0, 0],
  [-1,-1,-1]            [-1,-1,-1]
]                     ]

bias[1] = 1
```

**Output Size:**
```
H_out = floor((6 - 3) / 2) + 1 = 2
W_out = floor((6 - 3) / 2) + 1 = 2
C_out = 2

Output shape: (2, 2, 2)
```

**Calculate Output[0,0,0] (Filter 1, position 0,0):**

**From Channel 0:**
```
Region (rows 0-2, cols 0-2):
[1,  2,  3]
[7,  8,  9]
[13,14, 15]

With Filter[:,:,0,0]:
[1,  0, -1]
[1,  0, -1]
[1,  0, -1]

Sum_ch0 = (1×1) + (2×0) + (3×-1) +
          (7×1) + (8×0) + (9×-1) +
          (13×1) + (14×0) + (15×-1)
        = 1 + 0 - 3 + 7 + 0 - 9 + 13 + 0 - 15
        = -6
```

**From Channel 1:**
```
Region (rows 0-2, cols 0-2):
[2,  3,  4]
[8,  9, 10]
[14,15, 16]

With Filter[:,:,1,0]:
[0.5,  0, -0.5]
[0.5,  0, -0.5]
[0.5,  0, -0.5]

Sum_ch1 = (2×0.5) + (3×0) + (4×-0.5) +
          (8×0.5) + (9×0) + (10×-0.5) +
          (14×0.5) + (15×0) + (16×-0.5)
        = 1 + 0 - 2 + 4 + 0 - 5 + 7 + 0 - 8
        = -3
```

**Total:**
```
Output[0,0,0] = Sum_ch0 + Sum_ch1 + bias[0]
              = -6 + (-3) + 0
              = -9
```

**Complete Forward Pass Visual:**

```
      Input (6×6×2)                    Output (2×2×2)
         
   Ch0        Ch1                  Ch0          Ch1
  ┌──────┐  ┌──────┐             ┌────┐      ┌────┐
  │ A  B │  │ A  B │             │ a b│      │ a b│
  │ A  B │  │ A  B │    ────►    │ c d│      │ c d│
  │ A  B │  │ A  B │             └────┘      └────┘
  │ C  D │  │ C  D │          Filter 1    Filter 2
  │ C  D │  │ C  D │
  │ C  D │  │ C  D │
  └──────┘  └──────┘
     ↓  ↓      ↓  ↓
  Stride=2 samples non-overlapping regions
  A, B, C, D each become one output value per filter
```

**Complete Output:**

For Filter 1 (edge detector):
```
Output[:,:,0] = [
  [-9,  -3],
  [-9,  -3]
]
```

For Filter 2 (horizontal edge):
```
Output[:,:,1] = [
  [-35, -35],
  [-35, -35]
]
```

---

## Forward Propagation with Stride

### Forward Complete Process

**Plain English Explanation:**

Forward propagation with stride is nearly identical to standard convolution, with one critical difference: the filter doesn't visit every possible position. Instead, it "hops" by s positions each time, where s is the stride. This skipping of positions is what causes the output to be smaller and computation to be faster.

**Think of it like this:** If stride=1 is like reading every word in a book, stride=2 is like reading every other word, and stride=3 is like reading every third word. You cover the book faster but get less detailed information.

### Forward Algorithm

```
Algorithm: Strided Convolution Forward Pass

Input:
  - I: Input (H_in × W_in × C_in)
  - F: Filters (f × f × C_in × C_out)
  - b: Biases (C_out,)
  - s: Stride
  - p: Padding

Steps:

1. Add padding if p > 0:
   I_padded = pad(I, p)
   Size: (H_in + 2p) × (W_in + 2p) × C_in

2. Calculate output dimensions:
   H_out = floor((H_in + 2p - f) / s) + 1
   W_out = floor((W_in + 2p - f) / s) + 1

3. Initialize output:
   Output = zeros(H_out, W_out, C_out)

4. For each filter k from 0 to (C_out - 1):
   For each output row i from 0 to (H_out - 1):
      For each output column j from 0 to (W_out - 1):
         
         # KEY DIFFERENCE: Multiply by stride!
         row_start = i × s  # ← Stride applied here
         col_start = j × s  # ← Stride applied here
         row_end = row_start + f
         col_end = col_start + f
         
         # Extract region and convolve
         region = I_padded[row_start:row_end, col_start:col_end, :]
         
         sum = 0
         For each channel c:
            For each filter position (m,n):
               sum += region[m,n,c] × F[m,n,c,k]
         
         Output[i,j,k] = sum + b[k]

5. Apply activation:
   Output = activation(Output)

6. Return Output
```

**Critical Observation:** The only difference from stride=1 is the multiplication `i×s` and `j×s` when calculating `row_start` and `col_start`. This causes us to skip positions in the input.

### Forward Numerical Example

**Detailed walkthrough with stride=2:**

**Input (8×8):**
```
I = [
  [ 1,  2,  3,  4,  5,  6,  7,  8],
  [ 9, 10, 11, 12, 13, 14, 15, 16],
  [17, 18, 19, 20, 21, 22, 23, 24],
  [25, 26, 27, 28, 29, 30, 31, 32],
  [33, 34, 35, 36, 37, 38, 39, 40],
  [41, 42, 43, 44, 45, 46, 47, 48],
  [49, 50, 51, 52, 53, 54, 55, 56],
  [57, 58, 59, 60, 61, 62, 63, 64]
]
```

**Filter (3×3):**
```
F = [
  [0, 0, 0],
  [0, 1, 0],
  [0, 0, 0]
]
```
(Center pixel extractor)

**Stride = 2, Padding = 0**

**Output Size:**
```
H_out = floor((8 - 3) / 2) + 1 = 3
Output: 3×3
```

**Position Mapping:**

```
Output Position → Input Starting Position

i=0, j=0 → row = 0×2 = 0, col = 0×2 = 0
i=0, j=1 → row = 0×2 = 0, col = 1×2 = 2
i=0, j=2 → row = 0×2 = 0, col = 2×2 = 4

i=1, j=0 → row = 1×2 = 2, col = 0×2 = 0
i=1, j=1 → row = 1×2 = 2, col = 1×2 = 2
i=1, j=2 → row = 1×2 = 2, col = 2×2 = 4

i=2, j=0 → row = 2×2 = 4, col = 0×2 = 0
i=2, j=1 → row = 2×2 = 4, col = 1×2 = 2
i=2, j=2 → row = 2×2 = 4, col = 2×2 = 4
```

**Visual Illustration:**

```
Filter positions (stride=2):

(0,0): Start at input[0,0]
┌─────────────────────────────┐
│ ┏━━━━━━━┓                   │
│ ┃ 1  2  3┃ 4  5  6  7  8    │
│ ┃ 9 10 11┃12 13 14 15 16    │
│ ┃17 18 19┃20 21 22 23 24    │
│ ┗━━━━━━━┛                   │
└─────────────────────────────┘
Extract center: 10
Output[0,0] = 10

(0,1): Start at input[0,2] (skipped column 1!)
┌─────────────────────────────┐
│  1  2 ┏━━━━━━━┓             │
│  9 10 ┃11 12 13┃14 15 16    │
│ 17 18 ┃19 20 21┃22 23 24    │
│       ┗━━━━━━━┛             │
└─────────────────────────────┘
Extract center: 20
Output[0,1] = 20

(0,2): Start at input[0,4]
┌─────────────────────────────┐
│  1  2  3  4 ┏━━━━━━━┓       │
│  9 10 11 12 ┃13 14 15┃16    │
│ 17 18 19 20 ┃21 22 23┃24    │
│             ┗━━━━━━━┛       │
└─────────────────────────────┘
Extract center: 22
Output[0,2] = 22

(1,0): Start at input[2,0] (skipped row 1!)
┌─────────────────────────────┐
│  1  2  3  4  5  6  7  8     │
│  9 10 11 12 13 14 15 16     │
│ ┏━━━━━━━┓                   │
│ ┃17 18 19┃20 21 22 23 24    │
│ ┃25 26 27┃28 29 30 31 32    │
│ ┃33 34 35┃36 37 38 39 40    │
│ ┗━━━━━━━┛                   │
└─────────────────────────────┘
Extract center: 26
Output[1,0] = 26

Continue this pattern...
```

**Complete Output (3×3):**
```
Output = [
  [10, 20, 22],
  [26, 28, 30],
  [42, 44, 46]
]
```

**Visual Summary:**

```
Stride=2 samples every other position:

Input (8×8):                     Output (3×3):
┌─────────────────┐             ┌─────────┐
│ X · X · X · · · │             │ O O O  │
│ · · · · · · · · │             │ O O O  │
│ X · X · X · · · │   ────►     │ O O O  │
│ · · · · · · · · │             └─────────┘
│ X · X · X · · · │             
│ · · · · · · · · │             Each O comes from
│ · · · · · · · · │             one X region
│ · · · · · · · · │
└─────────────────┘
X = Sampled (used)
· = Skipped (not used directly)
```

---

## Backward Propagation with Stride

### Backward Plain English

During backward propagation with stride, gradients flow back through a "sparse" pattern. Not every input position receives gradients directly - only those positions that were actually used during the forward pass receive gradient updates.

**Analogy:** Imagine you're a teacher grading homework, and you only graded every other student's paper (stride=2). When giving feedback, only those students whose papers you graded will receive corrections. The other students don't get direct feedback (though they might learn indirectly from their neighbors).

**Key Insight:** With stride s:
- Each output position corresponds to s² input positions in the receptive field
- Input positions not in any receptive field receive no gradient (stay zero)
- Input positions in overlapping receptive fields accumulate gradients from multiple outputs (when stride < filter_size)

### Backward Gradient Computation

**Three Gradients (as always):**

1. **∂L/∂F**: Filter gradients (computed using strided positions)
2. **∂L/∂b**: Bias gradients (unchanged by stride)
3. **∂L/∂I**: Input gradients (sparse pattern)

**Algorithm:**

```
Algorithm: Strided Convolution Backward Pass

Input:
  - ∂L/∂Output: (H_out × W_out × C_out)
  - I: Input from forward pass
  - F: Filters from forward pass
  - s: Stride
  - p: Padding

Output:
  - ∂L/∂F: Filter gradients
  - ∂L/∂b: Bias gradients
  - ∂L/∂I: Input gradients

Steps:

1. Initialize gradients:
   ∂L/∂F = zeros(f, f, C_in, C_out)
   ∂L/∂b = zeros(C_out)
   ∂L/∂I_padded = zeros((H_in+2p) × (W_in+2p) × C_in)

2. For each filter k:
   For each output position (i,j):
      
      # Calculate input region (using stride!)
      row_start = i × s
      col_start = j × s
      
      # Get gradient from output
      grad_out = ∂L/∂Output[i,j,k]
      
      # Update bias gradient
      ∂L/∂b[k] += grad_out
      
      # Update filter gradients
      For each channel c:
         For each filter position (m,n):
            ∂L/∂F[m,n,c,k] += I_padded[row_start+m, col_start+n, c] × grad_out
      
      # Update input gradients
      For each channel c:
         For each filter position (m,n):
            ∂L/∂I_padded[row_start+m, col_start+n, c] += F[m,n,c,k] × grad_out

3. Remove padding from input gradients:
   ∂L/∂I = ∂L/∂I_padded[p:H_in+p, p:W_in+p, :]

4. Return ∂L/∂F, ∂L/∂b, ∂L/∂I
```

**Key Difference from Stride=1:**
The `row_start = i × s` and `col_start = j × s` cause gradients to be distributed to **sparse** positions in the input, not densely to all positions.

### Backward Detailed Example

**Given from Forward Pass:**
- Input: 6×6 (no padding for simplicity)
- Filter: 3×3
- Stride: 2
- Output: 2×2

**Input:**
```
I = [
  [1,  2,  3,  4,  5,  6],
  [7,  8,  9, 10, 11, 12],
  [13,14, 15, 16, 17, 18],
  [19,20, 21, 22, 23, 24],
  [25,26, 27, 28, 29, 30],
  [31,32, 33, 34, 35, 36]
]
```

**Filter:**
```
F = [
  [1, 2, 1],
  [0, 0, 0],
  [1, 2, 1]
]
```

**Suppose ∂L/∂Output (2×2):**
```
∂L/∂Output = [
  [0.5, 0.3],
  [0.2, 0.4]
]
```

**Step 1: Compute Filter Gradients**

**For ∂L/∂F[0,0] (top-left weight):**

This weight was used at 4 positions (2×2 output):

At Output[0,0] (input region starting at 0,0):
- Input value: I[0,0] = 1
- Output gradient: 0.5
- Contribution: 1 × 0.5 = 0.5

At Output[0,1] (input region starting at 0,2):
- Input value: I[0,2] = 3
- Output gradient: 0.3
- Contribution: 3 × 0.3 = 0.9

At Output[1,0] (input region starting at 2,0):
- Input value: I[2,0] = 13
- Output gradient: 0.2
- Contribution: 13 × 0.2 = 2.6

At Output[1,1] (input region starting at 2,2):
- Input value: I[2,2] = 15
- Output gradient: 0.4
- Contribution: 15 × 0.4 = 6.0

```
∂L/∂F[0,0] = 0.5 + 0.9 + 2.6 + 6.0 = 10.0
```

**Visual Illustration:**

```
F[0,0] used at these input positions:

Output(0,0)        Output(0,1)
row=0×2=0         row=0×2=0
col=0×2=0         col=1×2=2
┏━━━━━┓           ┏━━━━━┓
┃[1] 2  3          3[3] 4  5
┃ 7  8  9         10 11 12
┃13 14 15         15 16 17
┗━━━━━┛           ┗━━━━━┛

Output(1,0)        Output(1,1)
row=1×2=2         row=1×2=2
col=0×2=0         col=1×2=2
┏━━━━━┓           ┏━━━━━┓
┃[13]14 15        15[15]16 17
┃19 20 21         21 22 23
┃25 26 27         27 28 29
┗━━━━━┛           ┗━━━━━┛

Each [X] shows where F[0,0] was multiplied
```

**For ∂L/∂F[1,1] (center weight):**

At Output[0,0]:
- Input value: I[1,1] = 8
- Output gradient: 0.5
- Contribution: 8 × 0.5 = 4.0

At Output[0,1]:
- Input value: I[1,3] = 10
- Output gradient: 0.3
- Contribution: 10 × 0.3 = 3.0

At Output[1,0]:
- Input value: I[3,1] = 20
- Output gradient: 0.2
- Contribution: 20 × 0.2 = 4.0

At Output[1,1]:
- Input value: I[3,3] = 22
- Output gradient: 0.4
- Contribution: 22 × 0.4 = 8.8

```
∂L/∂F[1,1] = 4.0 + 3.0 + 4.0 + 8.8 = 19.8
```

**Step 2: Compute Bias Gradients**

```
∂L/∂b = sum(∂L/∂Output)
      = 0.5 + 0.3 + 0.2 + 0.4
      = 1.4
```

**Step 3: Compute Input Gradients**

**Initialize:**
```
∂L/∂I = zeros(6, 6)
```

**For Output[0,0] with gradient 0.5:**

Region was rows 0-2, cols 0-2. Distribute gradient:

```
∂L/∂I[0,0] += F[0,0] × 0.5 = 1 × 0.5 = 0.5
∂L/∂I[0,1] += F[0,1] × 0.5 = 2 × 0.5 = 1.0
∂L/∂I[0,2] += F[0,2] × 0.5 = 1 × 0.5 = 0.5
∂L/∂I[1,0] += F[1,0] × 0.5 = 0 × 0.5 = 0
∂L/∂I[1,1] += F[1,1] × 0.5 = 0 × 0.5 = 0
∂L/∂I[1,2] += F[1,2] × 0.5 = 0 × 0.5 = 0
∂L/∂I[2,0] += F[2,0] × 0.5 = 1 × 0.5 = 0.5
∂L/∂I[2,1] += F[2,1] × 0.5 = 2 × 0.5 = 1.0
∂L/∂I[2,2] += F[2,2] × 0.5 = 1 × 0.5 = 0.5
```

**For Output[0,1] with gradient 0.3:**

Region was rows 0-2, cols 2-4:

```
∂L/∂I[0,2] += F[0,0] × 0.3 = 1 × 0.3 = 0.3
∂L/∂I[0,3] += F[0,1] × 0.3 = 2 × 0.3 = 0.6
∂L/∂I[0,4] += F[0,2] × 0.3 = 1 × 0.3 = 0.3
∂L/∂I[1,2] += F[1,0] × 0.3 = 0 × 0.3 = 0
∂L/∂I[1,3] += F[1,1] × 0.3 = 0 × 0.3 = 0
∂L/∂I[1,4] += F[1,2] × 0.3 = 0 × 0.3 = 0
∂L/∂I[2,2] += F[2,0] × 0.3 = 1 × 0.3 = 0.3
∂L/∂I[2,3] += F[2,1] × 0.3 = 2 × 0.3 = 0.6
∂L/∂I[2,4] += F[2,2] × 0.3 = 1 × 0.3 = 0.3
```

**Continue for Output[1,0] and Output[1,1]...**

**Partial ∂L/∂I after processing Output[0,0] and Output[0,1]:**

```
∂L/∂I = [
  [0.5, 1.0, 0.8, 0.6, 0.3,  0],
  [0,   0,   0,   0,   0,    0],
  [0.5, 1.0, 0.8, 0.6, 0.3,  0],
  [0,   0,   0,   0,   0,    0],
  [0,   0,   0,   0,   0,    0],
  [0,   0,   0,   0,   0,    0]
]

Notice: Rows 1, 3, 4, 5 still zero (not processed yet)
Notice: Row 0 and 2 have gradients (from first row of output)
```

**After processing ALL output positions:**

```
Complete ∂L/∂I = [
  [0.5, 1.0, 0.8, 0.6, 0.3,  0  ],
  [0,   0,   0,   0,   0,    0  ],
  [0.7, 1.4, 1.1, 1.2, 0.7,  0  ],
  [0,   0,   0,   0,   0,    0  ],
  [0.2, 0.4, 0.5, 0.8, 0.4,  0  ],
  [0,   0,   0,   0,   0,    0  ]
]
```

**Visual Illustration of Sparse Gradients:**

```
Input Gradient Pattern (stride=2):

┌─────────────────────────┐
│ G  G  G  G  G  0        │  ← Has gradients
│ 0  0  0  0  0  0        │  ← No gradients (skipped)
│ G  G  G  G  G  0        │  ← Has gradients
│ 0  0  0  0  0  0        │  ← No gradients (skipped)
│ G  G  G  G  G  0        │  ← Has gradients
│ 0  0  0  0  0  0        │  ← No gradients (last row not used)
└─────────────────────────┘

G = Receives gradient
0 = Zero gradient (never sampled in forward pass)

Pattern: Alternating rows/cols receive gradients
```

**Comparison with Stride=1:**

```
Stride=1 Gradients:        Stride=2 Gradients:
┌─────────────────┐       ┌─────────────────┐
│ G  G  G  G  G  │       │ G  G  G  G  G  │
│ G  G  G  G  G  │       │ 0  0  0  0  0  │
│ G  G  G  G  G  │       │ G  G  G  G  G  │
│ G  G  G  G  G  │       │ 0  0  0  0  0  │
│ G  G  G  G  G  │       │ G  G  G  G  G  │
└─────────────────┘       └─────────────────┘
   Dense pattern           Sparse pattern
   All positions           Half positions
```

**Mathematical Formulation:**

**Input Gradient:**
```
∂L/∂I[i,j,c] = Σ Σ (F[m,n,c,k] × ∂L/∂Output[⌊(i-m)/s⌋, ⌊(j-n)/s⌋, k])
               m n k

Only when:
  - (i-m) is divisible by s
  - (j-n) is divisible by s
  - Output indices are within bounds
```

**Plain English:** An input position at (i,j) only receives gradient if it was actually used in the forward pass. This happens when its position aligns with the strided sampling pattern.

**Filter Gradient:**
```
∂L/∂F[m,n,c,k] = Σ   Σ   I[i×s + m, j×s + n, c] × ∂L/∂Output[i,j,k]
                 i=0 j=0
                to H_out-1, W_out-1
```

**Plain English:** Sum over all output positions, but remember each output position i,j corresponds to input position i×s, j×s due to stride.

### Backward Detailed Example

**Continuing previous example:**

**Given:**
- Input: 6×6
- Filter: 3×3 with values as shown
- Stride: 2
- Output: 2×2
- ∂L/∂Output as given above

**Compute ∂L/∂I[1,1]:**

Which output positions used Input[1,1]?

**Check Output[0,0]:**
- Region: rows 0-2, cols 0-2
- Does it include position (1,1)? YES
- Filter position for (1,1): (m=1, n=1)
- Filter weight: F[1,1] = 0
- Contribution: 0 × 0.5 = 0

```
∂L/∂I[1,1] from Output[0,0] = 0
```

**Check Output[0,1]:**
- Region: rows 0-2, cols 2-4
- Does it include position (1,1)? NO (starts at column 2)
- No contribution

**Check Output[1,0]:**
- Region: rows 2-4, cols 0-2
- Does it include position (1,1)? NO (starts at row 2)
- No contribution

**Check Output[1,1]:**
- Region: rows 2-4, cols 2-4
- Does it include position (1,1)? NO
- No contribution

```
Total: ∂L/∂I[1,1] = 0
```

**Compute ∂L/∂I[2,2]:**

**Check Output[0,0]:**
- Region: rows 0-2, cols 0-2
- Includes (2,2)? YES
- Filter position: (m=2, n=2)
- Filter weight: F[2,2] = 1
- Contribution: 1 × 0.5 = 0.5

**Check Output[0,1]:**
- Region: rows 0-2, cols 2-4
- Includes (2,2)? YES
- Filter position: (m=2, n=0)
- Filter weight: F[2,0] = 1
- Contribution: 1 × 0.3 = 0.3

**Check Output[1,0]:**
- Region: rows 2-4, cols 0-2
- Includes (2,2)? YES
- Filter position: (m=0, n=2)
- Filter weight: F[0,2] = 1
- Contribution: 1 × 0.2 = 0.2

**Check Output[1,1]:**
- Region: rows 2-4, cols 2-4
- Includes (2,2)? YES
- Filter position: (m=0, n=0)
- Filter weight: F[0,0] = 1
- Contribution: 1 × 0.4 = 0.4

```
Total: ∂L/∂I[2,2] = 0.5 + 0.3 + 0.2 + 0.4 = 1.4
```

**Complete ∂L/∂I Pattern:**

```
∂L/∂I (6×6) with stride=2:

┌─────────────────────────────────────┐
│ 0.5  1.0  0.8  0.6  0.3  0.3       │
│ 0    0    0    0    0    0         │
│ 0.7  1.4  1.4  1.2  0.6  0.3       │
│ 0    0    0    0    0    0         │
│ 0.2  0.4  0.6  0.8  0.4  0.4       │
│ 0.2  0.4  0.4  0.8  0.4  0.4       │
└─────────────────────────────────────┘

Pattern:
- Rows 0,2,4 have gradients (used in forward pass)
- Rows 1,3 are zero (skipped by stride)
- Similar pattern in columns
```

**Visual Summary:**

```
Forward Pass (Stride=2):      Backward Pass (Stride=2):
Which inputs used?            Where gradients go?

┌───────────────┐            ┌───────────────┐
│ X  X  X  ·  · │            │ G  G  G  ·  · │
│ X  X  X  ·  · │            │ 0  0  0  ·  · │
│ X  X  X  ·  · │            │ G  G  G  ·  · │
│ ·  ·  ·  X  X │            │ 0  0  0  G  G │
│ ·  ·  ·  X  X │            │ ·  ·  ·  G  G │
│ ·  ·  ·  X  X │            │ ·  ·  ·  G  G │
└───────────────┘            └───────────────┘
X = Used in forward          G = Gets gradient
· = Not shown                0 = Zero gradient
                            
Gradient pattern matches usage pattern!
```

---

## Stride vs Pooling

### Comparison

Both stride and pooling reduce spatial dimensions, but they work differently:

**Strided Convolution:**
- Learnable parameters (filter weights)
- Network learns how to downsample
- More flexible
- Part of the convolution operation

**Pooling (Max/Average):**
- No learnable parameters
- Fixed operation (max or average)
- Computationally simpler
- Separate operation after convolution

**Visual Comparison:**

```
Same Input Region (4×4):
┌─────────────┐
│  1   5   2  │
│  3   9   1  │
│  4   2   8  │
│  7   3   6  │
└─────────────┘

Max Pooling (2×2, stride=2):
┌────────┐
│ 9   8 │  Takes maximum from each 2×2 block
│ 7   8 │  Fixed operation, no learning
└────────┘

Strided Conv (2×2 filter, stride=2):
With learned filter [w₁ w₂]
                    [w₃ w₄]
┌────────┐
│ a   b │  a = w₁×1 + w₂×5 + w₃×3 + w₄×9
│ c   d │  Network learned optimal combination!
└────────┘
```

**Performance Comparison:**

```
┌─────────────────┬──────────────┬────────────────┐
│                 │ Strided Conv │ Max Pooling    │
├─────────────────┼──────────────┼────────────────┤
│ Parameters      │ Has weights  │ None           │
│ Learning        │ Yes          │ No             │
│ Computation     │ More         │ Less           │
│ Flexibility     │ High         │ Low            │
│ Information     │ Learned      │ Fixed (max)    │
│ Gradient Flow   │ Smooth       │ Selective      │
│ Common Use      │ Modern CNNs  │ Classic CNNs   │
└─────────────────┴──────────────┴────────────────┘
```

### When to Use Each

**Use Strided Convolution When:**

1. **Building modern architectures**
   - ResNet variants
   - DenseNet
   - EfficientNet
   - Most new architectures

2. **Want learnable downsampling**
   - Network can optimize how to reduce resolution
   - Better gradient flow than pooling

3. **Need flexibility**
   - Can combine downsampling with feature extraction
   - One operation instead of two

4. **Semantic segmentation / dense prediction**
   - Better for tasks requiring precise spatial information

**Example:**
```
# Modern approach
Conv 3×3, stride=2, pad=1  →  Learnable downsampling
```

**Use Pooling When:**

1. **Want translation invariance**
   - Max pooling provides some robustness to small shifts
   - Good for classification

2. **Reduce overfitting**
   - No parameters to overfit
   - Can be regularizing

3. **Simplicity**
   - Less computation
   - Easier to understand and debug

4. **Following classic architectures**
   - VGG
   - AlexNet
   - Original architectures

**Example:**
```
# Classic approach
Conv 3×3, stride=1, pad=1  →  Extract features
MaxPool 2×2, stride=2      →  Downsample
```

### Combining Both

**Hybrid Approach:**

Many architectures use both:

```
Block Pattern:
┌────────────────────────────────┐
│ Conv 3×3, stride=1, pad=1      │  Extract features
│ BatchNorm                      │
│ ReLU                           │
│ Conv 3×3, stride=1, pad=1      │  More features
│ BatchNorm                      │
│ ReLU                           │
│ Conv 3×3, stride=2, pad=1      │  Downsample
└────────────────────────────────┘
   OR
┌────────────────────────────────┐
│ Conv 3×3, stride=1, pad=1      │  Extract features
│ BatchNorm                      │
│ ReLU                           │
│ MaxPool 2×2, stride=2          │  Downsample
└────────────────────────────────┘
```

**Modern Trend:**
- Replace pooling with strided convolutions
- Better gradient flow
- More learnable parameters
- Improved performance on many tasks

---

## Practical Guidelines

### Choosing Stride Values

**Decision Tree:**

```
Do you need to downsample?
    │
    ├─ No  → Use stride=1 (standard)
    │
    └─ Yes → How much?
           │
           ├─ By 2× → Use stride=2 (most common)
           │           With padding to control output size
           │
           ├─ By 4× → Either:
           │           - stride=4 (aggressive, rare)
           │           - Two stride=2 layers (preferred)
           │           - stride=2 + pooling
           │
           └─ More → Multiple stride=2 layers
                     OR stride=2 + pooling combinations
```

**Guidelines by Layer Depth:**

```
First Layer (Input → First Features):
├─ Small images (32×32): stride=1
├─ Medium images (224×224): stride=2
└─ Large images (512×512): stride=2 or 4

Middle Layers (Feature Extraction):
├─ Default: stride=1 (preserve resolution)
├─ Transition blocks: stride=2 (downsample)
└─ Very deep networks: alternate stride=1 and stride=2

Final Layers (Before Classification):
├─ Aggressive downsampling
└─ stride=2 or global pooling
```

### Common Architectures

**1. ResNet Pattern:**

```
Residual Block with Downsampling:

Input: 56×56×64
    ↓
Conv 1×1, 64 filters, stride=2, pad=0  → 28×28×64
    ↓
Conv 3×3, 64 filters, stride=1, pad=1  → 28×28×64
    ↓
Conv 1×1, 256 filters, stride=1, pad=0 → 28×28×256
    ↓
Output: 28×28×256

Skip connection also uses stride=2:
Input: 56×56×64
Conv 1×1, 256 filters, stride=2, pad=0 → 28×28×256
```

**2. VGG Pattern (Classic):**

```
Block Structure:
Conv 3×3, stride=1, pad=1  → Same size
Conv 3×3, stride=1, pad=1  → Same size
MaxPool 2×2, stride=2      → Halve size

Example:
224×224×3
    ↓ Conv+Conv+Pool
112×112×64
    ↓ Conv+Conv+Pool
56×56×128
    ↓ Conv+Conv+Conv+Pool
28×28×256
```

**3. Inception/GoogLeNet Pattern:**

```
Parallel paths with different strides:

Input
├─ Conv 1×1, stride=1 ─────┐
├─ Conv 3×3, stride=1 ─────┤
├─ Conv 5×5, stride=1 ─────┤→ Concatenate
└─ MaxPool 3×3, stride=1 ──┘

Then stride=2 for downsampling in reduction blocks
```

**4. MobileNet Pattern:**

```
Depthwise Separable Conv with Stride:

Input: 112×112×32
    ↓
Depthwise Conv 3×3, stride=2, pad=1 → 56×56×32
    ↓
Pointwise Conv 1×1, stride=1 → 56×56×64

Efficient downsampling with fewer parameters
```

### Best Practices

**1. Stride Selection Rules:**

```
✓ DO:
- Use stride=1 for most layers
- Use stride=2 for downsampling
- Keep stride ≤ filter_size
- Use padding with stride to control output size

❌ DON'T:
- Use stride > 3 (too aggressive)
- Use stride > filter_size (misses information)
- Forget to adjust padding when changing stride
- Use different strides in width and height (usually)
```

**2. Output Size Planning:**

```
To achieve target output size:

Given: Input_size, Target_output_size, Filter_size

Calculate stride:
s = (Input_size - Filter_size) / (Target_output_size - 1)

Then adjust padding:
p = (s × (Target_output_size - 1) + Filter_size - Input_size) / 2

Example:
Input: 224, Target: 112, Filter: 3
s = (224 - 3) / (112 - 1) = 221/111 ≈ 2
Use s=2, then:
p = (2×111 + 3 - 224) / 2 = 1
Result: stride=2, padding=1
```

**3. Gradient Flow Considerations:**

```
Stride=1:  Dense gradients    → Better gradient flow
Stride=2:  Sparse gradients   → Good balance
Stride≥3:  Very sparse        → Risk vanishing gradients

For very deep networks:
- Prefer multiple stride=2 over single stride=4
- Allows better gradient propagation
```

**4. Computational Efficiency:**

```
Reduction in Computations:

Input: 224×224, Filter: 3×3

Stride=1: 222×222 = 49,284 positions
Stride=2: 111×111 = 12,321 positions (4× faster)
Stride=4: 56×56 = 3,136 positions (16× faster)

But: Aggressive stride might hurt accuracy
Balance speed vs accuracy based on your needs
```

**5. Common Configurations:**

```
Standard Conv Block:
┌─────────────────────────────────┐
│ Conv 3×3, stride=1, pad=1       │
│ BatchNorm                       │
│ ReLU                            │
│                                 │
│ Repeat 2-3 times                │
│                                 │
│ Conv 3×3, stride=2, pad=1       │  ← Downsample here
│ BatchNorm                       │
│ ReLU                            │
└─────────────────────────────────┘

Output: Half the spatial size, ready for next block
```

**6. Debugging Tips:**

```python
# Pseudocode - Verify output size

def verify_output_size(input_size, filter_size, stride, padding):
    expected = (input_size + 2*padding - filter_size) // stride + 1
    
    # Test cases:
    assert verify_output_size(32, 3, 1, 1) == 32  # Same padding
    assert verify_output_size(32, 3, 2, 1) == 16  # Halving
    assert verify_output_size(224, 7, 2, 3) == 112 # Common first layer
    
    print(f"Input {input_size}×{input_size}")
    print(f"Filter {filter_size}×{filter_size}")
    print(f"Stride {stride}, Padding {padding}")
    print(f"Output {expected}×{expected}")
```

**7. Memory and Speed Trade-offs:**

```
Configuration      Memory    Speed    Accuracy
stride=1, pad=1    High      Slow     Best
stride=2, pad=1    Medium    Medium   Good
stride=2, no pad   Lower     Fast     Okay
stride=4          Lowest    Fastest   Risk

Choose based on constraints:
- Limited memory? → Higher stride
- Need accuracy? → Lower stride
- Real-time inference? → Higher stride
```

**8. Architecture-Specific Recommendations:**

```
Task: Image Classification
├─ Input 224×224
├─ Conv 7×7, stride=2, pad=3 → 112×112 (fast start)
├─ MaxPool 3×3, stride=2 → 56×56
├─ Conv blocks with stride=1 (features)
└─ Periodic stride=2 for downsampling

Task: Object Detection
├─ Need multiple scales
├─ Use Feature Pyramid Network (FPN)
├─ Minimal stride in backbone
└─ Upsample/downsample as needed

Task: Semantic Segmentation
├─ Preserve resolution as much as possible
├─ Use stride=1 in encoder
├─ Use transposed convolutions in decoder
└─ Skip connections to recover details
```

**9. Testing Checklist:**

```
✓ Verify output dimensions match expected:
  (input + 2×pad - filter) / stride + 1

✓ Check filter can fit with your stride:
  Last valid position should be within input

✓ Test edge cases:
  - Minimum input size
  - Maximum stride
  - Padding combinations

✓ Profile performance:
  - Memory usage
  - Forward pass time
  - Backward pass time

✓ Validate gradients:
  - Use numerical gradient checking
  - Ensure gradients flow properly
```

**10. Common Pitfalls:**

```
❌ Mistake 1: Incompatible dimensions
Input: 10, Filter: 3, Stride: 3, Pad: 0
Output = floor((10 - 3) / 3) + 1 = 3
But last filter position: 2×3 = 6
Covers rows 6-8, total 9 positions
Missing last row! → Use padding or adjust stride

✓ Fix: Add padding=1 → output = floor((12-3)/3)+1 = 4

❌ Mistake 2: Forgetting to scale stride with input size
Using stride=2 on 28×28 gives 13×13
Using stride=2 on 224×224 gives 111×111
Very different reductions! Plan accordingly.

✓ Fix: Consider input size when choosing stride

❌ Mistake 3: Too aggressive stride
Using stride=5 on 50×50 input
Output = floor((50-3)/5)+1 = 10×10
Might lose important information!

✓ Fix: Use multiple stride=2 layers:
50 → 25 → 13 (more gradual, better learning)
```

---

**Summary: Key Formulas**

```
Output Size:
H_out = floor((H_in + 2p - f) / s) + 1

Convolution with Stride:
Output[i,j,k] = Σ Σ Σ Input[i×s+m, j×s+n, c] × Filter[m,n,c,k] + bias[k]
               m n c

Gradient with respect to Input:
∂L/∂I[i,j,c] = Σ (Filter[m,n,c,k] × ∂L/∂Output[⌊(i-m)/s⌋, ⌊(j-n)/s⌋, k])
               m,n,k
Where (i-m) and (j-n) are divisible by s

Gradient with respect to Filter:
∂L/∂F[m,n,c,k] = Σ Σ Input[i×s+m, j×s+n, c] × ∂L/∂Output[i,j,k]
                 i j

Default Recommendation:
- Stride=1 for feature extraction layers
- Stride=2 for downsampling layers
- Padding=1 with 3×3 filters for same padding
```

---

**End of Strided Convolution Tutorial**

This completes the comprehensive guide to strided convolution in CNNs. The document covers:

1. **What stride is** and why it's essential for CNNs
2. **How it works** with detailed visual comparisons
3. **Mathematical foundations** with complete formulas
4. **Multiple examples** with different stride values and configurations
5. **Forward propagation** process with stride
6. **Backward propagation** including sparse gradient patterns
7. **Comparison with pooling** and when to use each
8. **Practical guidelines** for real-world applications

**Key Takeaways:**
- **Stride controls downsampling**: Larger stride = smaller output
- **Stride=2 is most common**: Good balance of efficiency and information
- **Sparse gradients**: Not all input positions receive gradients with stride>1
- **Modern trend**: Replace pooling with strided convolutions
- **Default choice**: stride=1 for features, stride=2 for downsampling

Remember: When in doubt, use **stride=1 with padding=1 for 3×3 filters** in feature extraction layers, and **stride=2 with padding=1** for downsampling layers!