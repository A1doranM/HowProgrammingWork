# YOLO Algorithm (You Only Look Once)

## Table of Contents

1. [Introduction to YOLO](#introduction-to-yolo)
2. [How YOLO Works - The Core Idea](#how-yolo-works---the-core-idea)
3. [YOLO Output Encoding](#yolo-output-encoding)
4. [YOLO Loss Function](#yolo-loss-function)
5. [Detailed Numerical Examples](#detailed-numerical-examples)
6. [Forward Propagation in YOLO](#forward-propagation-in-yolo)
7. [Backward Propagation in YOLO](#backward-propagation-in-yolo)

---

## Introduction to YOLO

### What is YOLO?

**Plain English Overview:**

YOLO (You Only Look Once) is a revolutionary object detection algorithm that treats detection as a single regression problem. Unlike traditional methods that involve multiple stages (region proposals, classification, refinement), YOLO processes the entire image in one forward pass through a single convolutional neural network and directly predicts bounding boxes and class probabilities.

**Analogy:** Imagine you're looking at a crowded photograph trying to find all the people:

- **Traditional approach** (two-stage detectors like R-CNN): First, scan the image to find regions that might contain objects (proposal stage), then examine each region carefully to classify it (classification stage). Like using a magnifying glass to find interesting spots, then examining each spot in detail.

- **YOLO approach** (single-stage): Look at the entire image once and immediately identify all objects, their locations, and their classes simultaneously. Like having trained eyes that can spot everything in one glance.

**Key Concept:** "You Only Look Once" - the name captures the essence. One forward pass through the network gives you all detections. No region proposals, no multiple stages, just one look.

### Why YOLO is Revolutionary

**Single-Stage vs Two-Stage Detection:**

```
TWO-STAGE DETECTOR (e.g., Faster R-CNN):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Stage 1: Region Proposal
Image â†’ CNN â†’ Feature Map â†’ RPN â†’ ~2000 region proposals
Time: T1

Stage 2: Classification
For each of ~2000 proposals:
  Crop region â†’ ROI Pooling â†’ FC layers â†’ Classification + Box refinement
Time: T2 (expensive! ~2000 operations)

Total Time: T1 + T2 (typically 100-500ms per image)
Advantages: Very accurate
Disadvantages: Slow, complex pipeline


SINGLE-STAGE DETECTOR (YOLO):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Single Stage: Direct Detection
Image â†’ CNN â†’ Feature Map â†’ Detection Layer â†’ All predictions
Time: T (typically 20-40ms per image)

Advantages: Fast (real-time capable!), simple pipeline
Disadvantages: Slightly less accurate than two-stage (but improving)

Speedup: 5-25Ã— faster than two-stage methods!
```

**YOLO's Key Innovations:**

1. **Unified Architecture:** Single network does everything
2. **Global Reasoning:** Sees entire image, understands context
3. **Grid-Based:** Divides image into grid, each cell predicts
4. **Direct Prediction:** No region proposals needed
5. **Real-Time Speed:** Can process 30-60 frames per second

**Visual Comparison:**

```
Faster R-CNN (Two-Stage):

Image
  â†“
CNN Backbone
  â†“
Feature Map
  â†“
Region Proposal Network
  â†“
~2000 Proposals
  â†“
For each proposal:
  â”œâ”€ ROI Pooling
  â”œâ”€ Classification
  â””â”€ Box Refinement
  â†“
Final Detections
  â†“
NMS

Total: Many steps, slow


YOLO (Single-Stage):

Image
  â†“
CNN (one pass!)
  â†“
Predictions (all at once!)
  â†“
Final Detections
  â†“
NMS

Total: Fewer steps, fast!
```

### Connection to Previous Topics

**From Sliding Windows:**

YOLO is essentially an efficient implementation of sliding windows with a grid structure. Instead of literally sliding windows and running the network multiple times, YOLO:
- Divides the image into a fixed grid (e.g., 7Ã—7)
- Runs the network once
- Each grid cell makes predictions

This is the convolutional implementation we learned about!

**From IoU:**

YOLO uses IoU extensively:
- Training: Match ground truth boxes to grid cells
- Evaluation: Determine if prediction is correct
- NMS: Remove duplicate detections

**From NMS:**

YOLO produces many predictions (grid_cells Ã— anchors_per_cell). NMS removes duplicates to get clean final detections.

**From Anchor Boxes:**

YOLOv2 and later use anchor boxes to predict multiple objects per grid cell. YOLOv1 used a simpler approach with fixed predictions per cell.

---

## How YOLO Works - The Core Idea

### Grid-Based Detection Approach

**Plain English Explanation:**

YOLO divides the input image into an SÃ—S grid (commonly 7Ã—7, 13Ã—13, or 19Ã—19 depending on version). Each grid cell is "responsible" for detecting objects whose center falls within that cell. The cell predicts bounding boxes and class probabilities for objects it's responsible for.

**Think of it like this:** Imagine dividing a city map into a 10Ã—10 grid of neighborhoods. Each neighborhood's police station is responsible for crimes that happen in their area (determined by where the crime's center point is). When a crime is reported, the responsible station handles it. Similarly, each YOLO grid cell handles objects whose centers fall in that cell.

**Visual Illustration:**

```
Original Image:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                     â”‚
â”‚         ğŸš—                          â”‚
â”‚        Car 1                        â”‚
â”‚                                     â”‚
â”‚                    ğŸš—               â”‚
â”‚                   Car 2             â”‚
â”‚                                     â”‚
â”‚   ğŸ§                               â”‚
â”‚  Person                             â”‚
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

YOLO Grid Division (7Ã—7):
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚ X â”‚   â”‚   â”‚   â”‚   â”‚  X = Car 1 center
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  â†’ Cell (1,2) responsible
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚ Y â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  Y = Car 2 center
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  â†’ Cell (2,5) responsible
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  Z = Person center
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  â†’ Cell (5,0) responsible
â”‚ Z â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Grid cell (1,2) predicts: "Car at (x,y) with confidence 0.95"
Grid cell (2,5) predicts: "Car at (x,y) with confidence 0.88"
Grid cell (5,0) predicts: "Person at (x,y) with confidence 0.92"
Other cells predict: "No object" (low confidence)
```

### Single Forward Pass Philosophy

**The YOLO Principle:**

```
Input Image (448Ã—448Ã—3)
         â†“
    Single CNN
    (24 conv layers)
         â†“
Feature Map (7Ã—7Ã—1024)
         â†“
  Detection Layers
         â†“
Predictions (7Ã—7Ã—30)
    â†“    â†“    â†“
  Cell  Cell  Cell
  (0,0) (0,1) (0,2) ... all 49 cells make predictions

ALL predictions computed in ONE forward pass!
```

**What Each Grid Cell Predicts:**

For YOLOv1 with B=2 bounding boxes and C=20 classes:

```
Each grid cell (i,j) predicts:

Box 1:
- x, y: Center coordinates (relative to cell)
- w, h: Width and height (relative to image)
- confidence: P(object) Ã— IoU(pred, truth)

Box 2:
- x, y: Center coordinates
- w, h: Width and height
- confidence: P(object) Ã— IoU(pred, truth)

Class probabilities:
- P(class_1 | object)
- P(class_2 | object)
- up to P(class_20 | object)

Total per cell: 2 boxes Ã— 5 values + 20 classes = 30 values
For 7Ã—7 grid: 7Ã—7Ã—30 = 1,470 values total
```

### Output Tensor Structure

**Visual Breakdown:**

```
YOLO Output Tensor: 7Ã—7Ã—30

Dimensions:
- 7Ã—7: Grid cells (spatial)
- 30: Predictions per cell

Breaking down the 30 values per cell:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Box 1 (5 values):                                       â”‚
â”‚ â”œâ”€ x: 0.0 to 1.0 (relative to cell)                    â”‚
â”‚ â”œâ”€ y: 0.0 to 1.0 (relative to cell)                    â”‚
â”‚ â”œâ”€ w: 0.0 to 1.0+ (relative to image width)            â”‚
â”‚ â”œâ”€ h: 0.0 to 1.0+ (relative to image height)           â”‚
â”‚ â””â”€ confidence: 0.0 to 1.0                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Box 2 (5 values):                                       â”‚
â”‚ â”œâ”€ x, y, w, h: Same format as Box 1                    â”‚
â”‚ â””â”€ confidence                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Class Probabilities (20 values):                        â”‚
â”‚ â””â”€ P(class_i | object) for i=1 to 20                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example values for cell (3,4):
[0.45, 0.67, 0.32, 0.28, 0.85,  â† Box 1: x,y,w,h,conf
 0.52, 0.61, 0.35, 0.30, 0.12,  â† Box 2: x,y,w,h,conf
 0.05, 0.02, 0.87, 0.01, ...]  â† Class probs (class 3 has 0.87)
```

### Plain English Process

**Complete YOLO Detection Process:**

**Step 1: Divide and Conquer**
The image is divided into a 7Ã—7 grid. Each cell is a "zone of responsibility."

**Step 2: Make Predictions**
Each cell predicts:
- "Do I have an object?" (confidence)
- "Where is it?" (bounding box coordinates)
- "What is it?" (class probabilities)

**Step 3: Compute Final Boxes**
Convert relative coordinates to absolute pixel coordinates.

**Step 4: Filter**
Keep only predictions with confidence above threshold (e.g., 0.3).

**Step 5: Remove Duplicates**
Apply NMS to remove multiple detections of the same object.

**Real-World Example:**

```
Photo of street scene:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸš—      ğŸ§   ğŸš—        ğŸš²             â”‚
â”‚  Car1   Person Car2    Bicycle         â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

YOLO divides into 7Ã—7 grid:
Each cell that contains an object center makes predictions

Cell (1,1): "Car, 95% confident, box at (x,y,w,h)"
Cell (2,3): "Person, 92% confident, box at (x,y,w,h)"
Cell (2,5): "Car, 88% confident, box at (x,y,w,h)"
Cell (3,6): "Bicycle, 85% confident, box at (x,y,w,h)"
Other cells: "No object" or low confidence

Result: 4 detections, one per actual object
```

---

## YOLO Output Encoding

### What the Network Predicts

**At Each Grid Cell:**

The network outputs a fixed-size vector encoding multiple predictions. For YOLOv1 (S=7, B=2, C=20):

```
Total output: 7Ã—7Ã—30

For grid cell at position (i, j):
Output[i,j,:] contains 30 values organized as:

Positions 0-4:   Box 1 [bx, by, bw, bh, conf]
Positions 5-9:   Box 2 [bx, by, bw, bh, conf]
Positions 10-29: Class probabilities [P(c1), P(c2), ..., P(c20)]
```

**Symbol Legend:**
- `S`: Grid size (7 for YOLOv1)
- `B`: Number of boxes per cell (2 for YOLOv1)
- `C`: Number of classes (20 for PASCAL VOC)
- `i, j`: Grid cell position
- `bx, by`: Box center coordinates (0 to 1, relative to cell)
- `bw, bh`: Box width and height (0 to 1, relative to image)
- `conf`: Confidence score (0 to 1)

### Bounding Box Encoding

**Coordinate System:**

```
Grid Cell (i,j) has normalized coordinates:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                        â”‚
â”‚  Cell (i,j):                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚          â”‚  Top-left: (i/S, j/S)    â”‚
â”‚  â”‚    Â·     â”‚  Where S = grid size     â”‚
â”‚  â”‚  (bx,by) â”‚                          â”‚
â”‚  â”‚          â”‚  bx, by: 0 to 1          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (relative to this cell) â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Absolute center coordinates:
center_x = (j + bx) / S
center_y = (i + by) / S

Where:
- j is column index (0 to S-1)
- i is row index (0 to S-1)
- bx, by are predicted offsets (0 to 1)
- S is grid size (7)
```

**Example:**

```
Prediction from cell (2, 3):
bx = 0.6
by = 0.4
bw = 0.3
bh = 0.2

Absolute coordinates:
center_x = (3 + 0.6) / 7 = 3.6 / 7 = 0.514 (51.4% across image)
center_y = (2 + 0.4) / 7 = 2.4 / 7 = 0.343 (34.3% down image)
width = 0.3 (30% of image width)
height = 0.2 (20% of image height)

For 448Ã—448 image:
center_x_pixels = 0.514 Ã— 448 = 230
center_y_pixels = 0.343 Ã— 448 = 154
width_pixels = 0.3 Ã— 448 = 134
height_pixels = 0.2 Ã— 448 = 90

Bounding box in pixels:
x_min = 230 - 134/2 = 163
y_min = 154 - 90/2 = 109
x_max = 230 + 134/2 = 297
y_max = 154 + 90/2 = 199
```

### Confidence Score Meaning

**Mathematical Definition:**

```
Confidence = P(Object) Ã— IoU(prediction, ground_truth)
```

**What This Means:**

- `P(Object)`: Probability that the cell contains an object
- `IoU`: How well the predicted box matches the actual object

During **training**:
- If object present: confidence_target = IoU with ground truth
- If no object: confidence_target = 0

During **inference** (testing):
- Network predicts confidence directly
- High confidence means: "I'm confident there's an object AND my box is accurate"

**Example:**

```
Cell (3,4) predictions:

Box 1: confidence = 0.85
Interpretation: "85% confident there's an object here and my box is accurate"

Box 2: confidence = 0.12
Interpretation: "Only 12% confident, probably no object or bad box"

We would keep Box 1 (threshold typically 0.3) and discard Box 2
```

### Class Probabilities

**Encoding:**

```
Class probabilities are CONDITIONAL:
P(class_i | object) = Probability of class i GIVEN that an object exists

NOT: P(class_i) = Probability that class i is present

This is important! The probabilities are class distribution
assuming an object is present.
```

**Example:**

```
Cell (2,5) class probabilities:
[0.05, 0.02, 0.88, 0.01, 0.01, 0.01, 0.01, 0.01, ...]
 \_/   \_/   \_/                        20 values total
 car  bike  person

Interpretation:
"IF there's an object in this cell, then:
 - 88% chance it's a person
 - 5% chance it's a car
 - 2% chance it's a bike
 - very low chance of other classes"

Combined with confidence:
Final class confidence = Box_confidence Ã— Class_probability

If box confidence = 0.9 and P(person|object) = 0.88:
Final person confidence = 0.9 Ã— 0.88 = 0.792

This is the final score used for NMS and filtering
```

### Complete Output Formulation

**Mathematical Structure:**

```
YOLO Output: Tensor of shape (S, S, BÃ—5 + C)

For YOLOv1: (7, 7, 2Ã—5 + 20) = (7, 7, 30)

At grid cell (i,j):
Output[i,j, 0:5] = [bxâ‚, byâ‚, bwâ‚, bhâ‚, confâ‚]
Output[i,j, 5:10] = [bxâ‚‚, byâ‚‚, bwâ‚‚, bhâ‚‚, confâ‚‚]
Output[i,j, 10:30] = [P(câ‚|obj), P(câ‚‚|obj), ..., P(câ‚‚â‚€|obj)]

Constraints:
- 0 â‰¤ bx, by â‰¤ 1 (within cell)
- 0 â‰¤ bw, bh (can be > 1 for large objects)
- 0 â‰¤ conf â‰¤ 1
- Î£ P(cáµ¢|obj) = 1 (probabilities sum to 1)
```

---

## YOLO Loss Function

### Multi-Part Loss

**Plain English Explanation:**

The YOLO loss function has three main components that the network tries to minimize:

1. **Localization Loss:** How far is the predicted box from the correct position/size?
2. **Confidence Loss:** How accurate is the confidence score?
3. **Classification Loss:** Did we predict the correct class?

Think of it like grading a student's test with three parts:
- Did they circle the correct area? (localization)
- Did they express appropriate confidence in their answer? (confidence)
- Did they identify the object correctly? (classification)

### Complete Loss Formula

**Mathematical Formulation:**

```
Loss = Î»_coord Ã— Localization_Loss
     + Confidence_Loss  
     + Î»_noobj Ã— No_Object_Loss
     + Classification_Loss
```

**Each Component:**

**1. Localization Loss (for cells WITH objects):**

```
L_loc = Î»_coord Ã— Î£   Î£   ğŸ™áµ¢â±¼áµ’áµ‡Ê² [(xáµ¢ - xÌ‚áµ¢)Â² + (yáµ¢ - Å·áµ¢)Â²
                  i=0 j=0
                  
                  + (âˆšwáµ¢ - âˆšÅµáµ¢)Â² + (âˆšháµ¢ - âˆšÄ¥áµ¢)Â²]
```

**Symbol Legend:**
- `Î»_coord`: Weight for coordinate loss (typically 5)
- `ğŸ™áµ¢â±¼áµ’áµ‡Ê²`: Indicator function (1 if cell i,j contains object, else 0)
- `(xáµ¢, yáµ¢)`: Predicted box center
- `(xÌ‚áµ¢, Å·áµ¢)`: Ground truth box center  
- `(wáµ¢, háµ¢)`: Predicted box width/height
- `(Åµáµ¢, Ä¥áµ¢)`: Ground truth width/height
- `âˆš`: Square root (makes loss more sensitive to small boxes)

**2. Confidence Loss (for cells WITH objects):**

```
L_conf_obj = Î£   Î£   ğŸ™áµ¢â±¼áµ’áµ‡Ê² (Cáµ¢ - Äˆáµ¢)Â²
             i=0 j=0
```

Where:
- `Cáµ¢`: Predicted confidence
- `Äˆáµ¢`: Target confidence = IoU(predicted_box, ground_truth)

**3. No-Object Loss (for cells WITHOUT objects):**

```
L_conf_noobj = Î»_noobj Ã— Î£   Î£   ğŸ™áµ¢â±¼â¿áµ’áµ’áµ‡Ê² (Cáµ¢ - 0)Â²
                         i=0 j=0
```

Where:
- `Î»_noobj`: Weight for no-object loss (typically 0.5)
- `ğŸ™áµ¢â±¼â¿áµ’áµ’áµ‡Ê²`: Indicator (1 if cell has NO object)
- Target is 0 (no object present)

**4. Classification Loss:**

```
L_class = Î£   Î£   ğŸ™áµ¢â±¼áµ’áµ‡Ê² Î£  (páµ¢(c) - pÌ‚áµ¢(c))Â²
          i=0 j=0         c
```

Where:
- `páµ¢(c)`: Predicted probability of class c
- `pÌ‚áµ¢(c)`: Ground truth (1 for correct class, 0 for others)

### Why Each Component Matters

**Coordinate Loss (with Î»_coord = 5):**

```
Why squared error on coordinates?
Directly penalizes position errors

Why square root on width/height?
Makes loss more sensitive to errors in small boxes

Example:
Small box (w=0.1): Error of 0.05 â†’ âˆš0.1 - âˆš0.15 = 0.32 - 0.39 = 0.07
Large box (w=0.8): Error of 0.05 â†’ âˆš0.8 - âˆš0.85 = 0.89 - 0.92 = 0.03

Same absolute error, but bigger relative penalty for small box!

Why Î»_coord = 5?
Localization is harder than classification, so weight it more
```

**Confidence Loss:**

```
For cells WITH object: Target = IoU with ground truth
Encourages high confidence when box is accurate

For cells WITHOUT object: Target = 0
Encourages low confidence when no object

Why Î»_noobj = 0.5?
Most cells have no object (imbalance problem)
Reduce their weight to prevent overwhelming the loss
Otherwise, network learns to always predict "no object"
```

**Classification Loss:**

```
Only applied to cells that contain objects
Squared error on class probabilities
Encourages correct class to have high probability
```

---

## Detailed Numerical Examples

### Example 1: Single Object Detection

**Scenario:** One car in image

**Setup:**
```
Image: 448Ã—448 pixels
Grid: 7Ã—7
Ground truth: Car at center (224, 224), size 100Ã—80 pixels
```

**Grid Assignment:**

```
Car center: (224, 224)
In normalized coordinates: (224/448, 224/448) = (0.5, 0.5)

Grid position: (floor(0.5 Ã— 7), floor(0.5 Ã— 7)) = (3, 3)

Cell (3,3) is responsible for this car!

â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  Grid (7Ã—7)
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼ X â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  X = car center
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  in cell (3,3)
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
```

**Ground Truth Encoding:**

```
For cell (3,3):

Determine box center relative to cell:
Cell (3,3) covers: x in [3/7, 4/7], y in [3/7, 4/7]
Cell center: (3.5/7, 3.5/7) = (0.5, 0.5) in image coordinates

Car center: (0.5, 0.5) in image coordinates

Relative to cell:
bx = (0.5 - 3/7) / (1/7) = (0.5 - 0.429) / 0.143 = 0.497 â‰ˆ 0.5
by = (0.5 - 3/7) / (1/7) = 0.5 (same calculation)

Box dimensions (relative to image):
bw = 100/448 = 0.223
bh = 80/448 = 0.179

Ground truth for cell (3,3):
Box 1: [0.5, 0.5, 0.223, 0.179, 1.0]
Box 2: [0, 0, 0, 0, 0] (unused)
Classes: [0,0,1,0,0,0,...,0] (class 2 = car)
```

**Suppose Network Predicts:**

```
Cell (3,3) predictions:
Box 1: [0.48, 0.52, 0.210, 0.165, 0.88]
Box 2: [0.30, 0.40, 0.150, 0.120, 0.15] (low confidence, ignore)
Classes: [0.02, 0.01, 0.91, 0.01, ...]
```

**Loss Calculation:**

```
For cell (3,3) - has object:

Coordinate loss (Box 1, chosen because higher confidence):
L_coord = Î»_coord Ã— [(0.48-0.5)Â² + (0.52-0.5)Â² 
                    + (âˆš0.210-âˆš0.223)Â² + (âˆš0.165-âˆš0.179)Â²]
        = 5 Ã— [(0.02)Â² + (0.02)Â² + (0.458-0.472)Â² + (0.406-0.423)Â²]
        = 5 Ã— [0.0004 + 0.0004 + 0.000196 + 0.000289]
        = 5 Ã— 0.001285
        = 0.00643

Confidence loss:
IoU between predicted and ground truth box â‰ˆ 0.82
L_conf = (0.88 - 0.82)Â² = 0.06Â² = 0.0036

Classification loss:
L_class = (0.02-0)Â² + (0.01-0)Â² + (0.91-1)Â² + ... (other 17 classes)
        = 0.0004 + 0.0001 + 0.0081 + (sum of small values)
        â‰ˆ 0.01
```

### Example 2: Multiple Objects

**Scenario:** Two cars and one person

**Visual:**

```
Image 448Ã—448 with 7Ã—7 grid overlay:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚ ğŸš—â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  Car 1 center in cell (1,1)
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚ğŸš— â”‚   â”‚  Car 2 center in cell (3,5)
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚ğŸ§ â”‚   â”‚   â”‚   â”‚   â”‚  Person center in cell (5,2)
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Three cells have objects: (1,1), (3,5), (5,2)
Other 46 cells should predict "no object"
```

**Ground Truth Encoding:**

```
Cell (1,1): Car
- Box: [bx, by, bw, bh, conf] = computed from car position
- Classes: [1, 0, 0, ...] (class 0 = car)

Cell (3,5): Car
- Box: [bx, by, bw, bh, conf] = computed from car position  
- Classes: [1, 0, 0, ...] (class 0 = car)

Cell (5,2): Person
- Box: [bx, by, bw, bh, conf] = computed from person position
- Classes: [0, 0, 1, ...] (class 2 = person)

Cells without objects (46 cells):
- Box: [0, 0, 0, 0, 0] (don't care about box coords)
- Confidence: 0 (no object)
- Classes: [0, 0, 0, ...] (don't care)
```

**Loss Calculation:**

```
Total loss = Sum over all 49 cells

Cells (1,1), (3,5), (5,2): Object present
- Contribute to: coordinate loss, confidence loss, classification loss
- Î»_coord = 5 weights coordinate errors heavily

Other 46 cells: No object
- Contribute to: no-object confidence loss only
- Î»_noobj = 0.5 reduces their impact

Example calculation:
If cell (1,1) predicts car perfectly: contribution â‰ˆ 0
If cell (1,1) predicts car poorly: contribution could be large
If cell (0,0) predicts no object: contribution â‰ˆ 0
If cell (0,0) falsely predicts object: penalized but with lower weight
```

---

## Forward Propagation in YOLO

### Complete Architecture Flow

**YOLOv1 Network Architecture:**

```
Input: 448Ã—448Ã—3
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Convolutional Layers (1-20)     â”‚
â”‚                                 â”‚
â”‚ Conv-Pool-Conv-Pool pattern     â”‚
â”‚ Extracts hierarchical features  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
Feature Map: 7Ã—7Ã—1024
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fully Connected Layers (21-24)  â”‚
â”‚                                 â”‚
â”‚ FC: 7Ã—7Ã—1024 â†’ 4096             â”‚
â”‚ FC: 4096 â†’ 7Ã—7Ã—30               â”‚
â”‚ (reshaped to grid)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
Output: 7Ã—7Ã—30
(Grid of predictions)
```

**Detailed Layer Breakdown:**

```
Layer   Type           Input           Output          Kernel  Stride
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input   Image          448Ã—448Ã—3       -              -       -
1       Conv+ReLU      448Ã—448Ã—3       448Ã—448Ã—64     7Ã—7     1
2       MaxPool        448Ã—448Ã—64      224Ã—224Ã—64     2Ã—2     2
3       Conv+ReLU      224Ã—224Ã—64      224Ã—224Ã—192    3Ã—3     1
4       MaxPool        224Ã—224Ã—192     112Ã—112Ã—192    2Ã—2     2
5-8     Conv blocks    112Ã—112Ã—192     112Ã—112Ã—512    various 1
9       MaxPool        112Ã—112Ã—512     56Ã—56Ã—512      2Ã—2     2
10-13   Conv blocks    56Ã—56Ã—512       56Ã—56Ã—1024     various 1
14      MaxPool        56Ã—56Ã—1024      28Ã—28Ã—1024     2Ã—2     2
15-20   Conv blocks    28Ã—28Ã—1024      14Ã—14Ã—1024     various 1
21      MaxPool        14Ã—14Ã—1024      7Ã—7Ã—1024       2Ã—2     2
22-23   Conv           7Ã—7Ã—1024        7Ã—7Ã—1024       3Ã—3     1
24      Conv (final)   7Ã—7Ã—1024        7Ã—7Ã—30         1Ã—1     1
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Final output: 7Ã—7Ã—30 (grid of predictions)
```

### Grid Cell Predictions

**At Each Cell, Network Outputs:**

```
For cell (i,j), network produces 30 values:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Direct network outputs:            â”‚
â”‚                                    â”‚
â”‚ Box 1: [x, y, w, h, conf]         â”‚
â”‚ Box 2: [x, y, w, h, conf]         â”‚
â”‚ Classes: [p1, p2, ..., p20]       â”‚
â”‚                                    â”‚
â”‚ All values pass through:           â”‚
â”‚ - Sigmoid for x,y,conf,classes    â”‚
â”‚   (squash to 0-1 range)           â”‚
â”‚ - Exponential or direct for w,h   â”‚
â”‚   (allow values > 1)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example Cell Output:**

```
Cell (3,4) raw network output (before activation):

Raw values: [0.2, 1.5, -0.8, -0.3, 2.1, -1.2, 0.5, ...]

After sigmoid on x, y, conf, class probs:
xâ‚ = sigmoid(0.2) = 0.55
yâ‚ = sigmoid(1.5) = 0.82
confâ‚ = sigmoid(2.1) = 0.89
class_probs = softmax([...])

After processing w, h:
wâ‚ = raw value or exp(raw) depending on implementation
hâ‚ = same

Final cell (3,4) output:
[0.55, 0.82, 0.25, 0.18, 0.89,  # Box 1
 0.30, 0.45, 0.20, 0.15, 0.12,  # Box 2
 0.05, 0.02, 0.87, ...]         # Classes
```

### Bounding Box Computation

**From Predictions to Pixel Coordinates:**

```
Given prediction from cell (i,j):
bx = 0.6  (0 to 1, offset within cell)
by = 0.4
bw = 0.3  (0 to 1+, fraction of image)
bh = 0.2

Step 1: Compute absolute center
center_x_norm = (j + bx) / S = (3 + 0.6) / 7 = 0.514
center_y_norm = (i + by) / S = (4 + 0.4) / 7 = 0.629

Step 2: Convert to pixels
center_x = 0.514 Ã— 448 = 230 pixels
center_y = 0.629 Ã— 448 = 282 pixels

Step 3: Compute box size in pixels
box_width = 0.3 Ã— 448 = 134 pixels
box_height = 0.2 Ã— 448 = 90 pixels

Step 4: Compute box corners
x_min = 230 - 134/2 = 163
y_min = 282 - 90/2 = 237
x_max = 230 + 134/2 = 297
y_max = 282 + 90/2 = 327

Final bounding box: (163, 237, 297, 327) in pixel coordinates
```

### Confidence Thresholding

**Filtering Low-Confidence Predictions:**

```
For SÃ—S grid with B boxes per cell:
Total predictions = S Ã— S Ã— B = 7 Ã— 7 Ã— 2 = 98 bounding boxes

Most will have low confidence (no object):

Example confidences from all 98 predictions:
[0.02, 0.01, 0.03, 0.88, 0.05, 0.92, 0.01, 0.04, ...]
                    â†‘           â†‘
                 Keep these (>0.3)
                 Discard others

After thresholding with threshold=0.3:
Maybe 5-10 boxes remain

These go to NMS for duplicate removal
```

---

## Backward Propagation in YOLO

### Gradient Flow Through Multi-Part Loss

**Plain English:**

During backpropagation, gradients from the four loss components flow back through the network. However, not all cells receive all gradients - only cells responsible for objects get localization and classification gradients.

**Gradient Structure:**

```
For cell (i,j) WITH object:
âˆ‚L/âˆ‚Output[i,j, 0:5] = Coordinate gradients + Confidence gradient
âˆ‚L/âˆ‚Output[i,j, 5:10] = Confidence gradient only (for box 2)
âˆ‚L/âˆ‚Output[i,j, 10:30] = Classification gradients

For cell (i,j) WITHOUT object:
âˆ‚L/âˆ‚Output[i,j, 0:5] = Confidence gradient only (push to 0)
âˆ‚L/âˆ‚Output[i,j, 5:10] = Confidence gradient only (push to 0)
âˆ‚L/âˆ‚Output[i,j, 10:30] = Zero (no classification needed)
```

### Responsibility Assignment

**Which Cell Handles Which Object:**

```
Rule: Cell containing object's CENTER is responsible

Example with 3 objects:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚ Ã—â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  â† Car 1, cell (1,1) responsible
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚ Ã—â”‚   â”‚  â† Car 2, cell (3,5) responsible
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚ Ã—â”‚   â”‚   â”‚   â”‚   â”‚  â† Person, cell (5,2) responsible
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Cells (1,1), (3,5), (5,2): Get full gradients
Other 46 cells: Get only no-object confidence gradients
```

### Detailed Gradient Calculation

**Example: Cell (3,3) with Car**

**Forward pass gave:**
```
Predicted: box=[0.48, 0.52, 0.21, 0.17], conf=0.88, classes=[0.02, 0.01, 0.91, ...]
Ground truth: box=[0.5, 0.5, 0.223, 0.179], conf=0.82 (IoU), classes=[0, 0, 1, ...]
```

**Coordinate Gradients:**

```
âˆ‚L_coord/âˆ‚bx = 2 Ã— Î»_coord Ã— (bx - bx_true)
             = 2 Ã— 5 Ã— (0.48 - 0.5)
             = 10 Ã— (-0.02)
             = -0.2

âˆ‚L_coord/âˆ‚by = 2 Ã— 5 Ã— (0.52 - 0.5) = 0.2

For width (using square root):
âˆ‚L_coord/âˆ‚bw = 2 Ã— 5 Ã— (âˆšbw - âˆšbw_true) Ã— (1/(2âˆšbw))
             = 10 Ã— (âˆš0.21 - âˆš0.223) Ã— (1/(2âˆš0.21))
             = 10 Ã— (0.458 - 0.472) Ã— (1/0.916)
             = 10 Ã— (-0.014) Ã— 1.092
             = -0.153

Similar for âˆ‚L/âˆ‚bh
```

**Confidence Gradient:**

```
âˆ‚L_conf/âˆ‚conf = 2 Ã— (conf - conf_true)
              = 2 Ã— (0.88 - 0.82)
              = 2 Ã— 0.06
              = 0.12
```

**Classification Gradients:**

```
For class 2 (car, correct class):
âˆ‚L_class/âˆ‚pâ‚‚ = 2 Ã— (0.91 - 1.0) = -0.18

For class 0 (incorrect class):
âˆ‚L_class/âˆ‚pâ‚€ = 2 Ã— (0.02 - 0.0) = 0.04

All gradients push toward correct answer
```

### Why Some Cells Get Zero Gradient

**Cells Without Objects:**

```
Cell (0,0) has no object:

âˆ‚L/âˆ‚coords[0,0] = 0 (don't care about box coordinates)
âˆ‚L/âˆ‚conf[0,0] = 2 Ã— Î»_noobj Ã— (pred_conf - 0)
               = 2 Ã— 0.5 Ã— pred_conf
               = pred_conf
âˆ‚L/âˆ‚classes[0,0] = 0 (don't care about classes)

If predicted conf=0.02 (correctly low):
  âˆ‚L/âˆ‚conf = 0.02 (small gradient, minor adjustment)

If predicted conf=0.8 (incorrectly high!):
  âˆ‚L/âˆ‚conf = 0.8 (large gradient, big correction needed)
```

**Visual Summary:**

```
Gradient Flow Pattern:

Cells WITH objects:          Cells WITHOUT objects:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âˆ‚L/âˆ‚box_coords  â”‚         â”‚ âˆ‚L/âˆ‚box = 0      â”‚
â”‚ âˆ‚L/âˆ‚confidence  â”‚         â”‚ âˆ‚L/âˆ‚conf (small) â”‚
â”‚ âˆ‚L/âˆ‚classes     â”‚         â”‚ âˆ‚L/âˆ‚classes = 0  â”‚
â”‚                  â”‚         â”‚                  â”‚
â”‚ Full gradients   â”‚         â”‚ Sparse gradients â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3 cells get full gradients
46 cells get minimal gradients

This focuses learning on relevant cells!
```

---

## Summary

**YOLO Core Principles:**

1. **Single Pass:** One forward pass detects all objects
2. **Grid-Based:** Divide image into SÃ—S grid
3. **Cell Responsibility:** Each cell detects objects whose center falls in it
4. **Direct Prediction:** Output is grid of class probabilities and boxes
5. **Fast:** Real-time capable (30-60 FPS)

**Output Structure:**
```
Shape: (S, S, BÃ—5 + C)
S = grid size (7)
B = boxes per cell (2)
C = number of classes (20)

Each cell predicts:
- B bounding boxes (each with 5 values: x,y,w,h,confidence)
- C class probabilities
```

**Loss Function:**
```
Weighted sum of:
- Coordinate loss (box position/size)
- Confidence loss (objectness)
- Classification loss (which class)

Different weights for:
- Object cells vs non-object cells
- Coordinate errors vs other errors
```

**Advantages:**
- Extremely fast (real-time detection)
- Sees full image (better context understanding)
- Simple unified architecture
- End-to-end trainable

**Trade-offs:**
- Each cell can only detect B objects (limitation for densely packed scenes)
- Struggles with small objects in groups
- Less accurate than slower two-stage methods (but improving)

This makes YOLO ideal for real-time applications like video surveillance, autonomous driving, and robotics!