# Residual Networks (ResNets)

## Table of Contents

1. [Introduction to Residual Networks](#introduction-to-residual-networks)
   - [What are ResNets?](#what-are-resnets)
   - [The Motivation](#the-motivation)
   - [Key Innovation: Skip Connections](#key-innovation-skip-connections)
   - [Connection to Previous Topics](#connection-to-previous-topics)

2. [The Degradation Problem](#the-degradation-problem)
   - [Plain English Explanation](#degradation-plain-english)
   - [The Deep Learning Paradox](#the-deep-learning-paradox)
   - [Mathematical Understanding](#degradation-mathematical-understanding)
   - [Why Traditional Networks Fail](#why-traditional-networks-fail)

3. [Skip Connections (Residual Connections)](#skip-connections-residual-connections)
   - [What are Skip Connections?](#what-are-skip-connections)
   - [How Skip Connections Work](#how-skip-connections-work)
   - [Mathematical Formulation](#skip-mathematical-formulation)
   - [Types of Skip Connections](#types-of-skip-connections)
   - [Why They Solve the Degradation Problem](#why-they-solve-degradation)

4. [Residual Block Architecture](#residual-block-architecture)
   - [Basic Residual Block](#basic-residual-block)
   - [Bottleneck Residual Block](#bottleneck-residual-block)
   - [Mathematical Details](#block-mathematical-details)
   - [When to Use Each Type](#when-to-use-each-block-type)

5. [Forward Propagation in ResNets](#forward-propagation-in-resnets)
   - [Plain English Explanation](#forward-plain-english)
   - [Through Basic Block](#forward-basic-block)
   - [Through Bottleneck Block](#forward-bottleneck-block)
   - [Complete Numerical Example](#forward-numerical-example)

6. [Backward Propagation in ResNets](#backward-propagation-in-resnets)
   - [Plain English Explanation](#backward-plain-english)
   - [Gradient Flow Through Skip Connections](#gradient-flow)
   - [Why Gradients Don't Vanish](#why-no-vanishing)
   - [Detailed Mathematical Derivation](#backward-mathematical-derivation)
   - [Numerical Example](#backward-numerical-example)

7. [Practical Guidelines](#practical-guidelines)
   - [When to Use ResNets](#when-to-use-resnets)
   - [Implementation Best Practices](#implementation-best-practices)
   - [Common Patterns and Variations](#common-patterns)
   - [Troubleshooting](#troubleshooting)

---

## Introduction to Residual Networks

### What are ResNets?

**Plain English Overview:**

Residual Networks (ResNets) are a revolutionary architecture in deep learning that allows us to train extremely deep neural networks (100+ layers) successfully. The key innovation is the "skip connection" or "shortcut connection" that allows the network to skip one or more layers by adding the input directly to the output.

**Analogy:** Think of learning to play a difficult piano piece. A traditional approach (regular neural network) would be: start from scratch and learn the entire piece note by note, getting more complex with each measure. The ResNet approach is: start with what you already can play (the input), then only learn the *difference* (residual) between what you can play and what the piece requires. This makes learning much easier because you're building on what you know rather than starting from zero.

**Key Concept:** Instead of learning a direct mapping H(x) from input x to output, ResNets learn a *residual mapping* F(x) = H(x) - x, where the final output is F(x) + x. This seemingly simple change has profound effects on training deep networks.

### The Motivation

**The Problem with Very Deep Networks:**

Before ResNets (pre-2015), researchers observed a puzzling phenomenon:

1. Deeper networks should theoretically be at least as good as shallow ones
2. A deep network could always learn to copy the shallow network's layers and use identity mappings for extra layers
3. **But in practice**: Very deep networks (50+ layers) performed WORSE than shallower networks (20 layers)
4. This wasn't overfitting (training error was also higher)

**Visual Illustration of the Problem:**

```
Theory:                           Reality (before ResNets):
                                 
20-layer network:                20-layer network:
Training Error: 15%              Training Error: 15%
Test Error: 20%                  Test Error: 20%
                                 
56-layer network:                56-layer network:
Training Error: ≤15% (better)    Training Error: 25% (WORSE!) 
Test Error: ≤20% (better)        Test Error: 30% (WORSE!)
       ↑                                ↑
   Should be better             Degradation problem!
```

**The ResNet Solution:**

ResNets solved this by introducing skip connections that make it easy for the network to learn identity mappings when needed, allowing very deep networks to train successfully.

**Results:**
- ResNet-152 (152 layers): Better than ResNet-34 (34 layers)
- ResNet-1001 (1001 layers): Trainable and effective
- Won ImageNet 2015 with 3.57% error (human performance: ~5%)

### Key Innovation: Skip Connections

**The Core Idea:**

Instead of trying to learn a mapping H(x), learn F(x) = H(x) - x, then compute H(x) = F(x) + x.

**Visual Illustration:**

```
Traditional Block:              Residual Block:
                               
     x                              x
     │                              │
     ↓                              ├─────────────┐
  ┌──────┐                         │             │
  │Layer1│                       ┌──────┐        │
  └──────┘                       │Layer1│        │
     │                           └──────┘        │
     ↓                              │            │ Skip
  ┌──────┐                          ↓            │ Connection
  │Layer2│                       ┌──────┐        │
  └──────┘                       │Layer2│        │
     │                           └──────┘        │
     ↓                              │            │
    H(x)                            ├────── + ───┘
    Output                          │
                                    ↓
                                  H(x) = F(x) + x
                                   Output

Learn everything               Learn only the residual
from scratch                  (difference from input)
```

**Why This Helps:**

If the optimal function is close to identity, it's easier to learn F(x) ≈ 0 than to learn H(x) ≈ x directly. The network can "choose" to skip layers when they're not needed.

### Connection to Previous Topics

**Building on CNN Foundations:**

ResNets use all the concepts we've learned:

1. **Convolution** (from document 1): Standard convolutional layers form the residual mapping F(x)
2. **Padding** (from document 2): Same padding maintains dimensions for skip connections
3. **Stride** (from document 3): Strided convolutions for downsampling, with projection shortcuts

**Key Requirements from Previous Topics:**

For skip connections to work by simple addition:
```
Input shape = Output shape

This requires:
- Same spatial dimensions (use padding in F(x))
- Same number of channels (use 1×1 conv when needed)
```

**Integration Example:**

```
Input: 56×56×64
    │
    ├─────────────────┐ Skip connection
    │                 │
    ↓                 │
Conv 3×3, 64 filters  │ F(x): Uses convolution,
stride=1, pad=1       │       padding, stride
    ↓                 │       from previous topics
Conv 3×3, 64 filters  │
stride=1, pad=1       │
    ↓                 │
    │                 │
    └────── + ────────┘ Add (requires same size!)
         ↓
    ReLU
         ↓
Output: 56×56×64
```

---

## The Degradation Problem

### Degradation Plain English

**The Puzzle:**

In traditional neural networks, researchers discovered something strange: when they made networks deeper (more layers), performance actually got *worse*, even on the training set. This wasn't overfitting (which affects test error but not training error) - it was something different.

**Analogy:** Imagine you're building a tower with blocks:
- With 10 blocks, you build it successfully
- With 20 blocks, you should be able to build the same 10-block tower plus add 10 identity layers (blocks that do nothing)
- But in practice, the 20-block tower keeps falling over during construction

The problem isn't that a taller tower is impossible - it's that the construction process (training) breaks down.

### The Deep Learning Paradox

**The Theoretical Argument:**

Consider two networks:
1. **Shallow network**: 20 layers, achieves some accuracy
2. **Deep network**: 56 layers

**Theoretical claim:** The deep network should be at least as good because:

```
Deep Network (56 layers) can learn:
┌────────────────────────────────┐
│ Layers 1-20: Copy shallow net  │  Learn what shallow net learned
├────────────────────────────────┤
│ Layers 21-56: Identity maps    │  Just pass through (do nothing)
└────────────────────────────────┘

Result: Same performance as shallow network (at minimum)
```

**Visual Proof:**

```
Shallow Network (20 layers):
Input → L1 → L2 → ... → L20 → Output
        
Deep Network (56 layers) - Theoretical Optimal:
Input → L1 → L2 → ... → L20 → Identity → ... → Identity → Output
                              (36 identity layers)

Should give at least same performance!
```

**The Reality:**

```
Experimental Results on CIFAR-10:

Network Depth    Training Error    Test Error
─────────────────────────────────────────────
20 layers           8.7%            9.1%
32 layers           7.8%            8.9%      ← Getting better
56 layers          11.2%           12.4%      ← Gets WORSE!

The paradox: More capacity → Worse performance
```

**Visual Illustration:**

```
Error vs Depth (Traditional Networks):

Error ↑
  │                        
  │                    ╱ 56-layer
  │                  ╱  (worse!)
  │                ╱
  │              ╱
  │            ╱
  │    ╲     ╱ 
  │      ╲ ╱  20-layer
  │       ╳   (better)
  │      ╱ ╲
  │    ╱     ╲
  │                        
  └─────────────────────────► Network Depth
         
Expected: Error should decrease or plateau
Reality: Error increases! (degradation)
```

### Degradation Mathematical Understanding

**Hypothesis: Difficulty Learning Identity**

The core problem is that it's difficult for a stack of nonlinear layers to learn an identity mapping.

**Why Identity is Hard to Learn:**

Consider a simple 2-layer block trying to learn identity:

```
Input: x

Layer 1: W₁x + b₁ → ReLU → a₁
Layer 2: W₂a₁ + b₂ → ReLU → a₂

For identity: a₂ = x

This requires:
W₂ · ReLU(W₁x + b₁) + b₂ = x  for all x

With ReLU: max(0, z), this is complex!
```

**Example of Difficulty:**

For input x = [1, 2, 3]:

```
Ideal identity: output = [1, 2, 3]

Attempt 1:
W₁ = [[1, 0, 0],      b₁ = [0]
      [0, 1, 0],           [0]
      [0, 0, 1]]           [0]

After Layer 1: a₁ = ReLU([1, 2, 3]) = [1, 2, 3] ✓

W₂ = [[1, 0, 0],      b₂ = [0]
      [0, 1, 0],           [0]
      [0, 0, 1]]           [0]

After Layer 2: a₂ = ReLU([1, 2, 3]) = [1, 2, 3] ✓

Works for positive values!

But for x = [-1, 2, -3]:
After Layer 1: a₁ = ReLU([-1, 2, -3]) = [0, 2, 0] 
                                          ↑      ↑
                                      Information LOST!

After Layer 2: a₂ = [0, 2, 0] ≠ [-1, 2, -3] ✗

ReLU kills negative values - can't perfectly learn identity!
```

**The Gradient Problem:**

During backpropagation through many layers:

```
∂L/∂x = ∂L/∂y × ∂y/∂h₅₆ × ∂h₅₆/∂h₅₅ × ... × ∂h₂/∂h₁ × ∂h₁/∂x
        
        56 terms being multiplied!

If each ∂hᵢ/∂hᵢ₋₁ < 1: gradient vanishes (→ 0)
If each ∂hᵢ/∂hᵢ₋₁ > 1: gradient explodes (→ ∞)

Perfect identity would give ∂hᵢ/∂hᵢ₋₁ = 1, but:
- Weights initialized randomly (not identity)
- ReLU and other nonlinearities complicate this
- Difficult to maintain during training
```

### Why Traditional Networks Fail

**Three Main Issues:**

**1. Vanishing/Exploding Gradients:**

```
Deep Network Gradient:

∂L     ∂L   ∂h₅₆   ∂h₅₅        ∂h₂   ∂h₁
─── = ─── × ──── × ──── × ... × ─── × ───
∂x    ∂y   ∂h₅₅   ∂h₅₄        ∂h₁   ∂x

Example with 56 layers:
If each term ≈ 0.9:
  0.9⁵⁶ ≈ 0.002 (vanishing!)

If each term ≈ 1.1:
  1.1⁵⁶ ≈ 213 (exploding!)
```

**2. Difficulty Optimizing Deep Networks:**

```
Loss landscape for deep networks is complex:

Shallow Network          Deep Network
    Loss                    Loss
     ↑                       ↑
     │    ╱╲                │    ╱╲  ╱╲╱╲
     │   ╱  ╲               │   ╱  ╲╱    ╲
     │  ╱    ╲              │  ╱          ╲
     │ ╱      ╲             │ ╱            ╲
     └──────────► params    └────────────────► params
     
   Smooth, easy to         Many local minima,
   optimize                hard to optimize
```

**3. Information Bottleneck:**

As information flows through many layers with nonlinearities, it can get corrupted or lost.

```
Information Flow:

Input (100% information)
   │
   ↓ Layer 1 (ReLU might zero some values)
 95% information preserved
   │
   ↓ Layer 2
 90% information preserved
   │
   ⋮  (information degrades)
   │
   ↓ Layer 56
 30% information preserved ← Too much loss!
```

---

## Skip Connections (Residual Connections)

### What are Skip Connections?

**Plain English Overview:**

A skip connection (also called shortcut connection or residual connection) is a direct path that bypasses one or more layers, connecting an earlier layer directly to a later layer. The output becomes the sum of the skipped path and the processed path.

**Analogy:** Imagine a highway with local roads:
- **Local roads** (layers with weights): Transform the data, add features
- **Highway** (skip connection): Data can travel unchanged, bypassing transformations
- **Junction** (addition): Combine both paths

The highway ensures that even if the local roads are congested (layers fail to learn), traffic (information/gradients) can still flow smoothly.

**Visual Representation:**

```
Traditional Multi-Layer Block:

     x (input)
     │
     ↓
  ┌──────┐
  │ Conv │  Transform
  │ BN   │
  │ ReLU │
  └──────┘
     │
     ↓
  ┌──────┐
  │ Conv │  Transform
  │ BN   │
  └──────┘
     │
     ↓
  ┌──────┐
  │ ReLU │
  └──────┘
     │
     ↓
   Output
   
All information must flow through transformations

Residual Block:

        x (input)
        │
        ├─────────────────────┐
        │                     │
        ↓                     │ Skip
     ┌──────┐                 │ Connection
     │ Conv │  F(x)           │ (Identity)
     │ BN   │                 │
     │ ReLU │                 │
     └──────┘                 │
        │                     │
        ↓                     │
     ┌──────┐                 │
     │ Conv │                 │
     │ BN   │                 │
     └──────┘                 │
        │                     │
        ↓                     │
        └───────── + ─────────┘ Addition
                   │
                   ↓
                ┌──────┐
                │ ReLU │
                └──────┘
                   │
                   ↓
              H(x) = F(x) + x
              
Information has TWO paths: through layers AND direct highway
```

### How Skip Connections Work

**The Residual Learning Framework:**

**Traditional approach:** Learn H(x) directly
```
H(x) = desired_output
Network must learn H from scratch
```

**Residual approach:** Learn F(x) where H(x) = F(x) + x
```
F(x) = H(x) - x  (the residual/difference)
Network learns only what needs to change
```

**Why This is Easier:**

If the optimal output is close to the input (identity or near-identity):

```
Traditional: Learn H(x) ≈ x
- Must learn weights that exactly reproduce input
- Difficult with nonlinearities (ReLU, etc.)
- Requires precise weight values

Residual: Learn F(x) ≈ 0
- Network learns to output near-zero
- Much easier to achieve
- Weights can be small or zero
- Default behavior (small random weights) is already close!
```

**Step-by-Step Process:**

```
Step 1: Input x enters the block
        x = [1, 2, 3, 4]

Step 2: x flows through the residual mapping F(x)
        (multiple conv layers)
        F(x) = [0.1, -0.2, 0.05, -0.1]
        (small residual corrections)

Step 3: x also flows directly via skip connection
        (unchanged)
        x = [1, 2, 3, 4]

Step 4: Add both paths
        H(x) = F(x) + x
             = [0.1, -0.2, 0.05, -0.1] + [1, 2, 3, 4]
             = [1.1, 1.8, 3.05, 3.9]

Output is input plus small corrections!
```

### Skip Mathematical Formulation

**Basic Residual Block:**

```
y = F(x, {Wᵢ}) + x
```

**Symbol Legend:**
- `x`: Input to the residual block
- `F(x, {Wᵢ})`: Residual mapping (what the layers learn)
- `Wᵢ`: Learnable parameters in the residual mapping
- `y`: Output of the residual block (before final activation)
- `+`: Element-wise addition

**Expanded Form for 2-Layer Block:**

```
F(x) = W₂ · σ(W₁ · x + b₁) + b₂

y = F(x) + x

Output = σ(y) = σ(F(x) + x)
```

**Symbol Legend:**
- `W₁, W₂`: Weight matrices for layers 1 and 2
- `b₁, b₂`: Bias vectors
- `σ`: Activation function (ReLU)
- `·`: Matrix multiplication or convolution

**With Batch Normalization:**

```
h₁ = Conv₁(x)
h₁ = BatchNorm(h₁)
h₁ = ReLU(h₁)

h₂ = Conv₂(h₁)
h₂ = BatchNorm(h₂)

y = h₂ + x  ← Skip connection adds original input
Output = ReLU(y)
```

**Key Mathematical Properties:**

1. **Dimensionality requirement:**
   ```
   x.shape = F(x).shape for element-wise addition
   ```

2. **Gradient flow:**
   ```
   ∂y/∂x = ∂F(x)/∂x + ∂x/∂x = ∂F(x)/∂x + I
   ```
   The identity term `I` ensures gradients can flow directly!

### Types of Skip Connections

**1. Identity Skip Connection:**

Used when input and output have the same dimensions.

```
Visual:
    x (C × H × W)
    │
    ├───────────────┐
    │               │
    ↓               │
  Conv(C→C)         │ Direct addition
  3×3, pad=1        │ (no transformation)
    ↓               │
  Conv(C→C)         │
  3×3, pad=1        │
    ↓               │
    └─────── + ─────┘
         ↓
      y (C × H × W)

Mathematical:
y = F(x) + x
```

**Example:**
```
Input: 64 channels, 28×28
Conv 3×3, 64 filters, pad=1 → 64 channels, 28×28
Conv 3×3, 64 filters, pad=1 → 64 channels, 28×28
Add input: Same dimensions ✓
Output: 64 channels, 28×28
```

**2. Projection Skip Connection:**

Used when dimensions change (different spatial size or number of channels).

```
Visual:
    x (C₁ × H₁ × W₁)
    │
    ├─────────────────────┐
    │                     │
    ↓                     │ 1×1 Conv
  Conv(C₁→C₂)             │ (projection)
  3×3, stride=2           │ to match
    ↓                     │ dimensions
  Conv(C₂→C₂)             │
  3×3, pad=1              │
    ↓                     ↓
    │                  Conv 1×1
    │                  C₁→C₂
    │                  stride=2
    │                     │
    └─────── + ───────────┘
         ↓
    y (C₂ × H₂ × W₂)

Mathematical:
y = F(x) + Wₛ·x

where Wₛ is the projection matrix (1×1 convolution)
```

**Example:**
```
Input: 64 channels, 56×56

Residual path F(x):
Conv 3×3, 128 filters, stride=2 → 128 channels, 28×28
Conv 3×3, 128 filters, pad=1 → 128 channels, 28×28

Skip path:
Conv 1×1, 128 filters, stride=2 → 128 channels, 28×28

Add: Both 128 channels, 28×28 ✓
Output: 128 channels, 28×28
```

**Comparison:**

```
┌──────────────────┬─────────────┬──────────────────┐
│                  │ Identity    │ Projection       │
├──────────────────┼─────────────┼──────────────────┤
│ When to use      │ Same dims   │ Different dims   │
│ Parameters       │ None        │ Wₛ (1×1 conv)   │
│ Computation      │ None (free!)│ Small overhead   │
│ Use case         │ Most blocks │ Downsample/      │
│                  │             │ channel change   │
└──────────────────┴─────────────┴──────────────────┘
```

### Why They Solve Degradation

**Reason 1: Easy to Learn Identity**

With skip connections, learning identity is trivial:

```
Traditional: Learn H(x) = x
Requires: Complex weight configuration

Residual: Learn F(x) = 0
Requires: Just initialize weights to zero or small values
Default: Random small weights already ≈ 0!

Result: F(x) ≈ 0 → H(x) = F(x) + x ≈ x
```

**Reason 2: Gradient Highway**

Skip connections provide a direct path for gradients:

```
Backward Pass Without Skip Connection:

∂L     ∂L   ∂h₅₆   ∂h₅₅        ∂h₂
─── = ─── × ──── × ──── × ... × ───
∂x    ∂y   ∂h₅₅   ∂h₅₄        ∂x

Many multiplications → gradient vanishing/explosion

Backward Pass With Skip Connection:

∂L     ∂L   ∂y      ∂L   (∂F(x)      )
─── = ─── × ─── =  ─── × (───── + I  )
∂x    ∂y   ∂x      ∂y   ( ∂x         )

        ↑
    Identity term I ensures at least some gradient flows!
```

**Visual Illustration:**

```
Gradient Flow in Traditional Network:

Layer 56 ────┐
             ↓ gradient × 0.9
Layer 55 ────┤
             ↓ gradient × 0.9
Layer 54 ────┤
             ↓
    ...      │
             ↓ gradient × 0.9
Layer 2  ────┤
             ↓ gradient × 0.9
Layer 1  ────┘
             
Final gradient ≈ 0.9⁵⁶ ≈ 0.002 (vanished!)

Gradient Flow in ResNet:

Layer 56 ────┬──────────┐
             ↓          │
Layer 55 ────┤          │ Skip connection
             ↓          │ (gradient = 1)
Layer 54 ────┤          │
             ↓          │
    ...      │          │
             ↓          │
Layer 2  ────┤          │
             ↓          │
Layer 1  ────┴──────────┘
             
Gradient = 0.9⁵⁶ + 1 ≈ 1.002 (preserved!)
```

**Reason 3: Ensemble Behavior**

ResNets behave like ensembles of shallower networks:

```
A ResNet with skip connections at every 2 layers
effectively creates 2ⁿ paths of different lengths!

Example: 6 layers with 3 skip connections:

Possible paths:
1. Direct: Input → → → → → → Output (0 residual blocks)
2. Path 1: Input → Block1 → → → → Output (1 block)
3. Path 2: Input → → Block2 → → Output (1 block)
4. Path 3: Input → → → Block3 → Output (1 block)
5. Path 4: Input → Block1 → Block2 → → Output (2 blocks)
... and so on

2³ = 8 different paths!
Network trains ensemble of all these paths
```

---

## Residual Block Architecture

### Basic Residual Block

**Plain English Overview:**

The basic residual block consists of two convolutional layers with a skip connection that adds the input to the output. It's the fundamental building block of ResNet.

**Structure:**

```
Input: x
  │
  ├────────────────────────────┐
  │                            │
  ↓                            │
┌────────────────────┐         │
│ Conv 3×3, C filters│         │
│ BatchNorm          │         │  
│ ReLU               │         │
└────────────────────┘         │ Skip Connection
  │                            │ (Identity)
  ↓                            │
┌────────────────────┐         │
│ Conv 3×3, C filters│         │
│ BatchNorm          │         │
└────────────────────┘         │
  │                            │
  ↓                            │
  └──────── + ────────────────┘ Element-wise add
            │
            ↓
         ┌──────┐
         │ ReLU │
         └──────┘
            │
            ↓
         Output: y
```

**Mathematical Formulation:**

```
# First convolution
h₁ = Conv₁(x)
h₁ = BatchNorm₁(h₁)
h₁ = ReLU(h₁)

# Second convolution
h₂ = Conv₂(h₁)
h₂ = BatchNorm₂(h₂)

# Skip connection and final activation
y = h₂ + x  ← Addition of residual and input
Output = ReLU(y)
```

**Detailed Formula:**

```
F(x) = W₂ · ReLU(BatchNorm(W₁ · x + b₁)) + b₂

y = F(x) + x

Output = ReLU(y)
```

**Symbol Legend:**
- `x`: Input feature map (C × H × W)
- `W₁, W₂`: Convolutional filter weights
- `b₁, b₂`: Biases
- `F(x)`: Residual mapping (what layers learn)
- `y`: Pre-activation output
- `C`: Number of channels
- `H, W`: Spatial dimensions

**Numerical Example:**

**Given:**
- Input: 4×4×2 (H × W × C)
- Filters: 3×3×2 with 2 output channels
- Padding: 1 (same padding)
- Stride: 1

**Input:**
```
x channel 0:           x channel 1:
[1, 2, 3, 4]          [0.5, 1.0, 1.5, 2.0]
[5, 6, 7, 8]          [2.5, 3.0, 3.5, 4.0]
[9,10,11,12]          [4.5, 5.0, 5.5, 6.0]
[13,14,15,16]         [6.5, 7.0, 7.5, 8.0]
```

**After Conv1 + BatchNorm + ReLU:**
```
h₁ channel 0:         h₁ channel 1:
[2, 3, 4, 3]          [1, 2, 3, 2]
[4, 5, 6, 5]          [3, 4, 5, 4]
[6, 7, 8, 7]          [5, 6, 7, 6]
[5, 6, 7, 6]          [4, 5, 6, 5]
```

**After Conv2 + BatchNorm:**
```
F(x) channel 0:       F(x) channel 1:
[0.2, 0.3, 0.4, 0.2]  [0.1, 0.2, 0.3, 0.1]
[0.4, 0.5, 0.6, 0.4]  [0.3, 0.4, 0.5, 0.3]
[0.6, 0.7, 0.8, 0.6]  [0.5, 0.6, 0.7, 0.5]
[0.4, 0.5, 0.6, 0.4]  [0.3, 0.4, 0.5, 0.3]
```

**Add Skip Connection:**
```
y = F(x) + x

y channel 0:                    y channel 1:
[0.2+1, 0.3+2, 0.4+3, 0.2+4]   [0.1+0.5, 0.2+1.0, ...]
= [1.2, 2.3, 3.4, 4.2]         = [0.6, 1.2, 1.8, 2.1]
  [4.4, 5.5, 6.6, 8.4]           [2.8, 3.4, 4.0, 4.4]
  [9.6,10.7,11.8,12.6]           [5.0, 5.6, 6.2, 6.6]
  [13.4,14.5,15.6,16.4]          [6.8, 7.4, 8.0, 8.4]
```

**After ReLU:**
```
Output = ReLU(y)
(No change since all positive)

Final output ≈ input + small corrections
```

### Bottleneck Residual Block

**Plain English Overview:**

The bottleneck block is a more efficient variant that uses three layers instead of two, with 1×1 convolutions to reduce and restore dimensions. This reduces computational cost while maintaining capacity.

**Motivation:**

For deep networks (50+ layers), basic blocks become computationally expensive:
- 2 × (3×3 conv with 256 channels) = 2 × 589,824 = 1,179,648 operations per position

Bottleneck design:
- 1×1 conv (256→64): Reduce channels
- 3×3 conv (64→64): Process with fewer channels
- 1×1 conv (64→256): Restore channels
- Total: Much fewer operations!

**Structure:**

```
Input: x (256 channels)
  │
  ├──────────────────────────────────┐
  │                                  │
  ↓                                  │
┌──────────────────────┐             │
│ Conv 1×1, 64 filters │             │
│ BatchNorm            │ Reduce      │
│ ReLU                 │ dimensions  │
└──────────────────────┘             │
  │ (64 channels)                    │
  ↓                                  │
┌──────────────────────┐             │ Skip
│ Conv 3×3, 64 filters │             │ Connection
│ BatchNorm            │ Process     │
│ ReLU                 │             │
└──────────────────────┘             │
  │ (64 channels)                    │
  ↓                                  │
┌──────────────────────┐             │
│ Conv 1×1, 256 filters│             │
│ BatchNorm            │ Restore     │
└──────────────────────┘ dimensions  │
  │ (256 channels)                   │
  ↓                                  │
  └──────── + ────────────────────────┘
            │
            ↓
         ┌──────┐
         │ ReLU │
         └──────┘
            │
            ↓
       Output (256 channels)
```

**Why "Bottleneck"?**

```
Channel dimensions through block:

256 → 64 → 64 → 256
 │     │    │     │
Wide  Narrow  Narrow  Wide
      (bottleneck)

Shape visualization:
┌─────────────┐ 256 channels
└──────┬──────┘
       │ 1×1 conv reduces
    ┌──┴──┐ 64 channels (narrow bottleneck)
    │     │ 3×3 conv processes
    └──┬──┘ 64 channels
       │ 1×1 conv expands
┌──────┴──────┐ 256 channels
└─────────────┘
```

**Mathematical Formulation:**

```
# Reduce dimensions
h₁ = Conv₁×₁(x)        # 256 → 64 channels
h₁ = BatchNorm(h₁)
h₁ = ReLU(h₁)

# Process at reduced dimensions
h₂ = Conv₃×₃(h₁)       # 64 → 64 channels
h₂ = BatchNorm(h₂)
h₂ = ReLU(h₂)

# Restore dimensions
h₃ = Conv₁×₁(h₂)       # 64 → 256 channels
h₃ = BatchNorm(h₃)

# Skip and activate
y = h₃ + x
Output = ReLU(y)
```

**Parameter Comparison:**

**Basic Block (2 layers):**
```
Conv 3×3, 256→256: 3 × 3 × 256 × 256 = 589,824 parameters
Conv 3×3, 256→256: 3 × 3 × 256 × 256 = 589,824 parameters
Total: 1,179,648 parameters
```

**Bottleneck Block (3 layers):**
```
Conv 1×1, 256→64:  1 × 1 × 256 × 64 = 16,384 parameters
Conv 3×3, 64→64:   3 × 3 × 64 × 64 = 36,864 parameters
Conv 1×1, 64→256:  1 × 1 × 64 × 256 = 16,384 parameters
Total: 69,632 parameters (17× fewer!)
```

### Block Mathematical Details

**Complete Forward Pass Through Basic Block:**

**Given:**
- Input x: shape (N, C, H, W) where N=batch size
- Conv1: 3×3, C input channels, C output channels
- Conv2: 3×3, C input channels, C output channels

**Step-by-step with shapes:**

```
x:    (N, C, H, W)
  ↓ Conv1(3×3, pad=1, stride=1)
h₁:   (N, C, H, W)  # Same size due to padding
  ↓ BatchNorm
h₁:   (N, C, H, W)
  ↓ ReLU
h₁:   (N, C, H, W)
  ↓ Conv2(3×3, pad=1, stride=1)
h₂:   (N, C, H, W)  # Same size
  ↓ BatchNorm
F(x): (N, C, H, W)
  ↓ Add skip connection
y = F(x) + x: (N, C, H, W)  # Element-wise addition
  ↓ ReLU
Output: (N, C, H, W)
```

**Complete Forward Pass Through Bottleneck Block:**

**Given:**
- Input x: (N, C_in, H, W)
- Reduction factor: 4 (typical)
- C_bottleneck = C_in / 4

```
x:    (N, C_in, H, W)              e.g., (N, 256, H, W)
  ↓ Conv1×1(C_in → C_bottleneck)
h₁:   (N, C_bottleneck, H, W)      (N, 64, H, W) ← Reduced
  ↓ BatchNorm + ReLU
h₁:   (N, C_bottleneck, H, W)
  ↓ Conv3×3(C_bottleneck → C_bottleneck, pad=1)
h₂:   (N, C_bottleneck, H, W)      (N, 64, H, W) ← Bottleneck
  ↓ BatchNorm + ReLU
h₂:   (N, C_bottleneck, H, W)
  ↓ Conv1×1(C_bottleneck → C_in)
h₃:   (N, C_in, H, W)              (N, 256, H, W) ← Restored
  ↓ BatchNorm
F(x): (N, C_in, H, W)
  ↓ Add skip
y = F(x) + x: (N, C_in, H, W)
  ↓ ReLU
Output: (N, C_in, H, W)
```

### When to Use Each Block Type

**Use Basic Block When:**

```
✓ Shallow to medium networks (ResNet-18, ResNet-34)
✓ Smaller number of channels (< 128)
✓ Limited computational resources
✓ Smaller datasets
✓ Simplicity is preferred

Example: ResNet-18
- 18 layers total
- 8 basic residual blocks
- Channels: 64 → 128 → 256 → 512
```

**Use Bottleneck Block When:**

```
✓ Very deep networks (ResNet-50, ResNet-101, ResNet-152)
✓ Larger number of channels (≥ 256)
✓ Computational efficiency needed
✓ Large-scale datasets (ImageNet)
✓ Maximum capacity with controlled parameters

Example: ResNet-50
- 50 layers total
- 16 bottleneck residual blocks
- Channels: 256 → 512 → 1024 → 2048
```

**Comparison:**

```
┌─────────────────┬──────────────┬───────────────────┐
│                 │ Basic Block  │ Bottleneck Block  │
├─────────────────┼──────────────┼───────────────────┤
│ Layers          │ 2            │ 3                 │
│ Parameters      │ More per     │ Fewer per block   │
│                 │ block        │                   │
│ Computation     │ Higher       │ Lower (4× less)   │
│ Depth           │ Shallow      │ Deep networks     │
│                 │ networks     │                   │
│ Typical         │ ResNet-18/34 │ ResNet-50/101/152 │
│ Channel flow    │ C → C        │ C → C/4 → C       │
└─────────────────┴──────────────┴───────────────────┘
```

---

## Forward Propagation in ResNets

### Forward Plain English

Forward propagation in ResNets is similar to standard CNNs but with an additional step: we save the input and add it to the output after processing through the residual mapping.

**Think of it like this:** You're editing a document. Instead of rewriting the entire document from scratch (traditional network), you:
1. Keep the original (skip connection)
2. Make edits/corrections (residual mapping F(x))
3. Combine original + edits (addition)

The final result is your original work plus improvements, not a complete rewrite.

### Forward Basic Block

**Complete Algorithm:**

```
Algorithm: Forward Pass Through Basic Residual Block

Input:
  - x: Input feature map (N, C, H, W)
  - W₁, b₁: First conv layer parameters
  - W₂, b₂: Second conv layer parameters
  - γ₁, β₁: First BatchNorm parameters
  - γ₂, β₂: Second BatchNorm parameters

Steps:

1. Save input for skip connection:
   x_skip = x  # Copy (or just keep reference)

2. First convolution:
   h₁ = Conv3×3(x, W₁, b₁, padding=1, stride=1)
   h₁ = BatchNorm(h₁, γ₁, β₁)
   h₁ = ReLU(h₁)

3. Second convolution:
   h₂ = Conv3×3(h₁, W₂, b₂, padding=1, stride=1)
   h₂ = BatchNorm(h₂, γ₂, β₂)

4. Add skip connection:
   y = h₂ + x_skip  # Element-wise addition

5. Final activation:
   Output = ReLU(y)

6. Return Output, save x_skip and h₂ for backprop
```

**Visual Flow:**

```
Detailed computation flow:

x: [N, C, 56, 56]
│
├──────────────────── x_skip (saved)
│                            │
↓                            │
Conv₁(3×3, C→C, pad=1)        │
[N, C, 56, 56]                │
↓                            │
BatchNorm₁                    │
↓                            │
ReLU                         │
↓                            │
h₁: [N, C, 56, 56]            │
↓                            │
Conv₂(3×3, C→C, pad=1)        │
[N, C, 56, 56]                │
↓                            │
BatchNorm₂                    │
↓                            │
F(x): [N, C, 56, 56]          │
│                            │
└──────────── + ─────────────┘
               │
               ↓
        y: [N, C, 56, 56]
               ↓
             ReLU
               ↓
      Output: [N, C, 56, 56]
```

### Forward Bottleneck Block

**Complete Algorithm:**

```
Algorithm: Forward Pass Through Bottleneck Block

Input:
  - x: (N, C_in, H, W)
  - Reduction: 4 (typical)
  - C_mid = C_in / 4

Steps:

1. Save input:
   x_skip = x

2. Dimension reduction (1×1 conv):
   h₁ = Conv1×1(x, C_in → C_mid)
   h₁ = BatchNorm(h₁)
   h₁ = ReLU(h₁)
   Shape: (N, C_mid, H, W)

3. Spatial processing (3×3 conv):
   h₂ = Conv3×3(h₁, C_mid → C_mid, pad=1)
   h₂ = BatchNorm(h₂)
   h₂ = ReLU(h₂)
   Shape: (N, C_mid, H, W)

4. Dimension restoration (1×1 conv):
   h₃ = Conv1×1(h₂, C_mid → C_in)
   h₃ = BatchNorm(h₃)
   Shape: (N, C_in, H, W)

5. Add skip connection:
   y = h₃ + x_skip

6. Final activation:
   Output = ReLU(y)
   Shape: (N, C_in, H, W)

7. Return Output
```

**Visual Flow with Channel Counts:**

```
x: [N, 256, 28, 28]
│
├─────────────────────── x_skip
│                              │
↓                              │
Conv₁×₁(256→64)                 │
[N, 64, 28, 28] ← Reduced!      │
↓                              │
BatchNorm + ReLU               │
↓                              │
Conv₃×₃(64→64, pad=1)           │
[N, 64, 28, 28] ← Bottleneck    │
↓                              │
BatchNorm + ReLU               │
↓                              │
Conv₁×₁(64→256)                 │
[N, 256, 28, 28] ← Restored!    │
↓                              │
BatchNorm                      │
│                              │
└──────────── + ───────────────┘
               │
        y: [N, 256, 28, 28]
               ↓
             ReLU
               ↓
      Output: [N, 256, 28, 28]
```

### Forward Numerical Example

**Detailed Calculation Through Basic Block:**

**Input (simplified 2×2, single channel):**
```
x = [
  [1.0, 2.0],
  [3.0, 4.0]
]
```

**Conv1 (2×2 filter, no padding for simplicity):**
```
W₁ = [
  [0.1, 0.2],
  [0.3, 0.4]
]

h₁[0,0] = (1.0×0.1) + (2.0×0.2) + (3.0×0.3) + (4.0×0.4)
        = 0.1 + 0.4 + 0.9 + 1.6
        = 3.0

Result after Conv1 (1×1 output):
h₁ = [3.0]
```

**After BatchNorm (simplified, assume normalized):**
```
h₁ = [3.0]  (assume BatchNorm doesn't change for this example)
```

**After ReLU:**
```
h₁ = ReLU([3.0]) = [3.0]
```

**Conv2 with 2×2 filter to restore size (transpose conv or different approach):**

For this example, let's assume we use 1×1 convolution that projects back:

```
h₂ = Conv₁×₁(h₁)
# Project back to 2×2 using learned mapping
F(x) = [
  [0.5, 0.6],
  [0.7, 0.8]
]
```

**Add Skip Connection:**
```
y = F(x) + x
  = [0.5, 0.6] + [1.0, 2.0]
    [0.7, 0.8]   [3.0, 4.0]
  
  = [1.5, 2.6]
    [3.7, 4.8]
```

**Final ReLU:**
```
Output = ReLU(y) = [1.5, 2.6]
                   [3.7, 4.8]

Notice: Output ≈ Input + small corrections
        [1.5, 2.6]   [1.0, 2.0]   [0.5, 0.6]
        [3.7, 4.8] ≈ [3.0, 4.0] + [0.7, 0.8]
```

**More Realistic Example (3×3 with padding):**

**Input (4×4, 2 channels):**
```
x[:,:,0] = [              x[:,:,1] = [
  [1, 2, 3, 4],             [5, 6, 7, 8],
  [5, 6, 7, 8],             [1, 2, 3, 4],
  [9,10,11,12],             [9,10,11,12],
  [13,14,15,16]             [5, 6, 7, 8]
]                         ]
```

**After full residual block:**

Suppose F(x) learns small corrections:
```
F(x)[:,:,0] = [           F(x)[:,:,1] = [
  [0.1, 0.2, 0.1, 0.2],     [0.05, 0.1, 0.15, 0.1],
  [0.2, 0.3, 0.2, 0.1],     [0.1, 0.15, 0.1, 0.05],
  [0.1, 0.2, 0.3, 0.2],     [0.05, 0.1, 0.2, 0.15],
  [0.2, 0.1, 0.2, 0.1]      [0.1, 0.05, 0.1, 0.05]
]                         ]
```

**Output = F(x) + x:**
```
Output[:,:,0] = [         Output[:,:,1] = [
  [1.1, 2.2, 3.1, 4.2],     [5.05, 6.1, 7.15, 8.1],
  [5.2, 6.3, 7.2, 8.1],     [1.1, 2.15, 3.1, 4.05],
  [9.1,10.2,11.3,12.2],     [9.05,10.1,11.2,12.15],
  [13.2,14.1,15.2,16.1]     [5.1, 6.05, 7.1, 8.05]
]                         ]
```

**Observation:** Output ≈ Input, with F(x) making small refinements.

---

## Backward Propagation in ResNets

### Backward Plain English

Backward propagation in ResNets has a crucial advantage: gradients can flow through two paths - through the residual layers F(x) AND directly through the skip connection. This dual path prevents gradient vanishing.

**Analogy:** Imagine water flowing through a pipe system:
- **Traditional network**: Water must flow through every filter and valve (layers). If any valve is partially closed (gradient < 1), less water gets through each time.
- **ResNet**: Water has TWO paths - through the valves AND through a bypass pipe (skip connection). Even if the valve path slows down, the bypass ensures water keeps flowing.

### Gradient Flow

**Mathematical Foundation:**

For a residual block:
```
y = F(x) + x
```

Taking the gradient with respect to x:
```
∂y     ∂F(x)     ∂x      ∂F(x)
─── = ───── + ───── = ───── + I
∂x      ∂x       ∂x       ∂x
```

**Symbol Legend:**
- `∂y/∂x`: Gradient of output with respect to input
- `∂F(x)/∂x`: Gradient through the residual mapping
- `I`: Identity matrix
- `+`: The critical addition!

**Key Insight:** The `+ I` term (identity) ensures that gradients can always flow backward, even if ∂F(x)/∂x becomes very small!

**Chain Rule Through Multiple Blocks:**

For n residual blocks:

```
x₀ → Block1 → x₁ → Block2 → x₂ → ... → xₙ

Traditional network:
∂L     ∂L   ∂xₙ   ∂xₙ₋₁        ∂x₁
─── = ─── × ─── × ───── × ... × ───
∂x₀   ∂xₙ  ∂xₙ₋₁  ∂xₙ₋₂        ∂x₀

If each term < 1: product vanishes

ResNet:
∂xᵢ     ∂F(xᵢ₋₁)
──── = ───────── + I
∂xᵢ₋₁    ∂xᵢ₋₁

∂L     ∂L   (∂F(xₙ₋₁)    ) (∂F(xₙ₋₂)    )      (∂F(x₀)    )
─── = ─── × (──────── + I) × (──────── + I) × ... × (────── + I)
∂x₀   ∂xₙ   ( ∂xₙ₋₁      ) ( ∂xₙ₋₂      )      ( ∂x₀      )

Even if ∂F/∂x terms are small, the I terms accumulate!
Minimum gradient: 1ⁿ = 1 (never vanishes!)
```

**Visual Illustration:**

```
Gradient Flow - Traditional vs ResNet:

Traditional (56 layers):
∂L/∂x₀ = ∂L/∂x₅₆ × 0.9⁵⁶
       ≈ 0.002 × ∂L/∂x₅₆  ← Vanished!

┌─────┐ ∂=0.9  ┌─────┐ ∂=0.9  ┌─────┐
│ L56 │◄───────│ L55 │◄───────│ L54 │◄...
└─────┘        └─────┘        └─────┘
   Gradient gets smaller and smaller...

ResNet (56 layers with skip connections):
∂L/∂x₀ = ∂L/∂x₅₆ × (0.9⁵⁶ + 1)
       ≈ 1.002 × ∂L/∂x₅₆  ← Preserved!

┌─────┐        ┌─────┐        ┌─────┐
│ L56 │◄───────│ L55 │◄───────│ L54 │◄...
└──┬──┘        └──┬──┘        └──┬──┘
   │∂=0.9         │∂=0.9         │
   │              │              │
   └──── + ───────┴──── + ──────┴─── Skip paths
         ∂=1            ∂=1
   
   Highway for gradients!
```

### Why No Vanishing

**Detailed Mathematical Explanation:**

Consider the gradient of loss L with respect to early layer input x₀:

**Without Skip Connections:**
```
∂L      ∂L    ∂xₙ   ∂xₙ₋₁        ∂x₁
─── =  ─── × ──── × ───── × ... × ───
∂x₀    ∂xₙ   ∂xₙ₋₁  ∂xₙ₋₂        ∂x₀

For each block, if ∂xᵢ/∂xᵢ₋₁ = 0.8:
  
∂L/∂x₀ = ∂L/∂xₙ × 0.8ⁿ

For n=50: 0.8⁵⁰ ≈ 1.4 × 10⁻⁵ (effectively zero!)
```

**With Skip Connections:**
```
For residual block: xᵢ = F(xᵢ₋₁) + xᵢ₋₁

∂xᵢ     ∂F(xᵢ₋₁)     ∂xᵢ₋₁     ∂F(xᵢ₋₁)
──── = ──────── + ────── = ──────── + I
∂xᵢ₋₁    ∂xᵢ₋₁      ∂xᵢ₋₁      ∂xᵢ₋₁

Even if ∂F/∂x is small, the +I ensures ∂xᵢ/∂xᵢ₋₁ ≈ 1

Chain through n blocks:
∂L      ∂L   (∂F(xₙ₋₁)    ) (∂F(xₙ₋₂)    )      (∂F(x₁)    ) (∂F(x₀)    )
─── =  ─── × (──────── + I) × (──────── + I) × ... × (────── + I) × (────── + I)
∂x₀    ∂xₙ   ( ∂xₙ₋₁      ) ( ∂xₙ₋₂      )      ( ∂x₁      ) ( ∂x₀      )

Expanding (simplified): 
≈ ∂L/∂xₙ × (I + I + ... + I + other terms)
≈ ∂L/∂xₙ × (n × I + ...)

Minimum: ∂L/∂x₀ ≥ ∂L/∂xₙ (never vanishes!)
```

**Numerical Example:**

Suppose ∂F/∂x = 0.1 for each block (small gradients in residual path):

```
Single Block:
∂x₁/∂x₀ = ∂F(x₀)/∂x₀ + I
        = 0.1 + 1
        = 1.1  ← Gradient amplified!

Through 50 Blocks:
∂L/∂x₀ ≈ ∂L/∂x₅₀ × 1.1⁵⁰
       ≈ ∂L/∂x₅₀ × 117  ← Gradient may even grow!

This is much better than vanishing to ~0
```

### Backward Mathematical Derivation

**Complete Derivation for Basic Block:**

**Forward equations:**
```
h₁ = ReLU(BN(W₁ * x + b₁))
h₂ = BN(W₂ * h₁ + b₂)
y = h₂ + x
Output = ReLU(y)
```

**Backward pass - given ∂L/∂Output:**

**Step 1: Gradient through final ReLU**

```
∂L     ∂L       ∂ReLU(y)     ∂L
─── = ───── × ───────── = ───── × I[y > 0]
∂y   ∂Output      ∂y       ∂Output

where I[y > 0] is indicator function (1 if y>0, else 0)
```

**Step 2: Gradient splits at addition**

```
y = h₂ + x

∂L     ∂L   ∂y     ∂L
─── = ─── × ─── = ───  (gradient through skip path)
∂x    ∂y   ∂x     ∂y

∂L      ∂L   ∂y     ∂L
──── = ─── × ──── = ───  (gradient through residual path)
∂h₂    ∂y   ∂h₂    ∂y
```

**Key point:** BOTH paths receive the same gradient ∂L/∂y!

```
Visual:
        ∂L/∂y
          │
    ┌─────┴─────┐
    │           │
    ↓           ↓
 ∂L/∂x      ∂L/∂h₂
(to skip)  (to layers)

Gradient splits and flows through BOTH paths
```

**Step 3: Continue backprop through residual path**

```
∂L      ∂L     ∂h₂
──── = ──── × ────
∂W₂    ∂h₂    ∂W₂

∂L      ∂L     ∂h₂     ∂h₁
──── = ──── × ──── × ────
∂W₁    ∂h₂    ∂h₁     ∂W₁
```

**Complete gradient for input:**

```
∂L     ∂L   (∂F(x)    )     ∂L
─── = ─── × (───── + I) = ─── × (computed_gradient + 1)
∂x    ∂y   ( ∂x       )    ∂y
```

The `+I` term ensures gradient flows even if ∂F(x)/∂x is small!

### Backward Numerical Example

**Given from forward pass:**
- Input x: [1.0, 2.0; 3.0, 4.0]
- F(x): [0.5, 0.6; 0.7, 0.8]
- y: [1.5, 2.6; 3.7, 4.8]
- Output: [1.5, 2.6; 3.7, 4.8]

**Suppose ∂L/∂Output:**
```
∂L/∂Output = [
  [0.1, 0.2],
  [0.3, 0.4]
]
```

**Step 1: Gradient through final ReLU**

Since y > 0 everywhere:
```
∂L/∂y = ∂L/∂Output × I[y > 0]
      = [0.1, 0.2] × [1, 1]
        [0.3, 0.4]   [1, 1]
      = [0.1, 0.2]
        [0.3, 0.4]
```

**Step 2: Gradient splits at addition y = h₂ + x**

```
∂y/∂h₂ = I  (identity)
∂y/∂x = I   (identity)

Therefore:
∂L/∂h₂ = ∂L/∂y = [0.1, 0.2]  ← To residual path
                  [0.3, 0.4]

∂L/∂x = ∂L/∂y = [0.1, 0.2]   ← To skip path
(direct)         [0.3, 0.4]
```

**Step 3: Gradient through residual layers (simplified)**

Suppose gradients through F(x) layers give:
```
∂L/∂x = [0.02, 0.03]
(through F)  [0.04, 0.05]
```

**Step 4: Total gradient at input**

```
∂L/∂x        ∂L/∂x          ∂L/∂x
─────  =  ──────────  +  ───────────
(total)   (through F)     (through skip)

= [0.02, 0.03] + [0.1, 0.2]
  [0.04, 0.05]   [0.3, 0.4]

= [0.12, 0.23]
  [0.34, 0.45]
```

**Key Observation:**

```
Skip contribution:    [0.1, 0.2]    (dominant)
                      [0.3, 0.4]

Residual contribution:[0.02, 0.03]  (smaller)
                      [0.04, 0.05]

Total gradient preserved by skip connection!
Even if residual path has small gradients,
skip path ensures strong gradient flow.
```

**Visual Summary:**

```
Backpropagation Flow:

         ∂L/∂Output
              │
              ↓
         ∂L/∂y = 0.1
              │
        ┌─────┴─────┐
        │           │
        │           │ Skip: ∂L/∂x = 0.1
        │           │
        │           ↓
        │      Preserved!
        │
        ↓ Through F(x)
     0.1 × 0.2 = 0.02
        │
        ↓
  ∂L/∂x (from F) = 0.02
        │
        └──→ Total: 0.1 + 0.02 = 0.12

The skip path (0.1) dominates and preserves gradient!
```

---

## Practical Guidelines

### When to Use ResNets

**Use ResNets When:**

1. **Building very deep networks (>20 layers)**
   - ResNets enable training of 50-1000+ layer networks
   - Traditional networks degrade beyond ~20 layers

2. **Dealing with complex tasks**
   - Image classification on large datasets (ImageNet)
   - Object detection
   - Semantic segmentation
   - Face recognition

3. **Need strong baseline**
   - ResNets are proven architecture
   - Widely studied and understood
   - Many pre-trained models available

4. **Transfer learning**
   - Pre-trained ResNets available for many tasks
   - Transfer well to new domains
   - Starting point for fine-tuning

5. **When gradient flow is critical**
   - Very deep networks
   - Tasks requiring end-to-end training
   - When batch sizes are small (gradients noisier)

**Don't Need ResNets When:**

1. **Shallow networks sufficient (<20 layers)**
   - Simple tasks
   - Small datasets
   - Computational constraints

2. **Other architectures better suited**
   - Transformer for NLP/Vision Transformers
   - U-Net for medical imaging
   - GAN architectures for generation

3. **Extreme efficiency needed**
   - MobileNets (depthwise separable)
   - EfficientNets (compound scaling)
   - Pruned/quantized models

### Implementation Best Practices

**1. Skip Connection Dimension Matching:**

```python
# Pseudocode

# Identity shortcut (dimensions match):
if input.channels == output.channels and input.size == output.size:
    shortcut = input  # Direct addition
    
# Projection shortcut (dimensions differ):
else:
    shortcut = Conv1x1(input, 
                       filters=output.channels,
                       stride=downsample_factor)
    shortcut = BatchNorm(shortcut)
    # Note: No ReLU on projection!

output = F(input) + shortcut
output = ReLU(output)
```

**2. Batch Normalization Placement:**

```
Recommended order in each conv layer:
Conv → BatchNorm → ReLU

Exception: Last layer before skip addition:
Conv → BatchNorm (NO ReLU here!)
Then: Add skip → ReLU

Why: ReLU after addition allows negative residuals
```

**3. Initialization:**

```python
# He initialization for Conv layers
std = sqrt(2.0 / (kernel_size² × input_channels))
weights ~ Normal(0, std)

# Special for last conv in residual block:
# Initialize to zero or very small values
# So initial F(x) ≈ 0 → easier training start

last_conv.weights = zeros() or Normal(0, 0.0001)
```

**4. Learning Rate Scheduling:**

```
ResNets benefit from:
- Warmup period (gradual LR increase)
- Cosine annealing
- Step decay at specific epochs

Example schedule:
Epochs 1-5: LR warmup 0 → 0.1
Epochs 6-30: LR = 0.1
Epochs 31-60: LR = 0.01
Epochs 61-90: LR = 0.001
```

**5. Data Augmentation:**

```
Essential for ResNets on image tasks:
✓ Random crops
✓ Random horizontal flips
✓ Color jittering
✓ Random erasing
✓ Mixup/Cutmix (advanced)

Why: ResNets have many parameters, need regularization
```

### Common Patterns

**Pattern 1: Progressive Downsampling**

```
Standard ResNet Pattern:

Input: 224×224×3
    ↓
Conv 7×7, stride=2 → 112×112×64
    ↓
MaxPool 3×3, stride=2 → 56×56×64
    ↓
Residual blocks (stride=1) → 56×56×64
    ↓
Residual block (stride=2) → 28×28×128  ← Downsample
    ↓
Residual blocks (stride=1) → 28×28×128
    ↓
Residual block (stride=2) → 14×14×256  ← Downsample
    ↓
... continue pattern

Key: Stride=2 blocks for downsampling,
     stride=1 blocks for feature extraction
```

**Pattern 2: Channel Doubling at Downsampling**

```
When spatial size halves, double channels:

56×56×64  → stride=2 → 28×28×128  (2× channels)
28×28×128 → stride=2 → 14×14×256  (2× channels)
14×14×256 → stride=2 → 7×7×512    (2× channels)

Why: Maintains computational balance
     (Halving spatial → 1/4 positions,
      Doubling channels → 2× computation per position
      Net: 1/2 total computation)
```

**Pattern 3: Residual Block Stacking**

```
Typical grouping:
┌─────────────────────────────────┐
│ Residual Block (stride=1)       │
│ Residual Block (stride=1)       │
│ Residual Block (stride=1)       │  3-4 blocks
│ Residual Block (stride=2)       │  ← Downsample
└─────────────────────────────────┘

Repeat with increasing channels:
64 → 128 → 256 → 512 → 1024 → 2048
```

**Pattern 4: Pre-activation vs Post-activation**

```
Original ResNet (Post-activation):
x → Conv → BN → ReLU → Conv → BN → (+x) → ReLU

Improved ResNet (Pre-activation):
x → BN → ReLU → Conv → BN → ReLU → Conv → (+x)

Pre-activation allows:
- Cleaner gradient flow
- No ReLU after addition
- Better regularization
```

### Troubleshooting

**Problem 1: NaN or Inf During Training**

```
Symptoms:
- Loss becomes NaN
- Gradients explode
- Training unstable

Solutions:
✓ Check learning rate (try 10× smaller)
✓ Use gradient clipping: clip_norm = 5.0
✓ Verify batch normalization is working
✓ Check for bugs in skip connection addition
✓ Ensure projection shortcuts use correct stride
```

**Problem 2: Skip Connections Not Helping**

```
Symptoms:
- ResNet performs same as network without skips
- Gradient flow still poor

Solutions:
✓ Verify skip connections actually bypass layers
✓ Check dimension matching (shapes must align)
✓ Ensure proper initialization of residual branch
✓ Verify BatchNorm placement
✓ Try pre-activation variant
```

**Problem 3: Memory Issues**

```
Symptoms:
- Out of memory errors
- Can't fit desired batch size

Solutions:
✓ Reduce batch size
✓ Use gradient checkpointing (trade compute for memory)
✓ Use mixed precision training (FP16)
✓ Reduce input image size
✓ Use bottleneck blocks instead of basic blocks
```

**Problem 4: Slow Convergence**

```
Symptoms:
- Training loss decreases slowly
- Takes many epochs to converge

Solutions:
✓ Use learning rate warmup
✓ Increase learning rate (if stable)
✓ Check data augmentation (not too aggressive)
✓ Verify batch normalization momentum
✓ Try different optimizer (Adam vs SGD)
```

**Debugging Checklist:**

```
✓ Verify skip connections work:
  - Check output = F(x) + x, not just F(x)
  - Ensure shapes match for addition
  - Test with identity blocks (should pass input through)

✓ Check gradient flow:
  - Monitor gradient magnitudes in early layers
  - Should NOT decrease drastically with depth
  - Use gradient norm tracking

✓ Validate architecture:
  - Print layer-by-layer shapes
  - Verify downsampling occurs as expected
  - Check parameter counts match expectations

✓ Monitor training:
  - Plot training and validation loss
  - Watch for overfitting (gap between train/val)
  - Check gradient norms don't explode or vanish
```

---

**Summary: Key Concepts**

**1. The Problem:**
```
Deep networks (>20 layers) degraded in performance
Not overfitting - training error also increased
Traditional networks couldn't learn identity mappings easily
```

**2. The Solution:**
```
Skip connections: y = F(x) + x
Learn residual F(x) instead of direct mapping H(x)
Easier to learn F(x) ≈ 0 than H(x) = x
```

**3. Key Benefits:**
```
✓ Enables very deep networks (100+ layers)
✓ Solves gradient vanishing: ∂y/∂x includes +I term
✓ Easier optimization (residual learning)
✓ State-of-the-art performance on many tasks
```

**4. Two Block Types:**
```
Basic: 2 layers (3×3 → 3×3)
  - For ResNet-18, ResNet-34
  - Simpler, more parameters per block

Bottleneck: 3 layers (1×1 → 3×3 → 1×1)
  - For ResNet-50, ResNet-101, ResNet-152
  - More efficient, fewer parameters
```

**5. Critical Implementation Details:**
```
- Same padding to maintain dimensions
- Projection shortcuts when dimensions change
- BatchNorm before ReLU
- No ReLU on skip connection
- Proper initialization
```

**6. Mathematical Core:**
```
Forward: y = F(x, {Wᵢ}) + x

Backward: ∂L/∂x = ∂L/∂y × (∂F(x)/∂x + I)
                            \___  ___/
                                \/
                          Never vanishes!
```

---

**End of Residual Networks Tutorial**

This completes the comprehensive guide to Residual Networks. The document covers:

1. **The degradation problem** and why deep networks failed before ResNets
2. **Skip connections** as the key innovation
3. **Two types of residual blocks** (basic and bottleneck)
4. **Forward propagation** with detailed examples
5. **Backward propagation** and why gradients don't vanish
6. **Practical implementation** guidelines and troubleshooting

**Key Takeaway:** ResNets revolutionized deep learning by making it possible to train very deep networks through the elegant solution of skip connections. The principle "learn the residual" is simpler than "learn the mapping" and has enabled many subsequent advances in deep learning.

Remember: When building deep CNNs (>20 layers), **always use residual connections** to ensure proper gradient flow and enable effective training!