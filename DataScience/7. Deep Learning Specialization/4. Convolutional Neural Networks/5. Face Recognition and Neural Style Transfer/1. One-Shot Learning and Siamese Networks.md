# One-Shot Learning and Siamese Networks

## Table of Contents

1. [One-Shot Learning](#one-shot-learning)
2. [Introduction to Siamese Networks](#introduction-to-siamese-networks)
3. [Siamese Network Architecture](#siamese-network-architecture)
4. [Similarity Learning](#similarity-learning)
5. [Training Siamese Networks](#training-siamese-networks)
6. [Detailed Examples](#detailed-examples)
7. [Forward Propagation](#forward-propagation)
8. [Backward Propagation](#backward-propagation)

---

## One-Shot Learning

### What is One-Shot Learning?

**Plain English Overview:**

One-shot learning is the ability to recognize or classify objects from just a single example. Unlike traditional machine learning that requires hundreds or thousands of examples per class, one-shot learning can learn from one (or very few) examples.

**Analogy:** Imagine meeting a new person at a party. After seeing their face just once, you can recognize them later in a crowd, even if they're wearing different clothes or the lighting is different. You don't need to see them 1,000 times to remember them - just once is enough. That's one-shot learning.

### The Face Recognition Problem

**Traditional Classification Problem:**

```
Training a normal face classifier:

Person 1: Need 100+ photos
Person 2: Need 100+ photos
Person 3: Need 100+ photos
Person 100: Need 100+ photos

Total: 10,000+ images for 100 people

Problems:
1. Need many images per person
2. Fixed number of people (100 classes)
3. Adding new person requires retraining entire network
4. Impractical for real-world face recognition systems
```

**Face Recognition Reality:**

```
Company with 10,000 employees:
- Each person has 1-2 photos in database
- New employees added daily
- Can't retrain network every time
- Need instant recognition with minimal examples

One-shot learning solves this!
```

**Visual Comparison:**

```
TRADITIONAL CLASSIFICATION:
Training:
Person A: [ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·...100 photos]
Person B: [ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·...100 photos]
Person C: [ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·ğŸ“·...100 photos]

Network learns: "These patterns = Person A, those = Person B"

Testing:
New photo â†’ Network â†’ "Person B" (from learned patterns)

Adding Person D:
Need 100 new photos + retrain entire network!


ONE-SHOT LEARNING:
Enrollment:
Person A: [ğŸ“·] single photo â†’ embedding vector [0.2, 0.8, 0.3, ...]
Person B: [ğŸ“·] single photo â†’ embedding vector [0.5, 0.1, 0.7, ...]
Person C: [ğŸ“·] single photo â†’ embedding vector [0.9, 0.3, 0.2, ...]

Store embeddings in database

Verification:
New photo â†’ embedding â†’ compare with stored embeddings â†’ match/no match

Adding Person D:
Just compute embedding for their photo, add to database!
No retraining needed!
```

### Why Traditional Approaches Fail

**The Mathematical Problem:**

For traditional softmax classification with N people:

```
Final layer: FC with N outputs (one per person)
Output[i] = P(person is person_i)

Problems:
1. Fixed number of outputs N
2. Needs many examples per person to learn weights
3. New person = change architecture + retrain

Example:
100 people Ã— 512 input features = 51,200 parameters in final layer
Each parameter needs sufficient data to train
â†’ Need many examples per person
```

**The One-Shot Solution:**

Instead of classifying to fixed categories, learn a similarity function:

```
Similarity(image1, image2) â†’ score (0 to 1)

High score: Same person
Low score: Different people

No fixed categories!
Just compare pairs of images.
```

---

## Introduction to Siamese Networks

### What are Siamese Networks?

**Plain English Overview:**

A Siamese network is a neural network architecture with two identical sub-networks (twins) that share the exact same weights. These twins process two different inputs separately, then we compare their outputs to determine how similar the inputs are.

**Analogy:** Think of fingerprint matching. You have two fingerprint scanners (the twin networks) with identical technology. You scan the left thumb with one scanner and the right thumb with another scanner. Both scanners produce a numerical code (embedding) for the fingerprint pattern. Then you compare the codes - if they're similar, the fingerprints match. The scanners use the same technology (shared weights), but process different inputs.

**Visual Concept:**

```
SIAMESE NETWORK STRUCTURE:

Image 1 (Person A)           Image 2 (Person B)
     ğŸ“·                           ğŸ“·
     â†“                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Network 1  â”‚             â”‚  Network 2  â”‚
â”‚             â”‚             â”‚             â”‚
â”‚  (Conv      â”‚             â”‚  (Conv      â”‚
â”‚   layers)   â”‚  IDENTICAL  â”‚   layers)   â”‚  â† Same weights!
â”‚             â”‚    TWINS    â”‚             â”‚
â”‚             â”‚             â”‚             â”‚
â”‚  Shared     â”‚             â”‚  Shared     â”‚
â”‚  Weights    â”‚             â”‚  Weights    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“                            â†“
Embedding 1:                Embedding 2:
[0.2, 0.7, 0.1, ...]       [0.5, 0.3, 0.8, ...]
     â†“                            â†“
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
          Compute Distance
                â”‚
                â†“
            Distance = 0.72
                â†“
        If distance < threshold:
           "Same person"
        Else:
           "Different people"
```

### Why Siamese Networks Solve One-Shot Learning

**The Key Insight:**

Instead of learning to classify faces into categories, Siamese networks learn a similarity metric. They transform faces into embeddings where similar faces have nearby embeddings and different faces have distant embeddings.

**Traditional vs Siamese:**

```
TRADITIONAL CLASSIFIER:
Training: Learn patterns for each specific person
Testing: Match new photo to learned patterns
Problem: Can't handle people not seen during training!

Database has Person A, B, C
New person D appears â†’ FAIL (not in training set)


SIAMESE NETWORK:
Training: Learn what makes faces similar/different
Testing: Compare embeddings of any two faces
Advantage: Works for NEW people not seen during training!

Training on: Person X, Y, Z (learning similarity metric)
Testing on: Person A, B, C, D (completely different people)
â†’ SUCCESS! Just needs to compare embeddings
```

**Why This Works:**

```
Network learns a general similarity function:

Training teaches:
"Same person images â†’ similar embeddings (close in space)"
"Different person images â†’ different embeddings (far in space)"

This knowledge transfers to new people!

Like learning to measure distances:
Once you know how to use a ruler, you can measure
ANY objects, not just the ones you practiced with.
```

---

## Siamese Network Architecture

### Twin Network Structure

**Complete Architecture:**

```
INPUT PAIR:
Image A (100Ã—100Ã—3)              Image B (100Ã—100Ã—3)
        â†“                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ENCODER NETWORK      â”‚  â”‚   ENCODER NETWORK      â”‚
â”‚   (Shared Weights!)    â”‚  â”‚   (Shared Weights!)    â”‚
â”‚                        â”‚  â”‚                        â”‚
â”‚   Conv 3Ã—3, 64        â”‚  â”‚   Conv 3Ã—3, 64        â”‚
â”‚   â†“                    â”‚  â”‚   â†“                    â”‚
â”‚   Pool 2Ã—2            â”‚  â”‚   Pool 2Ã—2            â”‚
â”‚   â†“                    â”‚  â”‚   â†“                    â”‚
â”‚   Conv 3Ã—3, 128       â”‚  â”‚   Conv 3Ã—3, 128       â”‚
â”‚   â†“                    â”‚  â”‚   â†“                    â”‚
â”‚   Pool 2Ã—2            â”‚  â”‚   Pool 2Ã—2            â”‚
â”‚   â†“                    â”‚  â”‚   â†“                    â”‚
â”‚   Conv 3Ã—3, 256       â”‚  â”‚   Conv 3Ã—3, 256       â”‚
â”‚   â†“                    â”‚  â”‚   â†“                    â”‚
â”‚   Flatten             â”‚  â”‚   Flatten             â”‚
â”‚   â†“                    â”‚  â”‚   â†“                    â”‚
â”‚   FC â†’ 128 dims       â”‚  â”‚   FC â†’ 128 dims       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                                â†“
    Embedding A                     Embedding B
    [eâ‚, eâ‚‚, ..., eâ‚â‚‚â‚ˆ]           [fâ‚, fâ‚‚, ..., fâ‚â‚‚â‚ˆ]
        â†“                                â†“
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
            DISTANCE COMPUTATION
                     â†“
        d = ||Embedding_A - Embedding_B||
                     â†“
              Distance = 2.47
                     â†“
        If d < threshold (e.g., 0.5):
            Output: "SAME person"
        Else:
            Output: "DIFFERENT people"
```

**Weight Sharing:**

```
Critical property: Both networks share the EXACT SAME weights!

Network 1 weights = Network 2 weights

Why?
- Ensures consistent embeddings
- Same face always maps to same embedding
- Embedding(Face_A, Network1) = Embedding(Face_A, Network2)

Implementation:
Actually use ONE network, pass images through it sequentially
Or create two network instances with tied weights
```

**Mathematical Formulation:**

```
Let f(x; Î¸) be the encoder network with parameters Î¸

For image pair (xâ‚, xâ‚‚):

Embeddingâ‚ = f(xâ‚; Î¸)  âˆˆ â„áµˆ
Embeddingâ‚‚ = f(xâ‚‚; Î¸)  âˆˆ â„áµˆ

Where:
- Î¸: Shared network parameters (weights)
- d: Embedding dimension (e.g., 128)
- f(): Encoder function (series of conv, pool, FC layers)

Distance:
D(xâ‚, xâ‚‚) = ||f(xâ‚; Î¸) - f(xâ‚‚; Î¸)||

Common distance metrics:
- L2: ||eâ‚ - eâ‚‚||â‚‚ = âˆš(Î£áµ¢(eâ‚áµ¢ - eâ‚‚áµ¢)Â²)
- L1: ||eâ‚ - eâ‚‚||â‚ = Î£áµ¢|eâ‚áµ¢ - eâ‚‚áµ¢|
- Cosine: 1 - (eâ‚Â·eâ‚‚)/(||eâ‚|| ||eâ‚‚||)
```

### Embedding Layer

**Purpose:**

The embedding layer transforms the input image into a fixed-dimensional vector (embedding) that captures the essential features of the face in a compact representation.

**Properties of Good Embeddings:**

```
1. Same person â†’ Similar embeddings
   f(person_A_photo1) â‰ˆ f(person_A_photo2)
   Small distance

2. Different people â†’ Different embeddings
   f(person_A) â‰  f(person_B)
   Large distance

3. Compact: 128-512 dimensions (vs 100Ã—100Ã—3 = 30,000 input dimensions)

4. Meaningful: Nearby embeddings = similar faces
```

**Visual Embedding Space:**

```
2D projection of 128D embedding space:

      â†‘
      â”‚
    Pâ‚â”‚  Ã— Ã—        Person 1's photos
      â”‚  Ã— Ã—        clustered together
      â”‚
    Pâ‚‚â”‚        ğŸ”· ğŸ”·  Person 2's photos
      â”‚        ğŸ”·     clustered together
      â”‚
    Pâ‚ƒâ”‚              â–  â–   Person 3's photos
      â”‚              â–     clustered together
      â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’

Key properties:
- Photos of same person cluster together
- Different people's clusters are separated
- Distance between clusters > distance within cluster
```

### Distance Computation

**L2 Distance (Euclidean):**

Most common in Siamese networks.

```
For embeddings eâ‚ = [eâ‚â‚, eâ‚â‚‚, ..., eâ‚â‚™] and eâ‚‚ = [eâ‚‚â‚, eâ‚‚â‚‚, ..., eâ‚‚â‚™]:

L2_distance = âˆš(Î£áµ¢(eâ‚áµ¢ - eâ‚‚áµ¢)Â²)

Step-by-step:
1. Subtract: diff = eâ‚ - eâ‚‚
2. Square each element: diffÂ²
3. Sum: Î£ diffÂ²
4. Square root: âˆšsum

Example:
eâ‚ = [0.5, 0.8, 0.3]
eâ‚‚ = [0.6, 0.7, 0.4]

diff = [0.5-0.6, 0.8-0.7, 0.3-0.4] = [-0.1, 0.1, -0.1]
diffÂ² = [0.01, 0.01, 0.01]
sum = 0.03
distance = âˆš0.03 = 0.173
```

---

## Similarity Learning

### Learning to Measure Similarity

**The Goal:**

Train the network so that:
```
Distance(same_person_photo1, same_person_photo2) < threshold
Distance(different_person_photo1, different_person_photo2) > threshold
```

**How Learning Works:**

```
Initially (random weights):
Person A photo 1 â†’ [random embedding]
Person A photo 2 â†’ [different random embedding]
Distance = large (BAD! Same person should be close)

Person A photo 1 â†’ [random embedding]
Person B photo 1 â†’ [different random embedding]
Distance = could be small (BAD! Different people should be far)

After training:
Person A photo 1 â†’ [0.2, 0.7, 0.5, ...]
Person A photo 2 â†’ [0.21, 0.69, 0.51, ...] â† Very similar!
Distance = 0.02 (GOOD! Same person, small distance)

Person A photo 1 â†’ [0.2, 0.7, 0.5, ...]
Person B photo 1 â†’ [0.8, 0.1, 0.9, ...] â† Very different!
Distance = 0.95 (GOOD! Different people, large distance)
```

### Embedding Space Concept

**What is Embedding Space?**

A high-dimensional space (e.g., 128 dimensions) where each face is represented as a point. The network learns to position points such that:
- Similar faces are close together
- Different faces are far apart

**Visual 2D Projection:**

```
Embedding Space (128D, shown in 2D):

        Dimension 2
            â†‘
            â”‚
      0.8   â”‚     A2
            â”‚  A1  Ã—
            â”‚  Ã—
      0.6   â”‚
            â”‚          B1
      0.4   â”‚      B2  â—‹
            â”‚       â—‹
      0.2   â”‚
            â”‚               C1
      0.0   â”‚              C2 â—†
            â”‚               â—†
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Dimension 1
            0.0   0.2  0.4  0.6  0.8

Where:
A1, A2 = Two photos of Person A (close together)
B1, B2 = Two photos of Person B (close together)
C1, C2 = Two photos of Person C (close together)

Distance within person: small (~0.1)
Distance between people: large (~0.5-0.8)
```

### Distance Metrics

**L1 Distance (Manhattan):**

```
d_L1 = Î£áµ¢ |eâ‚áµ¢ - eâ‚‚áµ¢|

Geometric interpretation: Distance traveling along grid lines

Example:
eâ‚ = [0.5, 0.8, 0.3]
eâ‚‚ = [0.6, 0.7, 0.4]

d_L1 = |0.5-0.6| + |0.8-0.7| + |0.3-0.4|
     = 0.1 + 0.1 + 0.1
     = 0.3
```

**L2 Distance (Euclidean):**

```
d_L2 = âˆš(Î£áµ¢(eâ‚áµ¢ - eâ‚‚áµ¢)Â²)

Geometric interpretation: Straight-line distance

Example (same vectors):
eâ‚ = [0.5, 0.8, 0.3]
eâ‚‚ = [0.6, 0.7, 0.4]

d_L2 = âˆš((0.1)Â² + (0.1)Â² + (0.1)Â²)
     = âˆš0.03
     = 0.173
```

**Cosine Similarity:**

```
similarity = (eâ‚ Â· eâ‚‚) / (||eâ‚|| ||eâ‚‚||)
distance = 1 - similarity

Measures angle between vectors, not magnitude

Example:
eâ‚ = [0.5, 0.8, 0.3]
eâ‚‚ = [0.6, 0.7, 0.4]

eâ‚ Â· eâ‚‚ = 0.5Ã—0.6 + 0.8Ã—0.7 + 0.3Ã—0.4 = 0.3 + 0.56 + 0.12 = 0.98
||eâ‚|| = âˆš(0.25 + 0.64 + 0.09) = âˆš0.98 = 0.99
||eâ‚‚|| = âˆš(0.36 + 0.49 + 0.16) = âˆš1.01 = 1.00

similarity = 0.98 / (0.99 Ã— 1.00) = 0.99
distance = 1 - 0.99 = 0.01 (very similar!)
```

### What Makes Good Embeddings

**Desired Properties:**

1. **Discriminative:** Different people â†’ far apart embeddings
2. **Invariant:** Same person (different poses/lighting) â†’ close embeddings
3. **Compact:** Low dimensionality (128-512 dims)
4. **Normalized:** Often unit vectors for cosine similarity

**Training Goal:**

```
Minimize intra-class distance (same person):
d(person_i_photo1, person_i_photo2) â†’ small

Maximize inter-class distance (different people):
d(person_i, person_j) â†’ large

Margin:
inter_class_distance - intra_class_distance > margin
Different people should be farther apart than threshold
```

---

## Training Siamese Networks

### Contrastive Loss Function

**Plain English:**

Contrastive loss trains the network by:
- Making embeddings close for similar pairs (same person)
- Making embeddings far for dissimilar pairs (different people)

**Mathematical Formula:**

```
L_contrastive = (1/2N) Ã— Î£ [y Ã— dÂ² + (1-y) Ã— max(margin - d, 0)Â²]
                        pairs

Symbol Legend:
- N: Number of pairs
- y: Label (1 if same person, 0 if different)
- d: Distance between embeddings
- margin: Minimum desired distance for different people (e.g., 1.0)
- max(margin - d, 0): Penalty when different pairs are too close
```

**Breaking Down the Formula:**

```
For SIMILAR pair (same person, y=1):
Loss = (1/2) Ã— dÂ²

Minimizes distance:
- d=0 â†’ loss=0 (perfect! embeddings identical)
- d=0.5 â†’ loss=0.125 (okay, somewhat close)
- d=2.0 â†’ loss=2.0 (bad! same person should be closer)

For DISSIMILAR pair (different people, y=0):
Loss = (1/2) Ã— max(margin - d, 0)Â²

Penalizes if distance < margin:
- d=2.0, margin=1.0 â†’ loss=0 (good! far enough)
- d=0.5, margin=1.0 â†’ loss=(1.0-0.5)Â²/2 = 0.125 (penalty for being too close)
- d=0.1, margin=1.0 â†’ loss=(1.0-0.1)Â²/2 = 0.405 (large penalty!)
```

**Example Calculation:**

```
Pair 1 (same person):
eâ‚ = [0.5, 0.8, 0.2]
eâ‚‚ = [0.52, 0.79, 0.21]
d = 0.0245
y = 1

Lossâ‚ = (1/2) Ã— (0.0245)Â² = 0.0003

Pair 2 (different people):
eâ‚ = [0.5, 0.8, 0.2]
eâ‚ƒ = [0.9, 0.1, 0.7]
d = 0.74
y = 0
margin = 1.0

Lossâ‚‚ = (1/2) Ã— max(1.0 - 0.74, 0)Â²
      = (1/2) Ã— (0.26)Â²
      = 0.0338

Total Loss = (Lossâ‚ + Lossâ‚‚) / 2 = 0.017
```

### Triplet Loss Function

**Plain English:**

Triplet loss uses three images at once:
- Anchor: Reference image
- Positive: Same person as anchor
- Negative: Different person from anchor

Goal: Make distance(anchor, positive) much smaller than distance(anchor, negative).

**Visual Concept:**

```
TRIPLET TRAINING EXAMPLE:

Anchor           Positive         Negative
(Person A,       (Person A,       (Person B)
 photo 1)         photo 2)
   ğŸ“·               ğŸ“·              ğŸ“·
   â†“                â†“               â†“
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
            â†“                        â†“
        Should be CLOSE          Should be FAR
            â†“                        â†“
     d(A,P) = small           d(A,N) = large
         
Goal: d(A,N) - d(A,P) > margin

If already satisfied: loss = 0
If not satisfied: push them apart more
```

**Mathematical Formula:**

```
L_triplet = Î£ max(d(A,P) - d(A,N) + margin, 0)
           triplets

Symbol Legend:
- A: Anchor image
- P: Positive image (same person as A)
- N: Negative image (different person from A)
- d(x,y): Distance between embeddings of x and y
- margin: Desired separation (e.g., 0.2)
- max(..., 0): Only penalize if margin violated
```

**Breaking Down:**

```
Ideal case:
d(A,P) = 0.1  (anchor and positive close)
d(A,N) = 0.9  (anchor and negative far)
margin = 0.2

d(A,N) - d(A,P) = 0.9 - 0.1 = 0.8 > 0.2 âœ“
Loss = max(0.1 - 0.9 + 0.2, 0) = max(-0.6, 0) = 0

Good case:
d(A,P) = 0.2
d(A,N) = 0.5
margin = 0.2

d(A,N) - d(A,P) = 0.5 - 0.2 = 0.3 > 0.2 âœ“
Loss = max(0.2 - 0.5 + 0.2, 0) = max(-0.1, 0) = 0

Bad case (needs correction):
d(A,P) = 0.4
d(A,N) = 0.5
margin = 0.2

d(A,N) - d(A,P) = 0.5 - 0.4 = 0.1 < 0.2 âœ—
Loss = max(0.4 - 0.5 + 0.2, 0) = max(0.1, 0) = 0.1

Network will adjust to push N farther or pull P closer
```

### Positive and Negative Pairs

**Creating Training Pairs:**

**For Contrastive Loss:**

```
Dataset: 100 people, 10 photos each = 1,000 images

Create pairs:

Positive pairs (same person):
Person 1: (photo1, photo2), (photo1, photo3), ..., (photo9, photo10)
         â†’ (10 choose 2) = 45 pairs per person
         Ã— 100 people = 4,500 positive pairs

Negative pairs (different people):
Person 1 photo 1 + Person 2 photo 1
Person 1 photo 1 + Person 3 photo 1
...
Approximately 100 Ã— 100 Ã— 10 Ã— 10 = 1,000,000 possible pairs
Sample subset: 10,000 negative pairs

Total training pairs: 4,500 positive + 10,000 negative = 14,500 pairs
```

**For Triplet Loss:**

```
Create triplets (Anchor, Positive, Negative):

Person 1: Anchor=photo1, Positive=photo2, Negative=Person2_photo1
Person 1: Anchor=photo1, Positive=photo3, Negative=Person3_photo1
...

Per person: 9 positives Ã— 99 negatives = 891 triplets
Ã— 100 people = 89,100 triplets

Sample subset for efficient training
```

### Hard Negative Mining

**The Problem:**

Most negative pairs are too easy (very different faces):

```
Easy negative: 
Anchor: Young woman
Negative: Old man with beard
Distance: 2.5 (very far)
Loss: 0 (already satisfies margin)

Network learns nothing from easy negatives!
```

**Hard Negative Mining:**

Select negatives that are difficult:

```
Hard negative:
Anchor: Person A
Negative: Person B who looks similar to A
Distance: 0.35 (too close for comfort!)
Loss: >0 (violates margin)

Network forced to learn subtle differences!
```

**Strategy:**

```
1. Online hard negative mining:
   During training, from a batch:
   - Compute all embeddings
   - For each anchor, find hardest negative
     (different person but closest embedding)
   - Use these hard negatives for loss

2. Semi-hard negative mining:
   Select negatives where:
   d(A,P) < d(A,N) < d(A,P) + margin
   
   Not too easy (d(A,N) >> d(A,P) + margin)
   Not too hard (d(A,N) < d(A,P))
```

---

## Detailed Examples

### Example 1: Simple Pair Comparison

**Scenario:** Verify if two face images are the same person

**Given:**
```
Image 1: 64Ã—64Ã—3 (Person A, frontal)
Image 2: 64Ã—64Ã—3 (Person A, side profile)
```

**Encoder Network:**
```
Input: 64Ã—64Ã—3
Conv 3Ã—3, 32 filters â†’ 64Ã—64Ã—32
MaxPool 2Ã—2 â†’ 32Ã—32Ã—32
Conv 3Ã—3, 64 filters â†’ 32Ã—32Ã—64
MaxPool 2Ã—2 â†’ 16Ã—16Ã—64
Conv 3Ã—3, 128 filters â†’ 16Ã—16Ã—128
MaxPool 2Ã—2 â†’ 8Ã—8Ã—128
Flatten â†’ 8Ã—8Ã—128 = 8,192
FC â†’ 128 (embedding dimension)

Output: 128-dimensional embedding vector
```

**Forward Pass:**

```
Image 1 â†’ Network â†’ Embedding 1: [0.12, 0.45, 0.67, 0.23, ...]  (128 values)
Image 2 â†’ Network â†’ Embedding 2: [0.15, 0.43, 0.69, 0.21, ...]  (128 values)

Compute L2 distance:
d = âˆš(Î£áµ¢(e1áµ¢ - e2áµ¢)Â²)
  = âˆš((0.12-0.15)Â² + (0.45-0.43)Â² + (0.67-0.69)Â² + ...)
  = âˆš(0.0009 + 0.0004 + 0.0004 + ... (128 terms))
  = âˆš0.0856
  = 0.293

Decision:
If threshold = 0.5:
  d = 0.293 < 0.5 â†’ "SAME person" âœ“
```

### Example 2: Computing Embeddings

**Step-by-Step Through Network:**

**Input Image:**
```
Person face (64Ã—64Ã—3)
Represented as matrix of pixel values [0-255]
Normalized to [0-1] range
```

**After Conv1:**
```
64Ã—64Ã—3 â†’ Conv 3Ã—3, 32 filters â†’ 64Ã—64Ã—32

Each of 32 channels detects different features:
Channel 0: Edge detection
Channel 1: Texture detection
Channel 5: Nose feature
Channel 12: Eye feature
(learned automatically during training)
```

**After Pooling and More Convs:**
```
â†’ 32Ã—32Ã—32 â†’ 16Ã—16Ã—64 â†’ 8Ã—8Ã—128

Final features: 8Ã—8Ã—128 = 8,192 values
Rich representation of face features
```

**Final Embedding:**
```
8,192 values â†’ FC layer â†’ 128 dimensions

This compression captures the essential face identity:
[0.12, 0.45, 0.67, 0.23, -0.15, 0.89, ...]

Each dimension captures some aspect of face identity
Combination uniquely represents this person
```

### Example 3: Distance Calculation

**Complete Example with Real Numbers:**

**Person A - Photo 1:**
```
Embedding_A1 = [0.50, 0.30, 0.80, 0.20, 0.60] (simplified to 5D)
```

**Person A - Photo 2:**
```
Embedding_A2 = [0.52, 0.28, 0.82, 0.19, 0.61]
```

**Person B - Photo 1:**
```
Embedding_B1 = [0.10, 0.90, 0.25, 0.85, 0.15]
```

**Calculations:**

```
Distance(A1, A2) - Same person:
Differences: [0.02, 0.02, 0.02, 0.01, 0.01]
Squared: [0.0004, 0.0004, 0.0004, 0.0001, 0.0001]
Sum: 0.0014
d(A1,A2) = âˆš0.0014 = 0.037

Distance(A1, B1) - Different people:
Differences: [0.40, 0.60, 0.55, 0.65, 0.45]
Squared: [0.16, 0.36, 0.3025, 0.4225, 0.2025]
Sum: 1.4475
d(A1,B1) = âˆš1.4475 = 1.203

Comparison:
d(A1,A2) = 0.037 << threshold (0.5) â†’ Same person âœ“
