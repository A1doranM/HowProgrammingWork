# Attention Model Intuition and Explanation

---

## Intuition — Why Attention?

Seq2Seq models without attention compress an entire source sentence into a **single fixed-length vector**. This bottleneck forces the encoder to squeeze all information into one state, which is especially harmful for long sentences.

**Attention** removes the bottleneck by letting the decoder **look back at all encoder states** and focus on the most relevant parts at each output step.

### Plain-English Problem Illustration

```
Sentence:  "The cat that the dog chased was black"

Decoder at word "black" needs "cat", not "dog".
Fixed vector: forced to remember everything equally.
Attention: dynamically points to the word "cat".
```

### Visual Intuition (Dynamic Focus)

```
Source words:  x1     x2     x3     x4     x5
              "The"  "cat"  "that" "the" "dog"

Encoder states (stored):
h1     h2     h3     h4     h5
│      │      │      │      │
└──────┴──────┴──────┴──────┴──→ memory bank

Decoder step t (producing y_t):
   asks: "Which encoder state is most relevant now?"
   attends mostly to h2 ("cat")
```

**Key Idea:** The decoder **does not rely on a single vector**. It builds a **context vector** at every time step, tailored to the word being generated.

---

## Core Idea — Alignment and Context

Attention introduces **alignment scores** between the current decoder state and each encoder state. These scores turn into **attention weights** that sum to 1, and the **context vector** is a weighted sum of encoder states.

### High-Level Flow

```
┌──────────────────────────────────────────────────────────────┐
│  At decoder time step t                                       │
├──────────────────────────────────────────────────────────────┤
│  1) Compare decoder state s_{t-1} with each encoder state h_i │
│  2) Convert scores to weights α_{t,i} (softmax)               │
│  3) Build context c_t = Σ α_{t,i} h_i                         │
│  4) Use c_t to produce next word y_t                          │
└──────────────────────────────────────────────────────────────┘
```

### Alignment Intuition

```
Alignment scores e_{t,i}
   ┌───────────────┐
   │ s_{t-1} (decoder query) │
   └───────────────┘
          │   compare with each h_i
          ▼
   h_1    h_2    h_3    ...    h_{T_x}
   │      │      │             │
   ▼      ▼      ▼             ▼
 e_{t,1} e_{t,2} e_{t,3} ... e_{t,T_x}
```

**Interpretation:** Higher $e_{t,i}$ means the decoder is more aligned with source position $i$ at step $t$.

---

## Step-by-Step Mechanics (Conceptual)

Below is a full conceptual pass for a single decoder time step $t$.

```
╔═══════════════════════════════════════════════════════════════╗
║                 ATTENTION STEP (Time t)                      ║
╚═══════════════════════════════════════════════════════════════╝

Inputs:
  - Encoder states: h_1, h_2, ..., h_{T_x}
  - Decoder previous state: s_{t-1}

Step 1: Alignment Scores
  e_{t,i} = score(s_{t-1}, h_i)

Step 2: Attention Weights (softmax)
  α_{t,i} = exp(e_{t,i}) / Σ_j exp(e_{t,j})

Step 3: Context Vector (weighted sum)
  c_t = Σ_i α_{t,i} h_i

Step 4: Decoder Update
  s_t = f(s_{t-1}, y_{t-1}, c_t)

Step 5: Output Token
  y_t = softmax(W_y s_t + b_y)
```

### Diagram: How the Context Vector is Formed

```
Encoder states:  h_1     h_2     h_3     ...     h_{T_x}
                 │       │       │               │
Attention:       α_{t,1} α_{t,2} α_{t,3}         α_{t,T_x}
                 │       │       │               │
                 ▼       ▼       ▼               ▼
             α_{t,1}h_1 α_{t,2}h_2 α_{t,3}h_3 ... α_{t,T_x}h_{T_x}
                 │       │       │               │
                 └───────┴───────┴─────── ... ────┘
                              ▼
                         c_t (context)
```

---

## Mathematical Formulation (Minimal, Clear)

We keep the math compact but precise, matching the style of the RNN/GRU/Beam Search docs.

### 1) Alignment Score

$$e_{t,i} = s_{t-1}^T h_i$$

**Symbol Legend**:
- $e_{t,i}$ = alignment score between decoder step $t$ and encoder position $i$
- $s_{t-1}$ = decoder hidden state from previous time step (vector in $\mathbb{R}^{n_s}$)
- $h_i$ = encoder hidden state at input position $i$ (vector in $\mathbb{R}^{n_h}$)
- $(\cdot)^T$ = vector transpose (dot product)

> Note: Other scoring functions exist (additive, scaled dot), but dot product is sufficient here.

### 2) Attention Weights (Softmax)

$$\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_{j=1}^{T_x} \exp(e_{t,j})}$$

**Symbol Legend**:
- $\alpha_{t,i}$ = attention weight for encoder position $i$ at decoder step $t$
- $T_x$ = length of input sequence
- $\exp(\cdot)$ = exponential function

**Key Property:**
$$\sum_{i=1}^{T_x} \alpha_{t,i} = 1$$

### 3) Context Vector

$$c_t = \sum_{i=1}^{T_x} \alpha_{t,i} h_i$$

**Symbol Legend**:
- $c_t$ = context vector used at decoder time $t$
- $\alpha_{t,i}$ = attention weights (sum to 1)
- $h_i$ = encoder state for position $i$

### 4) Decoder Update (Conceptual)

$$s_t = f(s_{t-1}, y_{t-1}, c_t)$$

**Symbol Legend**:
- $s_t$ = current decoder hidden state
- $y_{t-1}$ = previous output token (embedded)
- $f(\cdot)$ = decoder cell (RNN/GRU/LSTM)

---

## Worked Mini-Example

We’ll compute attention for **one decoder step** with small vectors.

### Setup

Assume 3 input tokens ($T_x = 3$) and 2D hidden states.

```
Encoder states:
h_1 = [1, 0]
h_2 = [0, 2]
h_3 = [1, 1]

Decoder previous state:
s_{t-1} = [1, 1]
```

### Step 1: Alignment Scores (Dot Product)

$$e_{t,i} = s_{t-1}^T h_i$$

```
e_{t,1} = [1,1]·[1,0] = 1
e_{t,2} = [1,1]·[0,2] = 2
e_{t,3} = [1,1]·[1,1] = 2
```

### Step 2: Softmax Weights

$$\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_j \exp(e_{t,j})}$$

```
exp(1) = 2.718
exp(2) = 7.389
exp(2) = 7.389

Sum = 2.718 + 7.389 + 7.389 = 17.496

α_{t,1} = 2.718 / 17.496 = 0.155
α_{t,2} = 7.389 / 17.496 = 0.422
α_{t,3} = 7.389 / 17.496 = 0.422
```

### Attention Weights Sum to 1

```
0.155 + 0.422 + 0.422 = 0.999 ≈ 1.0  ✔
```

### Step 3: Context Vector (Weighted Sum)

$$c_t = \sum_i \alpha_{t,i} h_i$$

```
c_t = 0.155·[1,0] + 0.422·[0,2] + 0.422·[1,1]

    = [0.155, 0.000] + [0.000, 0.844] + [0.422, 0.422]

    = [0.577, 1.266]
```

**Interpretation:** The context vector is a blend of encoder states, dominated by $h_2$ and $h_3$ since their weights are highest.

---

## Benefits and Limitations

### Benefits
- **Solves the fixed-vector bottleneck** by using all encoder states.
- **Improves long-sequence performance** (translation, summarization, QA).
- **Interpretable alignment** via attention weights ($\alpha_{t,i}$).
- **Differentiable end-to-end** (trained with standard backpropagation).

### Limitations
- **Computational cost**: attention scales with $T_x$ per decoder step.
- **Quadratic complexity** in full self-attention variants.
- **Soft alignment only** (weights are continuous, not hard alignments).

---

## Final Takeaway

Attention lets the decoder **choose what to look at** in the input sequence for each output token. It replaces the single-vector bottleneck with a **dynamic context vector**, improving accuracy and interpretability.

