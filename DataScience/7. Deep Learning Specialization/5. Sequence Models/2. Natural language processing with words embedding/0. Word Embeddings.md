# Word Embeddings

## Table of Contents

1. [Introduction to Word Embeddings](#introduction-to-word-embeddings)
   - [What is Word Embedding?](#what-is-word-embedding)
   - [Visual Comparison: One-Hot vs Embedding](#visual-comparison-one-hot-vs-embedding)
   - [Why Use Word Embeddings?](#why-use-word-embeddings)
   - [Intuitive Example: King, Queen, Man, Woman](#intuitive-example-king-queen-man-woman)
     - [One-Hot Encoding (Vocabulary size = 5)](#one-hot-encoding-vocabulary-size--5)
     - [Word Embeddings (3-dimensional)](#word-embeddings-3-dimensional)
     - [Complete Numerical Example Summary](#complete-numerical-example-summary)
2. [Properties of Word Embeddings](#properties-of-word-embeddings)
   - [Semantic Similarity](#semantic-similarity)
     - [Cosine Similarity Formula](#cosine-similarity-formula)
     - [Why Cosine Similarity and Euclidean Distance?](#why-cosine-similarity-and-euclidean-distance)
       - [The Role of Direction vs Magnitude in Embeddings](#the-role-of-direction-vs-magnitude-in-embeddings)
       - [Cosine Similarity: Why It Works](#cosine-similarity-why-it-works)
       - [Euclidean Distance: Why It Works](#euclidean-distance-why-it-works)
       - [Why NOT Sine?](#why-not-sine)
       - [Why NOT Tangent?](#why-not-tangent)
       - [Why NOT Other Complex Metrics?](#why-not-other-complex-metrics)
       - [Comparison Table of Distance Metrics](#comparison-table-of-distance-metrics)
       - [Why Cosine is the Standard Choice](#why-cosine-is-the-standard-choice)
       - [Practical Guideline](#practical-guideline)
     - [Numerical Example: Computing Cosine Similarity](#numerical-example-computing-cosine-similarity)
     - [Visualization of Word Clusters](#visualization-of-word-clusters)
   - [Analogies and Linear Relationships](#analogies-and-linear-relationships)
     - [Mathematical Explanation](#mathematical-explanation)
     - [Step-by-Step Calculation: King - Man + Woman ≈ Queen](#step-by-step-calculation-king---man--woman--queen)
     - [More Analogy Examples with Calculations](#more-analogy-examples-with-calculations)
     - [Geometric Interpretation with Diagrams](#geometric-interpretation-with-diagrams)
   - [Dimensionality and Information](#dimensionality-and-information)
     - [Common Embedding Dimensions](#common-embedding-dimensions)
     - [Trade-offs Between Dimensions](#trade-offs-between-dimensions)
     - [Choosing the Right Dimensionality](#choosing-the-right-dimensionality)
     - [Properties Summary](#properties-summary)
3. [Embedding Matrix](#embedding-matrix)
   - [What is an Embedding Matrix?](#what-is-an-embedding-matrix)
   - [Matrix Dimensions](#matrix-dimensions)
     - [ASCII Diagram of Embedding Matrix Structure](#ascii-diagram-of-embedding-matrix-structure)
   - [Mathematical Representation](#mathematical-representation)
   - [How Embedding Lookup Works](#how-embedding-lookup-works)
     - [Step-by-Step Process](#step-by-step-process)
     - [Mathematical Formulation](#mathematical-formulation)
     - [Complete Numerical Example](#complete-numerical-example)
   - [Embedding Matrix Dimensions Example](#embedding-matrix-dimensions-example)
   - [Parameter Count Analysis](#parameter-count-analysis)
   - [Initialization Methods](#initialization-methods)
4. [Learning Word Embeddings](#learning-word-embeddings)
   - [Overview of Learning Approaches](#overview-of-learning-approaches)
   - [Learning in Neural Networks](#learning-in-neural-networks)
     - [Embeddings as First Layer Parameters](#embeddings-as-first-layer-parameters)
     - [Backpropagation Updates](#backpropagation-updates)
     - [Gradient Flow Through Embedding Layer](#gradient-flow-through-embedding-layer)
     - [Mathematical Formulation](#mathematical-formulation-1)
     - [Complete Numerical Example with Small Network](#complete-numerical-example-with-small-network)
   - [Simple Neural Language Model](#simple-neural-language-model)
     - [Architecture](#architecture)
     - [Task: Predict Next Word](#task-predict-next-word)
     - [ASCII Diagram of Architecture](#ascii-diagram-of-architecture)
     - [Mathematical Formulation](#mathematical-formulation-2)
     - [Step-by-Step Example: Predict "learning" from "I love machine"](#step-by-step-example-predict-learning-from-i-love-machine)
   - [Word2Vec: Skip-gram Model](#word2vec-skip-gram-model)
     - [Introduction and Architecture](#introduction-and-architecture)
     - [ASCII Diagram](#ascii-diagram)
     - [Mathematical Formulation](#mathematical-formulation-3)
     - [Numerical Example with Small Vocabulary](#numerical-example-with-small-vocabulary)
     - [Training Objective and Loss Function](#training-objective-and-loss-function)
     - [Visualization of Training Process](#visualization-of-training-process)
   - [Word2Vec: CBOW Model](#word2vec-cbow-model)
     - [Introduction](#introduction)
     - [ASCII Diagram](#ascii-diagram-1)
     - [Mathematical Formulation](#mathematical-formulation-4)
     - [Comparison with Skip-gram](#comparison-with-skip-gram)
   - [Negative Sampling](#negative-sampling)
     - [Problem: Computational Cost of Softmax](#problem-computational-cost-of-softmax)
     - [Solution: Negative Sampling](#solution-negative-sampling)
     - [Complete Example with Calculations](#complete-example-with-calculations)
     - [Visualization of Training Process](#visualization-of-training-process-1)
   - [GloVe (Global Vectors)](#glove-global-vectors)
     - [Introduction to GloVe Approach](#introduction-to-glove-approach)
     - [Co-occurrence Matrix Concept](#co-occurrence-matrix-concept)
     - [Mathematical Formulation](#mathematical-formulation-5)
     - [How GloVe Differs from Word2Vec](#how-glove-differs-from-word2vec)
   - [Training Process Overview](#training-process-overview)
     - [Data Preparation](#data-preparation)
     - [Hyperparameters](#hyperparameters)
     - [Training Algorithm Steps](#training-algorithm-steps)
5. [Summary](#summary)

---

## Introduction to Word Embeddings

### What is Word Embedding?

**Word embeddings** are dense, low-dimensional vector representations of words that capture semantic meaning and relationships between words. They transform words from sparse, high-dimensional one-hot encodings into compact, continuous vectors where similar words have similar representations.

**Problem Statement**: Traditional one-hot encoding represents each word as a sparse vector with a single 1 and the rest 0s. For a vocabulary of 10,000 words, each word is represented by a 10,000-dimensional vector. This approach has several critical limitations:
- **No semantic information**: "king" and "queen" are just as different as "king" and "apple"
- **Extremely high dimensionality**: Vocabulary size can be 50,000+ words
- **Sparse representation**: 99.99% of values are zeros
- **No generalization**: Can't infer relationships between words

**Evolution from One-Hot to Embeddings**:

```
Traditional One-Hot Encoding (Sparse, High-Dimensional)
════════════════════════════════════════════════════════

Vocabulary: ["king", "queen", "man", "woman", "apple"]
Vocabulary Size: V = 5

"king"   = [1, 0, 0, 0, 0]  ← 5-dimensional, only 1 non-zero value
"queen"  = [0, 1, 0, 0, 0]  ← No relationship captured between king/queen
"man"    = [0, 0, 1, 0, 0]
"woman"  = [0, 0, 0, 1, 0]
"apple"  = [0, 0, 0, 0, 1]

Problems:
- Each word is orthogonal to every other word
- Distance between "king" and "queen" = distance between "king" and "apple"
- No semantic relationships encoded


Word Embeddings (Dense, Low-Dimensional)
═════════════════════════════════════════

Embedding Dimension: d = 3 (much smaller than V = 5)

"king"   = [0.8,  0.6,  0.1]  ← Dense, all values non-zero
"queen"  = [0.7,  0.5,  0.2]  ← Close to "king" in vector space
"man"    = [0.5,  0.3, -0.1]  ← Related to "king" (male)
"woman"  = [0.4,  0.2,  0.0]  ← Related to "queen" (female)
"apple"  = [-0.2, -0.1, 0.9]  ← Distant from royal/gender words

Advantages:
- Similar words have similar vectors
- Semantic relationships captured
- Lower dimensionality (3 vs 5, in practice 300 vs 50,000)
- Dense representation (all values meaningful)
```

### Visual Comparison: One-Hot vs Embedding

```
╔══════════════════════════════════════════════════════════════════════════╗
║                      ONE-HOT ENCODING                                    ║
╚══════════════════════════════════════════════════════════════════════════╝

Vocabulary of 10,000 words → Each word is 10,000-dimensional vector

Word: "cat"
┌─────────────────────────────────────────────────────────────┐
│ [0, 0, 0, ..., 0, 1, 0, ..., 0, 0, 0]                       │
│  ↑           ↑   ↑           ↑                              │
│  pos 0       ...  pos 4523   ...  pos 9999                  │
│                   └─ only non-zero value                    │
└─────────────────────────────────────────────────────────────┘
  
  Dimensionality: 10,000
  Non-zero values: 1 (0.01%)
  Semantic info: None


╔══════════════════════════════════════════════════════════════════════════╗
║                      WORD EMBEDDING                                      ║
╚══════════════════════════════════════════════════════════════════════════╝

Same vocabulary → Each word is 300-dimensional vector

Word: "cat"
┌──────────────────────────────────────────────────────────┐
│ [0.45, -0.23, 0.67, 0.12, -0.34, ..., 0.56, -0.11]      │
│  ↑     ↑      ↑     ↑     ↑          ↑      ↑           │
│  All values are meaningful (dense representation)        │
└──────────────────────────────────────────────────────────┘

  Dimensionality: 300 (33× smaller!)
  Non-zero values: 300 (100%)
  Semantic info: Rich (captures meaning, relationships)


╔══════════════════════════════════════════════════════════════════════════╗
║                    DIMENSIONALITY COMPARISON                             ║
╚══════════════════════════════════════════════════════════════════════════╝

One-Hot:  [▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓] 10,000 dimensions
          └─ Only 1 value is 1, rest are 0s

Embedding: [▓▓▓] 300 dimensions
           └─ All values are meaningful
```

### Why Use Word Embeddings?

**Limitations of One-Hot Encoding**:

1. **No Semantic Similarity**
   - Dot product between any two one-hot vectors is always 0
   - "cat" and "dog" are as different as "cat" and "democracy"
   
2. **Curse of Dimensionality**
   - Vocabulary of 50,000 words → 50,000 dimensions
   - Extremely sparse representation (99.998% zeros)
   - Computationally expensive
   
3. **No Generalization**
   - If model learns "The cat sat on the mat"
   - Cannot generalize to "The kitten sat on the mat" (different one-hot vectors)
   
4. **Memory Inefficiency**
   - Storing 50,000-dimensional vectors for millions of words is impractical

**Benefits of Word Embeddings**:

1. **Semantic Similarity**
   - Similar words have similar vectors
   - Cosine similarity between "cat" and "dog" ≈ 0.8
   - Cosine similarity between "cat" and "democracy" ≈ 0.1
   
2. **Dimensionality Reduction**
   - Typical embeddings: 50-300 dimensions (vs 50,000+)
   - 100-1000× compression
   - Faster computation
   
3. **Generalization**
   - Model learns "orange" → understands "fruit"
   - Knowledge transfers to "apple", "banana", "grape"
   
4. **Captures Relationships**
   - Analogies: "king" - "man" + "woman" ≈ "queen"
   - Gender, tense, plurality relationships encoded

### Intuitive Example: King, Queen, Man, Woman

Let's see a concrete example comparing one-hot encoding to word embeddings:

#### One-Hot Encoding (Vocabulary size = 5)

```
Vocabulary: ["king", "queen", "man", "woman", "apple"]

One-hot representations (5-dimensional):
═══════════════════════════════════════════

king   = [1, 0, 0, 0, 0]
queen  = [0, 1, 0, 0, 0]
man    = [0, 0, 1, 0, 0]
woman  = [0, 0, 0, 1, 0]
apple  = [0, 0, 0, 0, 1]


Computing similarity (dot product):
═══════════════════════════════════

king · queen  = 1×0 + 0×1 + 0×0 + 0×0 + 0×0 = 0  ← No relationship!
king · man    = 1×0 + 0×0 + 0×1 + 0×0 + 0×0 = 0  ← No relationship!
king · apple  = 1×0 + 0×0 + 0×0 + 0×0 + 0×1 = 0  ← Same as above!


Problem: All words are equally different from each other!
```

#### Word Embeddings (3-dimensional)

```
Learned embeddings (3-dimensional):
═══════════════════════════════════

              dim 0   dim 1   dim 2
              ─────   ─────   ─────
              (royal) (male)  (food)
              
king   =     [ 0.90,  0.80,  0.05]  ← High royal, high male
queen  =     [ 0.85,  0.20,  0.10]  ← High royal, low male (female)
man    =     [ 0.10,  0.90, -0.05]  ← Low royal, high male
woman  =     [ 0.05,  0.15,  0.00]  ← Low royal, low male (female)
apple  =     [-0.10, -0.05,  0.95]  ← Not royal, not gendered, food


Computing similarity (cosine similarity):
═════════════════════════════════════════

Cosine similarity formula: cos(θ) = (A · B) / (||A|| ||B||)

1. king vs queen:
   ─────────────
   Numerator:   0.90×0.85 + 0.80×0.20 + 0.05×0.10 = 0.765 + 0.160 + 0.005 = 0.930
   ||king||:    √(0.90² + 0.80² + 0.05²) = √(0.81 + 0.64 + 0.0025) = √1.4525 ≈ 1.205
   ||queen||:   √(0.85² + 0.20² + 0.10²) = √(0.7225 + 0.04 + 0.01) = √0.7725 ≈ 0.879
   
   cos(king, queen) = 0.930 / (1.205 × 0.879) = 0.930 / 1.059 ≈ 0.878
   
   ► Very high similarity! (Both royal)

2. king vs man:
   ────────────
   Numerator:   0.90×0.10 + 0.80×0.90 + 0.05×(-0.05) = 0.09 + 0.72 - 0.0025 = 0.8075
   ||man||:     √(0.10² + 0.90² + (-0.05)²) = √(0.01 + 0.81 + 0.0025) = √0.8225 ≈ 0.907
   
   cos(king, man) = 0.8075 / (1.205 × 0.907) = 0.8075 / 1.093 ≈ 0.739
   
   ► High similarity (both male)

3. king vs apple:
   ──────────────
   Numerator:   0.90×(-0.10) + 0.80×(-0.05) + 0.05×0.95 = -0.09 - 0.04 + 0.0475 = -0.0825
   ||apple||:   √((-0.10)² + (-0.05)² + 0.95²) = √(0.01 + 0.0025 + 0.9025) = √0.915 ≈ 0.957
   
   cos(king, apple) = -0.0825 / (1.205 × 0.957) = -0.0825 / 1.153 ≈ -0.072
   
   ► Near zero (very different concepts!)


Visualization in 2D (first two dimensions):
════════════════════════════════════════════

   male (dim 1)
   ↑
1.0│              man•
   │
0.8│    king•
   │
0.6│
   │
0.4│
   │
0.2│         queen•
   │    woman•
0.0├────────────────────────────► royal (dim 0)
   0   0.2  0.4  0.6  0.8  1.0

         apple• (off chart, at food dimension)

Notice:
- king and man are close (both male)
- king and queen are close (both royal)
- apple is far from all (different concept)
```

#### Complete Numerical Example Summary

```
╔═══════════════════════════════════════════════════════════════════════╗
║                    WORD EMBEDDING SUMMARY                             ║
╠═══════════════════════════════════════════════════════════════════════╣
║                                                                       ║
║  Word         One-Hot (5D)         Embedding (3D)      Meaning       ║
║  ─────────────────────────────────────────────────────────────────   ║
║  king         [1,0,0,0,0]          [0.90, 0.80, 0.05]  Royal+Male   ║
║  queen        [0,1,0,0,0]          [0.85, 0.20, 0.10]  Royal+Female ║
║  man          [0,0,1,0,0]          [0.10, 0.90,-0.05]  Male         ║
║  woman        [0,0,0,1,0]          [0.05, 0.15, 0.00]  Female       ║
║  apple        [0,0,0,0,1]          [-0.10,-0.05,0.95]  Food         ║
║                                                                       ║
║  Similarities (Cosine):                                               ║
║  ─────────────────────────────────────────────────────────────────   ║
║                     One-Hot        Embedding      Interpretation     ║
║  king ↔ queen         0.000          0.878       Related (royal)    ║
║  king ↔ man           0.000          0.739       Related (male)     ║
║  king ↔ apple         0.000         -0.072       Unrelated          ║
║  queen ↔ woman        0.000          0.512       Related (female)   ║
║  man ↔ woman          0.000          0.338       Somewhat related   ║
║  apple ↔ king         0.000         -0.072       Unrelated          ║
║                                                                       ║
║  Key Insight: Embeddings capture semantic relationships that         ║
║               one-hot encoding completely misses!                    ║
╚═══════════════════════════════════════════════════════════════════════╝
```

---

## Properties of Word Embeddings

### Semantic Similarity

**Definition**: Words with similar meanings have similar vector representations. The similarity can be measured using cosine similarity or Euclidean distance.

#### Cosine Similarity Formula

**Mathematical Definition**:

$$\text{cosine\_similarity}(A, B) = \frac{A \cdot B}{||A|| \cdot ||B||} = \frac{\sum_{i=1}^{d} A_i B_i}{\sqrt{\sum_{i=1}^{d} A_i^2} \cdot \sqrt{\sum_{i=1}^{d} B_i^2}}$$

**Complete Symbol Legend**:
- $A, B$ = Word embedding vectors being compared (each of dimension $d$)
- $A \cdot B$ = Dot product of vectors A and B
- $A_i, B_i$ = The $i$-th component of vectors A and B
- $||A||$ = Euclidean norm (magnitude) of vector A: $||A|| = \sqrt{\sum_{i=1}^{d} A_i^2}$
- $||B||$ = Euclidean norm (magnitude) of vector B
- $d$ = Dimensionality of the embedding space
- $\text{cosine\_similarity}$ = Measure of similarity, range: [-1, 1]
  - 1 = Identical direction (most similar)
  - 0 = Orthogonal (unrelated)
  - -1 = Opposite direction (antonyms)

#### Why Cosine Similarity and Euclidean Distance?

When measuring similarity between word embeddings, we primarily use **cosine similarity** and **Euclidean distance**. But why these specific metrics and not others like sine, tangent, or more complex functions? Let's understand the mathematical and practical reasons.

##### The Role of Direction vs Magnitude in Embeddings

Word embeddings represent meaning through **position in high-dimensional space**. Two key aspects matter:
- **Direction**: What "semantic direction" the vector points (what the word means)
- **Magnitude**: How "strong" or frequent the word is

```
Conceptual Understanding:
════════════════════════

Think of word embeddings as arrows in space:
- Where the arrow POINTS → meaning/semantics
- How LONG the arrow is → frequency/importance

Example:
  "king" = [0.8, 0.6] (points toward "royal" + "male" direction)
  "emperor" = [1.6, 1.2] (points in SAME direction, but twice as long)
  
  These have similar meaning despite different magnitudes!
```

##### Cosine Similarity: Why It Works

**What Cosine Measures**: The angle between two vectors, **ignoring their lengths**.

$$\cos(\theta) = \frac{A \cdot B}{||A|| \cdot ||B||}$$

**Key Properties**:

1. **Range**: [-1, 1]
   - cos(θ) = 1  → vectors point same direction (0°)
   - cos(θ) = 0  → vectors perpendicular (90°)
   - cos(θ) = -1 → vectors opposite direction (180°)

2. **Scale Invariant**: Multiplying a vector by any positive scalar doesn't change cosine
   - cos([2, 4], [1, 2]) = 1 (same direction!)
   - This is crucial because word frequency shouldn't affect semantic similarity

3. **Interpretable**: Clear geometric meaning (angle between vectors)
   - Small angle → similar meaning
   - Large angle → different meaning

**Why Cosine Works for Word Embeddings**:

```
Scenario: Comparing word frequency vs meaning
══════════════════════════════════════════════

Word: "cat" (common, appears 10,000 times)
  Embedding: [0.5, 0.3]
  Magnitude: 0.583

Word: "feline" (rare, appears 100 times)
  Embedding: [0.4, 0.24]  (same direction, smaller magnitude)
  Magnitude: 0.467

Euclidean distance: √[(0.5-0.4)² + (0.3-0.24)²] = 0.117 (different)
Cosine similarity: (0.5×0.4 + 0.3×0.24)/(0.583×0.467) = 0.997 ≈ 1.0 (nearly identical!)

✓ Cosine correctly identifies semantic similarity despite frequency difference
```

**Visual Explanation**:

```
     y
     ↑
     │
 1.0 │     • vector_A [0.8, 0.6]
     │    ╱│
 0.8 │   ╱ │
     │  ╱  │
 0.6 │ ╱   │• vector_B [0.4, 0.3]
     │╱    │╱
 0.4 │    ╱│     Both point in SAME direction
     │   ╱ │     Small angle ≈ 0°
 0.2 │  ╱  │     Cosine ≈ 1.0 ✓
     │ ╱   │
 0.0 └─────────────────────► x
     0  0.2 0.4 0.6 0.8 1.0

Insight: Despite different lengths, the direction (meaning) is the same!


Compare with different directions:
══════════════════════════════════

     y
     ↑
 1.0 │         • dog [0.2, 0.9]
     │        ╱│
 0.8 │       ╱ │
     │      ╱  │ Large angle ≈ 70°
 0.6 │     ╱   │ Cosine ≈ 0.3 (different meanings)
     │    ╱    │
 0.4 │   ╱     │
     │  ╱      │
 0.2 │ ╱_______│___• car [0.9, 0.2]
     │╱        │  ╱
 0.0 └─────────────────────► x
     0  0.2 0.4 0.6 0.8 1.0

Insight: Different directions = different semantic meanings
```

##### Euclidean Distance: Why It Works

**What Euclidean Measures**: Straight-line distance between two points in space.

$$d(A, B) = ||A - B|| = \sqrt{\sum_{i=1}^{d} (A_i - B_i)^2}$$

**Key Properties**:

1. **Range**: [0, ∞)
   - d = 0 → identical vectors
   - d → ∞ → very different vectors

2. **Considers Both Direction AND Magnitude**: Unlike cosine, Euclidean captures both aspects
   - Useful when magnitude matters (e.g., word importance, confidence)

3. **Metric Properties**: Satisfies triangle inequality
   - d(A,B) ≤ d(A,C) + d(C,B)
   - Enables spatial reasoning about relationships

**When to Use Euclidean Distance**:

```
Use Case 1: Magnitude Matters
═════════════════════════════

Embeddings with learned magnitude encoding confidence:

"certain" = [0.9, 0.1]  (high magnitude = high certainty)
"probable" = [0.6, 0.1] (lower magnitude = less certain)
"maybe" = [0.3, 0.1]    (low magnitude = uncertain)

Euclidean distances:
  d("certain", "probable") = 0.3 (somewhat similar)
  d("certain", "maybe") = 0.6 (more different)
  d("probable", "maybe") = 0.3 (somewhat similar)

✓ Euclidean captures the certainty gradient


Use Case 2: Dense Regions
══════════════════════════

When word clusters have different "densities":

  High-frequency cluster (tight):
    "the" [0.5, 0.5]
    "a"   [0.52, 0.51]
    "an"  [0.51, 0.52]
    
  Low-frequency cluster (sparse):
    "consequently" [2.0, 2.0]
    "therefore"    [2.1, 2.05]
    
Euclidean preserves the density information that cosine loses.
```

**Comparison Visualization**:

```
Comparing Two Vectors with SAME direction, DIFFERENT magnitude:
═══════════════════════════════════════════════════════════════

Vector A: [1, 1]
Vector B: [3, 3]

     y
     ↑
 3.0 │           • B [3, 3]
     │          ╱│
 2.5 │         ╱ │
     │        ╱  │
 2.0 │       ╱   │  Both on same line (same direction)
     │      ╱    │  but different lengths
 1.5 │     ╱     │
     │    ╱      │
 1.0 │   •───────┘ A [1, 1]
     │  ╱        │
 0.5 │ ╱         │
     │╱          │
 0.0 └───────────────────────► x
     0   1   2   3

Cosine similarity = 1.0       (identical direction, ignores magnitude)
Euclidean distance = 2.828    (captures magnitude difference)

Interpretation:
  Cosine: "These words have identical meaning"
  Euclidean: "These words differ in magnitude/importance"


Comparing Two Vectors with DIFFERENT direction, SAME magnitude:
═══════════════════════════════════════════════════════════════

Vector C: [1, 0]
Vector D: [0, 1]

     y
     ↑
 1.0 │     • D [0, 1]
     │     │
     │     │
     │     │  90° angle
     │     │  Same length
 0.5 │     │
     │     │
     │     └───────• C [1, 0]
     │
 0.0 └───────────────────────► x
     0   0.5   1.0

Cosine similarity = 0         (orthogonal, different meanings)
Euclidean distance = 1.414    (both capture the difference)

Interpretation:
  Cosine: "Completely unrelated meanings"
  Euclidean: "Moderate distance apart"
```

##### Why NOT Sine?

**Sine Function**: $\sin(\theta)$ where $\theta$ is angle between vectors

**Problems with Sine**:

1. **Periodic and Symmetric**: $\sin(\theta) = \sin(180° - \theta)$
   ```
   sin(30°) = 0.5
   sin(150°) = 0.5  (same value!)
   
   But:
   cos(30°) = 0.866 (similar)
   cos(150°) = -0.866 (opposite)
   
   ✗ Sine cannot distinguish similar from opposite meanings!
   ```

2. **Maximum at 90°**: Sine peaks when vectors are orthogonal
   ```
   sin(0°) = 0   (identical vectors → low sine!)
   sin(90°) = 1  (orthogonal vectors → high sine!)
   
   ✗ This is backwards! We want HIGH values for similar words.
   ```

**Visual Comparison**:

```
Sine vs Cosine for Word Similarity:
════════════════════════════════════

     Value
     ↑
 1.0 │     cos(θ)                      sin(θ)
     │     ╲                            ╱╲
     │      ╲                          ╱  ╲
 0.5 │       ╲                        ╱    ╲
     │        ╲                      ╱      ╲
     │         ╲                    ╱        ╲
 0.0 ├──────────╲──────────────────╱──────────╲────────► θ (angle)
     │           ╲                ╱            ╲
     │            ╲              ╱              ╲
-0.5 │             ╲            ╱                ╲
     │              ╲          ╱
-1.0 │               ╲        ╱
     └─────────────────────────────────────────────
     0°    45°   90°   135°  180°

At 0° (identical):     cos=1.0 ✓,  sin=0.0 ✗
At 45° (similar):      cos=0.7 ✓,  sin=0.7 ✓
At 90° (unrelated):    cos=0.0 ✓,  sin=1.0 ✗ (wrong!)
At 135° (different):   cos=-0.7 ✓, sin=0.7 ✗ (same as 45°!)
At 180° (opposite):    cos=-1.0 ✓, sin=0.0 ✗ (same as 0°!)

✗ Sine gives the SAME value for opposite and identical vectors!
✓ Cosine correctly distinguishes all cases
```

##### Why NOT Tangent?

**Tangent Function**: $\tan(\theta) = \frac{\sin(\theta)}{\cos(\theta)}$

**Problems with Tangent**:

1. **Undefined at 90°**: $\tan(90°) = \frac{\sin(90°)}{\cos(90°)} = \frac{1}{0} = \infty$
   ```
   ✗ Breaks for orthogonal vectors (common in high-dimensional spaces)
   ```

2. **Unbounded Range**: $\tan(\theta) \in (-\infty, \infty)$
   ```
   tan(0°) = 0
   tan(45°) = 1
   tan(89°) = 57.3
   tan(89.9°) = 573
   tan(89.99°) = 5,730
   
   ✗ Values explode near 90°, making comparisons unstable
   ```

3. **Periodic with Wrong Period**: Repeats every 180° (not 360°)
   ```
   tan(0°) = 0
   tan(180°) = 0 (same!)
   
   ✗ Can't distinguish between parallel and anti-parallel vectors
   ```

4. **Numerical Instability**:
   ```
   For nearly orthogonal vectors (common in high dimensions):
   θ ≈ 90° → tan(θ) → ±∞
   
   ✗ Gradient descent becomes unstable
   ✗ Overflow errors in computation
   ```

##### Why NOT Other Complex Metrics?

**Manhattan Distance** (L1): $d(A,B) = \sum_{i=1}^{d} |A_i - B_i|$

- **Problem**: Doesn't capture angular relationships
- **Use case**: Sometimes used for sparsity-inducing regularization, not similarity

**Minkowski Distance** (Lp): $d(A,B) = (\sum_{i=1}^{d} |A_i - B_i|^p)^{1/p}$

- **Problem**: No clear semantic advantage over L2 (Euclidean)
- **Use case**: Theoretical generalization, rarely used in practice

**Mahalanobis Distance**: Accounts for correlations between dimensions

- **Problem**: Requires computing covariance matrix (expensive for high dimensions)
- **Problem**: Assumes Gaussian distribution (not true for embeddings)
- **Use case**: When you explicitly model feature correlations

**Kullback-Leibler Divergence**: Measures difference between probability distributions

- **Problem**: Requires probability distributions (embeddings are vectors, not distributions)
- **Problem**: Asymmetric: KL(P||Q) ≠ KL(Q||P)
- **Use case**: Comparing probability distributions, not vector embeddings

**Custom Neural Distance Metrics**: Learned similarity functions

- **Problem**: Requires training additional neural networks
- **Problem**: Lack interpretability
- **Problem**: Computationally expensive
- **Use case**: When simple metrics fail and you have lots of labeled similarity data

##### Comparison Table of Distance Metrics

```
╔═══════════════════════════════════════════════════════════════════════════════╗
║                     DISTANCE METRICS COMPARISON                               ║
╠═══════════════════════════════════════════════════════════════════════════════╣
║                                                                               ║
║ Metric          Range       What It Measures          When to Use             ║
║ ─────────────────────────────────────────────────────────────────────────     ║
║ Cosine         [-1, 1]      Angle between vectors     • Semantic similarity   ║
║ Similarity                  (direction only)          • Ignore magnitude      ║
║                                                        • Text classification  ║
║                1 = same direction                     • Most NLP tasks        ║
║                0 = orthogonal                         ✓ DEFAULT CHOICE        ║
║                -1 = opposite                                                  ║
║                                                                               ║
║ ────────────────────────────────────────────────────────────────────────────  ║
║                                                                               ║
║ Euclidean      [0, ∞)       Straight-line distance    • Magnitude matters     ║
║ Distance                    (direction + magnitude)   • Clustering            ║
║                                                        • Nearest neighbors    ║
║                0 = identical                          • When scale is         ║
║                ∞ = very different                       meaningful            ║
║                                                                               ║
║ ────────────────────────────────────────────────────────────────────────────  ║
║                                                                               ║
║ Sine           [-1, 1]      Angle (wrong scale)       ✗ DON'T USE             ║
║                                                        • Ambiguous (sin(θ)    ║
║                Peaks at 90° (orthogonal!)               = sin(180°-θ))        ║
║                Can't distinguish opposite             • Peaks at orthogonal   ║
║                from identical                         • Poor discrimination   ║
║                                                                               ║
║ ────────────────────────────────────────────────────────────────────────────  ║
║                                                                               ║
║ Tangent        (-∞, ∞)      Ratio of components       ✗ DON'T USE             ║
║                                                        • Undefined at 90°     ║
║                Undefined at 90°!                      • Unbounded range       ║
║                Unbounded and unstable                 • Numerically unstable  ║
║                                                                               ║
║ ────────────────────────────────────────────────────────────────────────────  ║
║                                                                               ║
║ Dot Product    (-∞, ∞)      Projection + magnitude    • Attention scores      ║
║                             (unnormalized cosine)     • Neural networks       ║
║                                                        • Not normalized       ║
║                Affected by vector lengths             ⚠ Use with caution     ║
║                                                                               ║
║ ────────────────────────────────────────────────────────────────────────────  ║
║                                                                               ║
║ Manhattan      [0, ∞)       Sum of absolute           • Sparse data           ║
║ (L1)                        differences               • Regularization        ║
║                                                        • Less common for      ║
║                Grid distance                            similarity            ║
║                                                                               ║
╚═══════════════════════════════════════════════════════════════════════════════╝


Key Insights:
═════════════

✓ Cosine: Best for capturing semantic meaning (most widely used)
✓ Euclidean: Best when magnitude/scale is important
✗ Sine: Ambiguous and peaks at wrong angle
✗ Tangent: Undefined and unstable
⚠ Others: Specialized use cases, not general similarity
```

##### Why Cosine is the Standard Choice

**Practical Reasons**:

1. **Scale Invariance**: Word embeddings often have varying magnitudes based on frequency
   - Common words: larger magnitude (seen more during training)
   - Rare words: smaller magnitude (less training exposure)
   - Cosine ignores this artifact and focuses on meaning

2. **Normalized Range**: [-1, 1] is easy to interpret and threshold
   - similarity > 0.7 → similar
   - similarity ≈ 0 → unrelated
   - similarity < 0 → opposite (though rare in practice)

3. **Computational Efficiency**: Can be computed efficiently in batches
   ```python
   # Vectorized computation for all pairs
   similarities = (embeddings @ embeddings.T) / (norms[:, None] * norms[None, :])
   ```

4. **Geometric Intuition**: Angle is a natural measure of "pointing in same direction"
   - 0° = same concept
   - 90° = orthogonal concept
   - 180° = opposite concept

5. **Proven Track Record**: Used successfully in:
   - Word2Vec, GloVe, FastText
   - Sentence embeddings (Skip-Thought, InferSent)
   - Document similarity
   - Information retrieval

**When to Use Euclidean Instead**:

1. **Magnitude is Meaningful**: When vector length encodes something important
   - Confidence scores
   - Frequency-weighted embeddings
   - Hierarchical embeddings (closer to root = more general)

2. **Clustering**: K-means and similar algorithms naturally use Euclidean
3. **Nearest Neighbor Search**: ANN algorithms (like FAISS) optimized for L2
4. **When Vectors are Normalized**: If all vectors have ||v|| = 1, Euclidean and cosine are equivalent (up to monotonic transform)

##### Practical Guideline

```
╔═══════════════════════════════════════════════════════════════════════╗
║                    DECISION FLOWCHART                                 ║
╚═══════════════════════════════════════════════════════════════════════╝

Are you comparing word embeddings for semantic similarity?
│
├─ YES: Do you care about word frequency/magnitude?
│  │
│  ├─ NO (most cases):  Use COSINE SIMILARITY ✓
│  │                    • Captures pure semantic meaning
│  │                    • Industry standard
│  │                    • Works with pre-trained embeddings
│  │
│  └─ YES (special cases): Use EUCLIDEAN DISTANCE
│                           • When magnitude encodes information
│                           • For clustering algorithms
│                           • When vectors are normalized anyway
│
└─ NO: Other considerations
   │
   ├─ Need probability distributions? → Use KL Divergence
   ├─ Learned similarity? → Train neural distance metric
   └─ Feature correlations important? → Use Mahalanobis
```

#### Numerical Example: Computing Cosine Similarity

Let's compute similarity between words with 4-dimensional embeddings:

```
Given embeddings:
════════════════

cat     = [0.5, 0.3, 0.8, 0.1]
dog     = [0.6, 0.4, 0.7, 0.2]
car     = [0.2, 0.9, 0.1, 0.6]
vehicle = [0.3, 0.8, 0.2, 0.5]


Step 1: Compute cat · dog (dot product)
═══════════════════════════════════════

cat · dog = (0.5 × 0.6) + (0.3 × 0.4) + (0.8 × 0.7) + (0.1 × 0.2)
          = 0.30 + 0.12 + 0.56 + 0.02
          = 1.00


Step 2: Compute ||cat|| (magnitude of cat)
═══════════════════════════════════════════

||cat|| = √(0.5² + 0.3² + 0.8² + 0.1²)
        = √(0.25 + 0.09 + 0.64 + 0.01)
        = √0.99
        = 0.995


Step 3: Compute ||dog|| (magnitude of dog)
═══════════════════════════════════════════

||dog|| = √(0.6² + 0.4² + 0.7² + 0.2²)
        = √(0.36 + 0.16 + 0.49 + 0.04)
        = √1.05
        = 1.025


Step 4: Compute cosine similarity
══════════════════════════════════

cos(cat, dog) = (cat · dog) / (||cat|| × ||dog||)
              = 1.00 / (0.995 × 1.025)
              = 1.00 / 1.020
              ≈ 0.980

► Very high similarity! (Both are animals/pets)


Similarly, compute cat · car:
═════════════════════════════

cat · car = (0.5 × 0.2) + (0.3 × 0.9) + (0.8 × 0.1) + (0.1 × 0.6)
          = 0.10 + 0.27 + 0.08 + 0.06
          = 0.51

||car|| = √(0.2² + 0.9² + 0.1² + 0.6²)
        = √(0.04 + 0.81 + 0.01 + 0.36)
        = √1.22
        = 1.105

cos(cat, car) = 0.51 / (0.995 × 1.105)
              = 0.51 / 1.099
              ≈ 0.464

► Lower similarity (different concepts)


Compute car · vehicle:
══════════════════════

car · vehicle = (0.2 × 0.3) + (0.9 × 0.8) + (0.1 × 0.2) + (0.6 × 0.5)
              = 0.06 + 0.72 + 0.02 + 0.30
              = 1.10

||vehicle|| = √(0.3² + 0.8² + 0.2² + 0.5²)
            = √(0.09 + 0.64 + 0.04 + 0.25)
            = √1.02
            = 1.010

cos(car, vehicle) = 1.10 / (1.105 × 1.010)
                  = 1.10 / 1.116
                  ≈ 0.986

► Very high similarity! (Both transportation)
```

#### Visualization of Word Clusters

```
Semantic Space Visualization (2D projection of embeddings)
══════════════════════════════════════════════════════════

                    Animals/Pets Cluster
                    ┌────────────────────┐
                    │  dog•    cat•      │
                    │      puppy•        │
                    │  kitten•           │
                    └────────────────────┘
                           ↕
                     (high similarity)


      Food Cluster              Transportation Cluster
      ┌──────────────┐          ┌──────────────────────┐
      │ apple•       │          │  car•    vehicle•    │
      │   banana•    │          │      truck•          │
      │     orange•  │          │  automobile•         │
      └──────────────┘          └──────────────────────┘
            ↕                            ↕
      (high similarity)           (high similarity)


Similarity Matrix (Cosine Similarities):
════════════════════════════════════════

           cat    dog   puppy  apple   car  vehicle
cat      1.000  0.980  0.920  0.120  0.464  0.450
dog      0.980  1.000  0.950  0.100  0.420  0.410
puppy    0.920  0.950  1.000  0.090  0.380  0.370
apple    0.120  0.100  0.090  1.000  0.050  0.040
car      0.464  0.420  0.380  0.050  1.000  0.986
vehicle  0.450  0.410  0.370  0.040  0.986  1.000

Key Observations:
─────────────────
1. Within-cluster similarity is HIGH (> 0.9)
   - cat ↔ dog: 0.980
   - car ↔ vehicle: 0.986

2. Cross-cluster similarity is LOW (< 0.5)
   - cat ↔ apple: 0.120
   - dog ↔ car: 0.420

3. Semantic relationships are captured automatically!
```

### Analogies and Linear Relationships

**Key Discovery**: Word embeddings exhibit linear structure. Relationships between words can be expressed as vector arithmetic.

**Famous Example**: "king" - "man" + "woman" ≈ "queen"

This means the vector difference between "king" and "man" is approximately the same as the difference between "queen" and "woman". This encodes the relationship: "king is to man as queen is to woman."

#### Mathematical Explanation

**Analogy Formula**:

$$\text{arg}\max_{w} \text{similarity}(e_w, e_a - e_b + e_c)$$

Where we want to find word $w$ such that: $a : b :: c : w$

**Complete Symbol Legend**:
- $e_a, e_b, e_c$ = Embedding vectors for words $a$, $b$, $c$
- $e_w$ = Embedding vector for the word $w$ we're searching for
- $\text{arg}\max$ = Find the word that maximizes the expression
- $\text{similarity}$ = Cosine similarity function
- $a : b :: c : w$ = Analogy notation: "$a$ is to $b$ as $c$ is to $w$"

#### Step-by-Step Calculation: King - Man + Woman ≈ Queen

```
Given embeddings (3-dimensional):
═════════════════════════════════

              dim 0    dim 1    dim 2
              ──────   ──────   ──────
              (royal)  (male)   (age)

king    =    [0.90,    0.80,    0.60]
queen   =    [0.85,    0.20,    0.55]
man     =    [0.10,    0.90,    0.50]
woman   =    [0.05,    0.15,    0.48]
prince  =    [0.80,    0.75,    0.20]  ← For comparison


Step 1: Compute "king - man"
═════════════════════════════

king - man = [0.90, 0.80, 0.60] - [0.10, 0.90, 0.50]
           = [0.90 - 0.10, 0.80 - 0.90, 0.60 - 0.50]
           = [0.80, -0.10, 0.10]

Interpretation: This vector represents "royalty without maleness"
                dim 0: +0.80 (strong royal)
                dim 1: -0.10 (less male)
                dim 2: +0.10 (slightly older)


Step 2: Compute "(king - man) + woman"
═══════════════════════════════════════

(king - man) + woman = [0.80, -0.10, 0.10] + [0.05, 0.15, 0.48]
                     = [0.80 + 0.05, -0.10 + 0.15, 0.10 + 0.48]
                     = [0.85, 0.05, 0.58]

This should be close to "queen"!


Step 3: Compare with actual "queen" embedding
══════════════════════════════════════════════

Computed:    [0.85, 0.05, 0.58]
Actual queen: [0.85, 0.20, 0.55]

Difference:   [0.00, -0.15, 0.03]  ← Very small!

Euclidean distance: √(0² + 0.15² + 0.03²) = √0.0234 ≈ 0.153


Step 4: Verify this is closest to "queen"
══════════════════════════════════════════

Target vector: [0.85, 0.05, 0.58]

Distance to queen:  √[(0.85-0.85)² + (0.05-0.20)² + (0.58-0.55)²]
                  = √[0 + 0.0225 + 0.0009]
                  = √0.0234 ≈ 0.153  ✓ Closest!

Distance to king:   √[(0.85-0.90)² + (0.05-0.80)² + (0.58-0.60)²]
                  = √[0.0025 + 0.5625 + 0.0004]
                  = √0.5654 ≈ 0.752

Distance to man:    √[(0.85-0.10)² + (0.05-0.90)² + (0.58-0.50)²]
                  = √[0.5625 + 0.7225 + 0.0064]
                  = √1.2914 ≈ 1.136

Distance to woman:  √[(0.85-0.05)² + (0.05-0.15)² + (0.58-0.48)²]
                  = √[0.64 + 0.01 + 0.01]
                  = √0.66 ≈ 0.812

Distance to prince: √[(0.85-0.80)² + (0.05-0.75)² + (0.58-0.20)²]
                  = √[0.0025 + 0.49 + 0.1444]
                  = √0.6369 ≈ 0.798

Result: "queen" is the closest word! ✓
```

#### More Analogy Examples with Calculations

**Example 1: Paris - France + Italy ≈ Rome**

```
Given embeddings (3D: [geography, culture, size]):
══════════════════════════════════════════════════

Paris   = [0.80, 0.75, 0.60]  ← Capital of France
France  = [0.50, 0.70, 0.85]  ← Country
Rome    = [0.78, 0.80, 0.55]  ← Capital of Italy
Italy   = [0.48, 0.78, 0.80]  ← Country


Calculation:
════════════

Paris - France = [0.80, 0.75, 0.60] - [0.50, 0.70, 0.85]
               = [0.30, 0.05, -0.25]

This vector represents "capital-ness" (what makes a city a capital)

(Paris - France) + Italy = [0.30, 0.05, -0.25] + [0.48, 0.78, 0.80]
                         = [0.78, 0.83, 0.55]

Compare with Rome: [0.78, 0.80, 0.55]

Distance: √[(0.78-0.78)² + (0.83-0.80)² + (0.55-0.55)²]
        = √[0 + 0.0009 + 0]
        = 0.03  ✓ Very close!


Interpretation:
═══════════════
The relationship "Paris is to France" is the same as "Rome is to Italy"
(capital city relationship)
```

**Example 2: Walking - Walk + Swim ≈ Swimming**

```
Given embeddings (3D: [verb-ness, activity-type, tense]):
═════════════════════════════════════════════════════════

walking  = [0.90, 0.60, 0.85]  ← -ing form (progressive)
walk     = [0.88, 0.58, 0.20]  ← Base form
swimming = [0.92, 0.75, 0.88]  ← -ing form (progressive)
swim     = [0.90, 0.73, 0.22]  ← Base form


Calculation:
════════════

walking - walk = [0.90, 0.60, 0.85] - [0.88, 0.58, 0.20]
               = [0.02, 0.02, 0.65]

This vector represents the "-ing" transformation (progressive tense)

(walking - walk) + swim = [0.02, 0.02, 0.65] + [0.90, 0.73, 0.22]
                        = [0.92, 0.75, 0.87]

Compare with swimming: [0.92, 0.75, 0.88]

Distance: √[(0.92-0.92)² + (0.75-0.75)² + (0.87-0.88)²]
        = √[0 + 0 + 0.0001]
        = 0.01  ✓ Almost exact!


Interpretation:
═══════════════
The grammatical relationship (base → progressive) is consistently encoded
```

#### Geometric Interpretation with Diagrams

```
╔═══════════════════════════════════════════════════════════════════╗
║           GEOMETRIC INTERPRETATION OF ANALOGIES                   ║
╚═══════════════════════════════════════════════════════════════════╝

Visualizing "king - man + woman ≈ queen" in 2D:
════════════════════════════════════════════════

    Royal (dim 0)
    ↑
    │
1.0 │        king•────────────────┐
    │         │                   │
0.9 │         │                   │
    │         │(king-man)         │
0.8 │         │                   │ queen•
    │         │                   │  ↑
0.7 │         │                   │  │
    │         ↓                   │  │
0.6 │        man•                 │  │
    │         │                   │  │
0.5 │         │                   └──┘
    │         │(woman)              (result)
0.4 │         │
    │         ↓
0.3 │        woman•
    │
0.2 │
    │
0.1 │
    │
0.0 └─────────────────────────────────────► Male (dim 1)
    0    0.2   0.4   0.6   0.8   1.0

Key Observations:
─────────────────
1. Vector from "man" to "king" has direction [+royal, -male_offset]
2. Vector from "woman" to "queen" has same direction and magnitude!
3. This creates a parallelogram: man-king-queen-woman
4. The relationships are parallel: they encode "royalty"


Another View: Multiple Analogies Form Parallel Lines
═════════════════════════════════════════════════════

    Royal
    ↑
    │     king•─────────→queen•
    │      │              │
    │      │              │  (parallel: gender shift)
    │      │              │
    │     prince•─────────→princess•
    │      │              │
    │      │              │  (parallel: same gender shift)
    │      │              │
    │     man•──────────→woman•
    │
    └──────────────────────────────► Male

All horizontal arrows have the same direction and represent
the "male to female" transformation!


3D Visualization: Country-Capital Relationships
════════════════════════════════════════════════

        Size
        ↑
        │
        │         France•
        │         ╱│
        │       ╱  │
        │     ╱    │ (same vector:
        │   ╱      │  capital relationship)
        │ ╱        ↓
        │Paris•   Italy•
        │         ╱
        │       ╱
        │     ╱
        │   ╱
        │ ╱
        │Rome•
        │
        └────────────────────► Geography/Culture

Vector "France → Paris" ≈ Vector "Italy → Rome"
Both represent the "country to capital" relationship
```

### Dimensionality and Information

Word embeddings typically use dimensions ranging from 50 to 300 (sometimes up to 600 for specialized applications).

#### Common Embedding Dimensions

```
╔═══════════════════════════════════════════════════════════════════════╗
║              EMBEDDING DIMENSIONS COMPARISON                          ║
╠═══════════════════════════════════════════════════════════════════════╣
║                                                                       ║
║  Dimensions    Use Case              Pros              Cons           ║
║  ──────────────────────────────────────────────────────────────────   ║
║  50-100        • Small datasets      • Fast training   • Less info    ║
║                • Limited compute     • Low memory      • Fewer        ║
║                • Mobile apps         • Quick inference   nuances      ║
║                                                                       ║
║  200-300       • General NLP         • Good balance    • More         ║
║                • Most applications   • Rich semantics    compute      ║
║                • Production systems  • Captures most   • Higher       ║
║                                        relationships     memory       ║
║                                                                       ║
║  400-600       • Large vocabularies  • Maximum info    • Slow         ║
║                • Domain-specific     • Fine details    • High         ║
║                • Research systems    • Best analogies    memory       ║
║                                                         • Overfitting ║
║                                                           risk        ║
╚═══════════════════════════════════════════════════════════════════════╝


Memory Requirements:
════════════════════

Vocabulary size: V = 50,000 words
Embedding dimension: d

d = 50:   Storage = 50,000 × 50 × 4 bytes = 10 MB     ✓ Small
d = 100:  Storage = 50,000 × 100 × 4 bytes = 20 MB    ✓ Medium
d = 200:  Storage = 50,000 × 200 × 4 bytes = 40 MB    ✓ Acceptable
d = 300:  Storage = 50,000 × 300 × 4 bytes = 60 MB    ✓ Standard
d = 600:  Storage = 50,000 × 600 × 4 bytes = 120 MB   ⚠ Large

(Assuming 32-bit floats)
```

#### Trade-offs Between Dimensions

**What Do Dimensions Represent?**

While dimensions in word embeddings don't always have clear interpretations, research has shown that different dimensions capture different semantic and syntactic properties:

```
Example interpretation of learned dimensions (hypothetical):
═══════════════════════════════════════════════════════════

Dimension 0:  Animacy (living vs non-living)
  dog: +0.9, cat: +0.8, car: -0.7, rock: -0.9

Dimension 1:  Size (large vs small)
  elephant: +0.9, mouse: -0.8, building: +0.7

Dimension 2:  Gender (male vs female)
  king: +0.8, queen: -0.7, man: +0.9, woman: -0.8

Dimension 3:  Sentiment (positive vs negative)
  love: +0.9, hate: -0.9, joy: +0.8, sad: -0.7

Dimension 4:  Abstraction (concrete vs abstract)
  democracy: +0.9, freedom: +0.8, apple: -0.9, chair: -0.7

...

Dimension 299: (Complex combination of features)

Note: Most dimensions don't have clean interpretations!
They capture complex combinations of semantic features.
```

#### Choosing the Right Dimensionality

```
Decision Tree for Choosing Embedding Dimensions:
═════════════════════════════════════════════════

                  Start
                    │
                    ▼
           ┌────────────────┐
           │ Vocabulary Size?│
           └────────┬────────┘
                    │
      ┌─────────────┼─────────────┐
      ▼             ▼             ▼
    V < 10K       10K < V < 50K   V > 50K
      │             │             │
      ▼             ▼             ▼
   = 50-100     d = 100-200   d = 200-300
      │             │             │
      └─────────────┴─────────────┘
                    │
                    ▼
            ┌─────────────────┐
            │ Memory Budget?  │
            └────────┬────────┘
                     │
       ┌─────────────┼─────────────┐
       ▼             ▼             ▼
    Tight (<50MB)  Medium       Flexible
       │            (50-200MB)    (>200MB)
       ▼             │             │
   Use lower d       │             ▼
      │              ▼         Use higher d
      │         Use middle       (300-600)
      │            (200)           │
      └─────────────┴──────────────┘
                    │
                    ▼
            Final Embedding Size
```

### Properties Summary

```
╔═══════════════════════════════════════════════════════════════════════╗
║                  KEY PROPERTIES OF WORD EMBEDDINGS                    ║
╠═══════════════════════════════════════════════════════════════════════╣
║                                                                       ║
║  1. COMPOSITIONALITY                                                  ║
║     ───────────────                                                   ║
║     Word meanings combine linearly:                                   ║
║       "very" + "good" ≈ "excellent"                                   ║
║       "un" + "happy" ≈ "sad"                                          ║
║                                                                       ║
║  2. LINEAR SUBSTRUCTURES                                              ║
║     ────────────────────                                              ║
║     Semantic relationships form parallel lines:                       ║
║       king - man + woman ≈ queen                                      ║
║       Paris - France + Italy ≈ Rome                                   ║
║                                                                       ║
║  3. TRANSFER LEARNING                                                 ║
║     ─────────────────                                                 ║
║     Pre-trained embeddings work across tasks:                         ║
║       • Sentiment analysis                                            ║
║       • Named entity recognition                                      ║
║       • Machine translation                                           ║
║       • Question answering                                            ║
║                                                                       ║
║  4. SEMANTIC SIMILARITY                                               ║
║     ───────────────────                                               ║
║     Similar words cluster together:                                   ║
║       cos(cat, dog) > 0.9                                             ║
║       cos(cat, automobile) < 0.3                                      ║
║                                                                       ║
║  5. DIMENSIONALITY REDUCTION                                          ║
║     ────────────────────────                                          ║
║     Compress sparse one-hot to dense vectors:                         ║
║       50,000 dimensions → 300 dimensions                              ║
║       166× compression with MORE information!                         ║
║                                                                       ║
║  6. CONTEXTUALIZATION (in modern embeddings)                          ║
║     ─────────────────────────────────────                             ║
║     Same word, different contexts:                                    ║
║       "bank" (river) ≠ "bank" (finance)                               ║
║       Handled by contextual models (BERT, GPT)                        ║
╚═══════════════════════════════════════════════════════════════════════╝
```

---

## Embedding Matrix

### What is an Embedding Matrix?

The **embedding matrix** is a learned lookup table that maps each word in the vocabulary to its corresponding dense vector representation. It's a fundamental component of neural NLP models.

**Concept**: Think of it as a dictionary where:
- **Keys**: Words (represented as integer indices)
- **Values**: Dense vectors (embeddings)

```
Lookup Table Visualization:
═══════════════════════════

Vocabulary (words)          Embedding Matrix E          Embedding Vectors
──────────────             ──────────────────          ─────────────────

Index 0: "hello"    ──►    Row 0: [0.2, 0.5, 0.1]  ──► e_hello
Index 1: "world"    ──►    Row 1: [0.3, 0.1, 0.8]  ──► e_world
Index 2: "good"     ──►    Row 2: [0.7, 0.4, 0.2]  ──► e_good
Index 3: "bad"      ──►    Row 3: [0.1, 0.6, 0.9]  ──► e_bad
   ...                        ...                          ...
Index V-1: "the"    ──►    Row V-1: [0.5, 0.3, 0.4] ──► e_the
```

### Matrix Dimensions

The embedding matrix has shape: **(vocabulary_size × embedding_dimension)**

$$E \in \mathbb{R}^{d \times V}$$

**Complete Symbol Legend**:
- $E$ = Embedding matrix (the full lookup table)
- $\mathbb{R}$ = Set of real numbers
- $d$ = Embedding dimension (e.g., 50, 100, 300)
- $V$ = Vocabulary size (number of unique words)
- $d \times V$ = Matrix dimensions: $d$ rows, $V$ columns
- Each column $E_{:,i}$ is the embedding for word $i$

**Note on Notation**: Sometimes written as $E \in \mathbb{R}^{V \times d}$ (transposed), depending on implementation. We'll use $\mathbb{R}^{d \times V}$ where each column is a word embedding.

#### ASCII Diagram of Embedding Matrix Structure

```
╔═══════════════════════════════════════════════════════════════════════╗
║                     EMBEDDING MATRIX STRUCTURE                        ║
╚═══════════════════════════════════════════════════════════════════════╝

          Vocabulary (V words)
          ←─────────────────────────────────────────────────→
          w_0      w_1      w_2      w_3     ...      w_{V-1}
        (hello)  (world)  (good)   (bad)              (the)

      ↑  ┌──────┬──────┬──────┬──────┬─────┬──────┐
      │  │ 0.24 │ 0.31 │ 0.72 │ 0.09 │ ... │ 0.53 │  ← dim 0
      │  ├──────┼──────┼──────┼──────┼─────┼──────┤
      │  │ 0.51 │ 0.15 │ 0.43 │ 0.61 │ ... │ 0.28 │  ← dim 1
      d  ├──────┼──────┼──────┼──────┼─────┼──────┤
      │  │ 0.08 │ 0.82 │ 0.19 │ 0.88 │ ... │ 0.44 │  ← dim 2
      │  ├──────┼──────┼──────┼──────┼─────┼──────┤
      │  │  ... │  ... │  ... │  ... │ ... │  ... │
      │  ├──────┼──────┼──────┼──────┼─────┼──────┤
      ↓  │ 0.67 │ 0.22 │ 0.55 │ 0.33 │ ... │ 0.71 │  ← dim d-1
         └──────┴──────┴──────┴──────┴─────┴──────┘
           ↑      ↑      ↑      ↑            ↑
           │      │      │      │            │
        e_hello e_world e_good  e_bad      e_the
        (col 0) (col 1) (col 2) (col 3)  (col V-1)


Each column is a d-dimensional embedding vector for one word.
Total parameters: d × V
```

### Mathematical Representation

The embedding matrix $E$ can be written as a concatenation of column vectors:

$$E = [e_0 \; e_1 \; e_2 \; \ldots \; e_{V-1}]$$

where each $e_i \in \mathbb{R}^d$ is the embedding vector for word $i$.

**Alternative notation**:
$$E_{i,j} = \text{the } i\text{-th dimension of the } j\text{-th word's embedding}$$

**Complete Symbol Legend**:
- $e_i$ = Embedding vector for word with index $i$ (a $d$-dimensional column vector)
- $E_{i,j}$ = Element at row $i$, column $j$ of the embedding matrix
- $i$ = Dimension index (ranges from 0 to $d-1$)
- $j$ = Word index (ranges from 0 to $V-1$)
- $[e_0 \; e_1 \; \ldots \; e_{V-1}]$ = Horizontal concatenation of column vectors

### How Embedding Lookup Works

#### Step-by-Step Process

```
┌────────────────────────────────────────────────────────────────┐
│                    EMBEDDING LOOKUP PROCESS                    │
├────────────────────────────────────────────────────────────────┤
│                                                                │
│  Step 1: Convert word to index                                 │
│  ─────────────────────────────                                 │
│    Word "good" → Vocabulary lookup → Index 2                   │
│                                                                │
│  Step 2: Create one-hot vector                                 │
│  ──────────────────────────                                    │
│    Index 2 → One-hot [0, 0, 1, 0, 0] (V-dimensional)          │
│                                                                │
│  Step 3: Matrix multiplication                                 │
│  ─────────────────────────                                     │
│    e_good = E · o_good                                         │
│    Where o_good is one-hot vector                              │
│                                                                │
│  Step 4: Extract embedding vector                              │
│  ────────────────────────────────                              │
│    Result is d-dimensional vector                              │
│                                                                │
└────────────────────────────────────────────────────────────────┘
```

#### Mathematical Formulation

The embedding lookup can be expressed as a matrix-vector multiplication:

$$e_w = E \cdot o_w$$

**Complete Symbol Legend**:
- $e_w$ = Embedding vector for word $w$ (output, dimension $d \times 1$)
- $E$ = Embedding matrix (dimension $d \times V$)
- $o_w$ = One-hot encoded vector for word $w$ (dimension $V \times 1$)
- $\cdot$ = Matrix-vector multiplication
- Result: $e_w$ is the column of $E$ corresponding to word $w$

**Why this works**:
```
One-hot vector selects a single column from E:

E · o_w = [e_0  e_1  e_2  ...  e_{V-1}] · [0]
                                          [0]
                                          [1]  ← position of word w
                                          [0]
                                          [0]
        = 0·e_0 + 0·e_1 + 1·e_2 + 0·e_3 + ... + 0·e_{V-1}
        = e_2  ← Only the w-th column is selected!
```

#### Complete Numerical Example

Let's work through a concrete example with a small vocabulary and embedding dimension:

**Setup**:
```
Vocabulary: ["hello", "world", "good", "bad", "the"]
Vocabulary size: V = 5
Embedding dimension: d = 3

Word to index mapping:
  "hello" → 0
  "world" → 1
  "good"  → 2
  "bad"   → 3
  "the"   → 4
```

**Embedding Matrix** (3 × 5):

```
           hello   world   good    bad     the
           (0)     (1)     (2)     (3)     (4)
         ┌──────┬──────┬──────┬──────┬──────┐
    dim 0│ 0.24 │ 0.31 │ 0.72 │ 0.09 │ 0.53 │
         ├──────┼──────┼──────┼──────┼──────┤
E = dim 1│ 0.51 │ 0.15 │ 0.43 │ 0.61 │ 0.28 │
         ├──────┼──────┼──────┼──────┼──────┤
    dim 2│ 0.08 │ 0.82 │ 0.19 │ 0.88 │ 0.44 │
         └──────┴──────┴──────┴──────┴──────┘
```

**Task**: Look up the embedding for word "good"

**Step 1: Convert "good" to index**
```
"good" → index 2
```

**Step 2: Create one-hot vector**
```
Index 2 → One-hot encoding (5-dimensional):

o_good = [0]  ← position 0 (hello)
         [0]  ← position 1 (world)
         [1]  ← position 2 (good) ✓
         [0]  ← position 3 (bad)
         [0]  ← position 4 (the)
```

**Step 3: Perform matrix multiplication E · o_good**

```
Matrix-vector multiplication:
════════════════════════════

E · o_good = [0.24  0.31  0.72  0.09  0.53] × [0]
             [0.51  0.15  0.43  0.61  0.28]   [0]
             [0.08  0.82  0.19  0.88  0.44]   [1]
                                              [0]
                                              [0]

Computing row by row:
────────────────────

Row 0: (0.24×0) + (0.31×0) + (0.72×1) + (0.09×0) + (0.53×0) = 0.72
Row 1: (0.51×0) + (0.15×0) + (0.43×1) + (0.61×0) + (0.28×0) = 0.43
Row 2: (0.08×0) + (0.82×0) + (0.19×1) + (0.88×0) + (0.44×0) = 0.19

Result:
───────

e_good = [0.72]
         [0.43]
         [0.19]

This is exactly column 2 of the embedding matrix E! ✓
```

### Embedding Matrix Dimensions Example

Let's consider a realistic scenario:

```
╔═══════════════════════════════════════════════════════════════════════╗
║           REALISTIC EMBEDDING MATRIX DIMENSIONS                       ║
╚═══════════════════════════════════════════════════════════════════════╝

Scenario: English NLP Task
═══════════════════════════

Vocabulary size: V = 10,000 common English words
Embedding dimension: d = 300 (standard choice)

Embedding Matrix Shape: E ∈ ℝ^(300 × 10,000)


Visualization (not to scale):
═════════════════════════════

        ←─────── 10,000 words ────────→
   ↑   ┌────────────────────────────────┐
   │   │                                │
   │   │                                │
 300   │     Embedding Matrix E         │
 dims  │     300 × 10,000 = 3M params   │
   │   │                                │
   │   │                                │
   ↓   └────────────────────────────────┘

Each column: 300 floating-point numbers
Total parameters: 300 × 10,000 = 3,000,000 parameters


Memory Requirements:
════════════════════

Each parameter: 32-bit float = 4 bytes

Total memory = 3,000,000 parameters × 4 bytes
             = 12,000,000 bytes
             = 12 MB

For 64-bit float: 24 MB

This is quite manageable for modern hardware! ✓


Comparison with One-Hot:
════════════════════════

One-hot encoding: Each word is 10,000-dimensional
Word embedding:   Each word is 300-dimensional

Dimensionality reduction: 10,000 → 300 (33× smaller!)

But information content: Embeddings contain MORE semantic information
                        despite being much smaller!
```

### Parameter Count Analysis

```
Parameters in Embedding Layer vs Other Layers:
══════════════════════════════════════════════

Example Neural Network for Text Classification:
───────────────────────────────────────────────

Layer 1: Embedding Layer
  Vocabulary: 10,000
  Embedding dim: 300
  Parameters: 10,000 × 300 = 3,000,000       ← 85% of total!

Layer 2: LSTM Layer
  Hidden units: 128
  Parameters: 4 × (300 + 128) × 128 = 219,136

Layer 3: Dense Layer
  Input: 128, Output: 64
  Parameters: 128 × 64 + 64 = 8,256

Layer 4: Output Layer
  Input: 64, Output: 5 (5 classes)
  Parameters: 64 × 5 + 5 = 325

Total parameters: 3,227,717
Embedding layer: 3,000,000 (93% of total!)


Key Observation:
═══════════════
The embedding layer often contains the MAJORITY of parameters
in NLP models. This is why:
  1. Pre-trained embeddings (Word2Vec, GloVe) are so valuable
  2. Transfer learning is effective in NLP
  3. Freezing embeddings during fine-tuning is common
```

### Initialization Methods

How should we initialize the embedding matrix before training?

```
╔═══════════════════════════════════════════════════════════════════════╗
║              EMBEDDING MATRIX INITIALIZATION METHODS                  ║
╠═══════════════════════════════════════════════════════════════════════╣
║                                                                       ║
║  Method 1: Random Initialization                                      ║
║  ─────────────────────────────                                        ║
║    Initialize with small random values from normal distribution:      ║
║      E_{i,j} ~ N(0, σ²)  where σ = 0.01 or 1/√d                       ║
║                                                                       ║
║    Pros: Simple, works for small datasets                             ║
║    Cons: Requires lots of training data to learn good embeddings      ║
║                                                                       ║
║  Method 2: Pre-trained Embeddings (Transfer Learning)                 ║
║  ────────────────────────────────────────────────                     ║
║    Load embeddings trained on large corpus (Word2Vec, GloVe, etc.):   ║
║      E = Load_pretrained("glove.6B.300d.txt")                         ║
║                                                                       ║
║    Pros: Excellent initialization, works with small datasets          ║
║    Cons: Fixed vocabulary, may not fit domain perfectly               ║
║                                                                       ║
║  Method 3: Trainable Pre-trained (Fine-tuning)                        ║
║  ──────────────────────────────────────────                           ║
║    Start with pre-trained, allow updates during training:             ║
║      E = Load_pretrained("glove.6B.300d.txt")                         ║
║      E.trainable = True                                               ║
║                                                                       ║
║    Pros: Best of both worlds, adapts to specific task                 ║
║    Cons: Risk of overfitting on small datasets                        ║
║                                                                       ║
║  Method 4: Frozen Pre-trained                                         ║
║  ───────────────────────────                                          ║
║    Use pre-trained, don't update during training:                     ║
║      E = Load_pretrained("glove.6B.300d.txt")                         ║
║      E.trainable = False                                              ║
║                                                                       ║
║    Pros: Prevents overfitting, faster training                        ║
║    Cons: Can't adapt to domain-specific terms                         ║
╚═══════════════════════════════════════════════════════════════════════╝


Initialization Example Code (Conceptual):
═════════════════════════════════════════

# Method 1: Random
E = np.random.normal(0, 0.01, size=(d, V))

# Method 2: Xavier/Glorot initialization
scale = np.sqrt(2.0 / (d + V))
E = np.random.uniform(-scale, scale, size=(d, V))

# Method 3: He initialization (for ReLU)
scale = np.sqrt(2.0 / d)
E = np.random.normal(0, scale, size=(d, V))


Choosing the Right Method:
══════════════════════════

Dataset Size │ Domain Match │ Recommendation
─────────────┼──────────────┼────────────────────────────
Small        │ Similar      │ Frozen pre-trained
Small        │ Different    │ Fine-tuned pre-trained
Medium       │ Similar      │ Fine-tuned pre-trained
Medium       │ Different    │ Random or fine-tuned
Large        │ Any          │ Random (train from scratch)
```

---

## Learning Word Embeddings

### Overview of Learning Approaches

Word embeddings can be learned in two main ways:

```
╔═══════════════════════════════════════════════════════════════════════╗
║                    LEARNING APPROACHES                                ║
╠═══════════════════════════════════════════════════════════════════════╣
║                                                                       ║
║  Approach 1: SUPERVISED LEARNING                                      ║
║  ───────────────────────────                                          ║
║                                                                       ║
║    Learn embeddings as part of a specific task (e.g., sentiment       ║
║    classification, named entity recognition)                          ║
║                                                                       ║
║    ┌─────────┐     ┌───────────┐     ┌────────┐     ┌────────┐        ║
║    │  Text   │────►│ Embedding │────►│  Task  │────►│ Labels │        ║
║    │  Input  │     │   Layer   │     │ Network│     │ (Gold) │        ║
║    └─────────┘     └───────────┘     └────────┘     └────────┘        ║
║                           ↑                               │           ║
║                           │                               │           ║
║                           └───── Backprop gradient ───────┘           ║
║                                                                       ║
║    Pros: Task-specific, optimal for the specific problem              ║
║    Cons: Requires labeled data, separate training per task            ║
║                                                                       ║
║  ──────────────────────────────────────────────────────────────       ║
║                                                                       ║
║  Approach 2: SELF-SUPERVISED LEARNING (Unsupervised)                  ║
║  ──────────────────────────────────────────────────                   ║
║                                                                       ║
║    Learn embeddings from raw text without labels by predicting        ║
║    words from context (or context from words)                         ║
║                                                                       ║
║    ┌─────────┐     ┌───────────┐     ┌────────────┐                   ║
║    │  Raw    │────►│ Embedding │────►│  Predict   │                   ║
║    │  Text   │     │   Layer   │     │Next/Context│                   ║
║    │(unlabel)│     └───────────┘     │    Word    │                   ║
║    └─────────┘            ↑          └────────────┘                   ║
║                           │                 │                         ║
║                           └──── Gradient ───┘                         ║
║                                                                       ║
║    Examples: Word2Vec, GloVe, FastText                                ║
║    Pros: No labeled data needed, generalizes across tasks             ║
║    Cons: May not capture task-specific nuances                        ║
╚═══════════════════════════════════════════════════════════════════════╝
```

### Learning in Neural Networks

When embeddings are learned as part of a neural network, they are treated as **learnable parameters** just like weights in other layers.

#### Embeddings as First Layer Parameters

```
Neural Network with Embedding Layer:
════════════════════════════════════

Input: "I love machine learning"
       ↓
Step 1: Convert to indices
       [34, 127, 892, 1045]
       ↓
Step 2: Embedding layer (lookup)
       ┌─────────────────────────────┐
       │   Embedding Matrix E        │
       │   (learnable parameters)    │
       │                             │
       │   [e_34, e_127, e_892, ...] │
       └─────────────────────────────┘
       ↓
Step 3: Pass embeddings to next layer
       [[0.2, 0.5, 0.1],   ← e_34 ("I")
        [0.3, 0.1, 0.8],   ← e_127 ("love")
        [0.7, 0.4, 0.2],   ← e_892 ("machine")
        [0.1, 0.6, 0.9]]   ← e_1045 ("learning")
       ↓
Step 4: LSTM/RNN/Transformer processing
       ↓
Step 5: Output layer
       ↓
Prediction
```

#### Backpropagation Updates

During training, gradients flow back to the embedding layer:

```
Forward Pass:
════════════

x (indices) → E (lookup) → h (hidden) → y (output) → L (loss)

Backward Pass:
═════════════

∂L/∂y → ∂L/∂h → ∂L/∂e → ∂L/∂E (embedding gradients)
                          ↑
                          Only update embeddings
                          for words in current batch!
```

#### Gradient Flow Through Embedding Layer

**Key Insight**: Only the embeddings of words that appear in the current batch receive gradient updates.

```
╔═══════════════════════════════════════════════════════════════════════╗
║              GRADIENT FLOW IN EMBEDDING LAYER                         ║
╚═══════════════════════════════════════════════════════════════════════╝

Embedding Matrix (V = 5, d = 3):
        word_0  word_1  word_2  word_3  word_4
      ┌──────┬──────┬──────┬──────┬──────┐
      │ 0.24 │ 0.31 │ 0.72 │ 0.09 │ 0.53 │
      ├──────┼──────┼──────┼──────┼──────┤
E =   │ 0.51 │ 0.15 │ 0.43 │ 0.61 │ 0.28 │
      ├──────┼──────┼──────┼──────┼──────┤
      │ 0.08 │ 0.82 │ 0.19 │ 0.88 │ 0.44 │
      └──────┴──────┴──────┴──────┴──────┘

Current batch contains: word_1 and word_3

Gradient from loss:
      ┌──────┬──────┬──────┬──────┬──────┐
      │  0   │-0.05 │  0   │ 0.03 │  0   │ ← Only columns 1 & 3
∂L    ├──────┼──────┼──────┼──────┼──────┤   get non-zero gradients!
── =  │  0   │ 0.02 │  0   │-0.01 │  0   │
∂E    ├──────┼──────┼──────┼──────┼──────┤
      │  0   │ 0.08 │  0   │-0.04 │  0   │
      └──────┴──────┴──────┴──────┴──────┘
         ↑      ↑      ↑      ↑      ↑
      unused  used  unused  used  unused

Update (with learning rate α = 0.1):
E_new = E - α · (∂L/∂E)

Only columns 1 and 3 are updated:
      ┌──────┬──────┬──────┬──────┬──────┐
      │ 0.24 │0.3105│ 0.72 │0.0897│ 0.53 │ ← Changed
      ├──────┼──────┼──────┼──────┼──────┤
E_new=│ 0.51 │0.1498│ 0.43 │0.6101│ 0.28 │ ← Changed
      ├──────┼──────┼──────┼──────┼──────┤
      │ 0.08 │0.8192│ 0.19 │0.8804│ 0.44 │ ← Changed
      └──────┴──────┴──────┴──────┴──────┘

Key Point: Embeddings for rare words update slowly because
          they appear in fewer batches!
```

#### Mathematical Formulation

**Forward Pass**:

$$e_w = E \cdot o_w$$

where $o_w$ is the one-hot vector for word $w$.

**Loss Function** (example: cross-entropy for classification):

$$L = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)$$

**Gradient with Respect to Embedding**:

$$\frac{\partial L}{\partial e_w} = \frac{\partial L}{\partial h} \cdot \frac{\partial h}{\partial e_w}$$

where $h$ is the hidden layer that receives the embedding.

**Gradient with Respect to Embedding Matrix**:

$$\frac{\partial L}{\partial E} = \frac{\partial L}{\partial e_w} \cdot o_w^T$$

This is an outer product that creates a matrix where only column $w$ is non-zero.

**Weight Update**:

$$E^{new} = E^{old} - \alpha \frac{\partial L}{\partial E}$$

**Complete Symbol Legend**:
- $L$ = Loss function value
- $e_w$ = Embedding for word $w$ (dimension $d \times 1$)
- $E$ = Embedding matrix (dimension $d \times V$)
- $o_w$ = One-hot vector for word $w$ (dimension $V \times 1$)
- $h$ = Hidden state in next layer
- $\frac{\partial L}{\partial e_w}$ = Gradient of loss w.r.t. word embedding (dimension $d \times 1$)
- $\frac{\partial L}{\partial E}$ = Gradient of loss w.r.t. embedding matrix (dimension $d \times V$)
- $o_w^T$ = Transpose of one-hot vector (dimension $1 \times V$)
- $\alpha$ = Learning rate
- $E^{new}, E^{old}$ = Updated and old embedding matrices

#### Complete Numerical Example with Small Network

Let's trace through a complete example:

**Setup**:
```
Vocabulary: ["I", "love", "NLP"]  (V = 3)
Embedding dimension: d = 2
Task: Binary classification (positive/negative sentiment)

Network architecture:
  Input → Embedding (2D) → Dense (1 neuron) → Sigmoid → Output
```

**Initial Parameters**:
```
Embedding Matrix E (2 × 3):
        "I"    "love"   "NLP"
      ┌─────┬───────┬───────┐
E =   │ 0.5 │  0.3  │  0.7  │ ← dim 0
      ├─────┼───────┼───────┤
      │ 0.2 │  0.8  │  0.4  │ ← dim 1
      └─────┴───────┴───────┘

Dense layer weights W_dense (1 × 2):
W_dense = [0.6, 0.4]

Bias: b_dense = 0.1
```

**Forward Pass**:
```
Input sentence: "I love NLP"
Indices: [0, 1, 2]

Step 1: Lookup embeddings
═════════════════════════

e_I   = E[:, 0] = [0.5, 0.2]
e_love= E[:, 1] = [0.3, 0.8]
e_NLP = E[:, 2] = [0.7, 0.4]


Step 2: Average embeddings (simple pooling)
════════════════════════════════════════════

e_avg = (e_I + e_love + e_NLP) / 3
      = ([0.5, 0.2] + [0.3, 0.8] + [0.7, 0.4]) / 3
      = [1.5, 1.4] / 3
      = [0.5, 0.467]


Step 3: Dense layer
═══════════════════

z = W_dense · e_avg + b_dense
  = [0.6, 0.4] · [0.5, 0.467] + 0.1
  = (0.6 × 0.5) + (0.4 × 0.467) + 0.1
  = 0.3 + 0.187 + 0.1
  = 0.587


Step 4: Sigmoid activation
═══════════════════════════

ŷ = σ(z) = 1 / (1 + exp(-z))
         = 1 / (1 + exp(-0.587))
         = 1 / (1 + 0.556)
         = 1 / 1.556
         = 0.643


Step 5: Compute loss (binary cross-entropy)
═══════════════════════════════════════════

True label: y = 1 (positive sentiment)

L = -[y log(ŷ) + (1-y) log(1-ŷ)]
  = -[1 × log(0.643) + 0 × log(0.357)]
  = -log(0.643)
  = -(-0.442)
  = 0.442
```

**Backward Pass**:
```
Step 1: Gradient of loss w.r.t. output
═══════════════════════════════════════

∂L/∂ŷ = -(y/ŷ - (1-y)/(1-ŷ))
      = -(1/0.643 - 0/0.357)
      = -1.556


Step 2: Gradient through sigmoid
═════════════════════════════════

∂L/∂z = ∂L/∂ŷ × ∂ŷ/∂z
      = ∂L/∂ŷ × ŷ(1-ŷ)
      = -1.556 × 0.643 × 0.357
      = -1.556 × 0.230
      = -0.357


Step 3: Gradient w.r.t. average embedding
══════════════════════════════════════════

∂L/∂e_avg = W_dense^T × ∂L/∂z
          = [0.6]^T × (-0.357)
            [0.4]
          = [-0.214]
            [-0.143]


Step 4: Gradient w.r.t. individual embeddings
══════════════════════════════════════════════

Since e_avg = (e_I + e_love + e_NLP) / 3:

∂L/∂e_I = ∂L/∂e_avg / 3 = [-0.214, -0.143] / 3 = [-0.071, -0.048]
∂L/∂e_love = ∂L/∂e_avg / 3 = [-0.071, -0.048]
∂L/∂e_NLP = ∂L/∂e_avg / 3 = [-0.071, -0.048]


Step 5: Gradient w.r.t. embedding matrix
═════════════════════════════════════════

∂L/∂E[:,0] = ∂L/∂e_I   = [-0.071, -0.048]
∂L/∂E[:,1] = ∂L/∂e_love = [-0.071, -0.048]
∂L/∂E[:,2] = ∂L/∂e_NLP  = [-0.071, -0.048]

Full gradient matrix (2 × 3):
         "I"      "love"    "NLP"
      ┌────────┬─────────┬─────────┐
∂L/∂E=│-0.071  │ -0.071  │ -0.071  │
      ├────────┼─────────┼─────────┤
      │-0.048  │ -0.048  │ -0.048  │
      └────────┴─────────┴─────────┘
```

**Parameter Updates** (learning rate α = 0.1):
```
Embedding matrix update:
════════════════════════

E_new = E_old - α × (∂L/∂E)

      ┌─────┬───────┬───────┐       ┌────────┬─────────┬─────────┐
E_new=│ 0.5 │  0.3  │  0.7  │ - 0.1×│-0.071  │ -0.071  │ -0.071  │
      ├─────┼───────┼───────┤       ├────────┼─────────┼─────────┤
      │ 0.2 │  0.8  │  0.4  │       │-0.048  │ -0.048  │ -0.048  │
      └─────┴───────┴───────┘       └────────┴─────────┴─────────┘

      ┌───────┬────────┬────────┐
    = │0.5071 │ 0.3071 │ 0.7071 │ ← All increased slightly
      ├───────┼────────┼────────┤
      │0.2048 │ 0.8048 │ 0.4048 │
      └───────┴────────┴────────┘


Dense layer update:
═══════════════════

∂L/∂W_dense = ∂L/∂z × e_avg^T
            = -0.357 × [0.5, 0.467]
            = [-0.179, -0.167]

W_dense_new = [0.6, 0.4] - 0.1 × [-0.179, -0.167]
            = [0.6179, 0.4167]


Summary:
════════
After one training step:
- Embeddings slightly adjusted to better represent sentiment
- All three word embeddings updated (since all appeared in sentence)
- Dense weights also updated
```

### Simple Neural Language Model

A **neural language model** learns to predict the next word in a sequence given the previous words. The embeddings are learned as a byproduct of this task.

#### Architecture

```
╔═══════════════════════════════════════════════════════════════════════╗
║              NEURAL LANGUAGE MODEL ARCHITECTURE                       ║
╚═══════════════════════════════════════════════════════════════════════╝

Task: Given context words, predict next word

Input:  "I love machine ___"  → Predict "learning"

Architecture:
═══════════

    Context words: ["I", "love", "machine"]
         ↓          ↓      ↓         ↓
    [Index 0]  [Index 1] [Index 2]
         ↓          ↓      ↓
    ┌────────────────────────────────┐
    │    Embedding Layer (E)         │
    │    Lookup embeddings           │
    └────────┬───────────────────────┘
             ↓
    [e_I, e_love, e_machine]  (each d-dimensional)
             ↓
    ┌────────────────────────────────┐
    │  Concatenate or Average        │
    │  Context representation        │
    └────────┬───────────────────────┘
             ↓
    ┌────────────────────────────────┐
    │    Hidden Layer (Dense)        │
    │    h = tanh(W_h · context + b_h)│
    └────────┬───────────────────────┘
             ↓
    ┌────────────────────────────────┐
    │    Output Layer (Dense)        │
    │    scores = W_o · h + b_o      │
    └────────┬───────────────────────┘
             ↓
    ┌────────────────────────────────┐
    │    Softmax                     │
    │    P(word|context)             │
    └────────┬───────────────────────┘
             ↓
    Probability distribution over vocabulary
    [P(w_0), P(w_1), ..., P(w_V-1)]
              ↑
              Most likely next word
```

#### Task: Predict Next Word

Given context: "I love machine"
Predict: "learning"

#### ASCII Diagram of Architecture

```
     Context Window (3 words)
     ────────────────────────
     
     "I"     "love"    "machine"
      │         │          │
      ▼         ▼          ▼
    [34]     [127]      [892]     ← Word indices
      │         │          │
      ├─────────┼──────────┤
      │         │          │
      ▼         ▼          ▼
    ┌──────────────────────────┐
    │   Embedding Matrix E     │
    │   (d × V parameters)     │
    │                          │
    │  [e_34, e_127, e_892]    │
    └─────────┬────────────────┘
              │
              ▼
    ┌──────────────────────────┐
    │ Concatenate              │
    │ [e_34; e_love; e_892]    │  ← 3d-dimensional
    └─────────┬────────────────┘
              │
              ▼
    ┌──────────────────────────┐
    │ Hidden Layer             │
    │ h = tanh(W_h·concat + b) │
    │ (n_h units)              │
    └─────────┬────────────────┘
              │
              ▼
    ┌──────────────────────────┐
    │ Output Layer             │
    │ scores = W_o·h + b_o     │
    │ (V-dimensional)          │
    └─────────┬────────────────┘
              │
              ▼
    ┌──────────────────────────┐
    │ Softmax                  │
    │ P(w|context)             │
    └─────────┬────────────────┘
              │
              ▼
    [0.001, 0.003, ..., 0.512, ...]
                         ↑
              Index 1045: "learning" (highest prob!)
```

#### Mathematical Formulation

**Step 1: Embed context words**

For context of $n$ words: $w_1, w_2, ..., w_n$

$$e_i = E \cdot o_{w_i} \quad \text{for } i = 1, 2, ..., n$$

**Step 2: Combine context embeddings**

Option A - Concatenation:
$$c = [e_1; e_2; ...; e_n] \in \mathbb{R}^{n \cdot d}$$

Option B - Average:
$$c = \frac{1}{n} \sum_{i=1}^{n} e_i \in \mathbb{R}^{d}$$

**Step 3: Hidden layer**

$$h = \tanh(W_h c + b_h)$$

**Step 4: Output layer**

$$z = W_o h + b_o$$

**Step 5: Softmax for probability distribution**

$$P(w_j | \text{context}) = \frac{\exp(z_j)}{\sum_{k=1}^{V} \exp(z_k)}$$

**Step 6: Loss (negative log-likelihood)**

$$L = -\log P(w_{\text{true}} | \text{context})$$

**Complete Symbol Legend**:
- $w_i$ = The $i$-th word in the context
- $o_{w_i}$ = One-hot encoding of word $w_i$
- $e_i$ = Embedding of word $w_i$ (dimension $d \times 1$)
- $E$ = Embedding matrix (dimension $d \times V$)
- $c$ = Combined context representation
- $n$ = Context window size (number of preceding words)
- $d$ = Embedding dimension
- $h$ = Hidden layer activation (dimension $n_h \times 1$)
- $W_h$ = Hidden layer weights (dimension $n_h \times (n \cdot d)$ for concat, or $n_h \times d$ for average)
- $b_h$ = Hidden layer bias (dimension $n_h \times 1$)
- $z$ = Output scores (logits) before softmax (dimension $V \times 1$)
- $W_o$ = Output layer weights (dimension $V \times n_h$)
- $b_o$ = Output layer bias (dimension $V \times 1$)
- $P(w_j | \text{context})$ = Probability of word $j$ given context
- $w_{\text{true}}$ = The actual next word (ground truth)
- $V$ = Vocabulary size
- $L$ = Loss (negative log-likelihood)

#### Step-by-Step Example: Predict "learning" from "I love machine"

**Setup**:
```
Vocabulary: ["I", "love", "machine", "learning", "deep"]  (V = 5)
Embedding dimension: d = 2
Context window: n = 3 words
Hidden units: n_h = 3
```

**Parameters**:
```
Embedding Matrix E (2 × 5):
        "I"   "love" "machine" "learning" "deep"
         0      1       2         3         4
      ┌────┬──────┬────────┬──────────┬──────┐
E =   │0.1 │ 0.3  │  0.7   │   0.4    │ 0.6  │
      ├────┼──────┼────────┼──────────┼──────┤
      │0.2 │ 0.8  │  0.5   │   0.6    │ 0.3  │
      └────┴──────┴────────┴──────────┴──────┘

Context window size: 3
We'll concatenate embeddings → 6-dimensional input

Hidden layer weights W_h (3 × 6):
      ┌──────────────────────────┐
W_h = │ 0.1  0.2  0.3  0.1  0.2  0.3│
      ├──────────────────────────┤
      │ 0.2  0.1  0.1  0.3  0.2  0.1│
      ├──────────────────────────┤
      │ 0.3  0.1  0.2  0.2  0.1  0.3│
      └──────────────────────────┘

b_h = [0.1, 0.1, 0.1]^T

Output weights W_o (5 × 3):
      ┌───────────────┐
W_o = │ 0.5  0.3  0.2 │ ← for "I"
      ├───────────────┤
      │ 0.4  0.5  0.3 │ ← for "love"
      ├───────────────┤
      │ 0.3  0.4  0.6 │ ← for "machine"
      ├───────────────┤
      │ 0.6  0.6  0.5 │ ← for "learning"
      ├───────────────┤
      │ 0.2  0.3  0.4 │ ← for "deep"
      └───────────────┘

b_o = [0, 0, 0, 0, 0]^T
```

**Forward Pass**:
```
Context: ["I", "love", "machine"]
Indices: [0, 1, 2]
True next word: "learning" (index 3)


Step 1: Lookup embeddings
══════════════════════════

e_I       = E[:, 0] = [0.1, 0.2]
e_love    = E[:, 1] = [0.3, 0.8]
e_machine = E[:, 2] = [0.7, 0.5]


Step 2: Concatenate context embeddings
═══════════════════════════════════════

c = [e_I; e_love; e_machine]
  = [0.1, 0.2, 0.3, 0.8, 0.7, 0.5]^T  (6-dimensional)


Step 3: Hidden layer computation
═════════════════════════════════

W_h · c:
  Row 1: 0.1×0.1 + 0.2×0.2 + 0.3×0.3 + 0.1×0.8 + 0.2×0.7 + 0.3×0.5
       = 0.01 + 0.04 + 0.09 + 0.08 + 0.14 + 0.15 = 0.51
  
  Row 2: 0.2×0.1 + 0.1×0.2 + 0.1×0.3 + 0.3×0.8 + 0.2×0.7 + 0.1×0.5
       = 0.02 + 0.02 + 0.03 + 0.24 + 0.14 + 0.05 = 0.50
  
  Row 3: 0.3×0.1 + 0.1×0.2 + 0.2×0.3 + 0.2×0.8 + 0.1×0.7 + 0.3×0.5
       = 0.03 + 0.02 + 0.06 + 0.16 + 0.07 + 0.15 = 0.49

W_h · c + b_h = [0.51, 0.50, 0.49] + [0.1, 0.1, 0.1]
              = [0.61, 0.60, 0.59]

h = tanh([0.61, 0.60, 0.59])
  = [0.544, 0.537, 0.531]


Step 4: Output layer computation
═════════════════════════════════

z = W_o · h + b_o

z_0 ("I"):        0.5×0.544 + 0.3×0.537 + 0.2×0.531 + 0 = 0.272 + 0.161 + 0.106 = 0.539
z_1 ("love"):     0.4×0.544 + 0.5×0.537 + 0.3×0.531 + 0 = 0.218 + 0.269 + 0.159 = 0.646
z_2 ("machine"):  0.3×0.544 + 0.4×0.537 + 0.6×0.531 + 0 = 0.163 + 0.215 + 0.319 = 0.697
z_3 ("learning"): 0.6×0.544 + 0.6×0.537 + 0.5×0.531 + 0 = 0.326 + 0.322 + 0.266 = 0.914 ← Highest!
z_4 ("deep"):     0.2×0.544 + 0.3×0.537 + 0.4×0.531 + 0 = 0.109 + 0.161 + 0.212 = 0.482

z = [0.539, 0.646, 0.697, 0.914, 0.482]


Step 5: Softmax
═══════════════

exp(z) = [exp(0.539), exp(0.646), exp(0.697), exp(0.914), exp(0.482)]
       = [1.714, 1.908, 2.008, 2.494, 1.620]

sum = 1.714 + 1.908 + 2.008 + 2.494 + 1.620 = 9.744

P(w|context) = exp(z) / sum
             = [1.714/9.744, 1.908/9.744, 2.008/9.744, 2.494/9.744, 1.620/9.744]
             = [0.176, 0.196, 0.206, 0.256, 0.166]
                                      ↑
                          Highest probability!

Prediction: word 3 ("learning") ✓ Correct!


Step 6: Compute loss
════════════════════

True word: "learning" (index 3)

L = -log(P(w_3 | context))
  = -log(0.256)
  = -(-1.363)
  = 1.363
```

This example shows how the neural language model learns embeddings while predicting the next word. The embeddings are adjusted via backpropagation to minimize the prediction error!

### Word2Vec: Skip-gram Model

Video explanation of Word2Vec and Negative Sampling (https://www.youtube.com/watch?v=viZrOnJclY0&t=720s)

Word2Vec is one of the most influential word embedding models, introduced by Mikolov et al. in 2013. The **Skip-gram** model predicts context words given a center word.

#### Introduction and Architecture

**Key Idea**: Words that appear in similar contexts have similar meanings. Skip-gram learns this by maximizing the probability of context words given a center word.

```
Skip-gram Task:
═══════════════

Given:   Center word (e.g., "machine")
Predict: Context words (e.g., "I", "love", "learning", "is")

Example sentence: "I love machine learning and deep learning"
                         ↑ center word

Context window = 2:
    Context: ["love", "learning"]  (2 words before and after)


Architecture:
════════════

  Center word: "machine"
       ↓
  [Index 892]
       ↓
  ┌──────────────┐
  │  Embedding   │ ← This is what we want to learn!
  │  e_machine   │
  └──────┬───────┘
         ↓
  ┌──────────────┐
  │  Project to  │
  │  V dimensions│ (vocabulary size)
  └──────┬───────┘
         ↓
  ┌──────────────┐
  │   Softmax    │
  └──────┬───────┘
         ↓
   Probability distribution
   [P(w_0|machine), P(w_1|machine), ..., P(w_V-1|machine)]
         ↓
   High probability for context words:
   P("love"|"machine") should be high
   P("learning"|"machine") should be high
```

#### ASCII Diagram

```
╔═══════════════════════════════════════════════════════════════════════╗
║                    SKIP-GRAM ARCHITECTURE                             ║
╚═══════════════════════════════════════════════════════════════════════╝

Sentence: "I love machine learning"
                   ↑
               Center word

Training pairs (window=2):
  ("machine", "I")
  ("machine", "love")
  ("machine", "learning")


For each pair ("machine", context_word):
────────────────────────────────────────

    Input: "machine" (index c)
         │
         ▼
    ┌─────────────────┐
    │  One-hot: o_c   │  (V-dimensional, only position c is 1)
    └────────┬────────┘
             │
             ▼
    ┌─────────────────┐
    │ Embedding Matrix│
    │       E         │  (d × V)
    │                 │
    │  e_c = E · o_c  │  Select column c
    └────────┬────────┘
             │
             ▼
    ┌─────────────────┐
    │ Context Matrix  │
    │       C         │  (V × d)
    │                 │
    │  scores = C·e_c │  (V-dimensional)
    └────────┬────────┘
             │
             ▼
    ┌─────────────────┐
    │    Softmax      │
    │                 │
    │ P(o|c) = softmax(scores)
    └────────┬────────┘
             │
             ▼
    Output: Probability distribution over V words
    
    High probability for actual context word ✓


Two embedding matrices:
───────────────────────
E: Center word embeddings (d × V)
C: Context word embeddings (V × d)

After training, typically use E or average (E + C^T)/2
```

#### Mathematical Formulation

**Objective**: Maximize the log probability of context words given the center word.

For a center word $c$ and context word $o$:

$$P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w=1}^V \exp(u_w^T v_c)}$$

**Complete Symbol Legend**:
- $P(o|c)$ = Probability of context word $o$ given center word $c$
- $v_c$ = Embedding vector for center word $c$ (from matrix $E$, dimension $d \times 1$)
- $u_o$ = Context embedding vector for word $o$ (from matrix $C$, dimension $d \times 1$)
- $u_o^T$ = Transpose of $u_o$ (dimension $1 \times d$)
- $u_o^T v_c$ = Dot product (scalar similarity score)
- $V$ = Vocabulary size
- $w$ = Index summing over all words in vocabulary
- $\exp()$ = Exponential function
- Numerator: Similarity between center and context word
- Denominator: Normalization over all possible words (softmax)

**Training Objective** (for entire corpus):

$$J = -\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \leq j \leq m, j \neq 0} \log P(w_{t+j} | w_t)$$

**Complete Symbol Legend**:
- $J$ = Training objective (to be minimized)
- $T$ = Total number of words in corpus
- $t$ = Position of center word in corpus
- $w_t$ = Center word at position $t$
- $w_{t+j}$ = Context word at relative position $j$
- $m$ = Context window size
- $-m \leq j \leq m, j \neq 0$ = All positions in window except center
- $\log P(w_{t+j} | w_t)$ = Log probability of context word given center word

#### Numerical Example with Small Vocabulary

**Setup**:
```
Vocabulary: ["king", "queen", "man", "woman"]  (V = 4)
Embedding dimension: d = 2
Sentence: "king and queen"
Center word: "king" (index 0)
Context window: 1
Context word: "queen" (index 1)
```

**Embeddings**:
```
Center word embedding matrix E (2 × 4):
        king  queen  man  woman
         0     1     2     3
      ┌────┬─────┬────┬─────┐
E =   │0.8 │ 0.7 │0.5 │ 0.4 │ ← dim 0
      ├────┼─────┼────┼─────┤
      │0.6 │ 0.5 │0.8 │ 0.7 │ ← dim 1
      └────┴─────┴────┴─────┘

Context embedding matrix C (4 × 2):
      ┌────┬────┐
      │0.9 │0.4 │ ← for "king"
C =   ├────┼────┤
      │0.8 │0.5 │ ← for "queen"
      ├────┼────┤
      │0.6 │0.7 │ ← for "man"
      ├────┼────┤
      │0.5 │0.6 │ ← for "woman"
      └────┴────┘
```

**Forward Pass**:
```
Center word: "king" (index 0)
Context word: "queen" (index 1)


Step 1: Get center word embedding
══════════════════════════════════

v_king = E[:, 0] = [0.8, 0.6]


Step 2: Compute scores (dot products with all context embeddings)
══════════════════════════════════════════════════════════════════

For each word w, compute u_w^T · v_king:

u_king^T · v_king   = [0.9, 0.4] · [0.8, 0.6] = 0.9×0.8 + 0.4×0.6 = 0.72 + 0.24 = 0.96
u_queen^T · v_king  = [0.8, 0.5] · [0.8, 0.6] = 0.8×0.8 + 0.5×0.6 = 0.64 + 0.30 = 0.94 ← Target
u_man^T · v_king    = [0.6, 0.7] · [0.8, 0.6] = 0.6×0.8 + 0.7×0.6 = 0.48 + 0.42 = 0.90
u_woman^T · v_king  = [0.5, 0.6] · [0.8, 0.6] = 0.5×0.8 + 0.6×0.6 = 0.40 + 0.36 = 0.76

scores = [0.96, 0.94, 0.90, 0.76]


Step 3: Apply softmax
═════════════════════

exp(scores) = [exp(0.96), exp(0.94), exp(0.90), exp(0.76)]
            = [2.611, 2.560, 2.460, 2.138]

sum = 2.611 + 2.560 + 2.460 + 2.138 = 9.769

P(w|king) = exp(scores) / sum
          = [2.611/9.769, 2.560/9.769, 2.460/9.769, 2.138/9.769]
          = [0.267, 0.262, 0.252, 0.219]
              ↑      ↑
            king  queen

P(queen|king) = 0.262


Step 4: Compute loss
════════════════════

L = -log P(queen|king)
  = -log(0.262)
  = -(-1.340)
  = 1.340

Goal: Minimize this loss by adjusting E and C so that
      P(queen|king) increases toward 1.0
```

#### Training Objective and Loss Function

The Skip-gram model learns by **maximizing** the probability of observing actual context words:

$$\text{maximize} \quad \prod_{(c,o) \in \mathcal{D}} P(o|c)$$

Equivalent to **minimizing** negative log-likelihood:

$$\text{minimize} \quad L = -\sum_{(c,o) \in \mathcal{D}} \log P(o|c)$$

where $\mathcal{D}$ is the set of all (center, context) pairs in the training corpus.

#### Visualization of Training Process

```
╔═══════════════════════════════════════════════════════════════════════╗
║                  SKIP-GRAM TRAINING PROCESS                           ║
╚═══════════════════════════════════════════════════════════════════════╝

Corpus: "I love machine learning and machine learning is fun"

Step 1: Generate training pairs (window=2)
═══════════════════════════════════════════

Center   →  Context
──────────────────────
"I"      →  "love"
"love"   →  "I", "machine"
"machine"→  "love", "learning"
"learning" → "machine", "and"
"and"    →  "learning", "machine"
...


Step 2: For each pair, update embeddings
═════════════════════════════════════════

Pair: ("machine", "learning")

Before training:
  v_machine = [0.1, 0.2]
  u_learning = [0.3, 0.4]
  
  P("learning"|"machine") = 0.15  (low, need to increase!)

After gradient descent step:
  v_machine_new = [0.12, 0.23]  ← Moved closer to u_learning
  u_learning_new = [0.31, 0.42] ← Moved closer to v_machine
  
  P("learning"|"machine") = 0.25  (increased! ✓)


Step 3: Repeat for all pairs over multiple epochs
══════════════════════════════════════════════════

Initially: Random embeddings, poor predictions
After 1000 steps: Embeddings start to cluster by meaning
After 10000 steps: Good embeddings, accurate predictions

Result:
  Words in similar contexts have similar embeddings
  "machine" and "learning" close together
  "love" and "hate" far apart but both emotion-related
```

### Word2Vec: CBOW Model

**Continuous Bag-of-Words (CBOW)** is the complement of Skip-gram: it predicts the center word given context words.

#### Introduction

**Key Idea**: Given surrounding words, predict the missing center word.

```
CBOW Task:
══════════

Given:   Context words (e.g., "I", "love", "learning", "is")
Predict: Center word (e.g., "machine")

Example: "I love ___ learning"
              → Fill in "machine"
```

#### ASCII Diagram

```
╔═══════════════════════════════════════════════════════════════════════╗
║                      CBOW ARCHITECTURE                                ║
╚═══════════════════════════════════════════════════════════════════════╝

Sentence: "I love machine learning"
                    ↑
               Target (center word)

Context words: ["I", "love", "learning"] (window=1 for simplicity)


    Context word 1: "I"
         │
         ▼
    ┌─────────────┐
    │ Embedding   │
    │   e_I       │
    └──────┬──────┘
           │
           │      Context word 2: "love"
           │           │
           │           ▼
           │      ┌─────────────┐
           │      │ Embedding   │
           │      │   e_love    │
           │      └──────┬──────┘
           │             │
           │             │      Context word 3: "learning"
           │             │           │
           │             │           ▼
           │             │      ┌─────────────┐
           │             │      │ Embedding   │
           │             │      │ e_learning  │
           │             │      └──────┬──────┘
           │             │             │
           └─────────────┴─────────────┘
                         │
                         ▼
               ┌──────────────────┐
               │    Average       │
               │                  │
               │ e_avg = (e_I +   │
               │  e_love +        │
               │  e_learning) / 3 │
               └────────┬─────────┘
                        │
                        ▼
               ┌──────────────────┐
               │  Project to V    │
               │  dimensions      │
               │                  │
               │ scores = W·e_avg │
               └────────┬─────────┘
                        │
                        ▼
               ┌──────────────────┐
               │    Softmax       │
               └────────┬─────────┘
                        │
                        ▼
          P(center|context) distribution
                        │
                        ▼
        Should predict "machine" with high probability!
```

#### Mathematical Formulation

**Step 1: Average context embeddings**

For context words $c_1, c_2, ..., c_m$:

$$\bar{v} = \frac{1}{m} \sum_{i=1}^{m} v_{c_i}$$

**Step 2: Predict center word**

$$P(o|\text{context}) = \frac{\exp(u_o^T \bar{v})}{\sum_{w=1}^V \exp(u_w^T \bar{v})}$$

**Complete Symbol Legend**:
- $\bar{v}$ = Average of context word embeddings (dimension $d \times 1$)
- $m$ = Number of context words
- $v_{c_i}$ = Embedding of $i$-th context word
- $o$ = Target center word
- $u_o$ = Context embedding for target word $o$
- $P(o|\text{context})$ = Probability of center word given context
- $V$ = Vocabulary size

#### Comparison with Skip-gram

```
╔═══════════════════════════════════════════════════════════════════════╗
║              SKIP-GRAM vs CBOW COMPARISON                             ║
╠═══════════════════════════════════════════════════════════════════════╣
║                                                                       ║
║  Aspect          Skip-gram                 CBOW                       ║
║  ──────────────────────────────────────────────────────────────────  ║
║  Input           Center word               Context words             ║
║  Output          Context words             Center word               ║
║  Prediction      One-to-Many               Many-to-One               ║
║  Training pairs  Multiple per word         One per word group        ║
║  Speed           Slower (more updates)     Faster (fewer updates)    ║
║  Small data      Better                    Worse                     ║
║  Frequent words  Worse                     Better                    ║
║  Rare words      Better (more training)    Worse (averaged out)      ║
║                                                                       ║
║  Example:                                                             ║
║  ─────────────────────────────────────────────────────────────────   ║
║  Sentence: "I love machine learning"                                 ║
║                                                                       ║
║  Skip-gram creates:                                                   ║
║    ("machine", "I")                                                   ║
║    ("machine", "love")                                                ║
║    ("machine", "learning")                                            ║
║    → 3 training examples per center word                              ║
║                                                                       ║
║  CBOW creates:                                                        ║
║    (["I", "love", "learning"], "machine")                             ║
║    → 1 training example per word                                      ║
║                                                                       ║
║  Which to choose?                                                     ║
║  ────────────────                                                     ║
║  • Small dataset + rare words  → Skip-gram                            ║
║  • Large dataset + speed       → CBOW                                 ║
║  • Most research uses Skip-gram (better quality)                      ║
╚═══════════════════════════════════════════════════════════════════════╝
```

### Negative Sampling

Video explanation of Word2Vec and Negative Sampling (https://www.youtube.com/watch?v=viZrOnJclY0&t=720s)

#### Problem: Computational Cost of Softmax

The main bottleneck in Word2Vec is computing the softmax over the entire vocabulary:

$$P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w=1}^V \exp(u_w^T v_c)}$$

**Problem**: The denominator requires computing $V$ dot products, where $V$ can be 50,000+ words!

```
Computational Cost Analysis:
════════════════════════════

For each training example:
  - Compute similarity with target word: O(d)
  - Compute similarity with ALL V words: O(V × d)  ← Expensive!
  - Apply softmax: O(V)

Total per example: O(V × d)

For 1 billion training pairs and V=100,000:
  Total operations ≈ 10^11 × d  (infeasible!)
```

#### Solution: Negative Sampling

Instead of computing probabilities for all $V$ words, sample a small number of "negative" examples.

**New Objective**: For each positive (center, context) pair, sample $k$ random "negative" words that aren't in the context.

$$J = \log \sigma(u_o^T v_c) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} [\log \sigma(-u_{w_i}^T v_c)]$$

**Complete Symbol Legend**:
- $\sigma(x) = \frac{1}{1 + e^{-x}}$ = Sigmoid function
- $o$ = Positive context word (actual context)
- $c$ = Center word
- $v_c$ = Embedding of center word
- $u_o$ = Context embedding of positive word
- $k$ = Number of negative samples (typically 5-20)
- $w_i$ = $i$-th negative sample word
- $P_n(w)$ = Noise distribution for sampling negatives
- $\mathbb{E}$ = Expectation (average over random samples)
- $u_{w_i}$ = Context embedding of negative word
- First term: Maximize probability of positive pair
- Second term: Minimize probability of negative pairs

```
Example:
════════

Positive pair: ("machine", "learning")
Negative samples (k=5): "apple", "run", "happy", "the", "politics"

Goal:
  - σ(u_learning^T · v_machine) → 1  (positive pair)
  - σ(u_apple^T · v_machine) → 0     (negative pair)
  - σ(u_run^T · v_machine) → 0       (negative pair)
  - ...
```

#### Complete Example with Calculations

**Setup**:
```
Vocabulary: ["machine", "learning", "apple", "run", "happy"]  (V = 5)
Embedding dimension: d = 2
Positive pair: ("machine", "learning")
Negative samples (k = 2): "apple", "run"
```

**Embeddings**:
```
Center embeddings:
v_machine = [0.8, 0.6]

Context embeddings:
u_learning = [0.7, 0.5]  (positive)
u_apple = [0.1, 0.2]     (negative)
u_run = [0.3, 0.1]       (negative)
```

**Calculation**:
```
Step 1: Positive term
═════════════════════

Compute u_learning^T · v_machine:
  = [0.7, 0.5] · [0.8, 0.6]
  = 0.7×0.8 + 0.5×0.6
  = 0.56 + 0.30
  = 0.86

Apply sigmoid:
  σ(0.86) = 1 / (1 + exp(-0.86))
          = 1 / (1 + 0.423)
          = 1 / 1.423
          = 0.703

Positive term: log(0.703) = -0.353


Step 2: Negative term (sample 1: "apple")
══════════════════════════════════════════

Compute u_apple^T · v_machine:
  = [0.1, 0.2] · [0.8, 0.6]
  = 0.1×0.8 + 0.2×0.6
  = 0.08 + 0.12
  = 0.20

Apply sigmoid to negative:
  σ(-0.20) = 1 / (1 + exp(0.20))
           = 1 / (1 + 1.221)
           = 1 / 2.221
           = 0.450

Negative term 1: log(0.450) = -0.799


Step 3: Negative term (sample 2: "run")
════════════════════════════════════════

Compute u_run^T · v_machine:
  = [0.3, 0.1] · [0.8, 0.6]
  = 0.3×0.8 + 0.1×0.6
  = 0.24 + 0.06
  = 0.30

Apply sigmoid to negative:
  σ(-0.30) = 1 / (1 + exp(0.30))
           = 1 / (1 + 1.350)
           = 1 / 2.350
           = 0.426

Negative term 2: log(0.426) = -0.854


Step 4: Total loss
══════════════════

J = (positive term) + (negative term 1) + (negative term 2)
  = -0.353 + (-0.799) + (-0.854)
  = -2.006

Goal: Minimize J (make it more negative)
  - Increase σ(u_learning^T · v_machine) → larger positive term
  - Increase σ(-u_apple^T · v_machine) → larger negative term
  - Increase σ(-u_run^T · v_machine) → larger negative term


Speedup:
════════

Without negative sampling: Compute V = 5 similarities
With negative sampling (k=2): Compute 1 + 2 = 3 similarities

For large vocabulary (V = 100,000, k = 10):
  Reduction: 100,000 → 11  (9,090× speedup!)
```

#### Visualization of Training Process

```
╔═══════════════════════════════════════════════════════════════════════╗
║           NEGATIVE SAMPLING TRAINING VISUALIZATION                    ║
╚═══════════════════════════════════════════════════════════════════════╝

Positive pair: ("machine", "learning")

Before training:
═══════════════

         v_machine         u_learning
            [0.5]              [0.4]
            [0.3]              [0.6]
              │                  │
              └─────┬────────────┘
                    │
            Dot product: 0.38 (low!)
            σ(0.38) = 0.59 (should be higher)


Negative sample: ("machine", "apple")

         v_machine          u_apple
            [0.5]              [0.9]
            [0.3]              [0.1]
              │                  │
              └─────┬────────────┘
                    │
            Dot product: 0.48
            σ(-0.48) = 0.38 (should be higher)


After gradient descent:
═══════════════════════

Positive pair:
  v_machine → [0.55, 0.35]  (moved closer to u_learning)
  u_learning → [0.45, 0.65] (moved closer to v_machine)
  
  New dot product: 0.475 (increased! ✓)
  σ(0.475) = 0.62

Negative pair:
  v_machine → [0.55, 0.35]  (moved away from u_apple)
  u_apple remains [0.9, 0.1] (not updated, or moves away)
  
  New dot product: 0.53
  σ(-0.53) = 0.37

Effect: Positive pairs attract, negative pairs repel!
```

### GloVe (Global Vectors)

**GloVe** (Pennington et al., 2014) is an alternative approach that combines the benefits of matrix factorization methods and context window methods.

#### Introduction to GloVe Approach

**Key Idea**: Word meanings can be captured by analyzing word co-occurrence statistics across the entire corpus.

**Difference from Word2Vec**:
- Word2Vec: Local context window (one window at a time)
- GloVe: Global co-occurrence matrix (entire corpus statistics)

#### Co-occurrence Matrix Concept

```
╔═══════════════════════════════════════════════════════════════════════╗
║                    CO-OCCURRENCE MATRIX                               ║
╚═══════════════════════════════════════════════════════════════════════╝

Corpus:
  "I love machine learning"
  "machine learning is fun"
  "I love deep learning"

Window size = 2

Co-occurrence counts X:
(How often word i appears near word j)

              I    love  machine learning  is   fun  deep
           ┌────┬──────┬───────┬────────┬────┬────┬────┐
    I      │  0 │   2  │   0   │   0    │  0 │  0 │  0 │
           ├────┼──────┼───────┼────────┼────┼────┼────┤
    love   │  2 │   0  │   1   │   1    │  0 │  0 │  1 │
           ├────┼──────┼───────┼────────┼────┼────┼────┤
    machine│  0 │   1  │   0   │   2    │  1 │  0 │  0 │
           ├────┼──────┼───────┼────────┼────┼────┼────┤
  learning │  0 │   1  │   2   │   0    │  1 │  1 │  1 │
           ├────┼──────┼───────┼────────┼────┼────┼────┤
    is     │  0 │   0  │   1   │   1    │  0 │  1 │  0 │
           ├────┼──────┼───────┼────────┼────┼────┼────┤
    fun    │  0 │   0  │   0   │   1    │  1 │  0 │  0 │
           ├────┼──────┼───────┼────────┼────┼────┼────┤
    deep   │  0 │   1  │   0   │   1    │  0 │  0 │  0 │
           └────┴──────┴───────┴────────┴────┴────┴────┘

Key observation:
  X["machine"]["learning"] = 2 (appear together twice)
  X["I"]["fun"] = 0 (never appear together)

This matrix captures global corpus statistics!
```

#### Mathematical Formulation

**GloVe Objective Function**:

$$J = \sum_{i,j=1}^{V} f(X_{ij}) (w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2$$

**Complete Symbol Legend**:
- $J$ = Cost function (to be minimized)
- $V$ = Vocabulary size
- $X_{ij}$ = Co-occurrence count (how often word $i$ appears near word $j$)
- $w_i$ = Word embedding for word $i$ (what we learn, dimension $d \times 1$)
- $\tilde{w}_j$ = Context embedding for word $j$ (dimension $d \times 1$)
- $b_i$ = Bias for word $i$ (scalar)
- $\tilde{b}_j$ = Bias for word $j$ (scalar)
- $f(X_{ij})$ = Weighting function (gives less weight to rare/very frequent co-occurrences)
- $\log X_{ij}$ = Log of co-occurrence count
- Goal: $w_i^T \tilde{w}_j + b_i + \tilde{b}_j \approx \log X_{ij}$

**Weighting Function**:

$$f(x) = \begin{cases}
(x/x_{\max})^\alpha & \text{if } x < x_{\max} \\
1 & \text{otherwise}
\end{cases}$$

where typically $x_{\max} = 100$ and $\alpha = 0.75$.

**Purpose of weighting**:
- Rare co-occurrences ($X_{ij}$ small) get lower weight (less reliable)
- Very frequent co-occurrences (like "the", "is") get capped weight
- Moderate co-occurrences get full weight (most informative)

#### How GloVe Differs from Word2Vec

```
╔═══════════════════════════════════════════════════════════════════════╗
║                  WORD2VEC vs GLOVE                                    ║
╠═══════════════════════════════════════════════════════════════════════╣
║                                                                       ║
║  Aspect         Word2Vec               GloVe                          ║
║  ────────────────────────────────────────────────────────────────    ║
║  Approach       Predictive              Count-based                   ║
║  Training       Local context windows   Global co-occurrence matrix   ║
║  Objective      Maximize probability    Minimize reconstruction error ║
║  Computation    Online (stochastic)     Batch (matrix factorization)  ║
║  Statistics     Implicit                Explicit                      ║
║  Memory         Low                     High (store matrix)           ║
║  Scalability    Better for huge corpus  Better for medium corpus      ║
║                                                                       ║
║  Example:                                                             ║
║  ─────────────────────────────────────────────────────────────────   ║
║  Corpus: "cat sat" appears 10 times in corpus                        ║
║                                                                       ║
║  Word2Vec:                                                            ║
║    - Processes each "cat sat" occurrence separately                   ║
║    - 10 separate training examples                                    ║
║    - Learns incrementally                                             ║
║                                                                       ║
║  GloVe:                                                               ║
║    - Records X["cat"]["sat"] = 10 once                                ║
║    - Single training constraint: w_cat^T w_sat ≈ log(10)              ║
║    - Uses global statistics                                           ║
║                                                                       ║
║  Performance:                                                         ║
║  ────────────                                                         ║
║  • Word similarity tasks: GloVe slightly better                       ║
║  • Word analogy tasks: Comparable                                     ║
║  • Training speed: Word2Vec faster for huge corpora                   ║
║  • Memory usage: GloVe requires storing co-occurrence matrix          ║
║                                                                       ║
║  Recommendation:                                                      ║
║  ───────────────                                                      ║
║  Both produce high-quality embeddings. Use pre-trained versions       ║
║  (GloVe pre-trained on Wikipedia, Common Crawl available)             ║
╚═══════════════════════════════════════════════════════════════════════╝
```

### Training Process Overview

#### Data Preparation

```
Raw Text Preparation:
════════════════════

Step 1: Tokenization
  "I love machine learning!" → ["I", "love", "machine", "learning", "!"]

Step 2: Cleaning
  - Lowercase: ["i", "love", "machine", "learning", "!"]
  - Remove punctuation: ["i", "love", "machine", "learning"]
  - Remove stopwords (optional): ["love", "machine", "learning"]

Step 3: Build vocabulary
  Vocabulary = {word: index for index, word in enumerate(unique_words)}
  
  Example:
    {"i": 0, "love": 1, "machine": 2, "learning": 3, ...}

Step 4: Convert to indices
  ["i", "love", "machine", "learning"] → [0, 1, 2, 3]

Step 5: Generate training pairs (for Word2Vec)
  Window size = 2
  
  Skip-gram pairs:
    (0, 1), (0, 2)        # "i" with context
    (1, 0), (1, 2), (1, 3) # "love" with context
    (2, 1), (2, 3)        # "machine" with context
    (3, 1), (3, 2)        # "learning" with context
```

#### Hyperparameters

```
╔═══════════════════════════════════════════════════════════════════════╗
║                      KEY HYPERPARAMETERS                              ║
╠═══════════════════════════════════════════════════════════════════════╣
║                                                                       ║
║  Parameter              Typical Value       Impact                    ║
║  ──────────────────────────────────────────────────────────────────  ║
║  Embedding dimension    50-300              Quality vs efficiency     ║
║                         (300 most common)                             ║
║                                                                       ║
║  Window size            5-10                Context range             ║
║                         (5 for skip-gram)   Larger = broader context ║
║                                                                       ║
║  Negative samples       5-20                Training speed            ║
║                         (5 for large corpus, 20 for small)            ║
║                                                                       ║
║  Learning rate          0.001-0.025         Convergence speed         ║
║                         (often use decay)                             ║
║                                                                       ║
║  Min word count         5-10                Vocabulary size           ║
║                         (filter rare words) Less noise                ║
║                                                                       ║
║  Epochs                 5-15                Training thoroughness     ║
║                                                                       ║
║  Batch size             256-1024            Memory vs speed           ║
║                                                                       ║
║  Subsampling           1e-5 to 1e-3        Handle frequent words     ║
║  threshold                                                            ║
╚═══════════════════════════════════════════════════════════════════════╝
```

#### Training Algorithm Steps

```
Training Algorithm (Word2Vec Skip-gram with Negative Sampling):
═══════════════════════════════════════════════════════════════

Input:
  - Corpus: List of sentences
  - Hyperparameters: d, window_size, k, α, epochs

Output:
  - Embedding matrix E (d × V)


Algorithm:
═════════

1. Initialize:
   ────────────
   E ← random matrix (d × V) with small values
   C ← random matrix (V × d) with small values
   
2. Build vocabulary:
   ─────────────────
   vocab ← unique words from corpus
   word_to_idx ← map words to indices

3. For each epoch:
   ───────────────
   
   3.1. Shuffle corpus
   
   3.2. For each sentence in corpus:
        
        3.2.1. Convert words to indices
        
        3.2.2. For each center word at position t:
               
               - Get context words within window
               - For each context word o:
                 
                 a. Sample k negative words
                 b. Compute v_c (center embedding)
                 c. Compute positive loss: -log σ(u_o^T v_c)
                 d. Compute negative loss: Σ -log σ(-u_neg^T v_c)
                 e. Total loss: L = positive_loss + negative_loss
                 f. Compute gradients: ∂L/∂v_c, ∂L/∂u_o, ∂L/∂u_neg
                 g. Update embeddings:
                    v_c ← v_c - α · (∂L/∂v_c)
                    u_o ← u_o - α · (∂L/∂u_o)
                    u_neg ← u_neg - α · (∂L/∂u_neg)

4. Return E (trained embeddings)


Pseudocode:
═══════════

for epoch in range(num_epochs):
    for sentence in shuffle(corpus):
        indices = [word_to_idx[w] for w in sentence]
        
        for t, center_idx in enumerate(indices):
            # Get context
            context_indices = get_context(indices, t, window_size)
            
            for context_idx in context_indices:
                # Positive pair
                v_c = E[:, center_idx]
                u_o = C[context_idx, :]
                
                # Sample negatives
                neg_indices = sample_negatives(k, vocab_size)
                
                # Forward pass
                pos_score = dot(u_o, v_c)
                pos_loss = -log(sigmoid(pos_score))
                
                neg_loss = 0
                for neg_idx in neg_indices:
                    u_neg = C[neg_idx, :]
                    neg_score = dot(u_neg, v_c)
                    neg_loss += -log(sigmoid(-neg_score))
                
                total_loss = pos_loss + neg_loss
                
                # Backward pass (gradients)
                grad_v_c = compute_gradient_v_c(...)
                grad_u_o = compute_gradient_u_o(...)
                grad_u_neg = compute_gradient_u_neg(...)
                
                # Update
                E[:, center_idx] -= learning_rate * grad_v_c
                C[context_idx, :] -= learning_rate * grad_u_o
                for neg_idx in neg_indices:
                    C[neg_idx, :] -= learning_rate * grad_u_neg
```

---

## Summary

```
╔═══════════════════════════════════════════════════════════════════════╗
║                    WORD EMBEDDINGS SUMMARY                            ║
╠═══════════════════════════════════════════════════════════════════════╣
║                                                                       ║
║  WHAT: Dense vector representations of words                          ║
║  ────                                                                 ║
║  • Transform sparse one-hot to dense low-dimensional vectors          ║
║  • Typical dimensions: 50-300 (vs vocabulary size 10K-100K)           ║
║  • Each word → continuous vector in ℝ^d                               ║
║                                                                       ║
║  WHY: Capture semantic relationships                                  ║
║  ───                                                                  ║
║  • Similar words have similar vectors (cosine similarity)             ║
║  • Enable analogies: king - man + woman ≈ queen                       ║
║  • Enable transfer learning across NLP tasks                          ║
║  • Dramatically reduce dimensionality with MORE information           ║
║                                                                       ║
║  HOW: Learn from context                                              ║
║  ───                                                                  ║
║  • Word2Vec (Skip-gram, CBOW): Predict context from word or vice versa
║  • GloVe: Factorize global co-occurrence matrix                       ║
║  • Modern: BERT, GPT (contextual embeddings)                          ║
║                                                                       ║
║  KEY PROPERTIES:                                                      ║
║  ──────────────                                                       ║
║  ✓ Semantic similarity (dog ≈ cat)                                    ║
║  ✓ Linear relationships (king:queen :: man:woman)                     ║
║  ✓ Compositionality (very + good ≈ excellent)                         ║
║  ✓ Transfer learning (pre-train once, use everywhere)                 ║
║  ✓ Efficient (300D vs 100,000D)                                       ║
║                                                                       ║
║  TRAINING:                                                            ║
║  ────────                                                             ║
║  1. Build vocabulary from corpus                                      ║
║  2. Initialize embedding matrix randomly                              ║
║  3. Generate training pairs (word-context)                            ║
║  4. Optimize embeddings to predict context                            ║
║  5. Use negative sampling for efficiency                              ║
║                                                                       ║
║  APPLICATIONS:                                                        ║
║  ────────────                                                         ║
║  • Sentiment analysis                                                 ║
║  • Machine translation                                                ║
║  • Named entity recognition                                           ║
║  • Question answering                                                 ║
║  • Text classification                                                ║
║  • Information retrieval                                              ║
╚═══════════════════════════════════════════════════════════════════════╝
```

**Modern Developments**:
- **Contextual embeddings** (ELMo, BERT, GPT): Embeddings depend on context
  - "bank" has different embedding in "river bank" vs "bank account"
- **Multilingual embeddings**: Shared space for multiple languages
- **Subword embeddings** (BPE, WordPiece): Handle rare words and morphology
- **Fine-tuning**: Adapt pre-trained embeddings to specific domains

Word embeddings revolutionized NLP by providing a way to represent word meaning in a computationally efficient and semantically rich format!
