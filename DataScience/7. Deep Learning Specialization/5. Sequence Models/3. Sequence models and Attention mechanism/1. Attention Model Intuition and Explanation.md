# Attention Models

## Table of Contents
1. [Introduction to Attention Models](#introduction-to-attention-models)
2. [The Sequence-to-Sequence Bottleneck](#the-sequence-to-sequence-bottleneck)
3. [Attention Mechanism - Core Concept](#attention-mechanism---core-concept)
4. [Mathematical Formulation](#mathematical-formulation)
5. [Types of Attention](#types-of-attention)
6. [Complete Step-by-Step Example](#complete-step-by-step-example)
7. [Self-Attention and Multi-Head Attention](#self-attention-and-multi-head-attention)
8. [Attention Visualizations](#attention-visualizations)
9. [Summary](#summary)

---

## Introduction to Attention Models

### What is Attention?

**Attention** is a mechanism that allows neural networks to focus on relevant parts of the input when producing each part of the output. Instead of compressing an entire input sequence into a single fixed-size vector, attention enables the model to "look back" at different parts of the input dynamically.

**Core Intuition**: When you translate a sentence, you don't memorize the entire sentence first and then translate. Instead, you look back at specific words as you write each word of the translation. Attention mimics this human behavior.

**Problem Statement**: Given an input sequence $x = [x_1, x_2, \ldots, x_n]$, compute a context-aware representation for generating each output token $y_t$ by selectively focusing on relevant input positions.

### Why is Attention Needed?

Traditional sequence-to-sequence models suffer from a **bottleneck problem**:
- The entire input sequence is compressed into a single fixed-length vector
- This vector must capture ALL information from the input
- Information loss is severe for long sequences

Attention solves this by:
- Allowing direct access to all encoder hidden states
- Computing a dynamic, position-specific context for each output step
- Enabling the model to "attend" to relevant input parts

### Human Attention Analogy

```
╔══════════════════════════════════════════════════════════════════════╗
║                    HUMAN ATTENTION ANALOGY                           ║
╚══════════════════════════════════════════════════════════════════════╝

Task: Translate "Le chat noir dort sur le canapé" → "The black cat sleeps on the sofa"

Human Translation Process:
─────────────────────────────────────────────────────────────────────────

When writing "The":
  "Le chat noir dort sur le canapé"
   ▲▲
   ││
   └┴─── Eyes focus on "Le" (the French article)

When writing "black":
  "Le chat noir dort sur le canapé"
          ▲▲▲▲
          ││││
          └┴┴┴─── Eyes jump to "noir" (black)

When writing "cat":
  "Le chat noir dort sur le canapé"
      ▲▲▲▲
      ││││
      └┴┴┴─── Eyes focus on "chat" (cat)

When writing "sleeps":
  "Le chat noir dort sur le canapé"
               ▲▲▲▲
               ││││
               └┴┴┴─── Eyes focus on "dort" (sleeps)

Key Insight:
┌─────────────────────────────────────────────────────────────────────┐
│ You DON'T memorize the whole sentence first, then translate.       │
│ You DYNAMICALLY look at relevant words for each output word.       │
│ This is EXACTLY what attention mechanisms do!                       │
└─────────────────────────────────────────────────────────────────────┘
```

### Where Attention is Used

#### 1. Neural Machine Translation (NMT)
**Description**: Translate text between languages with attention to source words.
**Application**: For each target word, attend to relevant source words (e.g., "cat" attends to "chat").
**Example**: Google Translate, DeepL

#### 2. Image Captioning
**Description**: Generate text descriptions by attending to image regions.
**Application**: When generating "dog", attend to the image region containing the dog.
**Example**: Automated alt-text, visual question answering

#### 3. Speech Recognition
**Description**: Convert audio to text by attending to relevant audio frames.
**Application**: Align acoustic features with text output dynamically.
**Example**: Whisper, Google Speech-to-Text

#### 4. Transformers and Large Language Models
**Description**: Self-attention allows tokens to attend to all other tokens.
**Application**: GPT, BERT, and all modern LLMs use attention as their core mechanism.
**Example**: ChatGPT, Claude, Llama

#### 5. Document Summarization
**Description**: Attend to important sentences when generating summaries.
**Application**: Focus on key information while ignoring redundant content.
**Example**: News summarization, meeting notes

---

## The Sequence-to-Sequence Bottleneck

### Encoder-Decoder Architecture Review

The classic **Encoder-Decoder** (Seq2Seq) architecture:

```
╔══════════════════════════════════════════════════════════════════════╗
║              CLASSIC ENCODER-DECODER ARCHITECTURE                    ║
╚══════════════════════════════════════════════════════════════════════╝

Input: "Je suis étudiant" (French)
Output: "I am a student" (English)

                    ENCODER                          DECODER
              ┌─────────────────┐              ┌─────────────────┐
              │                 │              │                 │
    "Je" ───► │ RNN │ h₁ ──────►│              │                 │
              │     │           │              │                 │
  "suis" ───► │ RNN │ h₂ ──────►│              │                 │
              │     │           │   Context    │                 │ ───► "I"
              │     │           │   Vector     │                 │
"étudiant"──► │ RNN │ h₃ ══════►│═════ c ═════►│ RNN             │ ───► "am"
              │     │           │              │                 │
              │                 │              │                 │ ───► "a"
              │                 │              │                 │
              └─────────────────┘              │                 │ ───► "student"
                                               └─────────────────┘

The Problem: Everything must pass through ONE vector c!

Symbol Legend:
─────────────────
h₁, h₂, h₃ = Hidden states at each encoder time step
c = Context vector (final encoder hidden state)
RNN = Recurrent Neural Network cell (LSTM/GRU)
```

### The Information Compression Problem

**The Bottleneck**: The context vector $c$ is typically the final hidden state $h_n$ of the encoder:

$$c = h_n$$

**Problem**: This single vector must encode:
- All words in the source sentence
- Their meanings
- Their relationships
- Word order information
- Grammar and syntax

**Formula**: For encoder with hidden size $d$:
$$c \in \mathbb{R}^{d}$$

A typical hidden size: $d = 512$ or $d = 1024$

**Symbol Legend**:
- $c$ = Context vector
- $h_n$ = Final hidden state of encoder
- $d$ = Dimension of hidden state
- $\mathbb{R}^{d}$ = d-dimensional real vector space

### Why Fixed-Length Context Vectors Fail

```
╔══════════════════════════════════════════════════════════════════════╗
║              INFORMATION LOSS IN LONG SEQUENCES                      ║
╚══════════════════════════════════════════════════════════════════════╝

Short Sequence (Works OK):
──────────────────────────────────────────────────────────────────────

Input: "Hello" (1 word)

  "Hello" ──► [h₁] ══► c (512 dims)
  
  Information density: 1 word / 512 dims = LOW
  Result: c captures "Hello" well ✓


Medium Sequence (Getting Harder):
──────────────────────────────────────────────────────────────────────

Input: "I am a student" (4 words)

  "I" ──► [h₁] ──► [h₂] ──► [h₃] ──► [h₄] ══► c (512 dims)
           │       │       │       │
         "am"    "a"  "student"
  
  Information density: 4 words / 512 dims = MODERATE
  Result: c captures most information ✓


Long Sequence (FAILS!):
──────────────────────────────────────────────────────────────────────

Input: 50-word paragraph

  w₁ → w₂ → w₃ → ... → w₄₈ → w₄₉ → w₅₀ ══► c (512 dims)
   │    │    │          │     │     │
   └────┴────┴──────────┴─────┴─────┘
              ALL must fit in c!
  
  Information density: 50 words / 512 dims = VERY HIGH
  Result: Early words are "forgotten" ✗

┌─────────────────────────────────────────────────────────────────────┐
│                     THE FORGETTING PROBLEM                          │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Information retained vs. position:                                 │
│                                                                     │
│  100% │█                                                            │
│       │██                                                           │
│   75% │███                                                          │
│       │████                                                         │
│   50% │█████                                                        │
│       │██████                                                       │
│   25% │███████                                                      │
│       │████████████                                                 │
│    0% │████████████████████████████████████                         │
│       └──────────────────────────────────────────►                  │
│         w₁  w₅  w₁₀  w₁₅  w₂₀  w₂₅  w₃₀  w₃₅  w₄₀  w₄₅  w₅₀         │
│         ▲                                              ▲            │
│         │                                              │            │
│     Early words                                   Recent words      │
│     (forgotten)                                   (remembered)      │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘

Empirical Evidence:
───────────────────
• BLEU scores drop significantly for sentences > 20 words
• Without attention, models struggle with long-range dependencies
• Information from early tokens gets "washed out"
```

### The Solution: Attention

Instead of using a single fixed context vector, **attention** computes a **different context vector for each output position**:

```
╔══════════════════════════════════════════════════════════════════════╗
║              ENCODER-DECODER WITH ATTENTION                          ║
╚══════════════════════════════════════════════════════════════════════╝

                    ENCODER                          DECODER
              ┌─────────────────┐              ┌─────────────────┐
              │                 │              │                 │
    "Je" ───► │ RNN │ h₁ ═══════╪══════════════╪═══► Attention   │
              │     │           │              │        │        │
  "suis" ───► │ RNN │ h₂ ═══════╪══════════════╪═══► Attention   │
              │     │           │              │        │        │
"étudiant"──► │ RNN │ h₃ ═══════╪══════════════╪═══► Attention   │
              │                 │              │        │        │
              └─────────────────┘              │        ▼        │
                                               │   c₁, c₂, c₃... │ ───► Outputs
                                               │   (dynamic!)    │
                                               └─────────────────┘

Key Difference:
┌─────────────────────────────────────────────────────────────────────┐
│ WITHOUT Attention: One context vector c for ALL decoder steps       │
│ WITH Attention: Different context cₜ for EACH decoder step t         │
└─────────────────────────────────────────────────────────────────────┘

Benefit: Each output word can "look at" relevant input words!
```

---

## Attention Mechanism - Core Concept

### Key Idea: "Looking Back" at All Encoder States

Instead of using only the final encoder state, attention gives the decoder **access to all encoder hidden states** and lets it decide which ones are relevant.

**Intuition**: For each decoder step, ask: "Which encoder states should I focus on?"

```
╔══════════════════════════════════════════════════════════════════════╗
║                    ATTENTION: THE BIG PICTURE                        ║
╚══════════════════════════════════════════════════════════════════════╝

Encoder Hidden States: h₁, h₂, h₃, h₄ (from input "I love this movie")
Decoder State: sₜ (current decoder hidden state)

Step 1: SCORE - How relevant is each encoder state?
───────────────────────────────────────────────────
        
        score(sₜ, h₁) = 2.1   ← "I" relevance
        score(sₜ, h₂) = 5.8   ← "love" relevance (HIGH!)
        score(sₜ, h₃) = 1.2   ← "this" relevance
        score(sₜ, h₄) = 0.9   ← "movie" relevance

Step 2: NORMALIZE - Convert scores to probabilities (softmax)
───────────────────────────────────────────────────────────────
        
        α₁ = 0.05   (5%)
        α₂ = 0.82   (82%)  ← Most attention on "love"!
        α₃ = 0.08   (8%)
        α₄ = 0.05   (5%)
        
        Sum = 1.0 ✓

Step 3: AGGREGATE - Compute weighted sum (context vector)
───────────────────────────────────────────────────────────
        
        cₜ = 0.05·h₁ + 0.82·h₂ + 0.08·h₃ + 0.05·h₄
        
        The context vector cₜ is MOSTLY h₂ ("love")!

Result: When generating output, the decoder focuses on "love"
```

### The Query-Key-Value Framework

Modern attention uses the **Query-Key-Value (QKV)** abstraction:

```
╔══════════════════════════════════════════════════════════════════════╗
║                    QUERY-KEY-VALUE FRAMEWORK                         ║
╚══════════════════════════════════════════════════════════════════════╝

Analogy: Library Search
───────────────────────

You have a QUERY (what you're looking for):
  "I need information about neural networks"

The library has KEYS (book titles/descriptions):
  Key₁: "Introduction to Machine Learning"
  Key₂: "Deep Neural Networks Explained"  ← Matches your query!
  Key₃: "Cooking for Beginners"
  Key₄: "Advanced Neural Network Design"  ← Also matches!

Each key has a VALUE (the actual book content):
  Value₁: [Content of ML book]
  Value₂: [Content of DNN book]
  Value₃: [Content of cooking book]
  Value₄: [Content of advanced NN book]

Attention Process:
  1. Compare QUERY with each KEY → get relevance scores
  2. Higher scores for Key₂ and Key₄ (they match!)
  3. Return weighted combination of VALUES
     → Mostly content from books 2 and 4

┌─────────────────────────────────────────────────────────────────────┐
│                    IN NEURAL NETWORKS                               │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Query (Q):  What the decoder is "looking for"                      │
│              → Current decoder state sₜ                              │
│                                                                     │
│  Keys (K):   What each encoder position "offers"                    │
│              → Encoder hidden states h₁, h₂, ..., hₙ                 │
│                                                                     │
│  Values (V): The actual information to retrieve                     │
│              → Same as keys (h₁, h₂, ..., hₙ) in basic attention    │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### Three-Step Attention Process

```
╔══════════════════════════════════════════════════════════════════════╗
║              THE THREE STEPS OF ATTENTION                            ║
╚══════════════════════════════════════════════════════════════════════╝

Given:
  • Query: q (decoder state sₜ)
  • Keys: K = [k₁, k₂, ..., kₙ] (encoder states)
  • Values: V = [v₁, v₂, ..., vₙ] (encoder states)

┌──────────────────────────────────────────────────────────────────────┐
│ STEP 1: SCORE                                                        │
│ ─────────────────                                                    │
│ Compute relevance score between query and each key                   │
│                                                                      │
│    eᵢ = score(q, kᵢ)    for i = 1, 2, ..., n                         │
│                                                                      │
│ Different score functions exist (dot product, additive, etc.)        │
└──────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌──────────────────────────────────────────────────────────────────────┐
│ STEP 2: NORMALIZE (Softmax)                                          │
│ ─────────────────────────────                                        │
│ Convert scores to probability distribution (attention weights)       │
│                                                                      │
│              exp(eᵢ)                                                 │
│    αᵢ = ─────────────────    for i = 1, 2, ..., n                    │
│          Σⱼ exp(eⱼ)                                                  │
│                                                                      │
│ Properties: αᵢ ≥ 0 and Σᵢ αᵢ = 1                                     │
└──────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌──────────────────────────────────────────────────────────────────────┐
│ STEP 3: AGGREGATE (Weighted Sum)                                     │
│ ─────────────────────────────────                                    │
│ Compute context vector as weighted sum of values                     │
│                                                                      │
│    c = Σᵢ αᵢ · vᵢ = α₁v₁ + α₂v₂ + ... + αₙvₙ                          │
│                                                                      │
│ Result: c contains information from values, weighted by relevance    │
└──────────────────────────────────────────────────────────────────────┘

Symbol Legend:
──────────────
q = Query vector (what we're searching with)
kᵢ = Key vector at position i
vᵢ = Value vector at position i
eᵢ = Raw attention score for position i
αᵢ = Normalized attention weight for position i (after softmax)
c = Context vector (output of attention)
n = Number of positions (sequence length)
exp(·) = Exponential function
Σ = Summation
```

### ASCII Visualization of Attention Flow

```
╔══════════════════════════════════════════════════════════════════════╗
║                    ATTENTION MECHANISM FLOW                          ║
╚══════════════════════════════════════════════════════════════════════╝

Input Sequence: "The cat sat"
Generating: 3rd output word

                         ENCODER STATES
              ┌─────┐    ┌─────┐    ┌─────┐
              │ h₁  │    │ h₂  │    │ h₃  │
              │"The"│    │"cat"│    │"sat"│
              └──┬──┘    └──┬──┘    └──┬──┘
                 │          │          │
                 │          │          │
    ┌────────────┼──────────┼──────────┼────────────┐
    │            ▼          ▼          ▼            │
    │  STEP 1: ┌─────┐   ┌─────┐   ┌─────┐          │
    │  SCORE   │e₁=1.2│   │e₂=3.5│   │e₃=2.1│       │
    │          └──┬──┘   └──┬──┘   └──┬──┘          │
    │             │          │          │           │
    │             ▼          ▼          ▼           │
    │  STEP 2: ┌─────┐   ┌─────┐   ┌─────┐          │
    │  SOFTMAX │α₁=.09│   │α₂=.68│   │α₃=.23│       │
    │          └──┬──┘   └──┬──┘   └──┬──┘          │
    │             │          │          │           │
    │             │    STEP 3: AGGREGATE            │
    │             │          │          │           │
    │             └────┬─────┴─────┬────┘           │
    │                  ▼           │                │
    │            ┌─────────────────┴──┐             │
    │            │    CONTEXT VECTOR  │             │
    │            │                    │             │
    │            │ c = 0.09·h₁        │             │
    │            │   + 0.68·h₂        │ ← Mostly h₂ │
    │            │   + 0.23·h₃        │   ("cat")   │
    │            │                    │             │
    │            └─────────┬──────────┘             │
    │     ATTENTION        │                        │
    │     MECHANISM        │                        │
    └──────────────────────┼────────────────────────┘
                           │
                           ▼
                    ┌─────────────┐
                    │   DECODER   │
                    │   STATE sₜ   │──────► Output word
                    │ (uses c!)   │
                    └─────────────┘
```

---

## Mathematical Formulation

### Score Functions

The **score function** measures how well a query matches each key. Different score functions have different properties.

#### 1. Dot-Product Attention

**Formula**:
$$e_i = q^\top k_i = \sum_{j=1}^{d} q_j \cdot k_{i,j}$$

**In matrix form** (for all positions at once):
$$e = q^\top K = q^\top [k_1, k_2, \ldots, k_n]$$

**Symbol Legend**:
- $e_i$ = Score for position $i$
- $q$ = Query vector $\in \mathbb{R}^d$
- $k_i$ = Key vector at position $i$ $\in \mathbb{R}^d$
- $d$ = Dimension of query/key vectors
- $q^\top$ = Transpose of $q$ (row vector)
- $K$ = Matrix of all keys $\in \mathbb{R}^{d \times n}$

**Properties**:
```
┌─────────────────────────────────────────────────────────────────────┐
│ DOT-PRODUCT ATTENTION                                               │
├─────────────────────────────────────────────────────────────────────┤
│ ✓ Simple and fast (just matrix multiplication)                      │
│ ✓ No learnable parameters                                           │
│ ✗ Requires query and key to have same dimension                     │
│ ✗ Scores can grow large with high dimensions (→ use scaling)        │
└─────────────────────────────────────────────────────────────────────┘
```

**Numerical Example**:
```
Query: q = [1.0, 2.0, 0.5]
Key:   k = [0.5, 1.0, 2.0]

Dot product:
e = q · k = (1.0 × 0.5) + (2.0 × 1.0) + (0.5 × 2.0)
         = 0.5 + 2.0 + 1.0
         = 3.5
```

#### 2. Scaled Dot-Product Attention

**Formula**:
$$e_i = \frac{q^\top k_i}{\sqrt{d_k}}$$

**Symbol Legend**:
- $d_k$ = Dimension of key vectors
- $\sqrt{d_k}$ = Square root of $d_k$ (scaling factor)

**Why Scale?**

```
╔══════════════════════════════════════════════════════════════════════╗
║                    WHY SCALING IS NEEDED                             ║
╚══════════════════════════════════════════════════════════════════════╝

Problem: For large dₖ, dot products can become very large.

Example with dₖ = 512:
─────────────────────
If q and k have elements ~ N(0, 1) (standard normal):
  • Each element product: mean 0, variance 1
  • Sum of 512 products: mean 0, variance 512
  • Standard deviation: √512 ≈ 22.6

So dot products can easily be ±50 or larger!

Why is this bad?
────────────────

Large scores → Softmax becomes "peaked":

  Small scores [-2, 1, 0.5]:
    softmax → [0.04, 0.67, 0.29]  ← Reasonable distribution
    
  Large scores [-40, 20, 10]:
    softmax → [0.00, 1.00, 0.00]  ← Almost one-hot!
    
┌─────────────────────────────────────────────────────────────────────┐
│ When softmax is too peaked:                                         │
│ • Gradients become very small (vanishing gradient problem)          │
│ • Model can't learn to distribute attention                         │
│ • Training becomes unstable                                         │
└─────────────────────────────────────────────────────────────────────┘

Solution: Divide by √dₖ
───────────────────────

Scaled score: e = (q · k) / √dₖ

With dₖ = 512:
  • Raw dot product std: √512 ≈ 22.6
  • After scaling: 22.6 / √512 = 22.6 / 22.6 = 1.0 ✓
  
Scaled scores have unit variance, keeping softmax well-behaved!
```

#### 3. Additive (Bahdanau) Attention

**Formula**:
$$e_i = v^\top \tanh(W_q q + W_k k_i)$$

**Alternative notation**:
$$e_i = v^\top \tanh(W_1 s_{t-1} + W_2 h_i)$$

**Symbol Legend**:
- $W_q \in \mathbb{R}^{d_a \times d_q}$ = Weight matrix for query
- $W_k \in \mathbb{R}^{d_a \times d_k}$ = Weight matrix for key
- $v \in \mathbb{R}^{d_a}$ = Weight vector for final score
- $d_a$ = Attention hidden dimension
- $\tanh$ = Hyperbolic tangent activation function
- $s_{t-1}$ = Previous decoder state (query)
- $h_i$ = Encoder hidden state at position $i$ (key)

**Properties**:
```
┌─────────────────────────────────────────────────────────────────────┐
│ ADDITIVE (BAHDANAU) ATTENTION                                       │
├─────────────────────────────────────────────────────────────────────┤
│ ✓ Query and key can have different dimensions                       │
│ ✓ More expressive (has learnable parameters)                        │
│ ✓ Works well in practice                                            │
│ ✗ Slower than dot-product (requires matrix multiplications + tanh)  │
│ ✗ More parameters to learn                                          │
└─────────────────────────────────────────────────────────────────────┘
```

**Visualization**:
```
          q (query)              kᵢ (key)
              │                      │
              ▼                      ▼
           ┌─────┐                ┌─────┐
           │ Wq  │                │ Wk  │
           └──┬──┘                └──┬──┘
              │                      │
              │      ┌───────┐       │
              └─────►│   +   │◄──────┘
                     └───┬───┘
                         │
                         ▼
                     ┌───────┐
                     │ tanh  │
                     └───┬───┘
                         │
                         ▼
                     ┌───────┐
                     │  vᵀ   │
                     └───┬───┘
                         │
                         ▼
                        eᵢ (score)
```

#### 4. Multiplicative (Luong) Attention

**General Form**:
$$e_i = s_t^\top W_a h_i$$

**Dot variant** (when dimensions match):
$$e_i = s_t^\top h_i$$

**Symbol Legend**:
- $s_t$ = Decoder state at time $t$ (query)
- $h_i$ = Encoder hidden state at position $i$ (key)
- $W_a \in \mathbb{R}^{d_s \times d_h}$ = Attention weight matrix

**Properties**:
```
┌─────────────────────────────────────────────────────────────────────┐
│ MULTIPLICATIVE (LUONG) ATTENTION                                    │
├─────────────────────────────────────────────────────────────────────┤
│ ✓ Simpler than additive (fewer operations)                          │
│ ✓ Faster computation                                                │
│ ✓ Works well with similar-dimension states                          │
│ ✗ Less flexible than additive                                       │
└─────────────────────────────────────────────────────────────────────┘
```

#### Score Functions Comparison

```
╔══════════════════════════════════════════════════════════════════════╗
║                 SCORE FUNCTIONS COMPARISON                           ║
╚══════════════════════════════════════════════════════════════════════╝

┌──────────────────┬─────────────────────────┬────────────┬────────────┐
│ Name             │ Formula                 │ Parameters │ Speed      │
├──────────────────┼─────────────────────────┼────────────┼────────────┤
│ Dot Product      │ qᵀk                     │ None       │ Fast ⚡⚡⚡
├──────────────────┼─────────────────────────┼────────────┼────────────┤
│ Scaled Dot       │ qᵀk / √dₖ               │ None       │ Fast ⚡⚡⚡
├──────────────────┼─────────────────────────┼────────────┼────────────┤
│ Additive         │ vᵀtanh(Wqq + Wkk)       │ Wq, Wk, v  │ Slow ⚡    │
├──────────────────┼─────────────────────────┼────────────┼────────────┤
│ Multiplicative   │ qᵀWk                    │ W          │ Medium ⚡⚡ 
└──────────────────┴─────────────────────────┴────────────┴────────────┘

Usage in Practice:
─────────────────
• Transformers: Scaled Dot-Product (fast, parallelizable)
• Original Seq2Seq: Additive/Bahdanau (first attention paper)
• Luong attention: Multiplicative variants
```

### Softmax Normalization

**Formula**:
$$\alpha_i = \frac{\exp(e_i)}{\sum_{j=1}^{n} \exp(e_j)}$$

**In vector form**:
$$\boldsymbol{\alpha} = \text{softmax}(\mathbf{e}) = \text{softmax}([e_1, e_2, \ldots, e_n])$$

**Symbol Legend**:
- $\alpha_i$ = Attention weight for position $i$
- $e_i$ = Raw score for position $i$
- $\exp(\cdot)$ = Exponential function
- $n$ = Number of positions
- $\boldsymbol{\alpha}$ = Vector of all attention weights

**Properties**:
1. All weights are non-negative: $\alpha_i \geq 0$
2. Weights sum to 1: $\sum_{i=1}^{n} \alpha_i = 1$
3. Can be interpreted as a probability distribution

**Numerical Example**:
```
Raw scores: e = [2.0, 1.0, 0.5, 3.0]

Step 1: Compute exponentials
  exp(2.0) = 7.389
  exp(1.0) = 2.718
  exp(0.5) = 1.649
  exp(3.0) = 20.086

Step 2: Sum of exponentials
  sum = 7.389 + 2.718 + 1.649 + 20.086 = 31.842

Step 3: Normalize
  α₁ = 7.389 / 31.842 = 0.232
  α₂ = 2.718 / 31.842 = 0.085
  α₃ = 1.649 / 31.842 = 0.052
  α₄ = 20.086 / 31.842 = 0.631  ← Highest score → highest weight

Result: α = [0.232, 0.085, 0.052, 0.631]
Sum check: 0.232 + 0.085 + 0.052 + 0.631 = 1.0 ✓
```

### Context Vector Calculation

**Formula**:
$$c = \sum_{i=1}^{n} \alpha_i \cdot v_i = \alpha_1 v_1 + \alpha_2 v_2 + \ldots + \alpha_n v_n$$

**In matrix form**:
$$c = V \boldsymbol{\alpha}$$

Where $V = [v_1, v_2, \ldots, v_n]$ is the matrix of value vectors.

**Symbol Legend**:
- $c$ = Context vector (output of attention) $\in \mathbb{R}^{d_v}$
- $\alpha_i$ = Attention weight for position $i$
- $v_i$ = Value vector at position $i$ $\in \mathbb{R}^{d_v}$
- $d_v$ = Dimension of value vectors
- $V$ = Matrix of all values $\in \mathbb{R}^{d_v \times n}$

**Numerical Example**:
```
Values (as row vectors for clarity):
  v₁ = [1.0, 0.0, 0.5]  (from "The")
  v₂ = [0.5, 1.0, 0.0]  (from "cat")
  v₃ = [0.0, 0.5, 1.0]  (from "sat")

Attention weights:
  α = [0.1, 0.7, 0.2]  (mostly attending to "cat")

Context vector:
  c = 0.1 · [1.0, 0.0, 0.5]
    + 0.7 · [0.5, 1.0, 0.0]
    + 0.2 · [0.0, 0.5, 1.0]

  c = [0.10, 0.00, 0.05]   (from v₁)
    + [0.35, 0.70, 0.00]   (from v₂) ← Main contribution!
    + [0.00, 0.10, 0.20]   (from v₃)
    ──────────────────────
    = [0.45, 0.80, 0.25]

The context vector c is dominated by v₂ ("cat") because α₂ = 0.7
```

### Complete Attention Formula

Combining all steps into one formula:

**Scaled Dot-Product Attention** (used in Transformers):

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$$

**Symbol Legend**:
- $Q \in \mathbb{R}^{m \times d_k}$ = Matrix of queries (m queries)
- $K \in \mathbb{R}^{n \times d_k}$ = Matrix of keys (n keys)
- $V \in \mathbb{R}^{n \times d_v}$ = Matrix of values (n values)
- $d_k$ = Dimension of keys/queries
- $d_v$ = Dimension of values
- $QK^\top \in \mathbb{R}^{m \times n}$ = Score matrix
- Output $\in \mathbb{R}^{m \times d_v}$ = Context vectors for all queries

```
╔══════════════════════════════════════════════════════════════════════╗
║            ATTENTION FORMULA BREAKDOWN                               ║
╚══════════════════════════════════════════════════════════════════════╝

                    Q K^T
            ┌─────────────────┐
            │                 │
            │   Score Matrix  │    Each entry (i,j) = qᵢ · kⱼ
            │    (m × n)      │
            │                 │
            └────────┬────────┘
                     │
                     │  ÷ √dₖ
                     ▼
            ┌─────────────────┐
            │                 │
            │ Scaled Scores   │    Prevent large values
            │    (m × n)      │
            │                 │
            └────────┬────────┘
                     │
                     │  softmax (row-wise)
                     ▼
            ┌─────────────────┐
            │                 │
            │ Attention       │    Each row sums to 1
            │ Weights (m × n) │    αᵢⱼ = attention from qᵢ to kⱼ
            │                 │
            └────────┬────────┘
                     │
                     │  × V
                     ▼
            ┌─────────────────┐
            │                 │
            │ Output          │    Weighted sum of values
            │ (m × dᵥ)        │    for each query
            │                 │
            └─────────────────┘
```

---

## Types of Attention

### Soft Attention vs Hard Attention

```
╔══════════════════════════════════════════════════════════════════════╗
║                 SOFT vs HARD ATTENTION                               ║
╚══════════════════════════════════════════════════════════════════════╝

SOFT ATTENTION (Standard):
──────────────────────────
• Uses weighted average of ALL values
• Weights are continuous (0 to 1)
• Differentiable → can use backpropagation
• Most commonly used

Example:
  Weights: [0.1, 0.6, 0.2, 0.1]
  Context: 0.1·v₁ + 0.6·v₂ + 0.2·v₃ + 0.1·v₄
  
        ░░▓▓▓▓▓▓▓▓▓▓▓▓▓░░░░░░░░░░░░
        v₁    v₂      v₃    v₄
           (60%)

HARD ATTENTION:
───────────────
• Selects ONE position (winner-take-all)
• Weights are discrete (0 or 1)
• NOT differentiable → needs reinforcement learning
• Used in some image attention models

Example:
  Weights: [0, 1, 0, 0]  (only v₂ selected)
  Context: v₂
  
        ░░████████████░░░░░░░░░░░░
        v₁    v₂      v₃    v₄
            (100%)

┌─────────────────────────────────────────────────────────────────────┐
│              COMPARISON TABLE                                       │
├─────────────────┬─────────────────────┬─────────────────────────────┤
│ Property        │ Soft Attention      │ Hard Attention              │
├─────────────────┼─────────────────────┼─────────────────────────────┤
│ Selection       │ Weighted average    │ Single selection            │
│ Weights         │ Continuous [0,1]    │ Binary {0,1}                │
│ Differentiable  │ Yes ✓               │ No ✗                       │
│ Training        │ Backpropagation     │ Reinforcement learning      │
│ Computation     │ Deterministic       │ Stochastic (sampling)       │
│ Use case        │ Most NLP tasks      │ Visual attention, RL        │
└─────────────────┴─────────────────────┴─────────────────────────────┘
```

### Cross-Attention vs Self-Attention

```
╔══════════════════════════════════════════════════════════════════════╗
║              CROSS-ATTENTION vs SELF-ATTENTION                       ║
╚══════════════════════════════════════════════════════════════════════╝

CROSS-ATTENTION (Encoder-Decoder Attention):
────────────────────────────────────────────
• Query comes from ONE sequence (decoder)
• Keys/Values come from ANOTHER sequence (encoder)
• Used to relate two different sequences

Example: Translation
  Decoder (English): "I" "am" "happy"
                      ↓   ↓    ↓
                   [Attention to French encoder]
                      ↓   ↓    ↓
  Encoder (French):  "Je" "suis" "content"

┌─────────────────────────────────────────────────────────────────────┐
│                                                                     │
│   DECODER                           ENCODER                         │
│   (Queries)                         (Keys & Values)                 │
│                                                                     │
│   "happy" q₃ ─────────────────────► h₁ h₂ h₃                        │
│   "am"    q₂ ─────────────────────► h₁ h₂ h₃                        │
│   "I"     q₁ ─────────────────────► h₁ h₂ h₃                        │
│                                    "Je""suis""content"              │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘


SELF-ATTENTION:
───────────────
• Query, Keys, Values ALL come from the SAME sequence
• Each position attends to all positions (including itself)
• Captures relationships within a single sequence

Example: Understanding "The cat sat on the mat"
  Each word attends to all other words:
  
  "cat" might attend strongly to "sat" (subject-verb)
  "mat" might attend to "on" and "the" (prepositional phrase)

┌─────────────────────────────────────────────────────────────────────┐
│                                                                     │
│              SAME SEQUENCE                                          │
│              (Q, K, V all from here)                                │
│                                                                     │
│   "The" ←──────────────────────────────────────┐                    │
│      ↓         ↓         ↓         ↓          │                     │
│   "cat" ←─────────────────────────────────────┤ All attend          │
│      ↓         ↓         ↓         ↓          │ to all!             │
│   "sat" ←─────────────────────────────────────┤                     │
│      ↓         ↓         ↓         ↓          │                     │
│   "on" ←──────────────────────────────────────┤                     │
│      ↓         ↓         ↓         ↓          │                     │
│   "the" ←─────────────────────────────────────┤                     │
│      ↓         ↓         ↓         ↓          │                     │
│   "mat" ←─────────────────────────────────────┘                     │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│              COMPARISON TABLE                                       │
├─────────────────┬─────────────────────┬─────────────────────────────┤
│ Property        │ Cross-Attention     │ Self-Attention              │
├─────────────────┼─────────────────────┼─────────────────────────────┤
│ Q source        │ Sequence A          │ Same sequence               │
│ K, V source     │ Sequence B          │ Same sequence               │
│ Purpose         │ Relate two seqs     │ Relate within one seq       │
│ Example         │ Encoder-Decoder     │ Transformer encoder         │
│ Use case        │ Translation         │ Language understanding      │
└─────────────────┴─────────────────────┴─────────────────────────────┘
```

### Global vs Local Attention

```
╔══════════════════════════════════════════════════════════════════════╗
║                 GLOBAL vs LOCAL ATTENTION                            ║
╚══════════════════════════════════════════════════════════════════════╝

GLOBAL ATTENTION:
─────────────────
• Attends to ALL positions in the source sequence
• Computes weights for every position
• More accurate but expensive for long sequences

Source: [w₁] [w₂] [w₃] [w₄] [w₅] [w₆] [w₇] [w₈] [w₉] [w₁₀]
         ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓     ↓
        .05  .10  .15  .20  .18  .12  .08  .05  .04   .03
         └────┴────┴────┴────┴────┴────┴────┴────┴─────┘
                              │
                              ▼
                    Context (from ALL)


LOCAL ATTENTION:
────────────────
• Attends to a WINDOW of positions around an aligned point
• Reduces computation for long sequences
• May miss long-range dependencies

Source: [w₁] [w₂] [w₃] [w₄] [w₅] [w₆] [w₇] [w₈] [w₉] [w₁₀]
                    ↓    ↓    ↓    ↓    ↓
        ----      .08  .25  .40  .20  .07      ----
                   └────┴────┴────┴────┘
                              │
                       Window of size 5
                              │
                              ▼
                    Context (from WINDOW)


┌─────────────────────────────────────────────────────────────────────┐
│              COMPARISON TABLE                                       │
├─────────────────┬─────────────────────┬─────────────────────────────┤
│ Property        │ Global Attention    │ Local Attention             │
├─────────────────┼─────────────────────┼─────────────────────────────┤
│ Scope           │ All positions       │ Window around position      │
│ Complexity      │ O(n)                │ O(window_size)              │
│ Long sequences  │ Expensive           │ Efficient                   │
│ Long-range deps │ Captures well       │ May miss                    │
│ Use case        │ Short-medium seqs   │ Very long sequences         │
└─────────────────┴─────────────────────┴─────────────────────────────┘

Hybrid approaches exist:
• Sparse attention (attend to selected positions)
• Sliding window + global tokens (Longformer)
• Chunked attention (BigBird)
```

---

## Complete Step-by-Step Example

### Setup

**Task**: Translate French "Je t'aime" to English "I love you"

**Architecture**: Encoder-Decoder with Bahdanau Attention

**Dimensions**:
- Encoder hidden size: $d_h = 4$
- Decoder hidden size: $d_s = 4$
- Attention hidden size: $d_a = 3$

**Input sequence**: ["Je", "t'", "aime"] (3 tokens)

**Output sequence**: ["I", "love", "you", "<EOS>"] (4 tokens)

### Step 1: Encode Input Sequence

The encoder (an RNN/LSTM) processes the input and produces hidden states:

```
╔══════════════════════════════════════════════════════════════════════╗
║                 STEP 1: ENCODER HIDDEN STATES                        ║
╚══════════════════════════════════════════════════════════════════════╝

Input: "Je t'aime"

  "Je" ──► ENCODER ──► h₁ = [0.2, 0.8, -0.1, 0.5]
  "t'" ──► ENCODER ──► h₂ = [0.9, 0.1, 0.4, -0.2]
"aime" ──► ENCODER ──► h₃ = [0.3, 0.6, 0.7, 0.1]

Encoder Hidden States Matrix H:
┌────────────────────────────────────────┐
│ Position │  h[0]  │  h[1]  │  h[2]  │  h[3]  │
├──────────┼────────┼────────┼────────┼────────┤
│ h₁ "Je"  │  0.2   │  0.8   │ -0.1   │  0.5   │
│ h₂ "t'"  │  0.9   │  0.1   │  0.4   │ -0.2   │
│ h₃ "aime"│  0.3   │  0.6   │  0.7   │  0.1   │
└────────────────────────────────────────┘

These hidden states will be used as Keys (K) and Values (V) for attention.
```

### Step 2: Initialize Decoder

The decoder starts with an initial state and will generate tokens one by one:

```
╔══════════════════════════════════════════════════════════════════════╗
║                 STEP 2: DECODER INITIALIZATION                       ║
╚══════════════════════════════════════════════════════════════════════╝

Initial decoder state (often last encoder state or learned):
  s₀ = [0.3, 0.6, 0.7, 0.1]  (same as h₃ in this example)

Attention parameters (learned):
  Wq ∈ R^(3×4): Maps decoder state to attention space
  Wk ∈ R^(3×4): Maps encoder states to attention space
  v ∈ R^3: Produces final scalar score

For this example:
     ┌                 ┐        ┌                 ┐        ┌     ┐
     │ 0.5  0.1 -0.2  0.3 │        │ 0.4 -0.1  0.2  0.1 │        │ 0.6 │
Wq = │-0.1  0.4  0.3 -0.2 │   Wk = │ 0.1  0.5 -0.3  0.2 │   v =  │-0.4 │
     │ 0.2 -0.3  0.1  0.5 │        │-0.2  0.3  0.4 -0.1 │        │ 0.3 │
     └                 ┘        └                 ┘        └     ┘
```

### Step 3: Generate First Output Token ("I")

#### 3.1 Compute Attention Scores

Using Bahdanau (additive) attention: $e_i = v^\top \tanh(W_q s_{t-1} + W_k h_i)$

```
╔══════════════════════════════════════════════════════════════════════╗
║         STEP 3.1: COMPUTING ATTENTION SCORES (t=1)                   ║
╚══════════════════════════════════════════════════════════════════════╝

Decoder state: s₀ = [0.3, 0.6, 0.7, 0.1]

First, compute Wq · s₀ (same for all positions):
─────────────────────────────────────────────────

Wq · s₀ = │ 0.5×0.3 + 0.1×0.6 + (-0.2)×0.7 + 0.3×0.1 │
          │(-0.1)×0.3 + 0.4×0.6 + 0.3×0.7 + (-0.2)×0.1│
          │ 0.2×0.3 + (-0.3)×0.6 + 0.1×0.7 + 0.5×0.1 │

        = │ 0.15 + 0.06 - 0.14 + 0.03 │   │ 0.10 │
          │-0.03 + 0.24 + 0.21 - 0.02 │ = │ 0.40 │
          │ 0.06 - 0.18 + 0.07 + 0.05 │   │ 0.00 │


Now compute Wk · hᵢ for each encoder position:
──────────────────────────────────────────────

For h₁ = [0.2, 0.8, -0.1, 0.5]:
Wk · h₁ = [0.4×0.2 + (-0.1)×0.8 + 0.2×(-0.1) + 0.1×0.5,
           0.1×0.2 + 0.5×0.8 + (-0.3)×(-0.1) + 0.2×0.5,
           (-0.2)×0.2 + 0.3×0.8 + 0.4×(-0.1) + (-0.1)×0.5]
        = [0.08 - 0.08 - 0.02 + 0.05, 0.02 + 0.40 + 0.03 + 0.10, -0.04 + 0.24 - 0.04 - 0.05]
        = [0.03, 0.55, 0.11]

For h₂ = [0.9, 0.1, 0.4, -0.2]:
Wk · h₂ = [0.4×0.9 + (-0.1)×0.1 + 0.2×0.4 + 0.1×(-0.2),
           0.1×0.9 + 0.5×0.1 + (-0.3)×0.4 + 0.2×(-0.2),
           (-0.2)×0.9 + 0.3×0.1 + 0.4×0.4 + (-0.1)×(-0.2)]
        = [0.36 - 0.01 + 0.08 - 0.02, 0.09 + 0.05 - 0.12 - 0.04, -0.18 + 0.03 + 0.16 + 0.02]
        = [0.41, -0.02, 0.03]

For h₃ = [0.3, 0.6, 0.7, 0.1]:
Wk · h₃ = [0.4×0.3 + (-0.1)×0.6 + 0.2×0.7 + 0.1×0.1,
           0.1×0.3 + 0.5×0.6 + (-0.3)×0.7 + 0.2×0.1,
           (-0.2)×0.3 + 0.3×0.6 + 0.4×0.7 + (-0.1)×0.1]
        = [0.12 - 0.06 + 0.14 + 0.01, 0.03 + 0.30 - 0.21 + 0.02, -0.06 + 0.18 + 0.28 - 0.01]
        = [0.21, 0.14, 0.39]


Compute scores using tanh and v:
────────────────────────────────

For position 1 ("Je"):
  Wq·s₀ + Wk·h₁ = [0.10, 0.40, 0.00] + [0.03, 0.55, 0.11] = [0.13, 0.95, 0.11]
  tanh([0.13, 0.95, 0.11]) = [0.129, 0.740, 0.110]
  e₁ = v · tanh(...) = 0.6×0.129 + (-0.4)×0.740 + 0.3×0.110
                     = 0.077 - 0.296 + 0.033 = -0.186

For position 2 ("t'"):
  Wq·s₀ + Wk·h₂ = [0.10, 0.40, 0.00] + [0.41, -0.02, 0.03] = [0.51, 0.38, 0.03]
  tanh([0.51, 0.38, 0.03]) = [0.470, 0.363, 0.030]
  e₂ = v · tanh(...) = 0.6×0.470 + (-0.4)×0.363 + 0.3×0.030
                     = 0.282 - 0.145 + 0.009 = 0.146

For position 3 ("aime"):
  Wq·s₀ + Wk·h₃ = [0.10, 0.40, 0.00] + [0.21, 0.14, 0.39] = [0.31, 0.54, 0.39]
  tanh([0.31, 0.54, 0.39]) = [0.300, 0.493, 0.371]
  e₃ = v · tanh(...) = 0.6×0.300 + (-0.4)×0.493 + 0.3×0.371
                     = 0.180 - 0.197 + 0.111 = 0.094

┌─────────────────────────────────────────────────────────────────────┐
│ RAW ATTENTION SCORES:                                               │
│   e₁ ("Je")   = -0.186                                              │
│   e₂ ("t'")   =  0.146  ← Highest!                                  │
│   e₃ ("aime") =  0.094                                              │
└─────────────────────────────────────────────────────────────────────┘
```

#### 3.2 Apply Softmax

```
╔══════════════════════════════════════════════════════════════════════╗
║         STEP 3.2: SOFTMAX NORMALIZATION (t=1)                        ║
╚══════════════════════════════════════════════════════════════════════╝

Scores: e = [-0.186, 0.146, 0.094]

Step 1: Compute exponentials
  exp(-0.186) = 0.830
  exp(0.146)  = 1.157
  exp(0.094)  = 1.099

Step 2: Sum
  sum = 0.830 + 1.157 + 1.099 = 3.086

Step 3: Normalize
  α₁ = 0.830 / 3.086 = 0.269  (26.9%)
  α₂ = 1.157 / 3.086 = 0.375  (37.5%)  ← Highest attention
  α₃ = 1.099 / 3.086 = 0.356  (35.6%)

┌─────────────────────────────────────────────────────────────────────┐
│ ATTENTION WEIGHTS (for generating "I"):                             │
│   α₁ ("Je")   = 0.269                                               │
│   α₂ ("t'")   = 0.375  ← Most attention                             │
│   α₃ ("aime") = 0.356                                               │
│                                                                     │
│   Sum check: 0.269 + 0.375 + 0.356 = 1.000 ✓                        │
└─────────────────────────────────────────────────────────────────────┘

Visualization:
                    "Je"        "t'"       "aime"
                   ┌────┐      ┌────┐      ┌────┐
Attention:         │26.9│      │37.5│      │35.6│  (%)
                   │░░░░│      │████│      │▓▓▓▓│
                   └────┘      └────┘      └────┘

To generate "I", the model attends roughly equally to "t'" and "aime",
with slightly less attention to "Je". This makes sense: "I" relates to
the subject which in "Je t'aime" is implicit in the verb conjugation.
```

#### 3.3 Compute Context Vector

```
╔══════════════════════════════════════════════════════════════════════╗
║         STEP 3.3: CONTEXT VECTOR COMPUTATION (t=1)                   ║
╚══════════════════════════════════════════════════════════════════════╝

Context = weighted sum of encoder hidden states

c₁ = α₁·h₁ + α₂·h₂ + α₃·h₃
   = 0.269·[0.2, 0.8, -0.1, 0.5]
   + 0.375·[0.9, 0.1, 0.4, -0.2]
   + 0.356·[0.3, 0.6, 0.7, 0.1]

Computing element by element:

c₁[0] = 0.269×0.2 + 0.375×0.9 + 0.356×0.3
      = 0.054 + 0.338 + 0.107 = 0.499

c₁[1] = 0.269×0.8 + 0.375×0.1 + 0.356×0.6
      = 0.215 + 0.038 + 0.214 = 0.467

c₁[2] = 0.269×(-0.1) + 0.375×0.4 + 0.356×0.7
      = -0.027 + 0.150 + 0.249 = 0.372

c₁[3] = 0.269×0.5 + 0.375×(-0.2) + 0.356×0.1
      = 0.135 - 0.075 + 0.036 = 0.096

┌─────────────────────────────────────────────────────────────────────┐
│ CONTEXT VECTOR c₁ = [0.499, 0.467, 0.372, 0.096]                    │
│                                                                     │
│ This vector is a blend of all encoder states, weighted by attention │
│ It will be combined with decoder state to predict "I"               │
└─────────────────────────────────────────────────────────────────────┘
```

#### 3.4 Generate Output

```
╔══════════════════════════════════════════════════════════════════════╗
║         STEP 3.4: OUTPUT GENERATION (t=1)                            ║
╚══════════════════════════════════════════════════════════════════════╝

The context vector c₁ is combined with decoder state s₀:

Combined = [s₀; c₁]  (concatenation)
         = [0.3, 0.6, 0.7, 0.1, 0.499, 0.467, 0.372, 0.096]

This goes through:
1. A neural network layer
2. Softmax over vocabulary

Result: High probability for "I"

                    Vocabulary
              ┌───────────────────┐
              │ "I"      : 0.85 ← │ ← Selected!
              │ "love"   : 0.05   │
              │ "you"    : 0.03   │
              │ "the"    : 0.02   │
              │ "..."    : ...    │
              └───────────────────┘

Output at t=1: "I" ✓

Update decoder state: s₁ = DECODER_RNN(s₀, "I", c₁)
                      s₁ = [0.5, 0.3, 0.2, 0.8]  (new state)
```

### Step 4: Generate Second Output Token ("love")

Now we repeat the attention process with the updated decoder state $s_1$:

```
╔══════════════════════════════════════════════════════════════════════╗
║         STEP 4: GENERATING "love" (t=2)                              ║
╚══════════════════════════════════════════════════════════════════════╝

Decoder state: s₁ = [0.5, 0.3, 0.2, 0.8]

[Detailed calculations similar to Step 3...]

After computing scores and softmax:

┌─────────────────────────────────────────────────────────────────────┐
│ ATTENTION WEIGHTS (for generating "love"):                          │
│   α₁ ("Je")   = 0.15                                                │
│   α₂ ("t'")   = 0.25                                                │
│   α₃ ("aime") = 0.60  ← MOST attention!                             │
└─────────────────────────────────────────────────────────────────────┘

Visualization:
                    "Je"        "t'"       "aime"
                   ┌────┐      ┌────┐      ┌────┐
Attention:         │ 15 │      │ 25 │      │ 60 │  (%)
                   │░░░░│      │▒▒▒▒│      │████│
                   └────┘      └────┘      └────┘

When generating "love", the model strongly attends to "aime" (French for
"love"). This is exactly what we'd expect! The attention mechanism has
learned to align "love" with its French translation.

Context vector: c₂ = 0.15·h₁ + 0.25·h₂ + 0.60·h₃
                   = [0.32, 0.52, 0.54, 0.14]

Output: "love" ✓
```

### Step 5: Generate Third Output Token ("you")

```
╔══════════════════════════════════════════════════════════════════════╗
║         STEP 5: GENERATING "you" (t=3)                               ║
╚══════════════════════════════════════════════════════════════════════╝

Decoder state: s₂ = [0.4, 0.5, 0.3, 0.6]

After computing scores and softmax:

┌─────────────────────────────────────────────────────────────────────┐
│ ATTENTION WEIGHTS (for generating "you"):                           │
│   α₁ ("Je")   = 0.20                                                │
│   α₂ ("t'")   = 0.65  ← MOST attention!                             │
│   α₃ ("aime") = 0.15                                                │
└─────────────────────────────────────────────────────────────────────┘

Visualization:
                    "Je"        "t'"       "aime"
                   ┌────┐      ┌────┐      ┌────┐
Attention:         │ 20 │      │ 65 │      │ 15 │  (%)
                   │░░░░│      │████│      │░░░░│
                   └────┘      └────┘      └────┘

When generating "you", the model strongly attends to "t'" (which means
"you" in French - it's the object pronoun). Perfect alignment!

Context vector: c₃ = 0.20·h₁ + 0.65·h₂ + 0.15·h₃
                   = [0.67, 0.32, 0.29, 0.01]

Output: "you" ✓
```

### Summary: Attention Alignment

```
╔══════════════════════════════════════════════════════════════════════╗
║              COMPLETE ATTENTION ALIGNMENT                            ║
╚══════════════════════════════════════════════════════════════════════╝

Translation: "Je t'aime" → "I love you"

                     SOURCE (French)
                    "Je"    "t'"   "aime"
                     │       │       │
        ┌────────────┼───────┼───────┼─────────────┐
        │            │       │       │             │
  "I"   │           0.27    0.38    0.35          │ → Distributed
        │            │       │       │             │
  "love"│           0.15    0.25   [0.60]         │ → "aime"
        │            │       │       │             │
  "you" │           0.20   [0.65]   0.15          │ → "t'"
        │            │       │       │             │
  TARGET└────────────┴───────┴───────┴─────────────┘
(English)

ATTENTION HEATMAP:
──────────────────

              "Je"     "t'"    "aime"
          ┌────────┬────────┬────────┐
    "I"   │ ▒▒▒    │ ▓▓▓▓   │ ▓▓▓    │  Distributed
          ├────────┼────────┼────────┤
  "love"  │ ░░     │ ▒▒▒    │ ████   │  Strong → "aime"
          ├────────┼────────┼────────┤
   "you"  │ ▒▒     │ ████   │ ░░     │  Strong → "t'"
          └────────┴────────┴────────┘

Legend: ░ = low attention, ▒ = medium, ▓ = high, █ = very high

Key Insights:
─────────────
1. "love" aligns with "aime" (both mean "love")
2. "you" aligns with "t'" (object pronoun)
3. "I" has distributed attention (subject is implicit in French verb)
4. Attention learns meaningful word alignments automatically!
```

---

## Self-Attention and Multi-Head Attention

### Self-Attention: Attending to the Same Sequence

In **self-attention**, the queries, keys, and values all come from the same sequence. This allows each position to attend to all other positions, capturing relationships within the sequence.

```
╔══════════════════════════════════════════════════════════════════════╗
║                    SELF-ATTENTION                                    ║
╚══════════════════════════════════════════════════════════════════════╝

Input: "The cat sat on the mat"

Each word creates Q, K, V vectors from itself:

Word    →    Q (Query)      K (Key)       V (Value)
─────────────────────────────────────────────────────
"The"   →    q₁             k₁            v₁
"cat"   →    q₂             k₂            v₂
"sat"   →    q₃             k₃            v₃
"on"    →    q₄             k₄            v₄
"the"   →    q₅             k₅            v₅
"mat"   →    q₆             k₆            v₆

Then each query attends to ALL keys:

     Keys:    k₁     k₂     k₃     k₄     k₅     k₆
            "The"  "cat"  "sat"  "on"   "the"  "mat"
              │      │      │      │      │      │
    q₁ ──────┼──────┼──────┼──────┼──────┼──────┤ → output₁
    q₂ ──────┼──────┼──────┼──────┼──────┼──────┤ → output₂
    q₃ ──────┼──────┼──────┼──────┼──────┼──────┤ → output₃
    q₄ ──────┼──────┼──────┼──────┼──────┼──────┤ → output₄
    q₅ ──────┼──────┼──────┼──────┼──────┼──────┤ → output₅
    q₆ ──────┼──────┼──────┼──────┼──────┼──────┤ → output₆

Each output is a weighted combination of ALL values!
```

### Why Self-Attention Matters

**Problem with RNNs**: Information must flow sequentially through all positions.

```
"The cat sat on the mat"
 h₁ → h₂ → h₃ → h₄ → h₅ → h₆

For "The" to influence "mat": 5 sequential steps!
Long-range dependencies are hard to learn.

Path length: O(n) for sequence length n
```

**Self-Attention**: Direct connections between all positions.

```
"The cat sat on the mat"
  ↕    ↕    ↕    ↕    ↕    ↕
  └────┴────┴────┴────┴────┘
     All-to-all connections!

For "The" to influence "mat": 1 attention step!
Long-range dependencies are easy.

Path length: O(1) constant!
```

### Scaled Dot-Product Self-Attention

**Formula**:
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$$

**Computing Q, K, V from input**:

Given input $X \in \mathbb{R}^{n \times d}$ (n positions, d dimensions):

$$Q = XW^Q, \quad K = XW^K, \quad V = XW^V$$

**Symbol Legend**:
- $X$ = Input sequence matrix
- $W^Q \in \mathbb{R}^{d \times d_k}$ = Query projection weights
- $W^K \in \mathbb{R}^{d \times d_k}$ = Key projection weights
- $W^V \in \mathbb{R}^{d \times d_v}$ = Value projection weights
- $d_k$ = Dimension of queries and keys
- $d_v$ = Dimension of values

```
╔══════════════════════════════════════════════════════════════════════╗
║           SCALED DOT-PRODUCT ATTENTION                               ║
╚══════════════════════════════════════════════════════════════════════╝

                                    ┌───────┐
                         ┌─────────►│MatMul │
                         │          └───┬───┘
                    ┌────┴────┐         │
                    │    Q    │         │ Q K^T
                    └────▲────┘         │
                         │              ▼
                    ┌────┴────┐    ┌─────────┐
            X ─────►│   W^Q   │    │ Scale   │  ÷ √dₖ
                    └─────────┘    │ (÷√dₖ)  │
                                   └────┬────┘
                    ┌─────────┐         │
            X ─────►│   W^K   │         ▼
                    └────┬────┘    ┌─────────┐
                         │         │ Softmax │
                    ┌────▼────┐    └────┬────┘
                    │    K    │         │
                    └────┬────┘         │ Attention
                         │              │ Weights
                         └──────────────┼───────┐
                                        ▼       │
                    ┌─────────┐    ┌───────┐    │
            X ─────►│   W^V   │───►│   V   │────┘
                    └─────────┘    └───┬───┘
                                       │
                                       ▼
                                  ┌───────┐
                                  │MatMul │ Weights × V
                                  └───┬───┘
                                      │
                                      ▼
                                   Output
```

### Multi-Head Attention

**Key Idea**: Instead of computing attention once, compute it **multiple times in parallel** with different learned projections. This allows the model to attend to information from different "perspectives".

**Formula**:
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O$$

Where each head is:
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

**Symbol Legend**:
- $h$ = Number of attention heads
- $\text{head}_i$ = Output of attention head $i$
- $W_i^Q \in \mathbb{R}^{d \times d_k}$ = Query projection for head $i$
- $W_i^K \in \mathbb{R}^{d \times d_k}$ = Key projection for head $i$
- $W_i^V \in \mathbb{R}^{d \times d_v}$ = Value projection for head $i$
- $W^O \in \mathbb{R}^{hd_v \times d}$ = Output projection
- Typically: $d_k = d_v = d/h$

```
╔══════════════════════════════════════════════════════════════════════╗
║                    MULTI-HEAD ATTENTION                              ║
╚══════════════════════════════════════════════════════════════════════╝

Why Multiple Heads?
───────────────────
Each head can learn to focus on different things:
• Head 1: Syntactic relationships (subject-verb)
• Head 2: Semantic similarity
• Head 3: Positional patterns
• Head 4: Coreference (pronouns → nouns)

Example with h=4 heads:

Input: "The cat sat on the mat because it was tired"

Head 1 (Syntax):
  "cat" attends to "sat" (subject-verb agreement)
  
Head 2 (Semantics):
  "cat" attends to "it" (coreference)
  
Head 3 (Local):
  "on" attends to "the" and "mat" (prepositional phrase)
  
Head 4 (Global):
  "tired" attends to "cat" (who is tired?)


Architecture Diagram:
─────────────────────

                              Input (Q, K, V)
                                    │
              ┌─────────────────────┼─────────────────────┐
              │                     │                     │
              ▼                     ▼                     ▼
        ┌──────────┐          ┌──────────┐          ┌──────────┐
        │  Head 1  │          │  Head 2  │    ...   │  Head h  │
        │          │          │          │          │          │
        │ Q₁ K₁ V₁ │          │ Q₂ K₂ V₂ │          │ Qₕ Kₕ Vₕ  │
        │    ↓     │          │    ↓     │          │    ↓     │
        │Attention │          │Attention │          │Attention │
        │    ↓     │          │    ↓     │          │    ↓     │
        │ Output₁  │          │ Output₂  │          │ Outputₕ  │
        └────┬─────┘          └────┬─────┘          └────┬─────┘
             │                     │                     │
             └─────────────────────┼─────────────────────┘
                                   │
                                   ▼
                            ┌─────────────┐
                            │ Concatenate │
                            └──────┬──────┘
                                   │
                                   ▼
                            ┌─────────────┐
                            │  Linear W^O │
                            └──────┬──────┘
                                   │
                                   ▼
                                Output


Dimension Flow (d=512, h=8):
────────────────────────────

Input: [batch, seq_len, 512]
           │
           ├──► Q, K, V projections: [batch, seq_len, 512]
           │
           ├──► Split into 8 heads: [batch, 8, seq_len, 64]
           │                              (512/8 = 64 per head)
           │
           ├──► Each head: attention on 64-dim
           │
           ├──► Concatenate: [batch, seq_len, 512]
           │                     (8 × 64 = 512)
           │
           └──► Output projection: [batch, seq_len, 512]
```

### Comparison: Single Head vs Multi-Head

```
╔══════════════════════════════════════════════════════════════════════╗
║            SINGLE HEAD vs MULTI-HEAD ATTENTION                       ║
╚══════════════════════════════════════════════════════════════════════╝

SINGLE HEAD (d=512):
────────────────────
• One 512-dim attention computation
• One set of attention weights
• Limited to one "view" of the data

Attention pattern:
  ┌─────────────────────────────┐
  │ ██░░░░▓▓▓░░░░██████░░░░░░░ │  One pattern
  │ ░░██░░░░░░▓▓▓░░░░░░████░░░ │
  │ ░░░░██▓▓▓░░░░░░░░░░░░░░███ │
  └─────────────────────────────┘


MULTI-HEAD (d=512, h=8):
────────────────────────
• Eight 64-dim attention computations
• Eight sets of attention weights
• Eight different "views" combined

Head 1:         Head 2:         Head 3:
┌─────────┐    ┌─────────┐    ┌─────────┐
│██░░░░░░░│    │░░░░██░░░│    │░░██░░██░│  ...
│░░██░░░░░│    │░░░░░░██░│    │░░░░░░░░░│
└─────────┘    └─────────┘    └─────────┘
(syntax)       (semantics)    (position)

Combined: Richer representation!


┌─────────────────────────────────────────────────────────────────────┐
│              COMPARISON TABLE                                       │
├─────────────────┬─────────────────────┬─────────────────────────────┤
│ Property        │ Single Head         │ Multi-Head                  │
├─────────────────┼─────────────────────┼─────────────────────────────┤
│ Parameters      │ 3 × d × d           │ Same (split across heads)   │
│ Attention views │ 1                   │ h (number of heads)         │
│ Per-head dim    │ d                   │ d/h                         │
│ Expressiveness  │ Limited             │ High (multiple patterns)    │
│ Parallelization │ Single computation  │ Highly parallel             │
│ Typical usage   │ Rare in modern NLP  │ Standard in Transformers    │
└─────────────────┴─────────────────────┴─────────────────────────────┘
```

### Connection to Transformers

Multi-head self-attention is the core building block of **Transformers**:

```
╔══════════════════════════════════════════════════════════════════════╗
║              TRANSFORMER ARCHITECTURE (Simplified)                   ║
╚══════════════════════════════════════════════════════════════════════╝

                    ENCODER                      DECODER
              ┌─────────────────┐          ┌─────────────────┐
              │                 │          │                 │
              │    Layer N      │          │    Layer N      │
              │       ↑         │          │       ↑         │
              │       │         │          │  ┌────┴────┐    │
              │  Feed Forward   │          │  │Cross-Attn│◄──┼───┐
              │       ↑         │          │  └────┬────┘    │   │
              │       │         │          │       │         │   │
              │  ┌────┴────┐    │          │  Feed Forward   │   │
              │  │Self-Attn│    │          │       ↑         │   │
              │  └────┬────┘    │          │       │         │   │
              │       │         │          │  ┌────┴────┐    │   │
              │       ↑         │          │  │Self-Attn│    │   │
              │    Layer 1      │          │  │(Masked) │    │   │
              │       ↑         │          │  └────┬────┘    │   │
              │       │         │          │       │         │   │
              │   Input Emb     │          │  Output Emb     │   │
              │   + Position    │          │   + Position    │   │
              │       ↑         │          │       ↑         │   │
              └───────┼─────────┘          └───────┼─────────┘   │
                      │                            │             │
              "Je t'aime"                   "<s> I love"         │
                                                                 │
              Encoder outputs ───────────────────────────────────┘

Key Attention Types in Transformers:
────────────────────────────────────

1. ENCODER SELF-ATTENTION
   • Each encoder position attends to all encoder positions
   • Builds rich representations of input

2. DECODER SELF-ATTENTION (Masked)
   • Each decoder position attends to previous decoder positions
   • Masked: can't see future tokens (autoregressive)

3. ENCODER-DECODER CROSS-ATTENTION
   • Decoder queries attend to encoder keys/values
   • Connects decoder to input representation
```

---

## Attention Visualizations

### Attention Heatmaps

Attention weights can be visualized as heatmaps, showing which input positions each output position attends to:

```
╔══════════════════════════════════════════════════════════════════════╗
║              ATTENTION HEATMAP VISUALIZATION                         ║
╚══════════════════════════════════════════════════════════════════════╝

Example: Translating "The black cat sat" → "Le chat noir assis"

                        SOURCE (English)
                    The   black   cat    sat
                ┌───────┬───────┬───────┬───────┐
           Le   │ ████  │ ░░░░  │ ░░░░  │ ░░░░  │  0.85  0.05  0.05  0.05
                ├───────┼───────┼───────┼───────┤
         chat   │ ░░░░  │ ░░░░  │ ████  │ ░░░░  │  0.05  0.10  0.80  0.05
  TARGET        ├───────┼───────┼───────┼───────┤
(French)  noir  │ ░░░░  │ ████  │ ▒▒▒▒  │ ░░░░  │  0.05  0.75  0.15  0.05
                ├───────┼───────┼───────┼───────┤
        assis   │ ░░░░  │ ░░░░  │ ░░░░  │ ████  │  0.05  0.05  0.05  0.85
                └───────┴───────┴───────┴───────┘

Legend:
  ████ = Very high attention (0.7-1.0)
  ▓▓▓▓ = High attention (0.4-0.7)
  ▒▒▒▒ = Medium attention (0.2-0.4)
  ░░░░ = Low attention (0.0-0.2)

Interpretation:
───────────────
• "Le" (French "The") → attends to "The" ✓
• "chat" (French "cat") → attends to "cat" ✓
• "noir" (French "black") → attends to "black" ✓
  (Also some attention to "cat" - adjective-noun relationship)
• "assis" (French "sat") → attends to "sat" ✓

The diagonal pattern shows word-level alignment learned automatically!
```

### Multi-Head Attention Patterns

Different heads learn different patterns:

```
╔══════════════════════════════════════════════════════════════════════╗
║              MULTI-HEAD ATTENTION PATTERNS                           ║
╚══════════════════════════════════════════════════════════════════════╝

Sentence: "The cat that sat on the mat was tired"

Head 1: POSITIONAL (attends to nearby words)
─────────────────────────────────────────────
         The cat that sat  on the mat was tired
    The  ███ ██  ░   ░    ░   ░   ░   ░   ░
    cat  ██  ███ ██  ░    ░   ░   ░   ░   ░
   that  ░   ██  ███ ██   ░   ░   ░   ░   ░
    sat  ░   ░   ██  ███  ██  ░   ░   ░   ░
     on  ░   ░   ░   ██   ███ ██  ░   ░   ░
    ...

Pattern: Strong diagonal = local/positional attention


Head 2: SYNTACTIC (subject-verb relationships)
──────────────────────────────────────────────
         The cat that sat  on the mat was tired
    The  ░   ░   ░   ░    ░   ░   ░   ░   ░
    cat  ░   ░   ░   ███  ░   ░   ░   ███ ░    ← cat→sat, cat→was
   that  ░   ░   ░   ░    ░   ░   ░   ░   ░
    sat  ░   ███ ░   ░    ░   ░   ░   ░   ░    ← sat→cat
     on  ░   ░   ░   ░    ░   ░   ███ ░   ░    ← on→mat
    mat  ░   ░   ░   ░    ███ ░   ░   ░   ░    ← mat→on
    was  ░   ███ ░   ░    ░   ░   ░   ░   ░    ← was→cat
  tired  ░   ███ ░   ░    ░   ░   ░   ░   ░    ← tired→cat

Pattern: Links subjects to verbs, preps to objects


Head 3: COREFERENCE (pronouns to antecedents)
─────────────────────────────────────────────
For: "The cat saw the dog. It was scared."

         The cat saw the dog  .  It was scared
    The  ░   ░   ░   ░   ░   ░   ░   ░   ░
    cat  ░   ░   ░   ░   ░   ░   ░   ░   ░
    saw  ░   ░   ░   ░   ░   ░   ░   ░   ░
    the  ░   ░   ░   ░   ░   ░   ░   ░   ░
    dog  ░   ░   ░   ░   ░   ░   ░   ░   ░
      .  ░   ░   ░   ░   ░   ░   ░   ░   ░
     It  ░   ███ ░   ░   ██  ░   ░   ░   ░    ← It→cat/dog
    was  ░   ░   ░   ░   ░   ░   ░   ░   ░
 scared  ░   ░   ░   ░   ░   ░   ░   ░   ░

Pattern: Pronouns attend to potential antecedents


Head 4: SEMANTIC (similar meanings)
───────────────────────────────────
For: "The big large enormous cat"

         The big large enormous cat
    The  ░   ░   ░     ░       ░
    big  ░   ░   ███   ███     ░     ← big→large, big→enormous
  large  ░   ███ ░     ███     ░     ← large→big, large→enormous
enormous ░   ███ ███   ░       ░     ← enormous→big, enormous→large
    cat  ░   ░   ░     ░       ░

Pattern: Synonyms/related words attend to each other
```

### What Attention Learns

```
╔══════════════════════════════════════════════════════════════════════╗
║              WHAT ATTENTION LEARNS                                   ║
╚══════════════════════════════════════════════════════════════════════╝

LEARNED PATTERNS (discovered automatically during training):
────────────────────────────────────────────────────────────

1. WORD ALIGNMENT (in translation)
   • "cat" ↔ "chat", "dog" ↔ "chien"
   • Handles word order differences

2. SYNTACTIC STRUCTURE
   • Subject-verb agreement: "The cats ARE" vs "The cat IS"
   • Dependency parsing emerges implicitly

3. COREFERENCE RESOLUTION
   • "John said HE was tired" → HE attends to John
   • Pronoun resolution without explicit training

4. SEMANTIC RELATIONSHIPS
   • Similar words attend to each other
   • Antonyms, synonyms, hypernyms

5. LONG-RANGE DEPENDENCIES
   • "The cat that the dog that the rat bit chased ran away"
   • Can track nested structures


WHY ATTENTION WORKS SO WELL:
────────────────────────────

┌─────────────────────────────────────────────────────────────────────┐
│                                                                     │
│ 1. DIRECT CONNECTIONS                                               │
│    Any two positions can interact directly (O(1) path length)       │
│                                                                     │
│ 2. DYNAMIC FOCUS                                                    │
│    Different context vectors for different queries                  │
│                                                                     │
│ 3. PARALLELIZATION                                                  │
│    All positions computed simultaneously (unlike RNNs)              │
│                                                                     │
│ 4. INTERPRETABILITY                                                 │
│    Attention weights show what the model "looks at"                 │
│                                                                     │
│ 5. FLEXIBILITY                                                      │
│    Works for variable-length inputs without modification            │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

---

## Summary

### Key Formulas Recap

```
╔══════════════════════════════════════════════════════════════════════╗
║                    KEY FORMULAS                                      ║
╚══════════════════════════════════════════════════════════════════════╝

1. ATTENTION SCORES (choose one):
─────────────────────────────────
   Dot Product:     eᵢ = qᵀkᵢ
   Scaled:          eᵢ = qᵀkᵢ / √dₖ
   Additive:        eᵢ = vᵀtanh(Wqq + Wkkᵢ)
   Multiplicative:  eᵢ = qᵀWkᵢ


2. ATTENTION WEIGHTS (softmax):
───────────────────────────────
                exp(eᵢ)
   αᵢ = ─────────────────
         Σⱼ exp(eⱼ)


3. CONTEXT VECTOR (weighted sum):
─────────────────────────────────
   c = Σᵢ αᵢ · vᵢ


4. SCALED DOT-PRODUCT ATTENTION (full):
───────────────────────────────────────
                         QKᵀ
   Attention(Q,K,V) = softmax(─────) V
                         √dₖ


5. MULTI-HEAD ATTENTION:
────────────────────────
   MultiHead(Q,K,V) = Concat(head₁, ..., headₕ)Wᴼ
   
   where headᵢ = Attention(QWᵢᵠ, KWᵢᴷ, VWᵢⱽ)
```

### Advantages Over Fixed Context Vectors

```
┌─────────────────────────────────────────────────────────────────────┐
│              WITHOUT ATTENTION              WITH ATTENTION          │
├─────────────────────────────────────────────────────────────────────┤
│ Fixed context vector c            │ Dynamic context cₜ per step      │
│ All info compressed into one vec  │ Access to all encoder states    │
│ Information lost for long seqs    │ No information bottleneck       │
│ Poor on sentences > 20 words      │ Works on long sequences         │
│ No alignment information          │ Learns word alignments          │
│ Hard to interpret                 │ Attention weights interpretable │
└─────────────────────────────────────────────────────────────────────┘
```

### Computational Complexity

```
╔══════════════════════════════════════════════════════════════════════╗
║              COMPUTATIONAL COMPLEXITY                                ║
╚══════════════════════════════════════════════════════════════════════╝

For sequence length n and dimension d:

┌─────────────────┬─────────────────┬─────────────────┬───────────────┐
│ Operation       │ Time Complexity │ Space Complexity│ Parallelizable│
├─────────────────┼─────────────────┼─────────────────┼───────────────┤
│ Self-Attention  │ O(n² · d)       │ O(n²)           │ Yes ✓         │
│ RNN             │ O(n · d²)       │ O(n · d)        │ No ✗          │
│ CNN (kernel k)  │ O(n · k · d²)   │ O(n · d)        │ Yes ✓         │
└─────────────────┴─────────────────┴─────────────────┴───────────────┘

Key insight:
• Self-attention is O(n²) in sequence length
• For long sequences, this can be expensive
• Solutions: Sparse attention, Linear attention, Chunked attention
```

### Applications Checklist

```
╔══════════════════════════════════════════════════════════════════════╗
║              ATTENTION APPLICATIONS                                  ║
╚══════════════════════════════════════════════════════════════════════╝

✓ Machine Translation (Google Translate, DeepL)
✓ Text Summarization (News, documents)
✓ Question Answering (Reading comprehension)
✓ Sentiment Analysis (Reviews, social media)
✓ Named Entity Recognition (Information extraction)
✓ Image Captioning (Visual understanding)
✓ Speech Recognition (Audio to text)
✓ Music Generation (Sequence modeling)
✓ Protein Structure Prediction (AlphaFold)
✓ Code Generation (GitHub Copilot, Codex)
✓ Large Language Models (GPT, Claude, Llama, Gemini)
✓ Vision Transformers (ViT for images)
✓ Multimodal Models (CLIP, Flamingo)

Attention is THE fundamental building block of modern AI!
```

---

## Final Summary

**Attention** is a mechanism that allows neural networks to dynamically focus on relevant parts of the input when producing output. Key points:

1. **Solves the bottleneck problem**: No more compressing everything into a single fixed vector

2. **Three-step process**: Score → Softmax → Weighted Sum

3. **Multiple variants**: Dot-product, additive, multiplicative, scaled

4. **Self-attention**: Allows positions to attend to all other positions in the same sequence

5. **Multi-head attention**: Multiple parallel attention "perspectives" for richer representations

6. **Foundation of Transformers**: The architecture powering all modern LLMs

**The key insight**: Instead of asking "what did the encoder output?", attention asks "what parts of the input are relevant for this specific output step?" This simple shift revolutionized NLP and beyond.
