# Beam Search

## Table of Contents
1. [Introduction to Beam Search](#introduction-to-beam-search)
2. [Beam Search Algorithm](#beam-search-algorithm)
3. [Mathematical Formulation](#mathematical-formulation)
4. [Complete Step-by-Step Example](#complete-step-by-step-example)

---

## Introduction to Beam Search

### What is Beam Search?

**Beam Search** is an approximate search algorithm used to find the most likely output sequence in sequence-to-sequence models. It's a heuristic search algorithm that explores the space of possible output sequences by keeping track of the top-B (beam width) most promising candidates at each generation step.

**Problem Statement**: Given an input sequence $x$, find the output sequence $y$ that maximizes the conditional probability $P(y|x)$.

**Why is Beam Search Needed?**

In sequence generation tasks, the model must produce one token at a time, and each choice affects all subsequent choices. Without proper search:
- **Greedy Search**: Picks the single best token at each step → gets stuck in local optima
- **Exhaustive Search**: Explores all possibilities → computationally infeasible

Beam Search provides a middle ground: more thorough than greedy search, more efficient than exhaustive search.

### The Sequence Generation Problem

#### Task Examples

**1. Machine Translation**
```
Input (French):  "Je suis étudiant"
Output (English): "I am a student"
```

**2. Text Generation**
```
Input:  "Once upon a"
Output: "time there was a princess"
```

**3. Image Captioning**
```
Input:  [Image of a dog playing]
Output: "A dog playing with a ball"
```

**4. Speech Recognition**
```
Input:  [Audio waveform]
Output: "Hello, how are you?"
```

**5. Text Summarization**
```
Input:  [Long article]
Output: "Brief summary of key points"
```

#### Search Space Explosion

The fundamental challenge: **exponential search space**

**Formula**: For a vocabulary of size $|V|$ and target sequence length $T$:

$$\text{Total possible sequences} = |V|^T$$

**Symbol Legend**:
- $|V|$ = Vocabulary size (number of possible tokens)
- $T$ = Target sequence length
- $|V|^T$ = Total number of possible sequences

**Concrete Example**:
```
Vocabulary size: |V| = 10,000 (typical for neural MT)
Sequence length: T = 10 words

Total sequences = 10,000^10 = 10^40 sequences

To put this in perspective:
- 10^40 is larger than the number of stars in the observable universe!
- Evaluating 1 billion sequences per second would take 3 × 10^23 years
```

#### Visualization of Search Space Explosion

```
╔═══════════════════════════════════════════════════════════════════════╗
║              SEQUENCE GENERATION SEARCH SPACE                         ║
╚═══════════════════════════════════════════════════════════════════════╝

Vocabulary Size: |V| = 4 tokens {A, B, C, D}
Target Length: T = 3

Time Step:      t=0          t=1              t=2              t=3
              ┌─────┐
              │START│
              └──┬──┘
                 │
         ┌───────┼───────┬────────┐
         │       │       │        │
        [A]     [B]     [C]      [D]        ← 4 choices
         │       │       │        │
    ┬────┼───┬   │       │        │
    │    │   │   │       │        │
   [A]  [B] [C] [D]     ...      ...        ← 4×4 = 16 choices
    │    │   │   │
  ┬─┼─┬  │   │   │
  │ │ │  │   │   │
 [A][B][C][D] ... ...             ...       ← 4×4×4 = 64 choices

Total paths = 4^3 = 64 sequences

With |V| = 10,000 and T = 10:
Total paths = 10,000^10 = 10^40 sequences (impossible to enumerate!)

╔═══════════════════════════════════════════════════════════════════════╗
║                    SEARCH STRATEGIES COMPARISON                       ║
╚═══════════════════════════════════════════════════════════════════════╝

EXHAUSTIVE SEARCH (Optimal but Infeasible):
├── Explores: ALL |V|^T sequences
├── Guarantees: Finds the absolute best sequence
└── Problem: Computationally impossible for real vocabularies

GREEDY SEARCH (Fast but Suboptimal):
├── Explores: Only 1 path (T steps)
├── At each step: Pick argmax P(y_t | y_{<t}, x)
└── Problem: Local optima - can't recover from early mistakes

Example of Greedy Failure:
  Step 1: Choose "The" (P=0.5) over "A" (P=0.4)
  Step 2: Must continue with "The ..." 
  
  But "A good solution" (overall P=0.3) might beat
  "The bad solution" (overall P=0.2)!
  
  Greedy can't backtrack to explore "A" path!

BEAM SEARCH (Balanced Approach):
├── Explores: B × T × |V| sequences (linear in T!)
├── Keeps: Top-B candidates at each step
└── Balance: Better than greedy, feasible unlike exhaustive
```

#### Why Greedy Search Fails

**Greedy Search** makes the locally optimal choice at each step:

$$y_t = \underset{y_t}{\text{argmax}} \; P(y_t | y_{<t}, x)$$

**Problem**: The best immediate choice may lead to poor overall sequence quality.

**Example**:
```
Translating "Je suis" to English:

Greedy Choice at t=1:
┌─────────────────────────────────┐
│ "I"     → P = 0.7               │ ← Greedy picks this
│ "We"    → P = 0.2               │
│ "They"  → P = 0.1               │
└─────────────────────────────────┘

But consider full sequences:
┌──────────────────────────────────────┐
│ "I am"       → P("I") × P("am"|"I")       = 0.7 × 0.6 = 0.42 │
│ "I are"      → P("I") × P("are"|"I")      = 0.7 × 0.1 = 0.07 │
│ "We are"     → P("We") × P("are"|"We")    = 0.2 × 0.8 = 0.16 │
│ "They are"   → P("They") × P("are"|"They")= 0.1 × 0.9 = 0.09 │
└──────────────────────────────────────┘

Greedy selects "I am" (0.42), but can never explore "We are" (0.16)
If continuing, "We are happy" might have higher total probability than
"I am happy", but greedy committed to "I" in step 1 and can't backtrack!
```


### Where Beam Search is Used

#### 1. Neural Machine Translation (NMT)
**Description**: Translate text from one language to another using encoder-decoder architectures.

**Application**: Given French sentence, generate English translation by maintaining B=5-10 candidate translations, selecting the one with highest probability after accounting for length.

**Example**: Google Translate, DeepL

#### 2. Text Generation and Language Modeling
**Description**: Generate coherent text continuations or completions.

**Application**: GPT-style models use beam search to generate diverse, high-quality text by exploring multiple promising continuations simultaneously rather than greedily picking one token at a time.

**Example**: GPT-3 text completion, story generation

#### 3. Image Captioning
**Description**: Generate natural language descriptions of images.

**Application**: CNN encoder extracts image features, RNN decoder with beam search generates captions by maintaining multiple candidate descriptions, ensuring fluent and accurate descriptions.

**Example**: Automated alt-text generation, photo organization

#### 4. Speech Recognition (ASR)
**Description**: Convert audio signals to text transcriptions.

**Application**: Maintain B candidate transcriptions as audio is processed frame-by-frame, selecting the most likely transcription considering acoustic and language model scores.

**Example**: Google Speech-to-Text, Whisper

#### 5. Code Generation and Program Synthesis
**Description**: Generate source code from natural language descriptions.

**Application**: Models like Codex use beam search to generate syntactically valid and semantically correct code by exploring multiple implementation paths.

**Example**: GitHub Copilot, AlphaCode

---

## Beam Search Algorithm

### Core Concept

**Beam Width (B)**: The number of candidate sequences (hypotheses) to maintain at each generation step.

**Key Idea**: At each time step $t$:
1. Expand each of the B current hypotheses by considering all $|V|$ possible next tokens
2. Score all $B \times |V|$ resulting sequences
3. Keep only the top-B highest-scoring sequences (prune the rest)
4. Repeat until termination

**Plain English Explanation**:

Imagine you're navigating a maze, trying to find the exit:
- **Greedy Search**: You always take the path that looks best right now. If it leads to a dead end, you're stuck.
- **Exhaustive Search**: You explore every single path. This guarantees finding the exit but takes forever.
- **Beam Search**: You explore the B most promising paths simultaneously. At each fork, you expand all B paths, then keep only the B best ones. This balances thoroughness with efficiency.

**Analogy - Chess**:
- **Greedy**: Only consider your best move right now (depth=1)
- **Beam Search**: Consider your best B moves and explore them several moves ahead
- **Exhaustive**: Consider every possible game (impossible!)

#### ASCII Visualization: Beam Expansion

```
╔═══════════════════════════════════════════════════════════════════════╗
║                    BEAM SEARCH VISUALIZATION                          ║
║                         (B = 3)                                       ║
╚═══════════════════════════════════════════════════════════════════════╝

Vocabulary: {A, B, C, D, E}  (|V| = 5)
Beam Width: B = 3

Step t=0: Start with <START> token
┌──────────┐
│ <START>  │
│ score: 0 │
└─────┬────┘
      │
      
Step t=1: Generate first token - expand to |V| = 5 candidates
      │
  ┌───┴───┬───┬───┬───┐
  │       │   │   │   │
 [A]     [B] [C] [D] [E]     ← Generate 5 candidates
 -0.3    -0.5 -1.2 -0.7 -1.5  ← Log probabilities
  │       │   │   │   │
  ✓       ✓   ✗  ✓   ✗      ← Keep top-3, prune 2
  
Kept: [A]:-0.3, [B]:-0.5, [D]:-0.7

Step t=2: Expand B=3 beams, each with |V|=5 options → 3×5=15 candidates
      
From [A]:              From [B]:              From [D]:
┌───┬───┬───┬───┬───  ┬───┬───┬───┬───┬───  ┬───┬───┬───┬───┬───┐
[AA][AB][AC][AD][AE]  [BA][BB][BC][BD][BE]  [DA][DB][DC][DD][DE]
-0.9-0.8-1.5-1.0-2.0  -1.0-1.2-1.8-0.9-2.1  -1.4-1.3-2.0-1.5-2.5
 │   │   │   │   │     │   │   │   │   │     │   │   │   │   │
 │   ✓   │   │   │     │   │   │   ✓  │     │   ✓   │   │   │  ← Keep top-3
 │       │   │   │     │   │   │       │     │       │   │   │     from all 15
 
Kept: [AB]:-0.8, [BD]:-0.9, [DB]:-1.3

Step t=3: Expand B=3 beams → 3×5=15 candidates
      
From [AB]:             From [BD]:             From [DB]:
[ABA][ABB]...[ABE]    [BDA][BDB]...[BDE]    [DBA][DBB]...[DBE]
  │    │       │        │    │       │        │    │       │
  └────┴───┬───┴────────┴────┴───┬───┴────────┴────┴───┬───┘
           │                     │                     │
           └─────────────────────┴─────────────────────┘
                                 │
                         Keep top-3 again
                         
Final Step: Select best complete sequence
┌─────────────────────────────────────────┐
│ Hypothesis 1: [A][B][E][<EOS>] -2.1   │
│ Hypothesis 2: [B][D][A][<EOS>] -2.3   │
│ Hypothesis 3: [D][B][B][<EOS>] -2.5   │
└─────────────────────────────────────────┘
                    │
              Select best: [A][B][E]

Key Observations:
━━━━━━━━━━━━━━━━━
├─ At each step: Evaluate B × |V| candidates (e.g., 3 × 5 = 15)
├─ Keep only: Top-B (3) sequences
├─ Pruned: B × |V| - B = B × (|V| - 1) sequences (e.g., 15 - 3 = 12)
└─ Efficiency: Linear in T, not exponential!
```

### Algorithm Steps

#### 1. Initialization

**Setup**:
- Start with a single hypothesis containing the start token: $[\text{<START>}]$
- Initialize score: $\text{score} = 0$ (or $\log P(\text{<START>}) = 0$)
- Set beam width $B$

**Data Structure**:
```
Hypothesis: {
  tokens: [<START>],
  score: 0.0,
  complete: False
}

Active beams: [Hypothesis]
Completed beams: []
```

#### 2. Expansion

**For each active hypothesis in the beam**:

At time step $t$, for each of the $B$ active hypotheses:
1. Pass the current sequence through the model
2. Get probability distribution over vocabulary: $P(y_t | y_{<t}, x)$
3. For each token $y_t \in V$, compute new hypothesis: $[y_1, \ldots, y_{t-1}, y_t]$
4. Compute new score: $\text{score}_{\text{new}} = \text{score}_{\text{old}} + \log P(y_t | y_{<t}, x)$

**Result**: $B \times |V|$ candidate hypotheses

**Formula**:
$$\text{score}(y_1, \ldots, y_t) = \sum_{i=1}^{t} \log P(y_i | y_{<i}, x)$$

**Symbol Legend**:
- $y_{<t}$ = Tokens generated before time $t$: $[y_1, y_2, \ldots, y_{t-1}]$
- $y_t$ = Token being generated at time $t$
- $P(y_t | y_{<t}, x)$ = Probability of token $y_t$ given previous tokens and input $x$
- $\log P(\cdot)$ = Log probability (for numerical stability)

#### 3. Pruning (Keeping Top-B)

**Selection Process**:
1. Collect all $B \times |V|$ candidate hypotheses from expansion
2. Sort by score (higher is better for log probabilities closer to 0)
3. Select top-B candidates
4. Discard remaining $B \times (|V| - 1)$ candidates

**Termination Handling**:
- If a hypothesis generates `<EOS>` (end-of-sequence) token:
  - Mark as complete
  - Move to completed hypotheses list
  - Don't expand further
- Continue with remaining active hypotheses

**Active Beam Management**:
```
Active beams = B hypotheses being expanded
Completed beams = Hypotheses that generated <EOS>

If active beams become empty:
  → All B beams have finished
  → Terminate algorithm
```

#### 4. Termination Conditions

**Algorithm stops when**:

**Condition 1**: All B beams generate `<EOS>` token
```
Active hypotheses = 0
Completed hypotheses = B
→ TERMINATE
```

**Condition 2**: Maximum length reached
```
Current length t = T_max
→ FORCE TERMINATION
→ Select best from active + completed hypotheses
```

**Condition 3**: Early stopping (optional optimization)
```
If all active hypotheses have score < best completed hypothesis:
→ Can't improve further
→ TERMINATE
```


## Mathematical Formulation

### Probability Scoring

#### Log Probability Formulation

The goal of sequence generation is to find the sequence $y = [y_1, y_2, \ldots, y_T]$ that maximizes the conditional probability given input $x$:

$$y^* = \underset{y}{\text{argmax}} \; P(y | x)$$

Using the chain rule of probability, we can decompose this:

$$P(y | x) = \prod_{t=1}^{T} P(y_t | y_1, y_2, \ldots, y_{t-1}, x) = \prod_{t=1}^{T} P(y_t | y_{<t}, x)$$

**Taking logarithm**:

$$\log P(y | x) = \sum_{t=1}^{T} \log P(y_t | y_{<t}, x)$$

**Complete Symbol Legend**:
- $y$ = Output sequence: $[y_1, y_2, \ldots, y_T]$
- $y^*$ = Best (optimal) output sequence
- $y_t$ = Token at position $t$ in the output sequence
- $y_{<t}$ = All tokens before position $t$: $[y_1, y_2, \ldots, y_{t-1}]$
- $x$ = Input sequence
- $P(y | x)$ = Probability of output sequence $y$ given input $x$
- $P(y_t | y_{<t}, x)$ = Conditional probability of token $y_t$ given previous tokens and input
- $T$ = Length of output sequence
- $\log$ = Natural logarithm
- $\prod$ = Product (multiply all terms)
- $\sum$ = Sum (add all terms)
- $\underset{y}{\text{argmax}}$ = Find the $y$ that maximizes the expression

#### Why Use Log Probabilities?

**Problem with Direct Probabilities**:

Probabilities are between 0 and 1. Multiplying many small probabilities leads to numerical underflow.

**Example**:
```
Consider a sequence of length T = 20

Typical token probability: P(y_t | y_{<t}, x) ≈ 0.3

Direct multiplication:
P(y|x) = 0.3 × 0.3 × 0.3 × ... (20 times)
       = (0.3)^20
       = 0.0000000000349
       ≈ 3.5 × 10^-11

Problems:
├─ Computer floating point: Limited precision
├─ Very small numbers: May round to zero
└─ Numerical underflow: Computation becomes unstable

Standard floating point (float32):
├─ Minimum positive value: ~10^-38
├─ But loses precision around: ~10^-7
└─ Multiplying 20 probabilities risks underflow
```

**Solution with Log Probabilities**:

$$\log P(y|x) = \sum_{t=1}^{T} \log P(y_t | y_{<t}, x)$$

**Advantages**:

**1. Numerical Stability**
```
Using logs:
log P(y|x) = log(0.3) + log(0.3) + ... (20 times)
           = 20 × log(0.3)
           = 20 × (-1.204)
           = -24.08

Result: -24.08 (stable, no underflow!)

Range: log probabilities are in (-∞, 0]
├─ Log of 1.0 (max probability): 0
├─ Log of 0.1: -2.3
├─ Log of 0.01: -4.6
└─ Much more stable for computation!
```

**2. Computational Efficiency**
```
Multiplication → Addition (faster operation)

Product of probabilities:
P = p_1 × p_2 × p_3 × ... × p_T  
(T-1 multiplications)

Sum of log probabilities:
log P = log(p_1) + log(p_2) + ... + log(p_T)
(T-1 additions, faster and more stable)
```

**3. Preserves Ordering**
```
Since log is monotonic increasing:
P(y_1|x) > P(y_2|x)  ⟺  log P(y_1|x) > log P(y_2|x)

Finding argmax in probability space = Finding argmax in log space!

Example:
P(y_1|x) = 0.2,  log P(y_1|x) = -1.609
P(y_2|x) = 0.3,  log P(y_2|x) = -1.204

P(y_2|x) > P(y_1|x)  ✓
log P(y_2|x) > log P(y_1|x)  ✓  (ordering preserved)
```

### Beam Score Calculation

#### Score for Each Hypothesis

At each time step $t$, for a hypothesis $h$ with current sequence $y_{1:t-1} = [y_1, \ldots, y_{t-1}]$:

**Current Score** (log probability of sequence so far):
$$\text{score}(y_{1:t-1}) = \sum_{i=1}^{t-1} \log P(y_i | y_{<i}, x)$$

**Extending the hypothesis** with new token $y_t$:

$$\text{score}(y_{1:t}) = \text{score}(y_{1:t-1}) + \log P(y_t | y_{<t}, x)$$

**Complete Symbol Legend**:
- $h$ = Hypothesis (candidate sequence being considered)
- $y_{1:t}$ = Sequence from position 1 to $t$: $[y_1, y_2, \ldots, y_t]$
- $y_{1:t-1}$ = Partial sequence before adding $y_t$
- $\text{score}(y_{1:t})$ = Cumulative log probability of sequence up to position $t$
- $\log P(y_t | y_{<t}, x)$ = Log probability of next token from model

#### Combining Previous Score with New Token Probability

**Recursive Formulation**:

$$\text{score}_{\text{new}} = \text{score}_{\text{old}} + \log P(y_{\text{new}} | y_{\text{current}}, x)$$

**Step-by-Step Process**:

1. **Start**: $\text{score}(\text{<START>}) = 0$ (or small value)

2. **Step 1**: Generate first token $y_1$
   $$\text{score}(y_1) = 0 + \log P(y_1 | \text{<START>}, x)$$

3. **Step 2**: Generate second token $y_2$
   $$\text{score}(y_1, y_2) = \text{score}(y_1) + \log P(y_2 | y_1, x)$$

4. **Step t**: Generate token $y_t$
   $$\text{score}(y_{1:t}) = \text{score}(y_{1:t-1}) + \log P(y_t | y_{1:t-1}, x)$$

**Symbol Legend**:
- $\text{score}_{\text{old}}$ = Score of hypothesis before extending
- $\text{score}_{\text{new}}$ = Score after adding new token
- $y_{\text{current}}$ = Current partial sequence
- $y_{\text{new}}$ = New token being added

#### Numerical Example: Score Calculation

**Setup**:
```
Task: Translate "Hello" → Spanish
Model outputs probability distribution at each step

Vocabulary: {Hola, Buenos, dias, <EOS>}
```

**Step-by-Step Score Computation**:

**Initial State**:
```
Hypothesis: ["<START>"]
Score: 0.0
```

**Step 1: Generate First Token**

Model gives probabilities:
```
P("Hola" | "<START>", x)   = 0.7  →  log(0.7)   = -0.357
P("Buenos" | "<START>", x) = 0.25 →  log(0.25)  = -1.386
P("dias" | "<START>", x)   = 0.04 →  log(0.04)  = -3.219
P("<EOS>" | "<START>", x)  = 0.01 →  log(0.01)  = -4.605
```

Compute scores:
```
Hypothesis 1: ["<START>", "Hola"]
  score = 0.0 + (-0.357) = -0.357

Hypothesis 2: ["<START>", "Buenos"]
  score = 0.0 + (-1.386) = -1.386

Hypothesis 3: ["<START>", "dias"]
  score = 0.0 + (-3.219) = -3.219

Hypothesis 4: ["<START>", "<EOS>"]
  score = 0.0 + (-4.605) = -4.605
```

With B=2, keep: ["Hola"] (-0.357), ["Buenos"] (-1.386)

**Step 2: Extend Each Beam**

**From "Hola"** (score: -0.357):
```
P("Hola" | "Hola", x)   = 0.05  →  log(0.05)  = -2.996
P("Buenos" | "Hola", x) = 0.10  →  log(0.10)  = -2.303
P("dias" | "Hola", x)   = 0.05  →  log(0.05)  = -2.996
P("<EOS>" | "Hola", x)  = 0.80  →  log(0.80)  = -0.223

Compute new scores:
["Hola", "Hola"]:   -0.357 + (-2.996) = -3.353
["Hola", "Buenos"]: -0.357 + (-2.303) = -2.660
["Hola", "dias"]:   -0.357 + (-2.996) = -3.353
["Hola", "<EOS>"]:  -0.357 + (-0.223) = -0.580  ← Good! Nearly done
```

**From "Buenos"** (score: -1.386):
```
P("Hola" | "Buenos", x)   = 0.05  →  log(0.05)  = -2.996
P("Buenos" | "Buenos", x) = 0.05  →  log(0.05)  = -2.996
P("dias" | "Buenos", x)   = 0.85  →  log(0.85)  = -0.163
P("<EOS>" | "Buenos", x)  = 0.05  →  log(0.05)  = -2.996

Compute new scores:
["Buenos", "Hola"]:   -1.386 + (-2.996) = -4.382
["Buenos", "Buenos"]: -1.386 + (-2.996) = -4.382
["Buenos", "dias"]:   -1.386 + (-0.163) = -1.549  ← Promising
["Buenos", "<EOS>"]:  -1.386 + (-2.996) = -4.382
```

**All 8 candidates**:
```
Rank Hypothesis              Score      Status
──────────────────────────────────────────────────
1.   ["Hola", "<EOS>"]       -0.580     COMPLETE ●
2.   ["Buenos", "dias"]      -1.549     Active
3.   ["Hola", "Buenos"]      -2.660     Active
4.   ["Hola", "Hola"]        -3.353     Pruned
5.   ["Hola", "dias"]        -3.353     Pruned
6.   ["Buenos", "Hola"]      -4.382     Pruned
7.   ["Buenos", "Buenos"]    -4.382     Pruned
8.   ["Buenos", "<EOS>"]     -4.382     Pruned

With B=2, keep top-2 active: 
  - ["Buenos", "dias"] (-1.549)
  - ["Hola", "Buenos"] (-2.660)
  
Also track completed:
  - ["Hola", "<EOS>"] (-0.580) COMPLETE
```

**Step 3: Continue from active beams**:

Continue this process until all beams are complete or max length reached.

**Key Observations**:
```
1. Scores accumulate: Each token adds its log probability
2. Scores are negative: log(p) < 0 for p < 1
3. Higher score = better: -0.580 > -1.549
4. Completed sequences compete with active ones
```

### Length Normalization

#### Problem: Longer Sequences Have Lower Probabilities

**Issue**: Since we sum log probabilities, longer sequences accumulate more negative values.

$$\text{score}(y) = \sum_{t=1}^{T} \log P(y_t | y_{<t}, x)$$

Each $\log P(y_t | y_{<t}, x) < 0$ (since $0 < P < 1$), so:
- Longer sequences → More negative scores
- Shorter sequences → Less negative scores (higher scores)

**This biases beam search toward shorter outputs!**

**Example of the Bias**:
```
Sequence 1: "I am happy"           (T=3)
  Scores: -0.5 + (-0.4) + (-0.3) = -1.2

Sequence 2: "I am very happy"      (T=4)
  Scores: -0.5 + (-0.4) + (-0.3) + (-0.2) = -1.4

Sequence 3: "I am extremely happy" (T=5)
  Scores: -0.5 + (-0.4) + (-0.3) + (-0.2) + (-0.3) = -1.7

Without normalization:
  Beam search prefers Sequence 1 (-1.2 is highest)
  
But Sequence 3 might be better! Its per-token probability is:
  -1.7 / 5 = -0.34 average
  
vs Sequence 1:
  -1.2 / 3 = -0.40 average
  
Sequence 3 has better average probability per token!
```

#### Length Normalization Formula

**Normalized Score**:

$$\text{score}_{\text{normalized}}(y) = \frac{1}{T^{\alpha}} \sum_{t=1}^{T} \log P(y_t | y_{<t}, x)$$

**Alternative Notation**:

$$\text{score}_{\text{normalized}}(y) = \frac{1}{T^{\alpha}} \cdot \text{score}(y)$$

**Complete Symbol Legend**:
- $\text{score}_{\text{normalized}}(y)$ = Length-normalized score for sequence $y$
- $T$ = Length of sequence $y$ (number of tokens)
- $\alpha$ = Length penalty hyperparameter ($0 \leq \alpha \leq 1$)
- $T^{\alpha}$ = Length penalty term (raises length to power $\alpha$)
- $\sum_{t=1}^{T} \log P(y_t | y_{<t}, x)$ = Original unnormalized score

#### Effect of α Parameter

The $\alpha$ parameter controls the strength of length normalization:

**α = 0**: No normalization
$$\text{score} = \frac{1}{T^0} \cdot \text{score}(y) = \text{score}(y)$$
- Favors shorter sequences heavily
- Original beam search behavior

**α = 1**: Full length normalization (average log probability)
$$\text{score} = \frac{1}{T^1} \cdot \text{score}(y) = \frac{\text{score}(y)}{T}$$
- Computes average log probability per token
- Treats all lengths equally

**0 < α < 1**: Partial normalization
$$\text{score} = \frac{1}{T^{\alpha}} \cdot \text{score}(y)$$
- Compromise between no normalization and full normalization
- Common choice: $\alpha = 0.6$ or $\alpha = 0.7$

**Visual Comparison**:
```
╔═══════════════════════════════════════════════════════════════╗
║         LENGTH NORMALIZATION EFFECT                           ║
╚═══════════════════════════════════════════════════════════════╝

Consider three hypotheses:
─────────────────────────────
Seq 1: [A, B, <EOS>]           Length T=2, Raw Score=-1.0
Seq 2: [C, D, E, <EOS>]        Length T=3, Raw Score=-1.2
Seq 3: [F, G, H, I, <EOS>]     Length T=4, Raw Score=-1.3


α = 0 (No Normalization):
──────────────────────────
Normalized Score = Raw Score / T^0 = Raw Score

Seq 1: -1.0 / 1 = -1.0  ← BEST (shortest wins)
Seq 2: -1.2 / 1 = -1.2
Seq 3: -1.3 / 1 = -1.3

Ranking: Seq 1 > Seq 2 > Seq 3
Bias: STRONG preference for shorter sequences


α = 0.5 (Partial Normalization):
────────────────────────────────
Normalized Score = Raw Score / T^0.5 = Raw Score / √T

Seq 1: -1.0 / √2 = -1.0 / 1.41 = -0.71
Seq 2: -1.2 / √3 = -1.2 / 1.73 = -0.69  ← BEST
Seq 3: -1.3 / √4 = -1.3 / 2.00 = -0.65  ← Actually best!

Ranking: Seq 3 > Seq 2 > Seq 1
Bias: MODERATE preference for balanced length


α = 0.7 (Common in Practice):
─────────────────────────────
Normalized Score = Raw Score / T^0.7

Seq 1: -1.0 / 2^0.7 = -1.0 / 1.62 = -0.62
Seq 2: -1.2 / 3^0.7 = -1.2 / 2.16 = -0.56  ← BEST
Seq 3: -1.3 / 4^0.7 = -1.3 / 2.64 = -0.49  ← Actually best!

Ranking: Seq 3 > Seq 2 > Seq 1
Bias: Good balance between quality and length


α = 1.0 (Full Normalization):
─────────────────────────────
Normalized Score = Raw Score / T (average per token)

Seq 1: -1.0 / 2 = -0.50
Seq 2: -1.2 / 3 = -0.40  ← BEST
Seq 3: -1.3 / 4 = -0.325 ← Actually best!

Ranking: Seq 3 > Seq 2 > Seq 1
Bias: NONE - pure average probability per token
```

#### Numerical Comparison: With vs Without Normalization

**Complete Example**:

**Setup**:
```
Task: Machine Translation
Candidates after beam search:
```

**Three complete hypotheses**:

**Hypothesis 1**: "I am" (T=2)
```
Tokens: ["I", "am", "<EOS>"]
Log probabilities:
  log P("I" | <START>, x)   = -0.35
  log P("am" | "I", x)      = -0.40
  log P("<EOS>" | "I am", x)= -0.25

Raw Score: -0.35 + (-0.40) + (-0.25) = -1.00
```

**Hypothesis 2**: "I am happy" (T=3)
```
Tokens: ["I", "am", "happy", "<EOS>"]
Log probabilities:
  log P("I" | <START>, x)      = -0.35
  log P("am" | "I", x)         = -0.40
  log P("happy" | "I am", x)   = -0.35
  log P("<EOS>" | "..happy", x)= -0.20

Raw Score: -0.35 + (-0.40) + (-0.35) + (-0.20) = -1.30
```

**Hypothesis 3**: "I am very happy" (T=4)
```
Tokens: ["I", "am", "very", "happy", "<EOS>"]
Log probabilities:
  log P("I" | <START>, x)        = -0.35
  log P("am" | "I", x)           = -0.40
  log P("very" | "I am", x)      = -0.30
  log P("happy" | "..very", x)   = -0.30
  log P("<EOS>" | "..happy", x)  = -0.15

Raw Score: -0.35 + (-0.40) + (-0.30) + (-0.30) + (-0.15) = -1.50
```

**Comparison Table**:

```
╔═════════════════════════════════════════════════════════════════════════╗
║              COMPARISON: WITH AND WITHOUT NORMALIZATION                 ║
╚═════════════════════════════════════════════════════════════════════════╝

Hypothesis         Length  Raw Score  α=0.0    α=0.6    α=0.7    α=1.0
────────────────────────────────────────────────────────────────────────
"I am"             2       -1.00      -1.00    -0.66    -0.62    -0.50
"I am happy"       3       -1.30      -1.30    -0.70    -0.65    -0.43
"I am very happy"  4       -1.50      -1.50    -0.68    -0.64    -0.38
────────────────────────────────────────────────────────────────────────

RANKINGS:
─────────

Without Normalization (α=0.0):
  1st: "I am"            (-1.00) ← SELECTED
  2nd: "I am happy"      (-1.30)
  3rd: "I am very happy" (-1.50)
  Problem: Unfairly favors shortest sequence!

With α=0.6:
  1st: "I am"            (-0.66) ← SELECTED
  2nd: "I am very happy" (-0.68)
  3rd: "I am happy"      (-0.70)
  Better: Rankings more balanced

With α=0.7 (commonly used):
  1st: "I am"            (-0.62) ← SELECTED
  2nd: "I am very happy" (-0.64)
  3rd: "I am happy"      (-0.65)
  Good balance: Moderate length preference

With Full Normalization (α=1.0):
  1st: "I am very happy" (-0.38) ← SELECTED
  2nd: "I am happy"      (-0.43)
  3rd: "I am"            (-0.50)
  Reversed: Now favors longer sequence with good per-token probability!
```

**Detailed Calculations for α=0.7**:

```
Hypothesis 1: "I am" (T=2)
  Normalized = -1.00 / 2^0.7
             = -1.00 / 1.6245
             = -0.6155 ≈ -0.62

Hypothesis 2: "I am happy" (T=3)
  Normalized = -1.30 / 3^0.7
             = -1.30 / 2.1598
             = -0.6019 ≈ -0.65

Hypothesis 3: "I am very happy" (T=4)
  Normalized = -1.50 / 4^0.7
             = -1.50 / 2.6390
             = -0.5685 ≈ -0.64

With α=0.7:
  Best: "I am" still wins, but margin is smaller
  The longer, fluent sentence is competitive!
```

**Key Insights**:
```
1. Raw scores favor shorter sequences
2. α=0.7 is common: Good balance in practice
3. α=1.0 may over-favor long sequences
4. Choice of α is task-dependent:
   - Machine Translation: α ≈ 0.6-0.7
   - Summarization: α ≈ 0.8-1.0 (want concise)
   - Text Generation: α ≈ 0.6 (want fluency)
```

---

## Complete Step-by-Step Example

### Setup

**Task**: Translate French "Je suis" to English

**Vocabulary**: 
$$V = \{\text{"I"}, \text{"am"}, \text{"a"}, \text{"student"}, \text{"happy"}, \text{"<EOS>"}\}$$
$$|V| = 6$$

**Beam Width**: $B = 3$

**Length Normalization**: $\alpha = 0.7$

**Model Probabilities**: 

We'll use a simplified model that gives us probability distributions. In practice, these come from a trained neural network (e.g., Transformer decoder), but here we define them explicitly for illustration.

**Probability Table 1**: $P(y_1 | \text{<START>}, x)$ (First token probabilities)

```
┌─────────────────────────────────────────┐
│ Token      │ Probability │ Log Prob    │
├────────────┼─────────────┼─────────────┤
│ "I"        │ 0.60        │ -0.511      │
│ "am"       │ 0.05        │ -2.996      │
│ "a"        │ 0.10        │ -2.303      │
│ "student"  │ 0.05        │ -2.996      │
│ "happy"    │ 0.05        │ -2.996      │
│ "<EOS>"    │ 0.15        │ -1.897      │
└─────────────────────────────────────────┘
```

**Probability Table 2**: $P(y_2 | y_1=\text{"I"}, x)$ (After "I")

```
┌─────────────────────────────────────────┐
│ Token      │ Probability │ Log Prob    │
├────────────┼─────────────┼─────────────┤
│ "I"        │ 0.02        │ -3.912      │
│ "am"       │ 0.70        │ -0.357      │
│ "a"        │ 0.05        │ -2.996      │
│ "student"  │ 0.03        │ -3.507      │
│ "happy"    │ 0.05        │ -2.996      │
│ "<EOS>"    │ 0.15        │ -1.897      │
└─────────────────────────────────────────┘
```

**Probability Table 3**: $P(y_2 | y_1=\text{"a"}, x)$ (After "a")

```
┌─────────────────────────────────────────┐
│ Token      │ Probability │ Log Prob    │
├────────────┼─────────────┼─────────────┤
│ "I"        │ 0.01        │ -4.605      │
│ "am"       │ 0.05        │ -2.996      │
│ "a"        │ 0.02        │ -3.912      │
│ "student"  │ 0.80        │ -0.223      │
│ "happy"    │ 0.10        │ -2.303      │
│ "<EOS>"    │ 0.02        │ -3.912      │
└─────────────────────────────────────────┘
```

**Probability Table 4**: $P(y_3 | y_{1:2}=[\text{"I"}, \text{"am"}], x)$ (After "I am")

```
┌─────────────────────────────────────────┐
│ Token      │ Probability │ Log Prob    │
├────────────┼─────────────┼─────────────┤
│ "I"        │ 0.01        │ -4.605      │
│ "am"       │ 0.02        │ -3.912      │
│ "a"        │ 0.15        │ -1.897      │
│ "student"  │ 0.20        │ -1.609      │
│ "happy"    │ 0.35        │ -1.050      │
│ "<EOS>"    │ 0.27        │ -1.309      │
└─────────────────────────────────────────┘
```

**Probability Table 5**: $P(y_3 | y_{1:2}=[\text{"a"}, \text{"student"}], x)$ (After "a student")

```
┌─────────────────────────────────────────┐
│ Token      │ Probability │ Log Prob    │
├────────────┼─────────────┼─────────────┤
│ "I"        │ 0.05        │ -2.996      │
│ "am"       │ 0.10        │ -2.303      │
│ "a"        │ 0.05        │ -2.996      │
│ "student"  │ 0.05        │ -2.996      │
│ "happy"    │ 0.10        │ -2.303      │
│ "<EOS>"    │ 0.65        │ -0.431      │
└─────────────────────────────────────────┘
```

### Step 0: Initialization

**Initial State**:
```
Active Hypotheses: 1
Completed Hypotheses: 0

┌──────────────────────────────────────┐
│ Hypothesis 1                         │
├──────────────────────────────────────┤
│ Tokens:  [<START>]                   │
│ Score:   0.0                         │
│ Length:  0 (not counting <START>)    │
│ Status:  Active ■                    │
└──────────────────────────────────────┘
```

**Visualization**:
```
╔═══════════════════════════════════════╗
║        STEP 0: INITIALIZATION         ║
╚═══════════════════════════════════════╝

           ┌─────────────┐
           │  <START>    │  ■ Active
           │  score: 0.0 │
           └──────┬──────┘
                  │
                  │ Ready to expand
                  ▼
```

### Step 1: First Token

#### Generate Candidates

Expand `<START>` to all $|V| = 6$ possible first tokens using Probability Table 1:

**All Candidates**:
```
┌────────────────────────────────────────────────────────────────┐
│ Rank │ Hypothesis           │ Token Prob │ Log Prob │ Score   │
├──────┼──────────────────────┼────────────┼──────────┼─────────┤
│  1   │ [<START>, "I"]       │ 0.60       │ -0.511   │ -0.511  │
│  2   │ [<START>, "<EOS>"]   │ 0.15       │ -1.897   │ -1.897  │
│  3   │ [<START>, "a"]       │ 0.10       │ -2.303   │ -2.303  │
│  4   │ [<START>, "am"]      │ 0.05       │ -2.996   │ -2.996  │
│  5   │ [<START>, "student"] │ 0.05       │ -2.996   │ -2.996  │
│  6   │ [<START>, "happy"]   │ 0.05       │ -2.996   │ -2.996  │
└────────────────────────────────────────────────────────────────┘
```

#### Calculate Scores

**Detailed Calculations**:

**Candidate 1**: ["I"]
$$\text{score} = 0.0 + \log P(\text{"I"} | \text{<START>}, x)$$
$$= 0.0 + (-0.511) = -0.511$$

**Candidate 2**: ["<EOS>"]
$$\text{score} = 0.0 + \log P(\text{"<EOS>"} | \text{<START>}, x)$$
$$= 0.0 + (-1.897) = -1.897$$

**Candidate 3**: ["a"]
$$\text{score} = 0.0 + \log P(\text{"a"} | \text{<START>}, x)$$
$$= 0.0 + (-2.303) = -2.303$$

**Candidate 4**: ["am"]
$$\text{score} = 0.0 + \log P(\text{"am"} | \text{<START>}, x)$$
$$= 0.0 + (-2.996) = -2.996$$

**Candidate 5**: ["student"]
$$\text{score} = 0.0 + \log P(\text{"student"} | \text{<START>}, x)$$
$$= 0.0 + (-2.996) = -2.996$$

**Candidate 6**: ["happy"]
$$\text{score} = 0.0 + \log P(\text{"happy"} | \text{<START>}, x)$$
$$= 0.0 + (-2.996) = -2.996$$

#### Select Top-3 Beams

With $B = 3$, keep the 3 highest-scoring hypotheses:

```
╔═══════════════════════════════════════════════════════════════╗
║              STEP 1: FIRST TOKEN SELECTION                    ║
╚═══════════════════════════════════════════════════════════════╝

Selected (Top-3):
┌────────────────────────────────────────────────┐
│ ✓ Beam 1: ["I"]       score: -0.511   ■      │
│ ✓ Beam 2: ["<EOS>"]   score: -1.897   ●      │ (complete)
│ ✓ Beam 3: ["a"]       score: -2.303   ■      │
└────────────────────────────────────────────────┘

Pruned:
┌────────────────────────────────────────────────┐
│ ✗ ["am"]       score: -2.996   (rank 4)       │
│ ✗ ["student"]  score: -2.996   (rank 5)       │
│ ✗ ["happy"]    score: -2.996   (rank 6)       │
└────────────────────────────────────────────────┘

Active Beams: 2  (["I"], ["a"])
Completed: 1     (["<EOS>"])
```

**Visualization**:
```
                    <START>(0.0)
                         │
      ┌──────┬───────────┼───────────┬───────┬─────────┐
      │      │           │           │       │         │
    ["I"]  [<EOS>]     ["a"]      ["am"] ["student"]["happy"]
    -0.511  -1.897     -2.303     -2.996  -2.996   -2.996
      ■       ●          ■          ✗       ✗        ✗
    KEEP   COMPLETE    KEEP      PRUNE   PRUNE    PRUNE
```

### Step 2: Second Token

#### Expand Active Beams

We have 2 active beams: ["I"] and ["a"]

Each expands to $|V| = 6$ candidates → $2 \times 6 = 12$ total candidates

#### From Beam 1: ["I"]

Using Probability Table 2: $P(y_2 | y_1=\text{"I"}, x)$

**All expansions**:
```
Previous: ["I"], score: -0.511

┌────────────────────────────────────────────────────────────────────┐
│ New Token  │ P(token│"I",x) │ Log Prob │ New Score              │
├────────────┼───────────────┼──────────┼────────────────────────┤
│ "I"        │ 0.02          │ -3.912   │ -0.511 + (-3.912) = -4.423│
│ "am"       │ 0.70          │ -0.357   │ -0.511 + (-0.357) = -0.868│
│ "a"        │ 0.05          │ -2.996   │ -0.511 + (-2.996) = -3.507│
│ "student"  │ 0.03          │ -3.507   │ -0.511 + (-3.507) = -4.018│
│ "happy"    │ 0.05          │ -2.996   │ -0.511 + (-2.996) = -3.507│
│ "<EOS>"    │ 0.15          │ -1.897   │ -0.511 + (-1.897) = -2.408│
└────────────────────────────────────────────────────────────────────┘

Results:
  ["I", "I"]:       -4.423  (very unlikely)
  ["I", "am"]:      -0.868  ← Excellent! Most natural
  ["I", "a"]:       -3.507
  ["I", "student"]: -4.018
  ["I", "happy"]:   -3.507
  ["I", "<EOS>"]:   -2.408  ● (complete)
```

#### From Beam 2: ["a"]

Using Probability Table 3: $P(y_2 | y_1=\text{"a"}, x)$

**All expansions**:
```
Previous: ["a"], score: -2.303

┌────────────────────────────────────────────────────────────────────┐
│ New Token  │ P(token│"a",x) │ Log Prob │ New Score              │
├────────────┼───────────────┼──────────┼────────────────────────┤
│ "I"        │ 0.01          │ -4.605   │ -2.303 + (-4.605) = -6.908│
│ "am"       │ 0.05          │ -2.996   │ -2.303 + (-2.996) = -5.299│
│ "a"        │ 0.02          │ -3.912   │ -2.303 + (-3.912) = -6.215│
│ "student"  │ 0.80          │ -0.223   │ -2.303 + (-0.223) = -2.526│
│ "happy"    │ 0.10          │ -2.303   │ -2.303 + (-2.303) = -4.606│
│ "<EOS>"    │ 0.02          │ -3.912   │ -2.303 + (-3.912) = -6.215│
└────────────────────────────────────────────────────────────────────┘

Results:
  ["a", "I"]:       -6.908
  ["a", "am"]:      -5.299
  ["a", "a"]:       -6.215
  ["a", "student"]: -2.526  ← Good! "a student" makes sense
  ["a", "happy"]:   -4.606
  ["a", "<EOS>"]:   -6.215  ● (complete)
```

#### Calculate Scores for All Candidates

**All 12 candidates ranked**:
```
╔═══════════════════════════════════════════════════════════════════════╗
║              STEP 2: ALL 12 CANDIDATES RANKED                         ║
╚═══════════════════════════════════════════════════════════════════════╝

┌──────┬─────────────────────┬──────────┬────────────┬──────────┐
│ Rank │ Hypothesis          │ Score    │ Length (T) │ Status   │
├──────┼─────────────────────┼──────────┼────────────┼──────────┤
│  1   │ ["I", "am"]         │ -0.868   │ 2          │ Active ■ │
│  2   │ ["I", "<EOS>"]      │ -2.408   │ 1          │ Done ●   │
│  3   │ ["a", "student"]    │ -2.526   │ 2          │ Active ■ │
│  4   │ ["I", "a"]          │ -3.507   │ 2          │ Pruned ✗ │
│  5   │ ["I", "happy"]      │ -3.507   │ 2          │ Pruned ✗ │
│  6   │ ["I", "student"]    │ -4.018   │ 2          │ Pruned ✗ │
│  7   │ ["I", "I"]          │ -4.423   │ 2          │ Pruned ✗ │
│  8   │ ["a", "happy"]      │ -4.606   │ 2          │ Pruned ✗ │
│  9   │ ["a", "am"]         │ -5.299   │ 2          │ Pruned ✗ │
│ 10   │ ["a", "a"]          │ -6.215   │ 2          │ Pruned ✗ │
│ 11   │ ["a", "<EOS>"]      │ -6.215   │ 1          │ Pruned ✗ │
│ 12   │ ["a", "I"]          │ -6.908   │ 2          │ Pruned ✗ │
└──────┴─────────────────────┴──────────┴────────────┴──────────┘
```

#### Keep Top-3

With $B = 3$:
- **Beam 1**: ["I", "am"] (-0.868) - Active ■
- **Beam 2**: ["I", "<EOS>"] (-2.408) - Complete ●
- **Beam 3**: ["a", "student"] (-2.526) - Active ■

**Note**: ["I", "<EOS>"] is complete but we keep it in our top-3 for final comparison.

```
╔═══════════════════════════════════════════════════════════════╗
║              STEP 2: SELECTION RESULTS                        ║
╚═══════════════════════════════════════════════════════════════╝

Active Beams (2):
┌────────────────────────────────────────────────┐
│ ✓ ["I", "am"]         -0.868   ■             │
│ ✓ ["a", "student"]    -2.526   ■             │
└────────────────────────────────────────────────┘

Completed Sequences (2):
┌────────────────────────────────────────────────┐
│ ● ["<EOS>"]           -1.897  (from step 1)   │
│ ● ["I", "<EOS>"]      -2.408  (from step 2)   │
└────────────────────────────────────────────────┘

Pruned (9 hypotheses)
```

**Visualization**:
```
                        <START>
                            │
        ┌───────────────────┼────────────────┐
        │                   │                │
      ["I"]              [<EOS>]           ["a"]
      -0.511              -1.897●          -2.303
        │                                    │
    ┌───┼──────┐                        ┌────┼─────┐
    │   │      │                        │    │     │
  ["am"]["<EOS>"]["a"]              ["student"]["am"]["happy"]
  -0.868 -2.408● -3.507✗            -2.526   -5.299✗ -4.606✗
    ■      ●                            ■
  KEEP  COMPLETE                      KEEP
```

### Step 3: Third Token

#### Expand Active Beams

We have 2 active beams: ["I", "am"] and ["a", "student"]

Each expands to $|V| = 6$ candidates → $2 \times 6 = 12$ total candidates

#### From Beam 1: ["I", "am"]

Using Probability Table 4: $P(y_3 | y_{1:2}=[\text{"I"}, \text{"am"}], x)$

**All expansions**:
```
Previous: ["I", "am"], score: -0.868

┌────────────────────────────────────────────────────────────────────┐
│ New Token  │ P(token│"I am",x) │ Log Prob │ New Score           │
├────────────┼──────────────────┼──────────┼─────────────────────┤
│ "I"        │ 0.01             │ -4.605   │ -0.868+(-4.605)=-5.473│
│ "am"       │ 0.02             │ -3.912   │ -0.868+(-3.912)=-4.780│
│ "a"        │ 0.15             │ -1.897   │ -0.868+(-1.897)=-2.765│
│ "student"  │ 0.20             │ -1.609   │ -0.868+(-1.609)=-2.477│
│ "happy"    │ 0.35             │ -1.050   │ -0.868+(-1.050)=-1.918│
│ "<EOS>"    │ 0.27             │ -1.309   │ -0.868+(-1.309)=-2.177│
└────────────────────────────────────────────────────────────────────┘

Results:
  ["I", "am", "I"]:       -5.473  
  ["I", "am", "am"]:      -4.780  
  ["I", "am", "a"]:       -2.765  
  ["I", "am", "student"]: -2.477  
  ["I", "am", "happy"]:   -1.918  ← Best extension!
  ["I", "am", "<EOS>"]:   -2.177  ● (complete)
```

#### From Beam 2: ["a", "student"]

Using Probability Table 5: $P(y_3 | y_{1:2}=[\text{"a"}, \text{"student"}], x)$

**All expansions**:
```
Previous: ["a", "student"], score: -2.526

┌────────────────────────────────────────────────────────────────────┐
│ New Token  │ P(token│"a student",x) │ Log Prob │ New Score       │
├────────────┼───────────────────────┼──────────┼─────────────────┤
│ "I"        │ 0.05                  │ -2.996   │ -2.526+(-2.996)=-5.522│
│ "am"       │ 0.10                  │ -2.303   │ -2.526+(-2.303)=-4.829│
│ "a"        │ 0.05                  │ -2.996   │ -2.526+(-2.996)=-5.522│
│ "student"  │ 0.05                  │ -2.996   │ -2.526+(-2.996)=-5.522│
│ "happy"    │ 0.10                  │ -2.303   │ -2.526+(-2.303)=-4.829│
│ "<EOS>"    │ 0.65                  │ -0.431   │ -2.526+(-0.431)=-2.957│
└────────────────────────────────────────────────────────────────────┘

Results:
  ["a", "student", "I"]:       -5.522
  ["a", "student", "am"]:      -4.829
  ["a", "student", "a"]:       -5.522
  ["a", "student", "student"]: -5.522
  ["a", "student", "happy"]:   -4.829
  ["a", "student", "<EOS>"]:   -2.957  ● (complete! Very likely ending)
```

#### Calculate Scores for All Candidates

**All 12 candidates ranked**:
```
╔═══════════════════════════════════════════════════════════════════════╗
║              STEP 3: ALL 12 CANDIDATES RANKED                         ║
╚═══════════════════════════════════════════════════════════════════════╝

┌──────┬──────────────────────────────┬──────────┬────────────┬──────────┐
│ Rank │ Hypothesis                   │ Score    │ Length (T) │ Status   │
├──────┼──────────────────────────────┼──────────┼────────────┼──────────┤
│  1   │ ["I", "am", "happy"]         │ -1.918   │ 3          │ Active ■ │
│  2   │ ["I", "am", "<EOS>"]         │ -2.177   │ 2          │ Done ●   │
│  3   │ ["I", "am", "student"]       │ -2.477   │ 3          │ Active ■ │
│  4   │ ["I", "am", "a"]             │ -2.765   │ 3          │ Active ■ │
│  5   │ ["a", "student", "<EOS>"]    │ -2.957   │ 2          │ Done ●   │
│  6   │ ["I", "am", "am"]            │ -4.780   │ 3          │ Pruned ✗ │
│  7   │ ["a", "student", "am"]       │ -4.829   │ 3          │ Pruned ✗ │
│  8   │ ["a", "student", "happy"]    │ -4.829   │ 3          │ Pruned ✗ │
│  9   │ ["I", "am", "I"]             │ -5.473   │ 3          │ Pruned ✗ │
│ 10   │ ["a", "student", "I"]        │ -5.522   │ 3          │ Pruned ✗ │
│ 11   │ ["a", "student", "a"]        │ -5.522   │ 3          │ Pruned ✗ │
│ 12   │ ["a", "student", "student"]  │ -5.522   │ 3          │ Pruned ✗ │
└──────┴──────────────────────────────┴──────────┴────────────┴──────────┘
```

#### Keep Top-3

```
╔═══════════════════════════════════════════════════════════════╗
║              STEP 3: SELECTION RESULTS                        ║
╚═══════════════════════════════════════════════════════════════╝

Top-3 Candidates:
┌────────────────────────────────────────────────────────────┐
│ 1. ["I", "am", "happy"]      -1.918   ■ Active           │
│ 2. ["I", "am", "<EOS>"]      -2.177   ● Complete         │
│ 3. ["I", "am", "student"]    -2.477   ■ Active           │
└────────────────────────────────────────────────────────────┘

All Completed Sequences (3 total):
┌────────────────────────────────────────────────────────────┐
│ ● ["<EOS>"]                  -1.897   (T=0)               │
│ ● ["I", "<EOS>"]             -2.408   (T=1)               │
│ ● ["I", "am", "<EOS>"]       -2.177   (T=2)               │
│ ● ["a", "student", "<EOS>"]  -2.957   (T=2)               │
└────────────────────────────────────────────────────────────┘
```

**Visualization**:
```
                            <START>
                                │
            ┌───────────────────┼────────────────┐
            │                   │                │
          ["I"]              [<EOS>]●          ["a"]
          -0.511              -1.897           -2.303
            │                                    │
        ┌───┼──────┐                        ┌────┴─────┐
        │   │      │                        │          │
    ["am"]["<EOS>"]                    ["student"]     ...
    -0.868 -2.408●                      -2.526
        │                                   │
    ┌───┼────────┬────────┐            ┌────┴─────┐
    │   │        │        │            │          │
 ["happy"]["<EOS>"]["student"]["a"] ["<EOS>"]    ...
 -1.918■  -2.177● -2.477■ -2.765   -2.957●
  ACTIVE  COMPLETE ACTIVE           COMPLETE
```

**Note**: We could continue expanding active beams (["I", "am", "happy"], etc.), but for this example, let's proceed to final selection since we have several completed sequences.

### Final Selection

We have collected completed sequences. Now apply length normalization and select the best.

#### All Completed Sequences

```
╔═══════════════════════════════════════════════════════════════════════╗
║                  COMPLETED SEQUENCES SUMMARY                          ║
╚═══════════════════════════════════════════════════════════════════════╝

┌────┬──────────────────────────────┬─────────────┬──────────┬──────────┐
│ ID │ Sequence                     │ Raw Score   │ Length   │ Tokens   │
├────┼──────────────────────────────┼─────────────┼──────────┼──────────┤
│ 1  │ ["<EOS>"]                    │ -1.897      │ T = 0    │ 0        │
│ 2  │ ["I", "<EOS>"]               │ -2.408      │ T = 1    │ 1        │
│ 3  │ ["I", "am", "<EOS>"]         │ -2.177      │ T = 2    │ 2        │
│ 4  │ ["a", "student", "<EOS>"]    │ -2.957      │ T = 2    │ 2        │
└────┴──────────────────────────────┴─────────────┴──────────┴──────────┘

Note: Length T counts only content tokens (excluding <START> and <EOS>)
```

#### Apply Length Normalization (α = 0.7)

**Formula**: 
$$\text{score}_{\text{normalized}} = \frac{\text{Raw Score}}{T^{0.7}}$$

**Sequence 1**: ["<EOS>"] (T=0)
```
Special case: Empty sequence, typically excluded
Or treat as T=1: -1.897 / 1^0.7 = -1.897
```

**Sequence 2**: ["I", "<EOS>"] (T=1)
$$\text{score}_{\text{norm}} = \frac{-2.408}{1^{0.7}} = \frac{-2.408}{1.000} = -2.408$$

**Sequence 3**: ["I", "am", "<EOS>"] (T=2)
$$\text{score}_{\text{norm}} = \frac{-2.177}{2^{0.7}} = \frac{-2.177}{1.6245} = -1.340$$

**Sequence 4**: ["a", "student", "<EOS>"] (T=2)
$$\text{score}_{\text{norm}} = \frac{-2.957}{2^{0.7}} = \frac{-2.957}{1.6245} = -1.820$$

**Detailed Calculation for T=2**:
```
2^0.7 calculation:
  2^0.7 = e^(0.7 × ln(2))
        = e^(0.7 × 0.693)
        = e^(0.485)
        = 1.6245

Sequence 3:
  -2.177 / 1.6245 = -1.340

Sequence 4:
  -2.957 / 1.6245 = -1.820
```

#### Final Ranking

```
╔═══════════════════════════════════════════════════════════════════════╗
║                  FINAL RANKING WITH LENGTH NORMALIZATION              ║
╚═══════════════════════════════════════════════════════════════════════╝

┌──────┬──────────────────────────────┬─────────────┬──────┬──────────────┐
│ Rank │ Sequence                     │ Raw Score   │ T    │ Norm Score   │
├──────┼──────────────────────────────┼─────────────┼──────┼──────────────┤
│  1   │ ["I", "am"]                  │ -2.177      │ 2    │ -1.340 ★★★  │
│  2   │ ["a", "student"]             │ -2.957      │ 2    │ -1.820 ★★   │
│  3   │ ["<EOS>"]                    │ -1.897      │ 0-1  │ -1.897 ★    │
│  4   │ ["I"]                        │ -2.408      │ 1    │ -2.408      │
└──────┴──────────────────────────────┴─────────────┴──────┴──────────────┘

═══════════════════════════════════════════════════════════════════════
FINAL SELECTED OUTPUT
═══════════════════════════════════════════════════════════════════════

★★★ BEST SEQUENCE: ["I", "am"]

Translation Result:
  Input:  "Je suis"
  Output: "I am"
  
  Normalized Score: -1.340
  Raw Score: -2.177
  Length: 2 tokens

Quality: Grammatically correct and semantically accurate!
```

#### Comparison: With vs Without Normalization

```
╔═══════════════════════════════════════════════════════════════════════╗
║          IMPACT OF LENGTH NORMALIZATION (α = 0.7)                     ║
╚═══════════════════════════════════════════════════════════════════════╝

WITHOUT Normalization (α = 0.0):
─────────────────────────────────
Ranking by Raw Score:
  1. ["<EOS>"]          -1.897  ← Would select empty output!
  2. ["I", "am"]        -2.177
  3. ["I"]              -2.408
  4. ["a", "student"]   -2.957

Problem: Incorrectly favors shorter/empty sequences

WITH Normalization (α = 0.7):
──────────────────────────────
Ranking by Normalized Score:
  1. ["I", "am"]        -1.340  ← Correct translation!
  2. ["a", "student"]   -1.820
  3. ["<EOS>"]          -1.897
  4. ["I"]              -2.408

Solution: Properly balances length and quality
```

#### Summary Statistics

```
╔═══════════════════════════════════════════════════════════════════════╗
║                     BEAM SEARCH STATISTICS                            ║
╚═══════════════════════════════════════════════════════════════════════╝

Configuration:
├─ Beam Width (B): 3
├─ Vocabulary Size (|V|): 6
├─ Length Normalization (α): 0.7
└─ Total Steps: 3

Candidates Evaluated:
├─ Step 1: 6 candidates (1 hypothesis × 6 tokens)
├─ Step 2: 12 candidates (2 hypotheses × 6 tokens)
├─ Step 3: 12 candidates (2 hypotheses × 6 tokens)
└─ Total: 30 candidates evaluated

Sequences Kept (B=3 per step):
├─ Step 1: 3 kept, 3 pruned
├─ Step 2: 3 kept, 9 pruned
├─ Step 3: 3 kept, 9 pruned
└─ Total: 21 sequences pruned

Efficiency:
├─ Candidates evaluated: 30
├─ Greedy would evaluate: 3 × 6 = 18
├─ Exhaustive would evaluate: 6^3 = 216
└─ Beam is 1.67× more than greedy, but 7.2× less than exhaustive!

Final Result:
├─ Selected: "I am"
├─ Translation Quality: Correct!
├─ Score (normalized): -1.340
└─ Tokens generated: 2
```

---

## Summary

**Beam Search** is a heuristic search algorithm that balances exploration and efficiency in sequence generation:

1. **Maintains B candidate sequences** (beams) at each step
2. **Expands each beam** by considering all possible next tokens
3. **Keeps only top-B** highest-scoring candidates (prunes the rest)
4. **Uses log probabilities** for numerical stability
5. **Applies length normalization** to avoid bias toward shorter sequences

**Key Formula**:
$$\text{score}_{\text{normalized}}(y) = \frac{1}{T^{\alpha}} \sum_{t=1}^{T} \log P(y_t | y_{<t}, x)$$

**Complexity**: $O(B \times T \times |V|)$ - linear in sequence length!

**Applications**: Machine translation, text generation, image captioning, speech recognition, and more.

**Advantages over alternatives**:
- **Better than greedy**: Explores multiple paths, avoids local optima
- **More efficient than exhaustive**: Evaluates only $B \times T \times |V|$ candidates vs $|V|^T$
- **Configurable trade-off**: Adjust beam width $B$ for quality vs speed

**Hyperparameters**:
- **Beam width ($B$)**: Typically 5-10 for translation, 1-5 for text generation
- **Length normalization ($\alpha$)**: Typically 0.6-0.7, task-dependent
