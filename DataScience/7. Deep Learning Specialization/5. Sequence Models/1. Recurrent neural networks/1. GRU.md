# Gated Recurrent Unit (GRU)

## Table of Contents
1. [Plain English Explanation](#plain-english-explanation)
2. [Mathematical Foundation](#mathematical-foundation)
3. [Forward Propagation](#forward-propagation)
4. [Backward Propagation Through Time (BPTT) for GRU](#backward-propagation-through-time-bptt-for-gru)
5. [GRU vs Vanilla RNN Comparison](#gru-vs-vanilla-rnn-comparison)
6. [Key Insights and Summary](#key-insights-and-summary)

---

## Plain English Explanation

### What is a GRU?

**Gated Recurrent Unit (GRU)** is an advanced type of recurrent neural network architecture designed to solve the fundamental problems of vanilla RNNs, particularly the **vanishing gradient problem**. Introduced by Cho et al. in 2014, GRUs use a gating mechanism to control the flow of information, making it easier to capture long-term dependencies in sequential data.

Think of a GRU as a smarter version of an RNN that can decide:
- **What to remember** from the past
- **What to forget** from the current memory
- **How much new information to add** to the memory

### Why Were GRUs Developed?

#### Problems with Vanilla RNNs:

1. **Vanishing Gradient Problem**: 
   - Gradients become extremely small when backpropagating through many time steps
   - Makes it nearly impossible to learn long-term dependencies
   - Example: In "The clouds are in the sky", by the time we process "sky", the gradient from "clouds" has vanished

2. **Difficulty Capturing Long-Term Dependencies**:
   - Information from early time steps gets diluted or lost
   - Network struggles to remember important context from far in the past

3. **Unstable Training**:
   - Exploding gradients can make training unstable
   - Requires careful initialization and learning rate tuning

#### GRU Solutions:

```
┌────────────────────────────────────────────────────────────┐
│ GRU Innovations to Solve RNN Problems                      │
├────────────────────────────────────────────────────────────┤
│                                                            │
│ 1. GATING MECHANISM                                        │
│    → Controls information flow with learned gates          │
│    → Decides what to keep and what to discard              │
│                                                            │
│ 2. UPDATE GATE (z_t)                                       │
│    → Controls how much of past memory to keep              │
│    → "How much should I remember from before?"             │
│                                                            │
│ 3. RESET GATE (r_t)                                        │
│    → Controls how much past information to use             │
│    → "Should I forget the past when computing new info?"   │
│                                                            │
│ 4. DIRECT PATH FOR GRADIENT FLOW                           │
│    → Gradients can flow directly through update gate       │
│    → Mitigates vanishing gradient problem                  │
│                                                            │
│ 5. SIMPLER THAN LSTM                                       │
│    → Fewer parameters (2 gates vs 3 gates in LSTM)        │
│    → Faster training and inference                         │
│    → Often similar performance to LSTM                     │
└────────────────────────────────────────────────────────────┘
```

### Key Innovations

**1. Gated Information Flow**: Unlike vanilla RNNs that always mix old and new information in the same way, GRUs use gates to adaptively control this mixing.

**2. Learned Forgetting**: The network learns when to forget irrelevant past information.

**3. Selective Memory Update**: Can preserve important information over many time steps while still accepting new relevant information.

### Basic Architecture

The GRU has **three main components**:
1. **Reset Gate** ($r_t$): Determines how much of the previous hidden state to "forget"
2. **Update Gate** ($z_t$): Decides how much to update the hidden state
3. **Candidate Hidden State** ($\tilde{h}_t$): New candidate information to potentially add

#### Folded (Compact) Representation:

```
         ┌─────────────────┐
     x_t │                 │ y_t
    ────►│   GRU Cell      │────►
         │  ┌─────────┐    │
         │  │ Gates:  │    │
         │  │  - z_t  │    │
         │  │  - r_t  │    │
         │  └─────────┘    │
         └────┬────────────┘
              │
              │ h_t (hidden state)
              │
              └──────┐
                     │
                     ▼
```

#### Unfolded (Over Time) Representation:

```
Time:    t=0           t=1           t=2           t=3
         ┌───┐        ┌───┐        ┌───┐        ┌───┐
   h_0   │   │  h_1   │   │  h_2   │   │  h_3   │   │
   ─────►│GRU├───────►│GRU├───────►│GRU├───────►│GRU│
         │   │        │   │        │   │        │   │
         └─┬─┘        └─┬─┘        └─┬─┘        └─┬─┘
           ▲            ▲            ▲            ▲
           │            │            │            │
          x_0          x_1          x_2          x_3
           │            │            │            │
           ▼            ▼            ▼            ▼
          y_0          y_1          y_2          y_3
```

### The Three Gates Explained

#### 1. Reset Gate ($r_t$)

**Purpose**: Decides how much of the previous hidden state should be used when computing the new candidate hidden state.

**Question it answers**: "How relevant is the past information for computing new memory?"

```
╔════════════════════════════════════════════════════════════╗
║                    RESET GATE (r_t)                        ║
╠════════════════════════════════════════════════════════════╣
║                                                            ║
║  Input: Current input (x_t) + Previous hidden state (h_t-1)║
║  Output: Reset gate value between 0 and 1                  ║
║                                                            ║
║  ┌──────────┐                                              ║
║  │  x_t     │                                              ║
║  │  h_{t-1} │                                              ║
║  └────┬─────┘                                              ║
║       │                                                    ║
║       │ Linear transformation                              ║
║       │ (W_r · [h_{t-1}, x_t])                            ║
║       ▼                                                    ║
║  ┌─────────┐                                               ║
║  │ Sigmoid │  ← Outputs value in [0, 1]                   ║
║  └────┬────┘                                               ║
║       │                                                    ║
║       ▼                                                    ║
║      r_t                                                   ║
║                                                            ║
║  r_t ≈ 0: "Forget the past! Start fresh!"                 ║
║  r_t ≈ 1: "Remember everything from the past!"            ║
║  r_t ≈ 0.5: "Consider some of the past"                   ║
║                                                            ║
║  Example Use Case:                                         ║
║  In "The cat, which was very fluffy, sat on the mat"      ║
║  When processing "sat", the reset gate might:             ║
║  - Keep high value for "cat" (subject is relevant)        ║
║  - Reduce value for "fluffy" (less relevant to action)    ║
╚════════════════════════════════════════════════════════════╝
```

**Visualization of Reset Gate Values**:

```
r_t = 0.0          r_t = 0.5          r_t = 1.0
───────────        ───────────        ───────────

h_{t-1} ━━━━►      h_{t-1} ━━━━►      h_{t-1} ━━━━►
  [5.0]              [5.0]              [5.0]
    │                  │                  │
    │ ×0.0             │ ×0.5             │ ×1.0
    ▼                  ▼                  ▼
  [0.0]              [2.5]              [5.0]

"Ignore past"    "Partial past"    "Keep all past"
```

#### 2. Update Gate ($z_t$)

**Purpose**: Controls how much of the previous hidden state to keep and how much new candidate information to add.

**Question it answers**: "How much should I update my memory with new information?"

```
╔════════════════════════════════════════════════════════════╗
║                   UPDATE GATE (z_t)                        ║
╠════════════════════════════════════════════════════════════╣
║                                                            ║
║  Input: Current input (x_t) + Previous hidden state (h_t-1)║
║  Output: Update gate value between 0 and 1                 ║
║                                                            ║
║  ┌──────────┐                                              ║
║  │  x_t     │                                              ║
║  │  h_{t-1} │                                              ║
║  └────┬─────┘                                              ║
║       │                                                    ║
║       │ Linear transformation                              ║
║       │ (W_z · [h_{t-1}, x_t])                            ║
║       ▼                                                    ║
║  ┌─────────┐                                               ║
║  │ Sigmoid │  ← Outputs value in [0, 1]                   ║
║  └────┬────┘                                               ║
║       │                                                    ║
║       ▼                                                    ║
║      z_t                                                   ║
║                                                            ║
║  z_t ≈ 0: "Update a lot! Use new information!"            ║
║  z_t ≈ 1: "Keep old memory! Ignore new information!"      ║
║  z_t ≈ 0.5: "Balance old and new equally"                 ║
║                                                            ║
║  Example Use Case:                                         ║
║  In "I grew up in France... I speak fluent [MASK]"        ║
║  When processing words after "France":                     ║
║  - z_t stays HIGH to remember "France" across many steps  ║
║  - Only updates when reaching the answer "French"         ║
╚════════════════════════════════════════════════════════════╝
```

**Visualization of Update Gate Effect**:

```
z_t = 0.0 (Update heavily)
──────────────────────────

h_{t-1}     (1-z_t) = 1.0          h_t
[3.0]  ────────×───────────┐    [7.0]
                            │
                            ├──► SUM ──►
                            │
h̃_t        z_t = 0.0        │
[7.0]  ────────×────────────┘

Result: New memory h_t = 7.0 (mostly new candidate)


z_t = 1.0 (Keep old memory)
───────────────────────────

h_{t-1}     (1-z_t) = 0.0          h_t
[3.0]  ────────×───────────┐    [3.0]
                            │
                            ├──► SUM ──►
                            │
h̃_t        z_t = 1.0        │
[7.0]  ────────×────────────┘

Result: New memory h_t = 3.0 (kept old memory entirely)


z_t = 0.5 (Balanced update)
───────────────────────────

h_{t-1}     (1-z_t) = 0.5          h_t
[3.0]  ────────×───────────┐    [5.0]
                  [1.5]     │
                            ├──► SUM ──►
                  [3.5]     │
h̃_t        z_t = 0.5        │
[7.0]  ────────×────────────┘

Result: New memory h_t = 1.5 + 3.5 = 5.0 (balanced)
```

#### 3. Candidate Hidden State ($\tilde{h}_t$)

**Purpose**: Proposes new information that could be added to the memory.

**Question it answers**: "What is the new information I want to potentially store?"

```
╔════════════════════════════════════════════════════════════╗
║              CANDIDATE HIDDEN STATE (h̃_t)                  ║
╠════════════════════════════════════════════════════════════╣
║                                                            ║
║  Input: Current input (x_t) + RESET previous state        ║
║  Output: Candidate values between -1 and 1                 ║
║                                                            ║
║  ┌──────────┐      ┌──────────┐                           ║
║  │  x_t     │      │ h_{t-1}  │                           ║
║  └────┬─────┘      └────┬─────┘                           ║
║       │                 │                                  ║
║       │                 │ ⊙ r_t  ← Reset gate applied!    ║
║       │                 ▼                                  ║
║       │          [ r_t ⊙ h_{t-1} ]                        ║
║       │                 │                                  ║
║       └────────┬────────┘                                  ║
║                │                                           ║
║                │ Linear transformation                     ║
║                │ (W_h · [r_t ⊙ h_{t-1}, x_t])             ║
║                ▼                                           ║
║           ┌────────┐                                       ║
║           │  tanh  │  ← Outputs value in [-1, 1]          ║
║           └───┬────┘                                       ║
║               │                                            ║
║               ▼                                            ║
║              h̃_t                                           ║
║                                                            ║
║  Key Difference from Vanilla RNN:                          ║
║  Uses RESET past: r_t ⊙ h_{t-1}, not just h_{t-1}        ║
║                                                            ║
║  This allows the network to "forget" irrelevant past      ║
║  information when computing new candidate!                 ║
╚════════════════════════════════════════════════════════════╝
```

### Step-by-Step: How GRU Works

Let's walk through a complete time step to see how all three components work together.

#### Complete GRU Cell Operation

```
═══════════════════════════════════════════════════════════════════════
                    GRU CELL: COMPLETE INFORMATION FLOW
═══════════════════════════════════════════════════════════════════════

INPUTS:
  x_t = Current input at time t
  h_{t-1} = Previous hidden state

OUTPUTS:
  h_t = New hidden state
  y_t = Output (if needed)


┌──────────────────────────────────────────────────────────────────┐
│  STEP 1: COMPUTE RESET GATE                                      │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│     x_t          h_{t-1}                                         │
│      │              │                                            │
│      └──────┬───────┘                                            │
│             │                                                    │
│             │ Concatenate: [h_{t-1}, x_t]                       │
│             ▼                                                    │
│      ┌─────────────┐                                             │
│      │   W_r · []  │  ← Apply weight matrix W_r                 │
│      │   + b_r     │  ← Add bias                                │
│      └──────┬──────┘                                             │
│             │                                                    │
│             ▼                                                    │
│      ┌───────────┐                                               │
│      │  σ(...)   │  ← Sigmoid activation                        │
│      └─────┬─────┘                                               │
│            │                                                     │
│            ▼                                                     │
│           r_t ∈ [0, 1]  ← Reset gate values                     │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘


┌──────────────────────────────────────────────────────────────────┐
│  STEP 2: COMPUTE UPDATE GATE                                     │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│     x_t          h_{t-1}                                         │
│      │              │                                            │
│      └──────┬───────┘                                            │
│             │                                                    │
│             │ Concatenate: [h_{t-1}, x_t]                       │
│             ▼                                                    │
│      ┌─────────────┐                                             │
│      │   W_z · []  │  ← Apply weight matrix W_z                 │
│      │   + b_z     │  ← Add bias                                │
│      └──────┬──────┘                                             │
│             │                                                    │
│             ▼                                                    │
│      ┌───────────┐                                               │
│      │  σ(...)   │  ← Sigmoid activation                        │
│      └─────┬─────┘                                               │
│            │                                                     │
│            ▼                                                     │
│           z_t ∈ [0, 1]  ← Update gate values                    │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘


┌──────────────────────────────────────────────────────────────────┐
│  STEP 3: COMPUTE CANDIDATE HIDDEN STATE                          │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│     x_t          h_{t-1}        r_t                              │
│      │              │            │                               │
│      │              └─────┬──────┘                               │
│      │                    │                                      │
│      │                    │ Element-wise multiply ⊙              │
│      │                    ▼                                      │
│      │             r_t ⊙ h_{t-1}  ← "Reset" past memory         │
│      │                    │                                      │
│      └──────┬─────────────┘                                      │
│             │                                                    │
│             │ Concatenate: [r_t ⊙ h_{t-1}, x_t]                │
│             ▼                                                    │
│      ┌─────────────┐                                             │
│      │   W_h · []  │  ← Apply weight matrix W_h                 │
│      │   + b_h     │  ← Add bias                                │
│      └──────┬──────┘                                             │
│             │                                                    │
│             ▼                                                    │
│      ┌───────────┐                                               │
│      │ tanh(...) │  ← Tanh activation                           │
│      └─────┬─────┘                                               │
│            │                                                     │
│            ▼                                                     │
│           h̃_t ∈ [-1, 1]  ← Candidate hidden state              │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘


┌──────────────────────────────────────────────────────────────────┐
│  STEP 4: UPDATE HIDDEN STATE (THE KEY STEP!)                     │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│     h_{t-1}          z_t              h̃_t                        │
│        │             │                 │                         │
│        │             │                 │                         │
│        │    ┌────────┴────────┐        │                         │
│        │    │                 │        │                         │
│        │    │  z_t ⊙ h_{t-1}  │        │  (1-z_t) ⊙ h̃_t         │
│        │    │   (keep old)    │        │  (add new)             │
│        │    │                 │        │                         │
│        └────┤                 ├────────┘                         │
│             │                 │                                  │
│             └────────┬────────┘                                  │
│                      │                                           │
│                      │ Element-wise addition                     │
│                      ▼                                           │
│             h_t = z_t ⊙ h_{t-1} + (1-z_t) ⊙ h̃_t                │
│                      │                                           │
│                      │                                           │
│                      ▼                                           │
│                 New hidden state h_t                             │
│                                                                  │
│  Interpretation:                                                 │
│  - When z_t ≈ 1: h_t ≈ h_{t-1}  (keep old memory)              │
│  - When z_t ≈ 0: h_t ≈ h̃_t      (use new candidate)            │
│  - When z_t ≈ 0.5: h_t is a blend of both                      │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘


┌──────────────────────────────────────────────────────────────────┐
│  STEP 5: COMPUTE OUTPUT (OPTIONAL)                               │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│           h_t                                                    │
│            │                                                     │
│            │ Apply output weights                                │
│            ▼                                                     │
│      ┌─────────────┐                                             │
│      │  W_y · h_t  │                                             │
│      │  + b_y      │                                             │
│      └──────┬──────┘                                             │
│             │                                                    │
│             │ Apply activation (e.g., softmax)                   │
│             ▼                                                    │
│            y_t  ← Output predictions                             │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘
```

#### Numerical Example Walk-Through

Let's trace through a concrete example with actual numbers:

**Setup**:
```
Input: x_t = [1.0, 0.5]  (2D input)
Previous hidden state: h_{t-1} = [0.8, -0.3, 0.5]  (3D hidden)

Weight matrices (simplified):
