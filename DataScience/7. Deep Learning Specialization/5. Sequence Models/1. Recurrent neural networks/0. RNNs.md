# Recurrent Neural Networks (RNNs)

## Table of Contents
1. [Plain English Explanation](#plain-english-explanation)
2. [Mathematical Foundation](#mathematical-foundation)
3. [Forward Propagation](#forward-propagation)
4. [Backward Propagation Through Time (BPTT)](#backward-propagation-through-time-bptt)

---

## Plain English Explanation

### What are RNNs?

**Recurrent Neural Networks (RNNs)** are a class of neural networks designed to work with **sequential data** - data where order matters. Unlike traditional feedforward neural networks that treat each input independently, RNNs have **memory** that allows them to remember information from previous inputs in the sequence.

Think of reading a sentence: to understand the current word, you need to remember the words that came before it. RNNs work the same way!

### Why Use RNNs?

RNNs are perfect for tasks involving sequences:
- **Natural Language Processing**: Text generation, translation, sentiment analysis
- **Time Series Prediction**: Stock prices, weather forecasting
- **Speech Recognition**: Converting audio to text
- **Video Analysis**: Understanding sequences of frames
- **Music Generation**: Creating melodies

### Basic Architecture

The key innovation of RNNs is the **recurrent connection** - the network's output loops back as input for the next time step.

#### Folded (Compact) Representation:
```
        ┌─────────┐
    x_t │         │ y_t
   ────►│  RNN    │────►
        │  Cell   │
        └────┬────┘
             │
             │ h_t (hidden state)
             │
             └──────┐
                    │
                    ▼
```

#### Unfolded (Over Time) Representation:
```
Time:    t=0         t=1         t=2         t=3
         ┌───┐      ┌───┐      ┌───┐      ┌───┐
   h_0   │   │ h_1  │   │ h_2  │   │ h_3  │   │
   ─────►│RNN├─────►│RNN├─────►│RNN├─────►│RNN│
         │   │      │   │      │   │      │   │
         └─┬─┘      └─┬─┘      └─┬─┘      └─┬─┘
           ▲          ▲          ▲          ▲
           │          │          │          │
          x_0        x_1        x_2        x_3
           │          │          │          │
           ▼          ▼          ▼          ▼
          y_0        y_1        y_2        y_3
```

### How Information Flows Through RNNs

**Key Concept: Hidden State** - The hidden state `h_t` acts as the network's "memory", carrying information from previous time steps.

```
┌─────────────────────────────────────────────────────────┐
│  At each time step t:                                   │
│                                                          │
│  1. Receive new input x_t                               │
│  2. Receive previous hidden state h_(t-1)               │
│  3. Combine them to create new hidden state h_t         │
│  4. Generate output y_t from h_t                        │
│  5. Pass h_t to next time step                          │
└─────────────────────────────────────────────────────────┘
```

#### Detailed Flow Visualization:
```
Input Sequence:  [x_0]  [x_1]  [x_2]  [x_3]
                   ↓      ↓      ↓      ↓
                 ┌───────────────────────┐
                 │  Processing at t=1    │
                 │                       │
                 │  h_0 ──┐              │
                 │        │              │
                 │        ├──► Combine   │
                 │        │    (W_hh)    │
                 │  x_1 ──┘              │
                 │         │              │
                 │         ▼              │
                 │    Activation (tanh)  │
                 │         │              │
                 │         ▼              │
                 │    h_1 (new memory)   │
                 │         │              │
                 │         ├──► y_1       │
                 │         │   (output)   │
                 │         │              │
                 │         └──► h_2       │
                 │          (to next step)│
                 └───────────────────────┘
```

### The Power of Recurrence

The recurrent connection allows information to persist:

```
Example: Understanding "The clouds are in the sky"

t=0: Input="The"    → h_0 stores: "article encountered"
t=1: Input="clouds" → h_1 stores: "article + noun (subject)"
t=2: Input="are"    → h_2 stores: "subject + verb"
t=3: Input="in"     → h_3 stores: "subject + verb + preposition"
t=4: Input="the"    → h_4 stores: "subject + verb + prep + article"
t=5: Input="sky"    → h_5 stores: complete sentence meaning

Each hidden state carries forward cumulative understanding!
```

### Types of RNN Architectures

```
1. One-to-One (Standard Neural Network)
   x → [RNN] → y

2. One-to-Many (e.g., Image Captioning)
   x → [RNN] → [RNN] → [RNN] → y, y, y

3. Many-to-One (e.g., Sentiment Analysis)
   x, x, x → [RNN] → [RNN] → [RNN] → y

4. Many-to-Many (e.g., Machine Translation)
   x, x, x → [RNN] → [RNN] → [RNN] → y, y, y

5. Many-to-Many Synced (e.g., Video Labeling)
   x → [RNN] → y
   x → [RNN] → y
   x → [RNN] → y
```

---

## Mathematical Foundation

### Core RNN Equations

The basic RNN consists of two main equations that compute the hidden state and output at each time step.

#### 1. Hidden State Update

**Plain English**: At each time step, the new hidden state is computed by:
1. Taking the current input `x_t` and multiplying it by input weights
2. Taking the previous hidden state `h_(t-1)` and multiplying it by recurrent weights
3. Adding them together with a bias
4. Applying an activation function (usually tanh)

**Formula**:
$$h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$

**Complete Legend**:
- $h_t$ = Hidden state at time step $t$ (vector of size $n_h$)
- $h_{t-1}$ = Previous hidden state at time step $t-1$ (vector of size $n_h$)
- $x_t$ = Input at time step $t$ (vector of size $n_x$)
- $W_{hh}$ = Weight matrix for hidden-to-hidden connections (size $n_h \times n_h$)
- $W_{xh}$ = Weight matrix for input-to-hidden connections (size $n_h \times n_x$)
- $b_h$ = Bias vector for hidden state (size $n_h$)
- $\tanh$ = Hyperbolic tangent activation function (outputs between -1 and 1)

**Alternative Notation**:
Sometimes written as:
$$h_t = \tanh(W_h [h_{t-1}, x_t] + b_h)$$

Where $W_h$ is a combined weight matrix and $[h_{t-1}, x_t]$ is the concatenation of the previous hidden state and current input.

#### 2. Output Computation

**Plain English**: The output at each time step is computed by:
1. Taking the current hidden state
2. Multiplying it by output weights
3. Adding a bias
4. Optionally applying an activation function (softmax for classification, etc.)

**Formula**:
$$y_t = \text{softmax}(W_{hy} h_t + b_y)$$

Or for regression tasks:
$$y_t = W_{hy} h_t + b_y$$

**Complete Legend**:
- $y_t$ = Output at time step $t$ (vector of size $n_y$)
- $W_{hy}$ = Weight matrix from hidden state to output (size $n_y \times n_h$)
- $b_y$ = Bias vector for output (size $n_y$)
- $\text{softmax}$ = Softmax activation function for classification

### Step-by-Step Visualization with Example Values

Let's work through a concrete example with small dimensions:

**Setup**:
- Input dimension: $n_x = 2$
- Hidden dimension: $n_h = 3$
- Output dimension: $n_y = 2$
- Sequence length: $T = 3$

**Initialize weights and inputs**:

```
Input sequence:
x_0 = [1.0]    x_1 = [0.5]    x_2 = [2.0]
      [0.5]          [1.5]          [0.3]

Initial hidden state (usually zeros):
h_0 = [0]
      [0]
      [0]

Weight matrices:
W_xh = [0.5  0.2 ]     (3×2 matrix)
       [0.3  0.1 ]
       [0.4  0.6 ]

W_hh = [0.1  0.2  0.3]     (3×3 matrix)
       [0.4  0.1  0.2]
       [0.2  0.3  0.1]

b_h = [0.1]
      [0.2]
      [0.1]

W_hy = [0.5  0.3  0.2]     (2×3 matrix)
       [0.4  0.2  0.1]

b_y = [0.1]
      [0.2]
```

#### Time Step t=0

**Step 1: Compute W_xh × x_0**
```
[0.5  0.2] × [1.0] = [0.5×1.0 + 0.2×0.5] = [0.60]
[0.3  0.1]   [0.5]   [0.3×1.0 + 0.1×0.5]   [0.35]
[0.4  0.6]           [0.4×1.0 + 0.6×0.5]   [0.70]
```

**Step 2: Compute W_hh × h_{-1}** (h_{-1} = zeros initially)
```
[0.1  0.2  0.3] × [0] = [0]
[0.4  0.1  0.2]   [0]   [0]
[0.2  0.3  0.1]   [0]   [0]
```

**Step 3: Add bias and apply tanh**
```
Before activation: [0.60] + [0] + [0.1] = [0.70]
                   [0.35]   [0]   [0.2]   [0.55]
                   [0.70]   [0]   [0.1]   [0.80]

h_0 = tanh([0.70]) = [0.604]
           [0.55]    [0.500]
           [0.80]    [0.664]
```

**Step 4: Compute output y_0**
```
W_hy × h_0 + b_y = [0.5  0.3  0.2] × [0.604] + [0.1]
                   [0.4  0.2  0.1]   [0.500]   [0.2]
                                      [0.664]

= [0.5×0.604 + 0.3×0.500 + 0.2×0.664] + [0.1]
  [0.4×0.604 + 0.2×0.500 + 0.1×0.664]   [0.2]

= [0.302 + 0.150 + 0.133] + [0.1] = [0.685]
  [0.242 + 0.100 + 0.066]   [0.2]   [0.608]

For classification, apply softmax:
y_0 = softmax([0.685]) = [0.519]
              [0.608]    [0.481]
```

#### Time Step t=1

**Step 1: Compute W_xh × x_1**
```
[0.5  0.2] × [0.5] = [0.55]
[0.3  0.1]   [1.5]   [0.30]
[0.4  0.6]           [0.95]
```

**Step 2: Compute W_hh × h_0** (now h_0 has values!)
```
[0.1  0.2  0.3] × [0.604] = [0.1×0.604 + 0.2×0.500 + 0.3×0.664]
[0.4  0.1  0.2]   [0.500]   [0.4×0.604 + 0.1×0.500 + 0.2×0.664]
[0.2  0.3  0.1]   [0.664]   [0.2×0.604 + 0.3×0.500 + 0.1×0.664]

= [0.060 + 0.100 + 0.199] = [0.359]
  [0.242 + 0.050 + 0.133]   [0.425]
  [0.121 + 0.150 + 0.066]   [0.337]
```

**Step 3: Add everything and apply tanh**
```
Before activation: [0.55] + [0.359] + [0.1] = [1.009]
                   [0.30]   [0.425]   [0.2]   [0.925]
                   [0.95]   [0.337]   [0.1]   [1.387]

h_1 = tanh([1.009]) = [0.763]
           [0.925]    [0.728]
           [1.387]    [0.881]
```

**Step 4: Compute output y_1**
```
y_1 (before softmax) = W_hy × h_1 + b_y
= [0.5×0.763 + 0.3×0.728 + 0.2×0.881] + [0.1]
  [0.4×0.763 + 0.2×0.728 + 0.1×0.881]   [0.2]

= [0.382 + 0.218 + 0.176] + [0.1] = [0.876]
  [0.305 + 0.146 + 0.088]   [0.2]   [0.739]

y_1 = softmax([0.876]) = [0.534]
              [0.739]    [0.466]
```

### Visualization of Weight Matrix Dimensions

```
Time step t:

Input x_t                 Hidden State h_{t-1}
(n_x × 1)                 (n_h × 1)
    │                         │
    │ W_xh                    │ W_hh
    │ (n_h × n_x)             │ (n_h × n_h)
    ▼                         ▼
    └────────► + ◄────────────┘
               │
               │ + b_h (n_h × 1)
               ▼
            tanh()
               │
               ▼
           h_t (n_h × 1)
               │
               │ W_hy (n_y × n_h)
               ▼
               + b_y (n_y × 1)
               │
               ▼
           y_t (n_y × 1)
```

### Activation Functions

#### Tanh (Hyperbolic Tangent)
**Formula**: 
$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

**Properties**:
- Output range: $[-1, 1]$
- Zero-centered (helps with gradient flow)
- S-shaped curve

**Why use tanh for hidden states?**
- Zero-centered outputs prevent bias shift
- Stronger gradients than sigmoid in the middle range
- Natural choice for representing "hidden memory" that can be positive or negative

#### Softmax (for classification output)
**Formula**:
$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n_y} e^{z_j}}$$

**Properties**:
- Converts scores to probability distribution
- All outputs sum to 1
- Each output is between 0 and 1

---

## Forward Propagation

### Overview

Forward propagation in RNNs processes the input sequence step by step, maintaining a hidden state that carries information across time steps.

```
Forward Pass Flow:
═══════════════════

Initialize: h_{-1} = 0 (or learned initial state)

For t = 0 to T-1:
    1. Receive x_t
    2. Compute h_t = tanh(W_hh·h_{t-1} + W_xh·x_t + b_h)
    3. Compute y_t = softmax(W_hy·h_t + b_y)
    4. Compute loss L_t for this time step
    
Total Loss: L = Σ L_t (sum over all time steps)
```

### Detailed Mathematical Formulation

#### Step-by-Step Equations

**For each time step t = 0, 1, 2, ..., T-1:**

1. **Hidden state pre-activation**:
   $$z_t = W_{hh} h_{t-1} + W_{xh} x_t + b_h$$

2. **Hidden state activation**:
   $$h_t = \tanh(z_t)$$

3. **Output pre-activation**:
   $$o_t = W_{hy} h_t + b_y$$

4. **Output activation** (for classification):
   $$y_t = \text{softmax}(o_t)$$

5. **Loss at time step** (cross-entropy for classification):
   $$L_t = -\sum_{i=1}^{n_y} \hat{y}_{t,i} \log(y_{t,i})$$

6. **Total loss**:
   $$L = \frac{1}{T} \sum_{t=0}^{T-1} L_t$$

**Complete Legend**:
- $z_t$ = Pre-activation hidden state (before tanh)
- $o_t$ = Pre-activation output (before softmax)
- $\hat{y}_t$ = True target/label at time $t$ (one-hot encoded)
- $L_t$ = Loss at time step $t$
- $L$ = Total loss over the sequence
- $T$ = Length of the sequence

### Concrete Numerical Example

Let's do a complete forward pass with a real example:

**Task**: Binary sentiment classification of a short sequence

**Setup**:
```
Vocabulary: {"good": 0, "bad": 1, "movie": 2}
Sequence: ["good", "movie"] → [0, 2]
Target: positive sentiment → [1, 0] at the final time step

Dimensions:
- n_x = 3 (vocabulary size, one-hot encoded)
- n_h = 4 (hidden units)
- n_y = 2 (binary classification)
- T = 2 (sequence length)
```

**Weights** (simplified for demonstration):
```
W_xh (4×3):
[0.1  0.2  0.3]
[0.4  0.1  0.2]
[0.2  0.3  0.1]
[0.3  0.2  0.4]

W_hh (4×4):
[0.1  0.2  0.1  0.2]
[0.3  0.1  0.2  0.1]
[0.2  0.1  0.3  0.2]
[0.1  0.3  0.2  0.1]

W_hy (2×4):
[0.5  0.3  0.2  0.4]
[0.4  0.2  0.5  0.3]

b_h = [0.1, 0.1, 0.1, 0.1]ᵀ
b_y = [0.0, 0.0]ᵀ
```

#### Time Step t=0 (Input: "good" = [1, 0, 0])

**Step 1: Prepare input**
```
x_0 = [1]  (one-hot encoding of "good")
      [0]
      [0]

h_{-1} = [0]  (initial hidden state)
         [0]
         [0]
         [0]
```

**Step 2: Compute W_xh × x_0**
```
[0.1  0.2  0.3] × [1] = [0.1]
[0.4  0.1  0.2]   [0]   [0.4]
[0.2  0.3  0.1]   [0]   [0.2]
[0.3  0.2  0.4]         [0.3]
```

**Step 3: Compute W_hh × h_{-1}**
```
[0.1  0.2  0.1  0.2] × [0] = [0]
[0.3  0.1  0.2  0.1]   [0]   [0]
[0.2  0.1  0.3  0.2]   [0]   [0]
[0.1  0.3  0.2  0.1]   [0]   [0]
```

**Step 4: Add bias and apply tanh**
```
z_0 = [0.1] + [0] + [0.1] = [0.2]
      [0.4]   [0]   [0.1]   [0.5]
      [0.2]   [0]   [0.1]   [0.3]
      [0.3]   [0]   [0.1]   [0.4]

h_0 = tanh(z_0) = [tanh(0.2)] = [0.197]
                  [tanh(0.5)]   [0.462]
                  [tanh(0.3)]   [0.291]
                  [tanh(0.4)]   [0.380]
```

**Step 5: Compute output**
```
o_0 = W_hy × h_0 + b_y

= [0.5  0.3  0.2  0.4] × [0.197] + [0]
  [0.4  0.2  0.5  0.3]   [0.462]   [0]
                          [0.291]
                          [0.380]

= [0.5×0.197 + 0.3×0.462 + 0.2×0.291 + 0.4×0.380] + [0]
  [0.4×0.197 + 0.2×0.462 + 0.5×0.291 + 0.3×0.380]   [0]

= [0.099 + 0.139 + 0.058 + 0.152] + [0] = [0.448]
  [0.079 + 0.092 + 0.146 + 0.114]   [0]   [0.431]
```

**Step 6: Apply softmax**
```
exp(o_0) = [exp(0.448), exp(0.431)] = [1.565, 1.539]

sum = 1.565 + 1.539 = 3.104

y_0 = [1.565/3.104] = [0.504]
      [1.539/3.104]   [0.496]
```

#### Time Step t=1 (Input: "movie" = [0, 0, 1])

**Step 1: Prepare input**
```
x_1 = [0]  (one-hot encoding of "movie")
      [0]
      [1]

h_0 = [0.197]  (from previous step)
      [0.462]
      [0.291]
      [0.380]
```

**Step 2: Compute W_xh × x_1**
```
[0.1  0.2  0.3] × [0] = [0.3]
[0.4  0.1  0.2]   [0]   [0.2]
[0.2  0.3  0.1]   [1]   [0.1]
[0.3  0.2  0.4]         [0.4]
```

**Step 3: Compute W_hh × h_0** (now using actual values!)
```
[0.1  0.2  0.1  0.2] × [0.197]
[0.3  0.1  0.2  0.1]   [0.462]
[0.2  0.1  0.3  0.2]   [0.291]
[0.1  0.3  0.2  0.1]   [0.380]

Row 1: 0.1×0.197 + 0.2×0.462 + 0.1×0.291 + 0.2×0.380 = 0.020 + 0.092 + 0.029 + 0.076 = 0.217
Row 2: 0.3×0.197 + 0.1×0.462 + 0.2×0.291 + 0.1×0.380 = 0.059 + 0.046 + 0.058 + 0.038 = 0.201
Row 3: 0.2×0.197 + 0.1×0.462 + 0.3×0.291 + 0.2×0.380 = 0.039 + 0.046 + 0.087 + 0.076 = 0.248
Row 4: 0.1×0.197 + 0.3×0.462 + 0.2×0.291 + 0.1×0.380 = 0.020 + 0.139 + 0.058 + 0.038 = 0.255

Result = [0.217]
         [0.201]
         [0.248]
         [0.255]
```

**Step 4: Add bias and apply tanh**
```
z_1 = [0.3] + [0.217] + [0.1] = [0.617]
      [0.2]   [0.201]   [0.1]   [0.501]
      [0.1]   [0.248]   [0.1]   [0.448]
      [0.4]   [0.255]   [0.1]   [0.755]

h_1 = tanh(z_1) = [tanh(0.617)] = [0.549]
                  [tanh(0.501)]   [0.463]
                  [tanh(0.448)]   [0.420]
                  [tanh(0.755)]   [0.636]
```

**Step 5: Compute output**
```
o_1 = W_hy × h_1 + b_y

= [0.5  0.3  0.2  0.4] × [0.549] + [0]
  [0.4  0.2  0.5  0.3]   [0.463]   [0]
                          [0.420]
                          [0.636]

= [0.5×0.549 + 0.3×0.463 + 0.2×0.420 + 0.4×0.636]
  [0.4×0.549 + 0.2×0.463 + 0.5×0.420 + 0.3×0.636]

= [0.275 + 0.139 + 0.084 + 0.254] = [0.752]
  [0.220 + 0.093 + 0.210 + 0.191]   [0.714]
```

**Step 6: Apply softmax**
```
exp(o_1) = [exp(0.752), exp(0.714)] = [2.121, 2.042]

sum = 2.121 + 2.042 = 4.163

y_1 = [2.121/4.163] = [0.509]
      [2.042/4.163]   [0.491]
```

#### Computing Loss

Assuming the target is positive sentiment [1, 0] at the final time step:

```
Target: ŷ_1 = [1]
              [0]

Predicted: y_1 = [0.509]
                 [0.491]

Cross-entropy loss:
L_1 = -[1×log(0.509) + 0×log(0.491)]
    = -log(0.509)
    = -(-0.675)
    = 0.675

If we evaluate loss at each time step:
L_0 = -log(0.504) = 0.685  (assuming target was also [1, 0])

Total Loss:
L = (L_0 + L_1) / 2 = (0.685 + 0.675) / 2 = 0.680
```

### Summary of Forward Propagation

```
┌─────────────────────────────────────────────────────────┐
│ Forward Propagation Algorithm                           │
├─────────────────────────────────────────────────────────┤
│ Input:                                                  │
│   - Sequence X = [x_0, x_1, ..., x_{T-1}]             │
│   - Weights: W_xh, W_hh, W_hy, b_h, b_y               │
│                                                         │
│ Process:                                                │
│   1. Initialize h_{-1} = 0                             │
│   2. For each time step t:                             │
│      a. z_t = W_hh·h_{t-1} + W_xh·x_t + b_h           │
│      b. h_t = tanh(z_t)                                │
│      c. o_t = W_hy·h_t + b_y                           │
│      d. y_t = softmax(o_t)                             │
│      e. L_t = -Σ ŷ_{t,i} log(y_{t,i})                 │
│                                                         │
│ Output:                                                 │
│   - Hidden states: H = [h_0, h_1, ..., h_{T-1}]       │
│   - Predictions: Y = [y_0, y_1, ..., y_{T-1}]         │
│   - Total loss: L = (1/T) Σ L_t                        │
└─────────────────────────────────────────────────────────┘
```

---

## Backward Propagation Through Time (BPTT)

### Overview

**Backpropagation Through Time (BPTT)** is the algorithm used to train RNNs. It's an extension of standard backpropagation that accounts for the temporal dependencies in recurrent networks.

**Key Challenge**: The hidden state at time $t$ depends on all previous hidden states, so gradients must flow backward through time.

```
Forward Flow:
h_0 → h_1 → h_2 → h_3 → ... → h_T

Backward Flow (BPTT):
∂L/∂h_0 ← ∂L/∂h_1 ← ∂L/∂h_2 ← ∂L/∂h_3 ← ... ← ∂L/∂h_T
```

### The Chain Rule Through Time

Since each hidden state depends on the previous one:
$$h_t = f(h_{t-1}, x_t)$$

The gradient of the loss with respect to $h_t$ must account for:
1. **Direct contribution**: How $h_t$ affects the current output $y_t$
2. **Future contribution**: How $h_t$ affects all future hidden states and outputs

```
Gradient flow visualization:

             ∂L/∂y_t        (direct loss gradient)
                │
                ▼
             ∂L/∂h_t  ◄────  ∂L/∂h_{t+1}  (gradient from future)
                │
        ┌───────┴───────┐
        ▼               ▼
    ∂L/∂W_hh        ∂L/∂W_xh
    ∂L/∂b_h
```

### Mathematical Formulation

#### 1. Output Layer Gradients

**Loss Function** (cross-entropy):
$$L_t = -\sum_{i=1}^{n_y} \hat{y}_{t,i} \log(y_{t,i})$$

**Gradient with respect to output scores** (before softmax):

**Plain English**: The gradient of the loss with respect to the pre-activation output is simply the difference between prediction and target.

$$\frac{\partial L_t}{\partial o_t} = y_t - \hat{y}_t$$

**Complete Legend**:
- $o_t$ = Pre-activation output at time $t$ (before softmax)
- $y_t$ = Predicted probability distribution (after softmax)
- $\hat{y}_t$ = True target (one-hot encoded for classification)

**Derivation**: For softmax + cross-entropy, this simplifies to $y_t - \hat{y}_t$

#### 2. Output Weight Gradients

**Plain English**: The gradient for output weights is the outer product of the error signal and the hidden state.

$$\frac{\partial L_t}{\partial W_{hy}} = \frac{\partial L_t}{\partial o_t} \cdot h_t^T = (y_t - \hat{y}_t) h_t^T$$

**Dimensions**:
- $\frac{\partial L_t}{\partial W_{hy}}$: $(n_y \times n_h)$
- $(y_t - \hat{y}_t)$: $(n_y \times 1)$
- $h_t^T$: $(1 \times n_h)$

**Output Bias Gradient**:
$$\frac{\partial L_t}{\partial b_y} = y_t - \hat{y}_t$$

#### 3. Hidden State Gradients

**Plain English**: The gradient for the hidden state comes from two sources:
1. The direct contribution to the current output
2. The contribution to the next hidden state (gradient flowing back from $h_{t+1}$)

$$\frac{\partial L}{\partial h_t} = \frac{\partial L_t}{\partial h_t} + \frac{\partial L}{\partial h_{t+1}} \frac{\partial h_{t+1}}{\partial h_t}$$

Breaking it down:

**Component 1: Direct gradient from output**
$$\frac{\partial L_t}{\partial h_t} = W_{hy}^T \frac{\partial L_t}{\partial o_t} = W_{hy}^T (y_t - \hat{y}_t)$$

**Component 2: Gradient from future**
$$\frac{\partial L}{\partial h_{t+1}} \frac{\partial h_{t+1}}{\partial h_t}$$

**Hidden state pre-activation gradient**:

**Plain English**: To get gradients for the weights, we need to account for the tanh activation.

$$\frac{\partial L}{\partial z_t} = \frac{\partial L}{\partial h_t} \odot (1 - h_t^2)$$

Where:
- $\odot$ denotes element-wise multiplication (Hadamard product)
- $(1 - h_t^2)$ is the derivative of $\tanh(z_t)$ evaluated at $h_t$
- $z_t = W_{hh} h_{t-1} + W_{xh} x_t + b_h$ is the pre-activation

**Complete Legend**:
- $\frac{\partial L}{\partial z_t}$ = Gradient of loss w.r.t. pre-activation hidden state
- $(1 - h_t^2)$ = Derivative of tanh (since $\frac{d}{dz}\tanh(z) = 1 - \tanh^2(z)$)

#### 4. Weight Gradients

**Recurrent Weight Gradient**:

**Plain English**: The gradient for recurrent weights accumulates contributions from all time steps.

$$\frac{\partial L}{\partial W_{hh}} = \sum_{t=0}^{T-1} \frac{\partial L_t}{\partial z_t} h_{t-1}^T$$

**Input Weight Gradient**:

**Plain English**: Similarly, the gradient for input weights accumulates over time.

$$\frac{\partial L}{\partial W_{xh}} = \sum_{t=0}^{T-1} \frac{\partial L_t}{\partial z_t} x_t^T$$

**Bias Gradient**:
$$\frac{\partial L}{\partial b_h} = \sum_{t=0}^{T-1} \frac{\partial L_t}{\partial z_t}$$

**Complete Legend**:
- $\sum_{t=0}^{T-1}$ = Sum over all time steps (accumulating gradients)
- Each term is an outer product of the gradient and the input

### BPTT Algorithm

```
┌─────────────────────────────────────────────────────────┐
│ Backpropagation Through Time Algorithm                 │
├─────────────────────────────────────────────────────────┤
│ Step 1: Forward Pass (store all intermediate values)   │
│   - Compute and store: h_0, h_1, ..., h_{T-1}         │
│   - Compute and store: y_0, y_1, ..., y_{T-1}         │
│   - Compute loss: L = Σ L_t                             │
│                                                         │
│ Step 2: Backward Pass (from t=T-1 to t=0)              │
│   - Initialize: ∂L/∂h_T = 0                            │
│   - For t = T-1 down to 0:                             │
│     a. Compute ∂L_t/∂o_t = y_t - ŷ_t                   │
│     b. Compute ∂L_t/∂W_hy, ∂L_t/∂b_y                   │
│     c. Compute ∂L_t/∂h_t from output                   │
│     d. Add gradient from future: ∂L/∂h_t += ∂L/∂h_{t+1}│
│     e. Compute ∂L/∂z_t using tanh derivative           │
│     f. Compute ∂L/∂W_hh, ∂L/∂W_xh, ∂L/∂b_h            │
│     g. Compute ∂L/∂h_{t-1} for next iteration          │
│                                                         │
│ Step 3: Update Weights                                 │
│   - W_hy := W_hy - α·∂L/∂W_hy                          │
│   - W_hh := W_hh - α·∂L/∂W_hh                          │
│   - W_xh := W_xh - α·∂L/∂W_xh                          │
│   - b_y  := b_y - α·∂L/∂b_y                            │
│   - b_h  := b_h - α·∂L/∂b_h                            │
└─────────────────────────────────────────────────────────┘
```

### Numerical Example of BPTT

Let's continue with our previous example and compute gradients. We'll use a shorter sequence for clarity.

**Setup** (from forward pass):
```
Sequence length: T = 2
Target: ŷ_1 = [1, 0] (positive sentiment at final step)

From forward pass, we have:
h_0 = [0.197, 0.462, 0.291, 0.380]ᵀ
h_1 = [0.549, 0.463, 0.420, 0.636]ᵀ
y_0 = [0.504, 0.496]ᵀ
y_1 = [0.509, 0.491]ᵀ

Learning rate: α = 0.01
```

#### Backward Pass: Time Step t=1

**Step 1: Compute output gradient**
```
∂L_1/∂o_1 = y_1 - ŷ_1 = [0.509] - [1] = [-0.491]
                         [0.491]   [0]   [ 0.491]
```

**Step 2: Compute ∂L_1/∂W_hy**
```
∂L_1/∂W_hy = (∂L_1/∂o_1) · h_1ᵀ

= [-0.491] × [0.549  0.463  0.420  0.636]
  [ 0.491]

= [-0.491×0.549  -0.491×0.463  -0.491×0.420  -0.491×0.636]
  [ 0.491×0.549   0.491×0.463   0.491×0.420   0.491×0.636]

= [-0.270  -0.227  -0.206  -0.312]
  [ 0.270   0.227   0.206   0.312]
```

**Step 3: Compute ∂L_1/∂b_y**
```
∂L_1/∂b_y = ∂L_1/∂o_1 = [-0.491]
                         [ 0.491]
```

**Step 4: Compute ∂L_1/∂h_1 (direct contribution)**
```
∂L_1/∂h_1 = W_hyᵀ · (∂L_1/∂o_1)

= [0.5  0.4]ᵀ × [-0.491]
  [0.3  0.2]    [ 0.491]
  [0.2  0.5]
  [0.4  0.3]

= [0.5×(-0.491) + 0.4×0.491]  = [-0.246 + 0.196]   = [-0.050]
  [0.3×(-0.491) + 0.2×0.491]    [-0.147 + 0.098]     [-0.049]
  [0.2×(-0.491) + 0.5×0.491]    [-0.098 + 0.246]     [ 0.148]
  [0.4×(-0.491) + 0.3×0.491]    [-0.196 + 0.147]     [-0.049]
```

Note: At t=1 (last time step), there's no gradient from future, so:
```
∂L/∂h_1 = ∂L_1/∂h_1 = [-0.050, -0.049, 0.148, -0.049]ᵀ
```

**Step 5: Compute ∂L/∂z_1 (account for tanh)**
```
∂L/∂z_1 = ∂L/∂h_1 ⊙ (1 - h_1²)

First, compute (1 - h_1²):
h_1² = [0.549²]  = [0.301]
       [0.463²]    [0.214]
       [0.420²]    [0.176]
       [0.636²]    [0.404]

1 - h_1² = [0.699]
           [0.786]
           [0.824]
           [0.596]

∂L/∂z_1 = [-0.050] ⊙ [0.699] = [-0.050 × 0.699] = [-0.035]
          [-0.049]   [0.786]   [-0.049 × 0.786]   [-0.039]
          [ 0.148]   [0.824]   [ 0.148 × 0.824]   [ 0.122]
          [-0.049]   [0.596]   [-0.049 × 0.596]   [-0.029]
```

**Step 6: Compute ∂L_1/∂W_hh**
```
∂L_1/∂W_hh = (∂L/∂z_1) · h_0ᵀ

= [-0.035] × [0.197  0.462  0.291  0.380]
  [-0.039]
  [ 0.122]
  [-0.029]

= [-0.035×0.197  -0.035×0.462  -0.035×0.291  -0.035×0.380]
  [-0.039×0.197  -0.039×0.462  -0.039×0.291  -0.039×0.380]
  [ 0.122×0.197   0.122×0.462   0.122×0.291   0.122×0.380]
  [-0.029×0.197  -0.029×0.462  -0.029×0.291  -0.029×0.380]

= [-0.007  -0.016  -0.010  -0.013]
  [-0.008  -0.018  -0.011  -0.015]
  [ 0.024   0.056   0.036   0.046]
  [-0.006  -0.013  -0.008  -0.011]
```

**Step 7: Compute ∂L_1/∂W_xh**
```
∂L_1/∂W_xh = (∂L/∂z_1) · x_1ᵀ

x_1 = [0, 0, 1]ᵀ (one-hot for "movie")

= [-0.035] × [0  0  1]
  [-0.039]
  [ 0.122]
  [-0.029]

= [0  0  -0.035]
  [0  0  -0.039]
  [0  0   0.122]
  [0  0  -0.029]
```

**Step 8: Compute ∂L/∂h_0 (for next iteration)**
```
∂L/∂h_0 = W_hhᵀ · (∂L/∂z_1)

= [0.1  0.3  0.2  0.1]ᵀ × [-0.035]
  [0.2  0.1  0.1  0.3]    [-0.039]
  [0.1  0.2  0.3  0.2]    [ 0.122]
  [0.2  0.1  0.2  0.1]    [-0.029]

= [0.1×(-0.035) + 0.3×(-0.039) + 0.2×0.122 + 0.1×(-0.029)]
  [0.2×(-0.035) + 0.1×(-0.039) + 0.1×0.122 + 0.3×(-0.029)]
  [0.1×(-0.035) + 0.2×(-0.039) + 0.3×0.122 + 0.2×(-0.029)]
  [0.2×(-0.035) + 0.1×(-0.039) + 0.2×0.122 + 0.1×(-0.029)]

= [-0.004 - 0.012 + 0.024 - 0.003]   = [ 0.005]
  [-0.007 - 0.004 + 0.012 - 0.009]     [-0.008]
  [-0.004 - 0.008 + 0.037 - 0.006]     [ 0.019]
  [-0.007 - 0.004 + 0.024 - 0.003]     [ 0.010]
```

#### Backward Pass: Time Step t=0

**Step 1: Compute output gradient** (if we compute loss at t=0)
```
∂L_0/∂o_0 = y_0 - ŷ_0 = [0.504] - [1] = [-0.496]
                         [0.496]   [0]   [ 0.496]
```

**Step 2: Compute ∂L_0/∂h_0 (direct contribution)**
```
∂L_0/∂h_0 = W_hyᵀ · (∂L_0/∂o_0)

= [0.5  0.4]ᵀ × [-0.496]
  [0.3  0.2]    [ 0.496]
  [0.2  0.5]
  [0.4  0.3]

= [-0.248 + 0.198]   = [-0.050]
  [-0.149 + 0.099]     [-0.050]
  [-0.099 + 0.248]     [ 0.149]
  [-0.198 + 0.149]     [-0.049]
```

**Step 3: Add gradient from future**
```
∂L/∂h_0 = ∂L_0/∂h_0 + (gradient from h_1)

= [-0.050]   [ 0.005]   [-0.045]
  [-0.050] + [-0.008] = [-0.058]
  [ 0.149]   [ 0.019]   [ 0.168]
  [-0.049]   [ 0.010]   [-0.039]
```

**Step 4: Compute ∂L/∂z_0**
```
1 - h_0² = 1 - [0.197²]  = [0.961]
               [0.462²]    [0.787]
               [0.291²]    [0.915]
               [0.380²]    [0.856]

∂L/∂z_0 = ∂L/∂h_0 ⊙ (1 - h_0²)

= [-0.045]   [0.961]   [-0.043]
  [-0.058] ⊙ [0.787] = [-0.046]
  [ 0.168]   [0.915]   [ 0.154]
  [-0.039]   [0.856]   [-0.033]
```

**Step 5: Compute ∂L_0/∂W_xh**
```
x_0 = [1, 0, 0]ᵀ (one-hot for "good")

∂L_0/∂W_xh = (∂L/∂z_0) · x_0ᵀ

= [-0.043] × [1  0  0]
  [-0.046]
  [ 0.154]
  [-0.033]

= [-0.043  0  0]
  [-0.046  0  0]
  [ 0.154  0  0]
  [-0.033  0  0]
```

#### Accumulate Gradients

**Total gradient for W_xh**:
```
∂L/∂W_xh = ∂L_0/∂W_xh + ∂L_1/∂W_xh

= [-0.043  0  0]     [0  0  -0.035]     [-0.043  0  -0.035]
  [-0.046  0  0]  +  [0  0  -0.039]  =  [-0.046  0  -0.039]
  [ 0.154  0  0]     [0  0   0.122]     [ 0.154  0   0.122]
  [-0.033  0  0]     [0  0  -0.029]     [-0.033  0  -0.029]
```

### Weight Updates

Using gradient descent with learning rate α = 0.01:

**Update W_hy**:
```
W_hy_new = W_hy - α · ∂L/∂W_hy

= [0.5  0.3  0.2  0.4]     [-0.270  -0.227  -0.206  -0.312]
  [0.4  0.2  0.5  0.3]  - 0.01 × [ 0.270   0.227   0.206   0.312]

= [0.5  0.3  0.2  0.4]     [0.0027  0.0023  0.0021  0.0031]
  [0.4  0.2  0.5  0.3]  +  [-0.0027 -0.0023 -0.0021 -0.0031]

= [0.5027  0.3023  0.2021  0.4031]
  [0.3973  0.1977  0.4979  0.2969]
```

**Update W_hh** (using gradient from t=1 only for this example):
```
W_hh_new = W_hh - α · ∂L_1/∂W_hh

= (similar calculation with small updates)
```

### Vanishing and Exploding Gradient Problem

#### The Problem

**Plain English**: When gradients flow backward through many time steps, they can either:
1. **Vanish**: Become extremely small (→ 0), making it impossible to learn long-term dependencies
2. **Explode**: Become extremely large (→ ∞), causing unstable training

#### Mathematical Explanation

Recall that the gradient flows back through the recurrent connections:

$$\frac{\partial h_t}{\partial h_{t-1}} = W_{hh}^T \cdot \text{diag}(1 - h_{t-1}^2)$$

When computing $\frac{\partial L}{\partial h_0}$ (gradient at the first time step), we need to multiply these Jacobians:

$$\frac{\partial L}{\partial h_0} = \frac{\partial L}{\partial h_T} \prod_{t=1}^{T} \frac{\partial h_t}{\partial h_{t-1}}$$

```
Gradient from t=T back to t=0:

∂L/∂h_0 = ∂L/∂h_T × (∂h_T/∂h_{T-1}) × (∂h_{T-1}/∂h_{T-2}) × ... × (∂h_1/∂h_0)
          └────────┴───────────────┴────────────────────┴────────┴──────────┘
                         Product of T matrices
```

**Why gradients vanish**:
- If eigenvalues of $W_{hh} < 1$, repeated multiplication makes gradients exponentially smaller
- $\tanh$ derivative $(1 - h_t^2) \leq 1$ further reduces gradients
- Over T time steps: gradient ≈ $(\lambda_{max})^T$ where $\lambda_{max}$ is largest eigenvalue

**Example of vanishing**:
```
Let's say λ_max = 0.9 and T = 100 time steps

Gradient scaling factor: (0.9)^100 ≈ 0.0000266

Original gradient: 1.0
After 100 steps: 0.0000266 (effectively zero!)
```

**Why gradients explode**:
- If eigenvalues of $W_{hh} > 1$, repeated multiplication makes gradients exponentially larger
- Over T time steps: gradient ≈ $(\lambda_{max})^T$ where $\lambda_{max} > 1$

**Example of exploding**:
```
Let's say λ_max = 1.1 and T = 100 time steps

Gradient scaling factor: (1.1)^100 ≈ 13,780.6

Original gradient: 1.0
After 100 steps: 13,780.6 (explodes!)
```

#### Visualization of the Problem

```
Vanishing Gradient:
═══════════════════

Time:    t=0    t=1    t=2   ...   t=99   t=100
Gradient: ▓▓▓▓ → ▓▓▓ → ▓▓ → ... → ▓ → • (nearly 0)
          1.0    0.9    0.81       0.0001  0.00002

Information from t=0 doesn't reach the loss at t=100!


Exploding Gradient:
═══════════════════

Time:    t=0    t=1    t=2   ...   t=99      t=100
Gradient: ▓ → ▓▓ → ▓▓▓ → ... → ▓▓▓▓▓▓▓ → ∞
          1.0  1.1   1.21       1000      13781

Gradients become so large they cause numerical overflow!
```

#### Solutions

**1. Gradient Clipping** (for exploding gradients):
```python
if ||gradient|| > threshold:
    gradient = (threshold / ||gradient||) × gradient
```

**2. Better Initialization**:
- Initialize $W_{hh}$ to be orthogonal or have eigenvalues near 1
- Helps keep gradient magnitudes stable

**3. Advanced Architectures**:
- **LSTM** (Long Short-Term Memory): Uses gates to control information flow
- **GRU** (Gated Recurrent Unit): Simplified gating mechanism
- **Residual connections**: Add skip connections to bypass some layers

**4. Gradient Monitoring**:
```
During training, monitor:
- Average gradient magnitude across layers
- Maximum gradient value
- Gradient-to-parameter ratio
```

### Complete BPTT Summary

```
┌──────────────────────────────────────────────────────────┐
│ BPTT Key Equations Summary                               │
├──────────────────────────────────────────────────────────┤
│                                                          │
│ Forward Pass:                                            │
│   h_t = tanh(W_hh·h_{t-1} + W_xh·x_t + b_h)             │
│   y_t = softmax(W_hy·h_t + b_y)                          │
│   L_t = -Σ ŷ_{t,i} log(y_{t,i})                         │
│                                                          │
│ Backward Pass (for each t from T-1 to 0):               │
│   ∂L/∂o_t = y_t - ŷ_t                                   │
│   ∂L/∂W_hy += (y_t - ŷ_t)·h_tᵀ                          │
│   ∂L/∂h_t = W_hyᵀ·(y_t - ŷ_t) + ∂L/∂h_{t+1}·W_hhᵀ      │
│   ∂L/∂z_t = ∂L/∂h_t ⊙ (1 - h_t²)                        │
│   ∂L/∂W_hh += ∂L/∂z_t·h_{t-1}ᵀ                          │
│   ∂L/∂W_xh += ∂L/∂z_t·x_tᵀ                              │
│   ∂L/∂b_h += ∂L/∂z_t                                     │
│                                                          │
│ Challenges:                                              │
│   - Vanishing gradients: (λ < 1)^T → 0                  │
│   - Exploding gradients: (λ > 1)^T → ∞                  │
│   - Long-term dependencies hard to learn                │
│                                                          │
│ Solutions:                                               │
│   - Gradient clipping                                   │
│   - Better initialization                               │
│   - Advanced architectures (LSTM, GRU)                  │
└──────────────────────────────────────────────────────────┘
```

---

## Summary

**Recurrent Neural Networks (RNNs)** are powerful models for sequential data that maintain a hidden state to remember information across time steps. The key innovations are:

1. **Recurrent connections**: Hidden state loops back as input for next step
2. **Parameter sharing**: Same weights used across all time steps
3. **Variable length handling**: Can process sequences of any length

**Forward propagation** processes the sequence step by step:
```
h_t = tanh(W_hh·h_{t-1} + W_xh·x_t + b_h)
y_t = softmax(W_hy·h_t + b_y)
```

**Backpropagation Through Time (BPTT)** trains the network by:
- Unrolling the network across time
- Computing gradients backwards through all time steps
- Accumulating gradients for weight updates

**Challenges**:
- Vanishing/exploding gradients limit learning of long-term dependencies
- Solutions include gradient clipping and advanced architectures (LSTM, GRU)

RNNs form the foundation for understanding modern sequence models and have applications in language modeling, translation, time series prediction, and more!
