# Recurrent Neural Networks (RNNs)

## Table of Contents
1. [Plain English Explanation](#plain-english-explanation)
2. [Mathematical Foundation](#mathematical-foundation)
3. [Forward Propagation](#forward-propagation)
4. [Backward Propagation Through Time (BPTT)](#backward-propagation-through-time-bptt)

---

## Plain English Explanation

### What are RNNs?

**Recurrent Neural Networks (RNNs)** are a class of neural networks designed to work with **sequential data** - data where order matters. Unlike traditional feedforward neural networks that treat each input independently, RNNs have **memory** that allows them to remember information from previous inputs in the sequence.

Think of reading a sentence: to understand the current word, you need to remember the words that came before it. RNNs work the same way!

### Why Use RNNs?

RNNs are perfect for tasks involving sequences:
- **Natural Language Processing**: Text generation, translation, sentiment analysis
- **Time Series Prediction**: Stock prices, weather forecasting
- **Speech Recognition**: Converting audio to text
- **Video Analysis**: Understanding sequences of frames
- **Music Generation**: Creating melodies

### Basic Architecture

The key innovation of RNNs is the **recurrent connection** - the network's output loops back as input for the next time step.

#### Folded (Compact) Representation:
```
        ┌─────────┐
    x_t │         │ y_t
   ────►│  RNN    │────►
        │  Cell   │
        └────┬────┘
             │
             │ h_t (hidden state)
             │
             └──────┐
                    │
                    ▼
```

#### Unfolded (Over Time) Representation:
```
Time:    t=0         t=1         t=2         t=3
         ┌───┐      ┌───┐      ┌───┐      ┌───┐
   h_0   │   │ h_1  │   │ h_2  │   │ h_3  │   │
   ─────►│RNN├─────►│RNN├─────►│RNN├─────►│RNN│
         │   │      │   │      │   │      │   │
         └─┬─┘      └─┬─┘      └─┬─┘      └─┬─┘
           ▲          ▲          ▲          ▲
           │          │          │          │
          x_0        x_1        x_2        x_3
           │          │          │          │
           ▼          ▼          ▼          ▼
          y_0        y_1        y_2        y_3
```

### How Information Flows Through RNNs

**Key Concept: Hidden State** - The hidden state `h_t` acts as the network's "memory", carrying information from previous time steps.

```
┌─────────────────────────────────────────────────────────┐
│  At each time step t:                                   │
│                                                          │
│  1. Receive new input x_t                               │
│  2. Receive previous hidden state h_(t-1)               │
│  3. Combine them to create new hidden state h_t         │
│  4. Generate output y_t from h_t                        │
│  5. Pass h_t to next time step                          │
└─────────────────────────────────────────────────────────┘
```

#### Detailed Flow Visualization:
```
Input Sequence:  [x_0]  [x_1]  [x_2]  [x_3]
                   ↓      ↓      ↓      ↓
                 ┌───────────────────────┐
                 │  Processing at t=1    │
                 │                       │
                 │  h_0 ──┐              │
                 │        │              │
                 │        ├──► Combine   │
                 │        │    (W_hh)    │
                 │  x_1 ──┘              │
                 │         │              │
                 │         ▼              │
                 │    Activation (tanh)  │
                 │         │              │
                 │         ▼              │
                 │    h_1 (new memory)   │
                 │         │              │
                 │         ├──► y_1       │
                 │         │   (output)   │
                 │         │              │
                 │         └──► h_2       │
                 │          (to next step)│
                 └───────────────────────┘
```

### The Power of Recurrence

The recurrent connection allows information to persist:

```
Example: Understanding "The clouds are in the sky"

t=0: Input="The"    → h_0 stores: "article encountered"
t=1: Input="clouds" → h_1 stores: "article + noun (subject)"
t=2: Input="are"    → h_2 stores: "subject + verb"
t=3: Input="in"     → h_3 stores: "subject + verb + preposition"
t=4: Input="the"    → h_4 stores: "subject + verb + prep + article"
t=5: Input="sky"    → h_5 stores: complete sentence meaning

Each hidden state carries forward cumulative understanding!
```

### Types of RNN Architectures

```
1. One-to-One (Standard Neural Network)
   x → [RNN] → y

2. One-to-Many (e.g., Image Captioning)
   x → [RNN] → [RNN] → [RNN] → y, y, y

3. Many-to-One (e.g., Sentiment Analysis)
   x, x, x → [RNN] → [RNN] → [RNN] → y

4. Many-to-Many (e.g., Machine Translation)
   x, x, x → [RNN] → [RNN] → [RNN] → y, y, y

5. Many-to-Many Synced (e.g., Video Labeling)
   x → [RNN] → y
   x → [RNN] → y
   x → [RNN] → y
```

---

## Mathematical Foundation

### Core RNN Equations

The basic RNN consists of two main equations that compute the hidden state and output at each time step.

#### 1. Hidden State Update

**Plain English**: At each time step, the new hidden state is computed by:
1. Taking the current input `x_t` and multiplying it by input weights
2. Taking the previous hidden state `h_(t-1)` and multiplying it by recurrent weights
3. Adding them together with a bias
4. Applying an activation function (usually tanh)

**Formula**:
$$h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$

**Complete Legend**:
- $h_t$ = Hidden state at time step $t$ (vector of size $n_h$)
- $h_{t-1}$ = Previous hidden state at time step $t-1$ (vector of size $n_h$)
- $x_t$ = Input at time step $t$ (vector of size $n_x$)
- $W_{hh}$ = Weight matrix for hidden-to-hidden connections (size $n_h \times n_h$)
- $W_{xh}$ = Weight matrix for input-to-hidden connections (size $n_h \times n_x$)
- $b_h$ = Bias vector for hidden state (size $n_h$)
- $\tanh$ = Hyperbolic tangent activation function (outputs between -1 and 1)

**Alternative Notation**:
Sometimes written as:
$$h_t = \tanh(W_h [h_{t-1}, x_t] + b_h)$$

Where $W_h$ is a combined weight matrix and $[h_{t-1}, x_t]$ is the concatenation of the previous hidden state and current input.

#### 2. Output Computation

**Plain English**: The output at each time step is computed by:
1. Taking the current hidden state
2. Multiplying it by output weights
3. Adding a bias
4. Optionally applying an activation function (softmax for classification, etc.)

**Formula**:
$$y_t = \text{softmax}(W_{hy} h_t + b_y)$$

Or for regression tasks:
$$y_t = W_{hy} h_t + b_y$$

**Complete Legend**:
- $y_t$ = Output at time step $t$ (vector of size $n_y$)
- $W_{hy}$ = Weight matrix from hidden state to output (size $n_y \times n_h$)
- $b_y$ = Bias vector for output (size $n_y$)
- $\text{softmax}$ = Softmax activation function for classification

### Step-by-Step Visualization with Example Values

Let's work through a concrete example with small dimensions:

**Setup**:
- Input dimension: $n_x = 2$
- Hidden dimension: $n_h = 3$
- Output dimension: $n_y = 2$
- Sequence length: $T = 3$

**Initialize weights and inputs**:

```
Input sequence:
x_0 = [1.0]    x_1 = [0.5]    x_2 = [2.0]
      [0.5]          [1.5]          [0.3]

Initial hidden state (usually zeros):
h_0 = [0]
      [0]
      [0]

Weight matrices:
W_xh = [0.5  0.2 ]     (3×2 matrix)
       [0.3  0.1 ]
       [0.4  0.6 ]

W_hh = [0.1  0.2  0.3]     (3×3 matrix)
       [0.4  0.1  0.2]
       [0.2  0.3  0.1]

b_h = [0.1]
      [0.2]
      [0.1]

W_hy = [0.5  0.3  0.2]     (2×3 matrix)
       [0.4  0.2  0.1]

b_y = [0.1]
      [0.2]
```

#### Time Step t=0

**Step 1: Compute W_xh × x_0**
```
[0.5  0.2] × [1.0] = [0.5×1.0 + 0.2×0.5] = [0.60]
[0.3  0.1]   [0.5]   [0.3×1.0 + 0.1×0.5]   [0.35]
[0.4  0.6]           [0.4×1.0 + 0.6×0.5]   [0.70]
```

**Step 2: Compute W_hh × h_{-1}** (h_{-1} = zeros initially)
```
[0.1  0.2  0.3] × [0] = [0]
[0.4  0.1  0.2]   [0]   [0]
[0.2  0.3  0.1]   [0]   [0]
```

**Step 3: Add bias and apply tanh**
```
Before activation: [0.60] + [0] + [0.1] = [0.70]
                   [0.35]   [0]   [0.2]   [0.55]
                   [0.70]   [0]   [0.1]   [0.80]

h_0 = tanh([0.70]) = [0.604]
           [0.55]    [0.500]
           [0.80]    [0.664]
```

**Step 4: Compute output y_0**
```
W_hy × h_0 + b_y = [0.5  0.3  0.2] × [0.604] + [0.1]
                   [0.4  0.2  0.1]   [0.500]   [0.2]
                                      [0.664]

= [0.5×0.604 + 0.3×0.500 + 0.2×0.664] + [0.1]
  [0.4×0.604 + 0.2×0.500 + 0.1×0.664]   [0.2]

= [0.302 + 0.150 + 0.133] + [0.1] = [0.685]
  [0.242 + 0.100 + 0.066]   [0.2]   [0.608]

For classification, apply softmax:
y_0 = softmax([0.685]) = [0.519]
              [0.608]    [0.481]
```

#### Time Step t=1

**Step 1: Compute W_xh × x_1**
```
[0.5  0.2] × [0.5] = [0.55]
[0.3  0.1]   [1.5]   [0.30]
[0.4  0.6]           [0.95]
```

**Step 2: Compute W_hh × h_0** (now h_0 has values!)
```
[0.1  0.2  0.3] × [0.604] = [0.1×0.604 + 0.2×0.500 + 0.3×0.664]
[0.4  0.1  0.2]   [0.500]   [0.4×0.604 + 0.1×0.500 + 0.2×0.664]
[0.2  0.3  0.1]   [0.664]   [0.2×0.604 + 0.3×0.500 + 0.1×0.664]

= [0.060 + 0.100 + 0.199] = [0.359]
  [0.242 + 0.050 + 0.133]   [0.425]
  [0.121 + 0.150 + 0.066]   [0.337]
```

**Step 3: Add everything and apply tanh**
```
Before activation: [0.55] + [0.359] + [0.1] = [1.009]
                   [0.30]   [0.425]   [0.2]   [0.925]
                   [0.95]   [0.337]   [0.1]   [1.387]

h_1 = tanh([1.009]) = [0.763]
           [0.925]    [0.728]
           [1.387]    [0.881]
```

**Step 4: Compute output y_1**
```
y_1 (before softmax) = W_hy × h_1 + b_y
= [0.5×0.763 + 0.3×0.728 + 0.2×0.881] + [0.1]
  [0.4×0.763 + 0.2×0.728 + 0.1×0.881]   [0.2]

= [0.382 + 0.218 + 0.176] + [0.1] = [0.876]
  [0.305 + 0.146 + 0.088]   [0.2]   [0.739]

y_1 = softmax([0.876]) = [0.534]
              [0.739]    [0.466]
```

### Visualization of Weight Matrix Dimensions

```
Time step t:

Input x_t                 Hidden State h_{t-1}
(n_x × 1)                 (n_h × 1)
    │                         │
    │ W_xh                    │ W_hh
    │ (n_h × n_x)             │ (n_h × n_h)
    ▼                         ▼
    └────────► + ◄────────────┘
               │
               │ + b_h (n_h × 1)
               ▼
            tanh()
               │
               ▼
           h_t (n_h × 1)
               │
               │ W_hy (n_y × n_h)
               ▼
               + b_y (n_y × 1)
               │
               ▼
           y_t (n_y × 1)
```

### Activation Functions

#### Tanh (Hyperbolic Tangent)
**Formula**: 
$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

**Properties**:
- Output range: $[-1, 1]$
- Zero-centered (helps with gradient flow)
- S-shaped curve

**Why use tanh for hidden states?**
- Zero-centered outputs prevent bias shift
- Stronger gradients than sigmoid in the middle range
- Natural choice for representing "hidden memory" that can be positive or negative

#### Softmax (for classification output)
**Formula**:
$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n_y} e^{z_j}}$$

**Properties**:
- Converts scores to probability distribution
- All outputs sum to 1
- Each output is between 0 and 1

---

## Forward Propagation

### Overview

Forward propagation in RNNs processes the input sequence step by step, maintaining a hidden state that carries information across time steps.

```
Forward Pass Flow:
═══════════════════

Initialize: h_{-1} = 0 (or learned initial state)

For t = 0 to T-1:
    1. Receive x_t
    2. Compute h_t = tanh(W_hh·h_{t-1} + W_xh·x_t + b_h)
    3. Compute y_t = softmax(W_hy·h_t + b_y)
    4. Compute loss L_t for this time step
    
Total Loss: L = Σ L_t (sum over all time steps)
```

### Detailed Mathematical Formulation

#### Step-by-Step Equations

**For each time step t = 0, 1, 2, ..., T-1:**

1. **Hidden state pre-activation**:
   $$z_t = W_{hh} h_{t-1} + W_{xh} x_t + b_h$$

2. **Hidden state activation**:
   $$h_t = \tanh(z_t)$$

3. **Output pre-activation**:
   $$o_t = W_{hy} h_t + b_y$$

4. **Output activation** (for classification):
   $$y_t = \text{softmax}(o_t)$$

5. **Loss at time step** (cross-entropy for classification):
   $$L_t = -\sum_{i=1}^{n_y} \hat{y}_{t,i} \log(y_{t,i})$$

6. **Total loss**:
   $$L = \frac{1}{T} \sum_{t=0}^{T-1} L_t$$

**Complete Legend**:
- $z_t$ = Pre-activation hidden state (before tanh)
- $o_t$ = Pre-activation output (before softmax)
- $\hat{y}_t$ = True target/label at time $t$ (one-hot encoded)
- $L_t$ = Loss at time step $t$
- $L$ = Total loss over the sequence
- $T$ = Length of the sequence

### Concrete Numerical Example

Let's do a complete forward pass with a real example:

**Task**: Binary sentiment classification of a short sequence

**Setup**:
```
Vocabulary: {"good": 0, "bad": 1, "movie": 2}
Sequence: ["good", "movie"] → [0, 2]
Target: positive sentiment → [1, 0] at the final time step

Dimensions:
- n_x = 3 (vocabulary size, one-hot encoded)
- n_h = 4 (hidden units)
- n_y = 2 (binary classification)
- T = 2 (sequence length)
```

**Weights** (simplified for demonstration):
```
W_xh (4×3):
[0.1  0.2  0.3]
[0.4  0.1  0.2]
[0.2  0.3  0.1]
[0.3  0.2  0.4]

W_hh (4×4):
[0.1  0.2  0.1  0.2]
[0.3  0.1  0.2  0.1]
[0.2  0.1  0.3  0.2]
[0.1  0.3  0.2  0.1]

W_hy (2×4):
[0.5  0.3  0.2  0.4]
[0.4  0.2  0.5  0.3]

b_h = [0.1, 0.1, 0.1, 0.1]ᵀ
b_y = [0.0, 0.0]ᵀ
```

#### Time Step t=0 (Input: "good" = [1, 0, 0])

**Step 1: Prepare input**
```
x_0 = [1]  (one-hot encoding of "good")
      [0]
      [0]

h_{-1} = [0]  (initial hidden state)
         [0]
         [0]
         [0]
```

**Step 2: Compute W_xh × x_0**
```
[0.1  0.2  0.3] × [1] = [0.1]
[0.4  0.1  0.2]   [0]   [0.4]
[0.2  0.3  0.1]   [0]   [0.2]
[0.3  0.2  0.4]         [0.3]
```

**Step 3: Compute W_hh × h_{-1}**
```
[0.1  0.2  0.1  0.2] × [0] = [0]
[0.3  0.1  0.2  0.1]   [0]   [0]
[0.2  0.1  0.3  0.2]   [0]   [0]
[0.1  0.3  0.2  0.1]   [0]   [0]
```

**Step 4: Add bias and apply tanh**
```
z_0 = [0.1] + [0] + [0.1] = [0.2]
      [0.4]   [0]   [0.1]   [0.5]
      [0.2]   [0]   [0.1]   [0.3]
      [0.3]   [0]   [0.1]   [0.4]

h_0 = tanh(z_0) = [tanh(0.2)] = [0.197]
                  [tanh(0.5)]   [0.462]
                  [tanh(0.3)]   [0.291]
                  [tanh(0.4)]   [0.380]
```

**Step 5: Compute output**
```
o_0 = W_hy × h_0 + b_y

= [0.5  0.3  0.2  0.4] × [0.197] + [0]
  [0.4  0.2  0.5  0.3]   [0.462]   [0]
                          [0.291]
                          [0.380]

= [0.5×0.197 + 0.3×0.462 + 0.2×0.291 + 0.4×0.380] + [0]
  [0.4×0.197 + 0.2×0.462 + 0.5×0.291 + 0.3×0.380]   [0]

= [0.099 + 0.139 + 0.058 + 0.152] + [0] = [0.448]
  [0.079 + 0.092 + 0.146 + 0.114]   [0]   [0.431]
```

**Step 6: Apply softmax**
```
exp(o_0) = [exp(0.448), exp(0.431)] = [1.565, 1.539]

sum = 1.565 + 1.539 = 3.104

y_0 = [1.565/3.104] = [0.504]
      [1.539/3.104]   [0.496]
```

#### Time Step t=1 (Input: "movie" = [0, 0, 1])

**Step 1: Prepare input**
```
x_1 = [0]  (one-hot encoding of "movie")
      [0]
      [1]

h_0 = [0.197]  (from previous step)
      [0.462]
      [0.291]
      [0.380]
```

**Step 2: Compute W_xh × x_1**
```
[0.1  0.2  0.3] × [0] = [0.3]
[0.4  0.1  0.2]   [0]   [0.2]
[0.2  0.3  0.1]   [1]   [0.1]
[0.3  0.2  0.4]         [0.4]
```

**Step 3: Compute W_hh × h_0** (now using actual values!)
```
[0.1  0.2  0.1  0.2] × [0.197]
[0.3  0.1  0.2  0.1]   [0.462]
[0.2  0.1  0.3  0.2]   [0.291]
[0.1  0.3  0.2  0.1]   [0.380]

Row 1: 0.1×0.197 + 0.2×0.462 + 0.1×0.291 + 0.2×0.380 = 0.020 + 0.092 + 0.029 + 0.076 = 0.217
Row 2: 0.3×0.197 + 0.1×0.462 + 0.2×0.291 + 0.1×0.380 = 0.059 + 0.046 + 0.058 + 0.038 = 0.201
Row 3: 0.2×0.197 + 0.1×0.462 + 0.3×0.291 + 0.2×0.380 = 0.039 + 0.046 + 0.087 + 0.076 = 0.248
Row 4: 0.1×0.197 + 0.3×0.462 + 0.2×0.291 + 0.1×0.380 = 0.020 + 0.139 + 0.058 + 0.038 = 0.255

Result = [0.217]
         [0.201]
         [0.248]
         [0.255]
```

**Step 4: Add bias and apply tanh**
```
z_1 = [0.3] + [0.217] + [0.1] = [0.617]
      [0.2]   [0.201]   [0.1]   [0.501]
      [0.1]   [0.248]   [0.1]   [0.448]
      [0.4]   [0.255]   [0.1]   [0.755]

h_1 = tanh(z_1) = [tanh(0.617)] = [0.549]
                  [tanh(0.501)]   [0.463]
                  [tanh(0.448)]   [0.420]
                  [tanh(0.755)]   [0.636]
```

**Step 5: Compute output**
```
o_1 = W_hy × h_1 + b_y

= [0.5  0.3  0.2  0.4] × [0.549] + [0]
  [0.4  0.2  0.5  0.3]   [0.463]   [0]
                          [0.420]
                          [0.636]

= [0.5×0.549 + 0.3×0.463 + 0.2×0.420 + 0.4×0.636]
  [0.4×0.549 + 0.2×0.463 + 0.5×0.420 + 0.3×0.636]

= [0.275 + 0.139 + 0.084 + 0.254] = [0.752]
  [0.220 + 0.093 + 0.210 + 0.191]   [0.714]
```

**Step 6: Apply softmax**
```
exp(o_1) = [exp(0.752), exp(0.714)] = [2.121, 2.042]

sum = 2.121 + 2.042 = 4.163

y_1 = [2.121/4.163] = [0.509]
      [2.042/4.163]   [0.491]
```

#### Computing Loss

Assuming the target is positive sentiment [1, 0] at the final time step:

```
Target: ŷ_1 = [1]
              [0]

Predicted: y_1 = [0.509]
                 [0.491]

Cross-entropy loss:
L_1 = -[1×log(0.509) + 0×log(0.491)]
    = -log(0.509)
    = -(-0.675)
    = 0.675

If we evaluate loss at each time step:
L_0 = -log(0.504) = 0.685  (assuming target was also [1, 0])

Total Loss:
L = (L_0 + L_1) / 2 = (0.685 + 0.675) / 2 = 0.680
```

### Summary of Forward Propagation

```
┌─────────────────────────────────────────────────────────┐
│ Forward Propagation Algorithm                           │
├─────────────────────────────────────────────────────────┤
│ Input:                                                  │
│   - Sequence X = [x_0, x_1, ..., x_{T-1}]             │
│   - Weights: W_xh, W_hh, W_hy, b_h, b_y               │
│                                                         │
│ Process:                                                │
│   1. Initialize h_{-1} = 0                             │
│   2. For each time step t:                             │
│      a. z_t = W_hh·h_{t-1} + W_xh·x_t + b_h           │
│      b. h_t = tanh(z_t)                                │
│      c. o_t = W_hy·h_t + b_y                           │
│      d. y_t = softmax(o_t)                             │
│      e. L_t = -Σ ŷ_{t,i} log(y_{t,i})                 │
│                                                         │
│ Output:                                                 │
│   - Hidden states: H = [h_0, h_1, ..., h_{T-1}]       │
│   - Predictions: Y = [y_0, y_1, ..., y_{T-1}]         │
│   - Total loss: L = (1/T) Σ L_t                        │
└─────────────────────────────────────────────────────────┘
```

---

## Backward Propagation Through Time (BPTT)

### Overview

**Backpropagation Through Time (BPTT)** is the algorithm used to train RNNs. It's an extension of standard backpropagation that accounts for the temporal dependencies in recurrent networks.

**Key Challenge**: The hidden state at time $t$ depends on all previous hidden states, so gradients must flow backward through time.

```
Forward Flow:
h_0 → h_1 → h_2 → h_3 → ... → h_T

Backward Flow (BPTT):
∂L/∂h_0 ← ∂L/∂h_1 ← ∂L/∂h_2 ← ∂L/∂h_3 ← ... ← ∂L/∂h_T
```

### The Chain Rule Through Time

Since each hidden state depends on the previous one:
$$h_t = f(h_{t-1}, x_t)$$

The gradient of the loss with respect to $h_t$ must account for:
1. **Direct contribution**: How $h_t$ affects the current output $y_t$
2. **Future contribution**: How $h_t$ affects all future hidden states and outputs

```
Gradient flow visualization:

             ∂L/∂y_t        (direct loss gradient)
                │
                ▼
             ∂L/∂h_t  ◄────  ∂L/∂h_{t+1}  (gradient from future)
                │
        ┌───────┴───────┐
        ▼               ▼
    ∂L/∂W_hh        ∂L/∂W_xh
    ∂L/∂b_h
```

### Mathematical Formulation

#### 1. Output Layer Gradients

**Loss Function** (cross-entropy):
$$L_t = -\sum_{i=1}^{n_y} \hat{y}_{t,i} \log(y_{t,i})$$

**Gradient with respect to output scores** (before softmax):

**Plain English**: The gradient of the loss with respect to the pre-activation output is simply the difference between prediction and target.

$$\frac{\partial L_t}{\partial o_t} = y_t - \hat{y}_t$$

**Complete Legend**:
- $o_t$ = Pre-activation output at time $t$ (before softmax)
- $y_t$ = Predicted probability distribution (after softmax)
- $\hat{y}_t$ = True target (one-hot encoded for classification)

**Derivation**: For softmax + cross-entropy, this simplifies to $y_t - \hat{y}_t$

#### 2. Output Weight Gradients

**Plain English**: The gradient for output weights is the outer product of the error signal and the hidden state.

$$\frac{\partial L_t}{\partial W_{hy}} = \frac{\partial L_t}{\partial o_t} \cdot h_t^T = (y_t - \hat{y}_t) h_t^T$$

**Dimensions**:
- $\frac{\partial L_t}{\partial W_{hy}}$: $(n_y \times n_h)$
- $(y_t - \hat{y}_t)$: $(n_y \times 1)$
- $h_t^T$: $(1 \times n_h)$

**Output Bias Gradient**:
$$\frac{\partial L_t}{\partial b_y} = y_t - \hat{y}_t$$

#### 3. Hidden State Gradients

**Plain English**: The gradient for the hidden state comes from two sources:
1. The direct contribution to the current output
2. The contribution to the next hidden state (gradient flowing back from $h_{t+1}$)

$$\frac{\partial L}{\partial h_t} = \frac{\partial L_t}{\partial h_t} + \frac{\partial L}{\partial h_{t+1}} \frac{\partial h_{t+1}}{\partial h_t}$$

Breaking it down:

**Component 1: Direct gradient from output**
$$\frac{\partial L_t}{\partial h_t} = W_{hy}^T \frac{\partial L_t}{\partial o_t} = W_{hy}^T (y_t - \hat{y}_t)$$

**Component 2: Gradient from future**
$$\frac{\partial L}{\partial h_{t+1}} \frac{\partial h_{t+1}}{\partial h_t}$$

**Hidden state pre-activation gradient**:

**Plain English**: To get gradients for the weights, we need to account for the tanh activation.

$$\frac{\partial L}{\partial z_t} = \frac{\partial L}{\partial h_t} \odot (1 - h_t^2)$$

Where:
- $\odot$ denotes element-wise multiplication (Hadamard product)
- $(1 - h_t^2)$ is the derivative of $\tanh(z_t)$ evaluated at $h_t$
- $z_t = W_{hh} h_{t-1} + W_{xh} x_t + b_h$ is the pre-activation

**Complete Legend**:
- $\frac{\partial L}{\partial z_t}$ = Gradient of loss w.r.t. pre-activation hidden state
- $(1 - h_t^2)$ = Derivative of tanh (since $\frac{d}{dz}\tanh(z) = 1 - \tanh^2(z)$)

#### 4. Weight Gradients

**Recurrent Weight Gradient**:

**Plain English**: The gradient for recurrent weights accumulates contributions from all time steps.

$$\frac{\partial L}{\partial W_{hh}} = \sum_{t=0}^{T-1} \frac{\partial L_t}{\partial z_t} h_{t-1}^T$$

**Input Weight Gradient**:

**Plain English**: Similarly, the gradient for input weights accumulates over time.

$$\frac{\partial L}{\partial W_{xh}} = \sum_{t=0}^{T-1} \frac{\partial L_t}{\partial z_t} x_t^T$$

**Bias Gradient**:
$$\frac{\partial L}{\partial b_h} = \sum_{t=0}^{T-1} \frac{\partial L_t}{\partial z_t}$$

**Complete Legend**:
- $\sum_{t=0}^{T-1}$ = Sum over all time steps (accumulating gradients)
- Each term is an outer product of the gradient and the input

### BPTT Algorithm

```
┌────────────────────────────────────────────────────────┐
│ Backpropagation Through Time Algorithm                 │
├────────────────────────────────────────────────────────┤
│ Step 1: Forward Pass (store all intermediate values)   │
│   - Compute and store: h_0, h_1, ..., h_{T-1}          │
│   - Compute and store: y_0, y_1, ..., y_{T-1}          │
│   - Compute loss: L = Σ L_t                            │
│                                                        │
│ Step 2: Backward Pass (from t=T-1 to t=0)              │
│   - Initialize: ∂L/∂h_T = 0                            │
│   - For t = T-1 down to 0:                             │
│     a. Compute ∂L_t/∂o_t = y_t - ŷ_t                   │
│     b. Compute ∂L_t/∂W_hy, ∂L_t/∂b_y                   │
│     c. Compute ∂L_t/∂h_t from output                   │
│     d. Add gradient from future: ∂L/∂h_t += ∂L/∂h_{t+1}│
│     e. Compute ∂L/∂z_t using tanh derivative           │
│     f. Compute ∂L/∂W_hh, ∂L/∂W_xh, ∂L/∂b_h             │
│     g. Compute ∂L/∂h_{t-1} for next iteration          │
│                                                        │
│ Step 3: Update Weights                                 │
│   - W_hy := W_hy - α·∂L/∂W_hy                          │
│   - W_hh := W_hh - α·∂L/∂W_hh                          │
│   - W_xh := W_xh - α·∂L/∂W_xh                          │
│   - b_y  := b_y - α·∂L/∂b_y                            │
│   - b_h  := b_h - α·∂L/∂b_h                            │
└────────────────────────────────────────────────────────┘
```

### Numerical Example of BPTT

Let's continue with our previous example and compute gradients. We'll use a shorter sequence for clarity.

**Setup** (from forward pass):
```
Sequence length: T = 2
Target: ŷ_1 = [1, 0] (positive sentiment at final step)

From forward pass, we have:
h_0 = [0.197, 0.462, 0.291, 0.380]ᵀ
h_1 = [0.549, 0.463, 0.420, 0.636]ᵀ
y_0 = [0.504, 0.496]ᵀ
y_1 = [0.509, 0.491]ᵀ

Learning rate: α = 0.01
```

### Step-by-Step BPTT Visualizations

To understand BPTT, we'll visualize how gradients flow backward through the network at each time step. We'll trace the gradients from the loss back to all parameters.

#### Complete Network Topology (T=2 time steps)

**Understanding the Complete Network Topology:**

This diagram shows the complete computational graph of an RNN unrolled over 2 time steps (t=0 and t=1). It visualizes both the forward propagation (computing outputs) and backward propagation (computing gradients) in a single view. Understanding this diagram is crucial because it reveals how information and gradients flow through the entire network.

**What You're Looking At:**

The network has three main layers at each time step:
1. **Input Layer**: Receives the input vector [`x_t`](0. RNNs.md:161) at each time step
2. **Hidden Layer**: Maintains the hidden state [`h_t`] that carries information through time
3. **Output Layer**: Produces predictions [`y_t`] and computes loss [`L_t`]

**Key Components and Connections:**

1. **Input-to-Hidden Connections** (`W_xh`): Transform current input [`x_t`] to hidden space
2. **Hidden-to-Hidden Connections** (`W_hh`): Carry information from previous time step `h_{t-1}` to current
3. **Hidden-to-Output Connections** (`W_hy`): Transform hidden state to output predictions
4. **Activation Functions**:
   - `tanh` squashes hidden state values to [-1, 1]
   - `softmax` converts output scores to probabilities

**Forward Propagation Flow (Left to Right):**

Information flows forward through time in this sequence:
1. At t=0: Input `x_0` enters → combines with initial hidden state → produces `h_0` → generates output `y_0`
2. At t=1: Input `x_1` enters → combines with previous `h_0` via `W_hh` → produces `h_1` → generates output `y_1`
3. The hidden state flows horizontally through time, carrying memory from past to future

**Backward Propagation Flow (Right to Left):**

Gradients flow backward through time during training (BPTT):
1. Starts at the final loss `L_1` at t=1
2. Flows backward through `softmax`, output layer, and `tanh` at t=1
3. Splits into two paths:
   - **Vertical path**: Updates `W_hy` and `W_xh` at t=1
   - **Horizontal path**: Flows backward through `W_hh` to t=0 (temporal gradient flow)
4. At t=0, gradients accumulate from both:
   - Local loss `L_0` (if computed)
   - Future time steps via `W_hh` (temporal contribution)

**Gradient Accumulation:**

The weight matrices `W_xh`, `W_hh`, `W_hy` are shared across all time steps. Their gradients accumulate contributions from every time step:
- `∂L/∂W_xh` = sum of gradients from t=0 and t=1
- `∂L/∂W_hh` = sum of gradients from t=0 and t=1
- `∂L/∂W_hy` = sum of gradients from t=0 and t=1

**What to Look For in the Diagram:**

- **Horizontal arrows**: Show temporal dependencies (how past affects future)
- **Vertical arrows**: Show layer-to-layer transformations at each time step
- **Transposed weights** (e.g., `W_hh^T`): Indicate backward gradient flow
- **Derivative notations** (e.g., tanh'): Mark where activation derivatives are applied
- **Gradient accumulation points**: Where gradients from multiple sources combine

```
╔════════════════════════════════════════════════════════════════════════════╗
║                    FORWARD PROPAGATION (Left → Right)                      ║
╚════════════════════════════════════════════════════════════════════════════╝

         TIME STEP t=0                           TIME STEP t=1
         ═════════════                           ═════════════

Input:       x_0                                       x_1
              │                                         │
              │ Multiply by W_xh                        │ Multiply by W_xh
              ↓                                         ↓
           ┌──────┐                                  ┌──────┐
   h_{-1}  │      │              h_0 carries         │      │
  (zeros)→ │  +   │◄────────── memory from ───────── │  +   │
           │ sum  │            previous step         │ sum  │
           └───┬──┘            (via W_hh)            └───┬──┘
               │                    ↑                    │
               │ Apply tanh         │                    │ Apply tanh
               ↓                    │                    ↓
Hidden:       h_0 ───────────────────                   h_1
           [0.197]             Recurrent             [0.549]
           [0.462]             Connection            [0.463]
           [0.291]             (W_hh)                [0.420]
           [0.380]                                   [0.636]
               │                                        │
               │ Multiply by W_hy                       │ Multiply by W_hy
               ↓                                        ↓
Pre-output:   o_0                                      o_1
               │                                        │
               │ Apply softmax                          │ Apply softmax
               ↓                                        ↓
Output:       y_0                                      y_1
           [0.504]                                  [0.509]
           [0.496]                                  [0.491]
               │                                        │
               │ Compute loss                           │ Compute loss
               ↓                                        ↓
Loss:         L_0                                      L_1
           (0.685)                                  (0.675)


╔════════════════════════════════════════════════════════════════════════════╗
║                   BACKWARD PROPAGATION (Right → Left)                      ║
╚════════════════════════════════════════════════════════════════════════════╝

         TIME STEP t=0                           TIME STEP t=1
         ═════════════                           ═════════════

Gradient:    ∂L/∂x_0                                 ∂L/∂x_1
              ↑                                        ↑
              │ Backprop via W_xh^T                    │ Backprop via W_xh^T
              │                                        │
           ┌──────┐                                   For ┌──────┐
           │tanh' │             Gradient flow        │tanh' │
           │ (1-h²)│◄────────── through time ────────│ (1-h²)│
           └───↑──┘            (via W_hh^T)          └───↑──┘
               │                    ↑                    │
               │                    │                    │
Hidden:    ∂L/∂h_0 ◄──────────────────              ∂L/∂h_1
          [-0.045]              Temporal            [-0.050]
          [-0.058]              Gradient            [-0.049]
          [ 0.168]              Flow                [ 0.148]
          [-0.039]              (W_hh^T)            [-0.049]
               ↑                                        ↑
               │ Backprop via W_hy^T                    │ Backprop via W_hy^T
               │                                        │
           ∂L/∂o_0                                  ∂L/∂o_1
               ↑                                        ↑
               │ Softmax derivative                     │ Softmax derivative
               │                                        │
          ∂L/∂y_0                                   ∂L/∂y_1
               ↑                                        ↑
               │ Loss gradient                          │ Loss gradient
               │                                        │
             ∂L_0                                     ∂L_1


╔════════════════════════════════════════════════════════════════════════════╗
║                        GRADIENT ACCUMULATION                               ║
╚════════════════════════════════════════════════════════════════════════════╝

Weight matrices are SHARED across time steps, so gradients ACCUMULATE:

    ∂L/∂W_xh  =  [contribution from t=0]  +  [contribution from t=1]
                 (∂L/∂z_0 · x_0^T)           (∂L/∂z_1 · x_1^T)

    ∂L/∂W_hh  =  [contribution from t=0]  +  [contribution from t=1]
                 (∂L/∂z_0 · h_{-1}^T)        (∂L/∂z_1 · h_0^T)

    ∂L/∂W_hy  =  [contribution from t=0]  +  [contribution from t=1]
                 (∂L_0/∂o_0 · h_0^T)         (∂L_1/∂o_1 · h_1^T)

Final weight update: W_new = W_old - learning_rate × accumulated_gradient


╔════════════════════════════════════════════════════════════════════════════╗
║                           KEY INSIGHTS                                     ║
╚════════════════════════════════════════════════════════════════════════════╝

1. TEMPORAL DEPENDENCY: Hidden state h_t depends on ALL previous inputs
   h_1 contains information from both x_0 AND x_1

2. GRADIENT FLOW PATHS: Gradients at t=0 come from TWO sources:
   - Direct: from its own output loss L_0
   - Temporal: from future time step t=1 via W_hh^T

3. SHARED PARAMETERS: W_xh, W_hh, W_hy are the SAME at t=0 and t=1
   → Enables processing variable-length sequences
   → Requires accumulating gradients across all time steps

4. VANISHING GRADIENT RISK: For long sequences (T >> 2), gradients
   flow through W_hh^T many times, potentially vanishing or exploding
```

#### Backward Pass: Time Step t=1

Now let's trace the gradient flow step-by-step with detailed visualizations.

---

##### **STEP 1: Output Gradient Computation at t=1**

**Visualization: Gradient Flow from Loss to Output Layer**

```
═══════════════════════════════════════════════════════════════════════
STEP 1: Computing ∂L_1/∂o_1 (Output Error Signal)
═══════════════════════════════════════════════════════════════════════

At time t=1:

                    Target         Prediction
                    ŷ_1 = [1]      y_1 = [0.509]
                           [0]            [0.491]
                            │              │
                            └──────┬───────┘
                                   │ (subtract)
                                   ▼
                         Error = y_1 - ŷ_1
                                   │
                         ┌─────────┴─────────┐
                         │  [-0.491]         │  ← Negative: predicted too low
                         │  [ 0.491]         │  ← Positive: predicted too high
                         └───────────────────┘
                                   │
                                   │ This is ∂L_1/∂o_1
                                   ▼
                    [Ready to backpropagate]


Gradient Flow Diagram at t=1:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

              Loss L_1 = 0.675
                    │
                    │ ∂L_1/∂L_1 = 1.0
                    ▼
              ┌───────────┐
              │ Softmax + │
              │   CE Loss │
              └─────┬─────┘
                    │ Chain rule simplifies!
                    │ ∂L_1/∂o_1 = y_1 - ŷ_1
                    ▼
          ┌──────────────────┐
          │   Output Error   │
          │  ∂L_1/∂o_1       │
          │  = [-0.491]      │
          │    [ 0.491]      │
          └──────────────────┘
                    │
                    │ Ready to flow backward to:
                    │ 1. Output weights W_hy
                    │ 2. Hidden state h_1
                    ▼

Key Insight: For softmax + cross-entropy, the gradient simplifies to
just the difference between prediction and target!
```

**Mathematical Formula:**
$$\frac{\partial L_1}{\partial o_1} = y_1 - \hat{y}_1 = \begin{bmatrix} 0.509 \\ 0.491 \end{bmatrix} - \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} -0.491 \\ 0.491 \end{bmatrix}$$

---

**Step 1: Compute output gradient**
```
∂L_1/∂o_1 = y_1 - ŷ_1 = [0.509] - [1] = [-0.491]
                         [0.491]   [0]   [ 0.491]
```

**Step 2: Compute ∂L_1/∂W_hy**
```
∂L_1/∂W_hy = (∂L_1/∂o_1) · h_1ᵀ

= [-0.491] × [0.549  0.463  0.420  0.636]
  [ 0.491]

= [-0.491×0.549  -0.491×0.463  -0.491×0.420  -0.491×0.636]
  [ 0.491×0.549   0.491×0.463   0.491×0.420   0.491×0.636]

= [-0.270  -0.227  -0.206  -0.312]
  [ 0.270   0.227   0.206   0.312]
```

**Step 3: Compute ∂L_1/∂b_y**
```
∂L_1/∂b_y = ∂L_1/∂o_1 = [-0.491]
                         [ 0.491]
```

---

##### **STEP 2: Hidden State Gradient at t=1**

**Visualization: Gradient Flow from Output to Hidden State**

```
═══════════════════════════════════════════════════════════════════════
STEP 2: Computing ∂L_1/∂h_1 (Backprop to Hidden State)
═══════════════════════════════════════════════════════════════════════

Gradient flows from output layer back to hidden state through W_hy:

              ∂L_1/∂o_1 = [-0.491]
                           [ 0.491]
                               │
                               │ Multiply by W_hy^T
                               ▼
           ┌──────────────────────────────────┐
           │     W_hy^T (transposed)          │
           │                                  │
           │  [0.5  0.4]   ← from 1st output  │
           │  [0.3  0.2]   ← from 2nd output  │
           │  [0.2  0.5]                      │
           │  [0.4  0.3]                      │
           └────────┬─────────────────────────┘
                    │ Matrix multiplication
                    ▼
          ┌─────────────────────┐
          │   ∂L_1/∂h_1         │
          │   = W_hy^T · ∂L_1/∂o_1
          │                     │
          │   = [-0.050]  ← Hidden unit 1
          │     [-0.049]  ← Hidden unit 2
          │     [ 0.148]  ← Hidden unit 3
          │     [-0.049]  ← Hidden unit 4
          └─────────────────────┘
                    │
                    │ Note: At t=1 (final timestep),
                    │ there's NO gradient from future!
                    │
                    ▼
              Total ∂L/∂h_1 = ∂L_1/∂h_1 only


Detailed Gradient Flow at t=1:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

                        L_1
                         │
                         ▼
        ┌────────────────────────────────┐
        │      Output Layer (t=1)        │
        │                                │
        │  o_1 = W_hy · h_1 + b_y       │
        │  y_1 = softmax(o_1)           │
        └────────┬───────────────────────┘
                 │
                 │ ∂L_1/∂o_1
                 │ = [-0.491, 0.491]
                 │
     ┌───────────┴────────────┐
     │                        │
     ▼                        ▼
∂L_1/∂W_hy              ∂L_1/∂h_1
(compute later)          = W_hy^T · ∂L_1/∂o_1
     │                        │
     │                        ▼
     │              ┌──────────────────┐
     │              │  Hidden State    │
     │              │  h_1 = [0.549]   │
     │              │        [0.463]   │
     │              │        [0.420]   │
     │              │        [0.636]   │
     │              └──────────────────┘
     │                        │
     │                        │ Gradient will flow:
     │                        │ 1. To weights W_hh, W_xh
     │                        │ 2. Backward in time to h_0
     ▼                        ▼
```

**Mathematical Formula:**
$$\frac{\partial L_1}{\partial h_1} = W_{hy}^T \cdot \frac{\partial L_1}{\partial o_1} = \begin{bmatrix} 0.5 & 0.4 \\ 0.3 & 0.2 \\ 0.2 & 0.5 \\ 0.4 & 0.3 \end{bmatrix} \cdot \begin{bmatrix} -0.491 \\ 0.491 \end{bmatrix} = \begin{bmatrix} -0.050 \\ -0.049 \\ 0.148 \\ -0.049 \end{bmatrix}$$

---

**Step 4: Compute ∂L_1/∂h_1 (direct contribution)**
```
∂L_1/∂h_1 = W_hyᵀ · (∂L_1/∂o_1)

= [0.5  0.4]ᵀ × [-0.491]
  [0.3  0.2]    [ 0.491]
  [0.2  0.5]
  [0.4  0.3]

= [0.5×(-0.491) + 0.4×0.491]  = [-0.246 + 0.196]   = [-0.050]
  [0.3×(-0.491) + 0.2×0.491]    [-0.147 + 0.098]     [-0.049]
  [0.2×(-0.491) + 0.5×0.491]    [-0.098 + 0.246]     [ 0.148]
  [0.4×(-0.491) + 0.3×0.491]    [-0.196 + 0.147]     [-0.049]
```

Note: At t=1 (last time step), there's no gradient from future, so:
```
∂L/∂h_1 = ∂L_1/∂h_1 = [-0.050, -0.049, 0.148, -0.049]ᵀ
```

**Step 5: Compute ∂L/∂z_1 (account for tanh)**
```
∂L/∂z_1 = ∂L/∂h_1 ⊙ (1 - h_1²)

First, compute (1 - h_1²):
h_1² = [0.549²]  = [0.301]
       [0.463²]    [0.214]
       [0.420²]    [0.176]
       [0.636²]    [0.404]

1 - h_1² = [0.699]
           [0.786]
           [0.824]
           [0.596]

∂L/∂z_1 = [-0.050] ⊙ [0.699] = [-0.050 × 0.699] = [-0.035]
          [-0.049]   [0.786]   [-0.049 × 0.786]   [-0.039]
          [ 0.148]   [0.824]   [ 0.148 × 0.824]   [ 0.122]
          [-0.049]   [0.596]   [-0.049 × 0.596]   [-0.029]
```

**Step 6: Compute ∂L_1/∂W_hh**
```
∂L_1/∂W_hh = (∂L/∂z_1) · h_0ᵀ

= [-0.035] × [0.197  0.462  0.291  0.380]
  [-0.039]
  [ 0.122]
  [-0.029]

= [-0.035×0.197  -0.035×0.462  -0.035×0.291  -0.035×0.380]
  [-0.039×0.197  -0.039×0.462  -0.039×0.291  -0.039×0.380]
  [ 0.122×0.197   0.122×0.462   0.122×0.291   0.122×0.380]
  [-0.029×0.197  -0.029×0.462  -0.029×0.291  -0.029×0.380]

= [-0.007  -0.016  -0.010  -0.013]
  [-0.008  -0.018  -0.011  -0.015]
  [ 0.024   0.056   0.036   0.046]
  [-0.006  -0.013  -0.008  -0.011]
```

**Step 7: Compute ∂L_1/∂W_xh**
```
∂L_1/∂W_xh = (∂L/∂z_1) · x_1ᵀ

x_1 = [0, 0, 1]ᵀ (one-hot for "movie")

= [-0.035] × [0  0  1]
  [-0.039]
  [ 0.122]
  [-0.029]

= [0  0  -0.035]
  [0  0  -0.039]
  [0  0   0.122]
  [0  0  -0.029]
```

---

##### **STEP 3: Backpropagation from t=1 to t=0**

**Visualization: Gradient Flow Backward Through Time via W_hh**

```
═══════════════════════════════════════════════════════════════════════
STEP 3: Computing ∂L/∂h_0 from future (Gradient flows through W_hh)
═══════════════════════════════════════════════════════════════════════

This is the KEY step where gradients flow backward through time!

Time t=1:                        Time t=0:
  
  ∂L/∂h_1 = [-0.050]
            [-0.049]
            [ 0.148]
            [-0.049]
              │
              │ First: Apply tanh derivative
              │ ∂L/∂z_1 = ∂L/∂h_1 ⊙ (1 - h_1²)
              ▼
  ∂L/∂z_1 = [-0.035]
            [-0.039]
            [ 0.122]
            [-0.029]
              │
              │
              │ Now backprop through W_hh
              │ (the recurrent connection!)
              ▼
     ┌─────────────────────┐
     │   W_hh^T            │
     │   (4×4 transposed)  │
     │                     │
     │  [0.1  0.3  0.2  0.1] ←─ contributes to h_0[0]
     │  [0.2  0.1  0.1  0.3] ←─ contributes to h_0[1]
     │  [0.1  0.2  0.3  0.2] ←─ contributes to h_0[2]
     │  [0.2  0.1  0.2  0.1] ←─ contributes to h_0[3]
     └──────────┬──────────┘
                │ Multiply: W_hh^T · ∂L/∂z_1
                ▼
    ∂L/∂h_0 (from future) = [ 0.005]
                             [-0.008]
                             [ 0.019]
                             [ 0.010]
                                │
                                │ This gradient travels
                                │ back in time from t=1 to t=0!
                                ▼
                        Arrives at h_0


Temporal Gradient Flow Diagram:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

        t=0                          t=1
         │                            │
         │                            │
    ┌────▼────┐         W_hh     ┌────▼────┐
    │   h_0   │ ──────────────► │   h_1   │
    │ [0.197] │                  │ [0.549] │
    │ [0.462] │  Forward Pass    │ [0.463] │
    │ [0.291] │ ═══════════════► │ [0.420] │
    │ [0.380] │                  │ [0.636] │
    └────▲────┘                  └────┬────┘
         │                            │
         │                            │ ∂L/∂h_1
         │         W_hh^T             │ = [-0.050]
         │  ◄──────────────────       │   [-0.049]
         │                            │   [ 0.148]
         │  Backward Pass             │   [-0.049]
         │  ◄═══════════════          │
         │                            ▼
    ∂L/∂h_0                       ∂L/∂z_1
    (from future)                 = [-0.035]
    = [ 0.005]                      [-0.039]
      [-0.008]                      [ 0.122]
      [ 0.019]                      [-0.029]
      [ 0.010]


Key Insight:
───────────
The gradient flows from t=1 back to t=0 through the recurrent weight
matrix W_hh^T. This is what makes RNN training "through time"!

For longer sequences (t=0 → t=1 → t=2 → ... → t=100), this gradient
gets multiplied by W_hh^T at each step, which can lead to vanishing
or exploding gradients!
```

**Mathematical Formula:**
$$\frac{\partial L}{\partial h_0}|_{\text{from future}} = W_{hh}^T \cdot \frac{\partial L}{\partial z_1}$$

---

**Step 8: Compute ∂L/∂h_0 (for next iteration)**
```
∂L/∂h_0 = W_hhᵀ · (∂L/∂z_1)

= [0.1  0.3  0.2  0.1]ᵀ × [-0.035]
  [0.2  0.1  0.1  0.3]    [-0.039]
  [0.1  0.2  0.3  0.2]    [ 0.122]
  [0.2  0.1  0.2  0.1]    [-0.029]

= [0.1×(-0.035) + 0.3×(-0.039) + 0.2×0.122 + 0.1×(-0.029)]
  [0.2×(-0.035) + 0.1×(-0.039) + 0.1×0.122 + 0.3×(-0.029)]
  [0.1×(-0.035) + 0.2×(-0.039) + 0.3×0.122 + 0.2×(-0.029)]
  [0.2×(-0.035) + 0.1×(-0.039) + 0.2×0.122 + 0.1×(-0.029)]

= [-0.004 - 0.012 + 0.024 - 0.003]   = [ 0.005]
  [-0.007 - 0.004 + 0.012 - 0.009]     [-0.008]
  [-0.004 - 0.008 + 0.037 - 0.006]     [ 0.019]
  [-0.007 - 0.004 + 0.024 - 0.003]     [ 0.010]
```

#### Backward Pass: Time Step t=0

**Step 1: Compute output gradient** (if we compute loss at t=0)
```
∂L_0/∂o_0 = y_0 - ŷ_0 = [0.504] - [1] = [-0.496]
                         [0.496]   [0]   [ 0.496]
```

**Step 2: Compute ∂L_0/∂h_0 (direct contribution)**
```
∂L_0/∂h_0 = W_hyᵀ · (∂L_0/∂o_0)

= [0.5  0.4]ᵀ × [-0.496]
  [0.3  0.2]    [ 0.496]
  [0.2  0.5]
  [0.4  0.3]

= [-0.248 + 0.198]   = [-0.050]
  [-0.149 + 0.099]     [-0.050]
  [-0.099 + 0.248]     [ 0.149]
  [-0.198 + 0.149]     [-0.049]
```

---

##### **STEP 4: Hidden State Gradient Accumulation at t=0**

**Visualization: Two Sources of Gradients Combine**

```
═══════════════════════════════════════════════════════════════════════
STEP 4: Computing Total ∂L/∂h_0 (Gradient Accumulation from Two Paths)
═══════════════════════════════════════════════════════════════════════

At t=0, the gradient comes from TWO sources:
1. Direct contribution from output y_0 (local gradient)
2. Contribution from future hidden state h_1 (temporal gradient)

Source 1: Direct from Output        Source 2: From Future
━━━━━━━━━━━━━━━━━━━━━━━━━━          ━━━━━━━━━━━━━━━━━━━━━

         L_0                              L_1
          │                                │
          ▼                                ▼
         o_0                              h_1
          │                                │
     ∂L_0/∂o_0                        ∂L/∂z_1
    = [-0.496]                       = [-0.035]
      [ 0.496]                         [-0.039]
          │                            [ 0.122]
          │ W_hy^T                     [-0.029]
          ▼                                │
    ∂L_0/∂h_0                             │ W_hh^T
   = [-0.050]                             ▼
     [-0.050]                    ∂L/∂h_0 (from future)
     [ 0.149]                    = [ 0.005]
     [-0.049]                      [-0.008]
          │                         [ 0.019]
          │                         [ 0.010]
          │                              │
          └──────────┬───────────────────┘
                     │ ADD THEM!
                     ▼
          ┌──────────────────────┐
          │  Total ∂L/∂h_0       │
          │  = ∂L_0/∂h_0 +       │
          │    ∂L/∂h_0(future)   │
          │                      │
          │  = [-0.050]  [0.005] │
          │    [-0.050]+ [-0.008]│
          │    [ 0.149]  [0.019] │
          │    [-0.049]  [0.010] │
          │                      │
          │  = [-0.045]          │
          │    [-0.058]          │
          │    [ 0.168]          │
          │    [-0.039]          │
          └──────────────────────┘
                     │
                     │ This total gradient accounts for:
                     │ - Current output error
                     │ - Future consequences
                     ▼


Complete Gradient Flow at t=0:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

                    Loss at t=0        Loss at t=1
                        L_0                 L_1
                         │                   │
                         ▼                   ▼
                    ┌────────┐         ┌────────┐
                    │  y_0   │         │  y_1   │
                    └───┬────┘         └───┬────┘
                        │                  │
                 W_hy^T │                  │ W_hy^T
                        │                  │
                        ▼                  ▼
          Path 1:  ∂L_0/∂h_0         ∂L_1/∂h_1
           (local) = [-0.050]        = [-0.050]
                     [-0.050]          [-0.049]
                     [ 0.149]          [ 0.148]
                     [-0.049]          [-0.049]
                        │                  │
                        │                  │ tanh'
                        │                  ▼
                        │             ∂L/∂z_1
                        │            = [-0.035]
                        │              [-0.039]
                        │              [ 0.122]
                        │              [-0.029]
                        │                  │
                        │                  │ W_hh^T
                        │                  │ (flows backward in time)
                        │                  ▼
                        │         Path 2: ∂L/∂h_0
                        │        (future) = [ 0.005]
                        │                   [-0.008]
                        │                   [ 0.019]
                        │                   [ 0.010]
                        │                   │
                        └────────┬──────────┘
                                 │ SUM
                                 ▼
                         Total ∂L/∂h_0
                         = [-0.045]
                           [-0.058]
                           [ 0.168]
                           [-0.039]


Key Insight:
───────────
This is the HEART of BPTT! Each hidden state receives gradients from:
1. Its own output (if it produces one)
2. All future time steps (through recurrent connections)

This accumulation happens at EVERY time step during backpropagation!
```

**Mathematical Formula:**
$$\frac{\partial L}{\partial h_0} = \underbrace{W_{hy}^T \cdot \frac{\partial L_0}{\partial o_0}}_{\text{direct from output}} + \underbrace{W_{hh}^T \cdot \frac{\partial L}{\partial z_1}}_{\text{from future}}$$

---

**Step 3: Add gradient from future**
```
∂L/∂h_0 = ∂L_0/∂h_0 + (gradient from h_1)

= [-0.050]   [ 0.005]   [-0.045]
  [-0.050] + [-0.008] = [-0.058]
  [ 0.149]   [ 0.019]   [ 0.168]
  [-0.049]   [ 0.010]   [-0.039]
```

**Step 4: Compute ∂L/∂z_0**
```
1 - h_0² = 1 - [0.197²]  = [0.961]
               [0.462²]    [0.787]
               [0.291²]    [0.915]
               [0.380²]    [0.856]

∂L/∂z_0 = ∂L/∂h_0 ⊙ (1 - h_0²)

= [-0.045]   [0.961]   [-0.043]
  [-0.058] ⊙ [0.787] = [-0.046]
  [ 0.168]   [0.915]   [ 0.154]
  [-0.039]   [0.856]   [-0.033]
```

**Step 5: Compute ∂L_0/∂W_xh**
```
x_0 = [1, 0, 0]ᵀ (one-hot for "good")

∂L_0/∂W_xh = (∂L/∂z_0) · x_0ᵀ

= [-0.043] × [1  0  0]
  [-0.046]
  [ 0.154]
  [-0.033]

= [-0.043  0  0]
  [-0.046  0  0]
  [ 0.154  0  0]
  [-0.033  0  0]
```

---

##### **STEP 5: Weight Gradient Accumulation Across Time Steps**

**Visualization: How Gradients from All Time Steps Combine**

```
═══════════════════════════════════════════════════════════════════════
STEP 5: Weight Gradient Accumulation (Final Step Before Update)
═══════════════════════════════════════════════════════════════════════

The weight matrices (W_hh, W_xh, W_hy) are SHARED across all time steps.
Therefore, their gradients must be ACCUMULATED from all time steps!


Example: Accumulating ∂L/∂W_xh
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Time t=0:                          Time t=1:
─────────                          ─────────

x_0 = [1]                          x_1 = [0]
      [0]  ← "good"                      [0]  ← "movie"
      [0]                                [1]
      ↓                                  ↓
  ┌────────┐                         ┌────────┐
  │  W_xh  │                         │  W_xh  │
  │ (same) │                         │ (same) │
  └────┬───┘                         └────┬───┘
       ↓                                  ↓
    Forward                            Forward
       ↑                                  ↑
       │                                  │
  ∂L/∂z_0                            ∂L/∂z_1
  = [-0.043]                         = [-0.035]
    [-0.046]                           [-0.039]
    [ 0.154]                           [ 0.122]
    [-0.033]                           [-0.029]
       │                                  │
       │ Compute                          │ Compute
       │ ∂L_0/∂W_xh                       │ ∂L_1/∂W_xh
       │ = ∂L/∂z_0 · x_0^T                │ = ∂L/∂z_1 · x_1^T
       ▼                                  ▼

Contribution from t=0:          Contribution from t=1:
─────────────────────          ─────────────────────

∂L_0/∂W_xh =                   ∂L_1/∂W_xh =
[-0.043  0  0]                 [0  0  -0.035]
[-0.046  0  0]                 [0  0  -0.039]
[ 0.154  0  0]                 [0  0   0.122]
[-0.033  0  0]                 [0  0  -0.029]
      │                              │
      └──────────┬───────────────────┘
                 │
                 │ ACCUMULATE (SUM)
                 ▼
         ┌──────────────────┐
         │  Total ∂L/∂W_xh  │
         │                  │
         │ [-0.043  0  -0.035]
         │ [-0.046  0  -0.039]
         │ [ 0.154  0   0.122]
         │ [-0.033  0  -0.029]
         └──────────────────┘
                 │
                 │ Used for weight update:
                 │ W_xh := W_xh - α · ∂L/∂W_xh
                 ▼


Visualizing Accumulation for ALL Weight Matrices:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

┌─────────────────────────────────────────────────────────────┐
│                    Time t=0          Time t=1               │
│                    ───────           ───────                │
│                       │                 │                   │
│  W_xh Gradient:       │                 │                   │
│  ∂L_0/∂W_xh ←─────────┘                 │                   │
│  = ∂L/∂z_0 · x_0^T                      │                   │
│                                         │                   │
│  ∂L_1/∂W_xh ←───────────────────────────┘                   │
│  = ∂L/∂z_1 · x_1^T                                          │
│                                                             │
│  Total: ∂L/∂W_xh = ∂L_0/∂W_xh + ∂L_1/∂W_xh                  │
│  ═══════════════════════════════════════════════════        │
│                       │                 │                   │
│  W_hh Gradient:       │                 │                   │
│  ∂L_0/∂W_hh ←─────────┘                 │                   │
│  = ∂L/∂z_0 · h_{-1}^T                   │                   │
│                                         │                   │
│  ∂L_1/∂W_hh ←───────────────────────────┘                   │
│  = ∂L/∂z_1 · h_0^T                                          │
│                                                             │
│  Total: ∂L/∂W_hh = ∂L_0/∂W_hh + ∂L_1/∂W_hh                  │
│  ═══════════════════════════════════════════════════        │
│                       │                 │                   │
│  W_hy Gradient:       │                 │                   │
│  ∂L_0/∂W_hy ←─────────┘                 │                   │
│  = ∂L_0/∂o_0 · h_0^T                    │                   │
│                                         │                   │
│  ∂L_1/∂W_hy ←───────────────────────────┘                   │
│  = ∂L_1/∂o_1 · h_1^T                                        │
│                                                             │
│  Total: ∂L/∂W_hy = ∂L_0/∂W_hy + ∂L_1/∂W_hy                  │
│  ═══════════════════════════════════════════════════        │
└─────────────────────────────────────────────────────────────┘


For Longer Sequences (T time steps):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

         t=0    t=1    t=2    ...    t=T-1
          │      │      │      │      │
          ▼      ▼      ▼      ▼      ▼
         ∇W_0   ∇W_1   ∇W_2   ...   ∇W_{T-1}
          │      │      │      │      │
          └──────┴──────┴──────┴──────┘
                     │
                     │ ACCUMULATE
                     ▼
              Total ∇W = Σ ∇W_t
                         t=0 to T-1
                     │
                     │ Apply gradient descent
                     ▼
              W_new = W_old - α · ∇W


Complete BPTT Gradient Accumulation Formula:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

∂L/∂W_xh = Σ (∂L/∂z_t · x_t^T)        Sum over all t
           t=0 to T-1

∂L/∂W_hh = Σ (∂L/∂z_t · h_{t-1}^T)    Sum over all t
           t=0 to T-1

∂L/∂W_hy = Σ (∂L_t/∂o_t · h_t^T)      Sum over all t
           t=0 to T-1

∂L/∂b_h = Σ (∂L/∂z_t)                  Sum over all t
          t=0 to T-1

∂L/∂b_y = Σ (∂L_t/∂o_t)                Sum over all t
          t=0 to T-1


Key Insights:
─────────────
1. Weight matrices are SHARED across time → gradients ACCUMULATE
2. Each time step contributes its own gradient to the total
3. The final gradient is the SUM of all per-timestep gradients
4. This is why BPTT can be expensive for long sequences!
```

---

#### Accumulate Gradients

**Total gradient for W_xh**:
```
∂L/∂W_xh = ∂L_0/∂W_xh + ∂L_1/∂W_xh

= [-0.043  0  0]     [0  0  -0.035]     [-0.043  0  -0.035]
  [-0.046  0  0]  +  [0  0  -0.039]  =  [-0.046  0  -0.039]
  [ 0.154  0  0]     [0  0   0.122]     [ 0.154  0   0.122]
  [-0.033  0  0]     [0  0  -0.029]     [-0.033  0  -0.029]
```

### Weight Updates

Using gradient descent with learning rate α = 0.01:

**Update W_hy**:
```
W_hy_new = W_hy - α · ∂L/∂W_hy

= [0.5  0.3  0.2  0.4]     [-0.270  -0.227  -0.206  -0.312]
  [0.4  0.2  0.5  0.3]  - 0.01 × [ 0.270   0.227   0.206   0.312]

= [0.5  0.3  0.2  0.4]     [0.0027  0.0023  0.0021  0.0031]
  [0.4  0.2  0.5  0.3]  +  [-0.0027 -0.0023 -0.0021 -0.0031]

= [0.5027  0.3023  0.2021  0.4031]
  [0.3973  0.1977  0.4979  0.2969]
```

**Update W_hh** (using gradient from t=1 only for this example):
```
W_hh_new = W_hh - α · ∂L_1/∂W_hh

= (similar calculation with small updates)
```

### Vanishing and Exploding Gradient Problem

#### The Problem

**Plain English**: When gradients flow backward through many time steps, they can either:
1. **Vanish**: Become extremely small (→ 0), making it impossible to learn long-term dependencies
2. **Explode**: Become extremely large (→ ∞), causing unstable training

#### Mathematical Explanation

Recall that the gradient flows back through the recurrent connections:

$$\frac{\partial h_t}{\partial h_{t-1}} = W_{hh}^T \cdot \text{diag}(1 - h_{t-1}^2)$$

When computing $\frac{\partial L}{\partial h_0}$ (gradient at the first time step), we need to multiply these Jacobians:

$$\frac{\partial L}{\partial h_0} = \frac{\partial L}{\partial h_T} \prod_{t=1}^{T} \frac{\partial h_t}{\partial h_{t-1}}$$

```
Gradient from t=T back to t=0:

∂L/∂h_0 = ∂L/∂h_T × (∂h_T/∂h_{T-1}) × (∂h_{T-1}/∂h_{T-2}) × ... × (∂h_1/∂h_0)
          └────────┴───────────────┴────────────────────┴────────┴──────────┘
                         Product of T matrices
```

**Why gradients vanish**:
- If eigenvalues of $W_{hh} < 1$, repeated multiplication makes gradients exponentially smaller
- $\tanh$ derivative $(1 - h_t^2) \leq 1$ further reduces gradients
- Over T time steps: gradient ≈ $(\lambda_{max})^T$ where $\lambda_{max}$ is largest eigenvalue

**Example of vanishing**:
```
Let's say λ_max = 0.9 and T = 100 time steps

Gradient scaling factor: (0.9)^100 ≈ 0.0000266

Original gradient: 1.0
After 100 steps: 0.0000266 (effectively zero!)
```

**Why gradients explode**:
- If eigenvalues of $W_{hh} > 1$, repeated multiplication makes gradients exponentially larger
- Over T time steps: gradient ≈ $(\lambda_{max})^T$ where $\lambda_{max} > 1$

**Example of exploding**:
```
Let's say λ_max = 1.1 and T = 100 time steps

Gradient scaling factor: (1.1)^100 ≈ 13,780.6

Original gradient: 1.0
After 100 steps: 13,780.6 (explodes!)
```

#### Visualization of the Problem

```
Vanishing Gradient:
═══════════════════

Time:    t=0    t=1    t=2   ...   t=99   t=100
Gradient: ▓▓▓▓ → ▓▓▓ → ▓▓ → ... → ▓ → • (nearly 0)
          1.0    0.9    0.81       0.0001  0.00002

Information from t=0 doesn't reach the loss at t=100!


Exploding Gradient:
═══════════════════

Time:    t=0    t=1    t=2   ...   t=99      t=100
Gradient: ▓ → ▓▓ → ▓▓▓ → ... → ▓▓▓▓▓▓▓ → ∞
          1.0  1.1   1.21       1000      13781

Gradients become so large they cause numerical overflow!
```

#### Solutions

**1. Gradient Clipping** (for exploding gradients):
```python
if ||gradient|| > threshold:
    gradient = (threshold / ||gradient||) × gradient
```

**2. Better Initialization**:
- Initialize $W_{hh}$ to be orthogonal or have eigenvalues near 1
- Helps keep gradient magnitudes stable

**3. Advanced Architectures**:
- **LSTM** (Long Short-Term Memory): Uses gates to control information flow
- **GRU** (Gated Recurrent Unit): Simplified gating mechanism
- **Residual connections**: Add skip connections to bypass some layers

**4. Gradient Monitoring**:
```
During training, monitor:
- Average gradient magnitude across layers
- Maximum gradient value
- Gradient-to-parameter ratio
```

### Complete BPTT Summary

```
┌──────────────────────────────────────────────────────────┐
│ BPTT Key Equations Summary                               │
├──────────────────────────────────────────────────────────┤
│                                                          │
│ Forward Pass:                                            │
│   h_t = tanh(W_hh·h_{t-1} + W_xh·x_t + b_h)              │
│   y_t = softmax(W_hy·h_t + b_y)                          │
│   L_t = -Σ ŷ_{t,i} log(y_{t,i})                          │ 
│                                                          │
│ Backward Pass (for each t from T-1 to 0):                │
│   ∂L/∂o_t = y_t - ŷ_t                                    │
│   ∂L/∂W_hy += (y_t - ŷ_t)·h_tᵀ                           │
│   ∂L/∂h_t = W_hyᵀ·(y_t - ŷ_t) + ∂L/∂h_{t+1}·W_hhᵀ        │
│   ∂L/∂z_t = ∂L/∂h_t ⊙ (1 - h_t²)                        │
│   ∂L/∂W_hh += ∂L/∂z_t·h_{t-1}ᵀ                           │
│   ∂L/∂W_xh += ∂L/∂z_t·x_tᵀ                               │
│   ∂L/∂b_h += ∂L/∂z_t                                     │
│                                                          │
│ Challenges:                                              │
│   - Vanishing gradients: (λ < 1)^T → 0                   │
│   - Exploding gradients: (λ > 1)^T → ∞                   │
│   - Long-term dependencies hard to learn                 │
│                                                          │
│ Solutions:                                               │
│   - Gradient clipping                                    │
│   - Better initialization                                │
│   - Advanced architectures (LSTM, GRU)                   │
└──────────────────────────────────────────────────────────┘
```

---

## Summary
DO
**Recurrent Neural Networks (RNNs)** are powerful models for sequential data that maintain a hidden state to remember information across time steps. The key innovations are:

1. **Recurrent connections**: Hidden state loops back as input for next step
2. **Parameter sharing**: Same weights used across all time steps
3. **Variable length handling**: Can process sequences of any length

**Forward propagation** processes the sequence step by step:
```
h_t = tanh(W_hh·h_{t-1} + W_xh·x_t + b_h)
y_t = softmax(W_hy·h_t + b_y)
```

**Backpropagation Through Time (BPTT)** trains the network by:
- Unrolling the network across time
- Computing gradients backwards through all time steps
- Accumulating gradients for weight updates

**Challenges**:
- Vanishing/exploding gradients limit learning of long-term dependencies
- Solutions include gradient clipping and advanced architectures (LSTM, GRU)

RNNs form the foundation for understanding modern sequence models and have applications in language modeling, translation, time series prediction, and more!
