# Transformer Network

## Table of Contents
1. [Introduction to Transformers](#introduction-to-transformers)
2. [Word Embeddings](#word-embeddings)
3. [Positional Encoding](#positional-encoding)
4. [Self-Attention](#self-attention)
5. [Building the Encoder](#building-the-encoder)
6. [Parallel Computation](#parallel-computation)
7. [The Decoder and Decoder Self-Attention](#the-decoder-and-decoder-self-attention)
8. [Encoder-Decoder Attention](#encoder-decoder-attention)
9. [Decoding Numbers into Words](#decoding-numbers-into-words)
10. [Generating the Complete Translation](#generating-the-complete-translation)

---

## Introduction to Transformers

### What is a Transformer?

The **Transformer** is a neural network architecture that revolutionized natural language processing. Introduced in the 2017 paper "Attention Is All You Need," it processes entire sequences simultaneously using a mechanism called **self-attention**, rather than reading words one by one like previous models.

Today, Transformers power virtually all modern AI language systems: GPT, BERT, Claude, Llama, and many others.

### Our Running Example

Throughout this entire document, we will follow **one simple example** from start to finish:

```
╔══════════════════════════════════════════════════════════════════════╗
║                      OUR TRANSLATION TASK                            ║
╚══════════════════════════════════════════════════════════════════════╝

    English Input:    "I love you"
                           │
                           ▼
                    ┌─────────────┐
                    │ TRANSFORMER │
                    └─────────────┘
                           │
                           ▼
    French Output:    "Je t'aime"
```

By following this single sentence through every step, you will see exactly how a Transformer converts "I love you" into "Je t'aime."

### The Journey Ahead

Here is the complete path our sentence will take:

```
╔══════════════════════════════════════════════════════════════════════╗
║                    THE TRANSFORMER JOURNEY                           ║
╚══════════════════════════════════════════════════════════════════════╝

INPUT: "I love you"
         │
         │  STEP 1: Word Embeddings
         │  Convert words to vectors of numbers
         ▼
    ┌─────────────────────────────────────────┐
    │ "I"    → [0.2, 0.5, 0.1, 0.8]           │
    │ "love" → [0.9, 0.1, 0.7, 0.3]           │
    │ "you"  → [0.3, 0.8, 0.2, 0.6]           │
    └─────────────────────────────────────────┘
         │
         │  STEP 2: Positional Encoding
         │  Add position information
         ▼
    ┌─────────────────────────────────────────┐
    │ Position 0: "I"    + PE[0]              │
    │ Position 1: "love" + PE[1]              │
    │ Position 2: "you"  + PE[2]              │
    └─────────────────────────────────────────┘
         │
         │  STEP 3: Encoder (Self-Attention + FFN)
         │  Each word learns about other words
         ▼
    ┌─────────────────────────────────────────┐
    │ Encoder Output: 3 context-rich vectors  │
    │ (Understanding of entire input)         │
    └─────────────────────────────────────────┘
         │
         │  STEP 4: Decoder
         │  Generate output one word at a time
         ▼
    ┌─────────────────────────────────────────┐
    │ Step 4a: <START>           → "Je"       │
    │ Step 4b: <START> Je        → "t'"       │
    │ Step 4c: <START> Je t'     → "aime"     │
    │ Step 4d: <START> Je t'aime → <END>      │
    └─────────────────────────────────────────┘
         │
         ▼
OUTPUT: "Je t'aime"
```

Let's begin!

---

## Word Embeddings

### Why We Need Embeddings

Computers cannot process words directly—they only understand numbers. We need a way to convert words into numerical representations that capture their meaning.

**Word embeddings** solve this problem: they represent each word as a list of numbers (a vector).

```
╔══════════════════════════════════════════════════════════════════════╗
║                    WORDS TO NUMBERS                                  ║
╚══════════════════════════════════════════════════════════════════════╝

The Problem:
    Computer sees: "love"
    Computer thinks: ??? (cannot process text)

The Solution:
    Computer sees: "love" → [0.9, 0.1, 0.7, 0.3]
    Computer thinks: I can do math with these numbers! ✓
```

### What is an Embedding?

An embedding is a vector (list of numbers) that represents a word's meaning. Words with similar meanings have similar vectors.

```
╔══════════════════════════════════════════════════════════════════════╗
║                    EMBEDDING INTUITION                               ║
╚══════════════════════════════════════════════════════════════════════╝

Similar words → Similar vectors:

    "king"  → [0.9, 0.8, 0.2, 0.1]
    "queen" → [0.9, 0.7, 0.3, 0.1]    ← Very similar to "king"!
    
    "apple" → [0.1, 0.2, 0.9, 0.8]    ← Very different from "king"

The numbers encode meaning:
    • Maybe dimension 1 encodes "royalty"
    • Maybe dimension 3 encodes "food-related"
    • (In practice, dimensions don't have clear interpretations)
```

### Our Toy Vocabulary

For our example, we'll use a small vocabulary of 10 words with embedding dimension **d = 4**:

```
╔══════════════════════════════════════════════════════════════════════╗
║                    TOY VOCABULARY                                    ║
║                    (10 words, d = 4)                                 ║
╚══════════════════════════════════════════════════════════════════════╝

┌─────────┬────────┬──────────────────────────────┐
│  Word   │ Token  │     Embedding Vector         │
│         │   ID   │   (4 dimensions)             │
├─────────┼────────┼──────────────────────────────┤
│ <START> │   0    │  [0.1, 0.1, 0.1, 0.1]        │
│ I       │   1    │  [0.2, 0.5, 0.1, 0.8]        │
│ love    │   2    │  [0.9, 0.1, 0.7, 0.3]        │
│ you     │   3    │  [0.3, 0.8, 0.2, 0.6]        │  
│ Je      │   4    │  [0.2, 0.4, 0.2, 0.7]        │
│ t'      │   5    │  [0.4, 0.6, 0.3, 0.5]        │
│ aime    │   6    │  [0.8, 0.2, 0.6, 0.4]        │
│ hello   │   7    │  [0.5, 0.5, 0.5, 0.5]        │
│ world   │   8    │  [0.6, 0.3, 0.8, 0.2]        │
│ <END>   │   9    │  [0.0, 0.0, 0.0, 0.0]        │
└─────────┴────────┴──────────────────────────────┘

Note: "love" [0.9, 0.1, 0.7, 0.3] and "aime" [0.8, 0.2, 0.6, 0.4] 
are similar—they mean the same thing in different languages!
```

### The Embedding Matrix

All embeddings are stored in a matrix **E** of size (vocabulary_size × embedding_dimension):

$$E \in \mathbb{R}^{V \times d}$$

**Symbol Legend:**
- $E$ = Embedding matrix
- $V$ = Vocabulary size (10 in our example)
- $d$ = Embedding dimension (4 in our example)

```
╔══════════════════════════════════════════════════════════════════════╗
║                    EMBEDDING MATRIX E                                ║
║                    (10 × 4)                                          ║
╚══════════════════════════════════════════════════════════════════════╝

                    Dimension
                  d₀    d₁    d₂    d₃
            ┌──────┬──────┬──────┬──────┐
  <START> 0 │ 0.1  │ 0.1  │ 0.1  │ 0.1  │
            ├──────┼──────┼──────┼──────┤
    I     1 │ 0.2  │ 0.5  │ 0.1  │ 0.8  │  ← Row 1
            ├──────┼──────┼──────┼──────┤
    love  2 │ 0.9  │ 0.1  │ 0.7  │ 0.3  │  ← Row 2
            ├──────┼──────┼──────┼──────┤
    you   3 │ 0.3  │ 0.8  │ 0.2  │ 0.6  │  ← Row 3
Token       ├──────┼──────┼──────┼──────┤
  ID  Je  4 │ 0.2  │ 0.4  │ 0.2  │ 0.7  │
            ├──────┼──────┼──────┼──────┤
     t'   5 │ 0.4  │ 0.6  │ 0.3  │ 0.5  │
            ├──────┼──────┼──────┼──────┤
    aime  6 │ 0.8  │ 0.2  │ 0.6  │ 0.4  │
            ├──────┼──────┼──────┼──────┤
    hello 7 │ 0.5  │ 0.5  │ 0.5  │ 0.5  │
            ├──────┼──────┼──────┼──────┤
    world 8 │ 0.6  │ 0.3  │ 0.8  │ 0.2  │
            ├──────┼──────┼──────┼──────┤
    <END> 9 │ 0.0  │ 0.0  │ 0.0  │ 0.0  │
            └──────┴──────┴──────┴──────┘
```

### Step-by-Step: Embedding "I love you"

Now let's convert our input sentence into embeddings:

```
╔══════════════════════════════════════════════════════════════════════╗
║            STEP-BY-STEP: EMBEDDING "I love you"                      ║
╚══════════════════════════════════════════════════════════════════════╝

INPUT SENTENCE: "I love you"


STEP 1: Tokenization (words → token IDs)
────────────────────────────────────────

    "I"    → Token ID: 1
    "love" → Token ID: 2
    "you"  → Token ID: 3
    
    Result: [1, 2, 3]


STEP 2: Embedding Lookup (token IDs → vectors)
──────────────────────────────────────────────

Look up each token ID in the embedding matrix E:

    Token ID 1 ("I"):    E[1] = [0.2, 0.5, 0.1, 0.8]
    Token ID 2 ("love"): E[2] = [0.9, 0.1, 0.7, 0.3]
    Token ID 3 ("you"):  E[3] = [0.3, 0.8, 0.2, 0.6]


STEP 3: Create Input Matrix X
─────────────────────────────

Stack the embedding vectors into a matrix:

                      d₀    d₁    d₂    d₃
                  ┌──────┬──────┬──────┬──────┐
    Position 0: I │ 0.2  │ 0.5  │ 0.1  │ 0.8  │
                  ├──────┼──────┼──────┼──────┤
    Position 1:   │ 0.9  │ 0.1  │ 0.7  │ 0.3  │
           love   ├──────┼──────┼──────┼──────┤
    Position 2:   │ 0.3  │ 0.8  │ 0.2  │ 0.6  │
           you    └──────┴──────┴──────┴──────┘

    X ∈ ℝ^(3 × 4)  (3 words, 4 dimensions each)


RESULT:
───────

    "I love you"  →  X = ┌                    ┐
                         │ 0.2  0.5  0.1  0.8 │  ← "I"
                         │ 0.9  0.1  0.7  0.3 │  ← "love"
                         │ 0.3  0.8  0.2  0.6 │  ← "you"
                         └                    ┘

We now have 3 vectors of 4 numbers each representing our sentence!
```

### Key Insight

At this point, we have converted words to numbers, but there's a problem:

```
╔══════════════════════════════════════════════════════════════════════╗
║                    THE POSITION PROBLEM                              ║
╚══════════════════════════════════════════════════════════════════════╝

These two sentences have the SAME embeddings in different order:

    "I love you"  →  [0.2, 0.5, 0.1, 0.8]  ← "I"
                     [0.9, 0.1, 0.7, 0.3]  ← "love"
                     [0.3, 0.8, 0.2, 0.6]  ← "you"

    "you love I"  →  [0.3, 0.8, 0.2, 0.6]  ← "you"
                     [0.9, 0.1, 0.7, 0.3]  ← "love"
                     [0.2, 0.5, 0.1, 0.8]  ← "I"

The embeddings don't know WHERE each word is in the sentence!

"I love you" and "you love I" mean different things!
We need to add POSITION information...
```

---

## Positional Encoding

### The Problem: Word Order Matters

Embeddings alone don't capture word position. But in language, position is crucial:

- "Dog bites man" ≠ "Man bites dog"
- "I love you" ≠ "You love I"

We need to tell the model where each word appears in the sentence.

### The Solution: Add Position Information

**Positional Encoding** adds a unique "position signal" to each word's embedding. Each position (0, 1, 2, ...) gets its own vector, which is added to the word embedding.

```
╔══════════════════════════════════════════════════════════════════════╗
║                    POSITIONAL ENCODING CONCEPT                       ║
╚══════════════════════════════════════════════════════════════════════╝

Final Input = Word Embedding + Positional Encoding

For "I love you":

Position 0 ("I"):
    Final[0] = Embedding("I") + PE[0]
             = [0.2, 0.5, 0.1, 0.8] + [PE values for position 0]

Position 1 ("love"):
    Final[1] = Embedding("love") + PE[1]
             = [0.9, 0.1, 0.7, 0.3] + [PE values for position 1]

Position 2 ("you"):
    Final[2] = Embedding("you") + PE[2]
             = [0.3, 0.8, 0.2, 0.6] + [PE values for position 2]
```

### Sinusoidal Positional Encoding

The original Transformer uses **sine and cosine functions** to generate positional encodings:

**Formulas:**

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)$$

$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)$$

**Symbol Legend:**
- $pos$ = Position in the sequence (0, 1, 2, ...)
- $i$ = Dimension index (0, 1, 2, ...)
- $d$ = Embedding dimension (4 in our example)
- $PE_{(pos, 2i)}$ = Value at position $pos$, even dimension $2i$
- $PE_{(pos, 2i+1)}$ = Value at position $pos$, odd dimension $2i+1$

**Why Sine and Cosine?**
- Each position gets a unique pattern
- The model can learn relative positions (position 5 relates to position 3 in a consistent way)
- Works for sequences longer than those seen during training

### Step-by-Step: Computing Positional Encodings

Let's calculate the positional encodings for positions 0, 1, and 2 with d=4:

```
╔══════════════════════════════════════════════════════════════════════╗
║         STEP-BY-STEP: POSITIONAL ENCODING CALCULATION                ║
╚══════════════════════════════════════════════════════════════════════╝

Parameters: d = 4, so we have dimensions 0, 1, 2, 3


DIMENSION FORMULAS:
───────────────────

Dimension 0 (2i=0, i=0):   PE[pos,0] = sin(pos / 10000^(0/4)) = sin(pos / 1) = sin(pos)
Dimension 1 (2i+1=1, i=0): PE[pos,1] = cos(pos / 10000^(0/4)) = cos(pos / 1) = cos(pos)
Dimension 2 (2i=2, i=1):   PE[pos,2] = sin(pos / 10000^(2/4)) = sin(pos / 100)
Dimension 3 (2i+1=3, i=1): PE[pos,3] = cos(pos / 10000^(2/4)) = cos(pos / 100)


POSITION 0 ("I"):
─────────────────

PE[0,0] = sin(0 / 1)   = sin(0)     = 0.000
PE[0,1] = cos(0 / 1)   = cos(0)     = 1.000
PE[0,2] = sin(0 / 100) = sin(0)     = 0.000
PE[0,3] = cos(0 / 100) = cos(0)     = 1.000

PE[0] = [0.000, 1.000, 0.000, 1.000]


POSITION 1 ("love"):
────────────────────

PE[1,0] = sin(1 / 1)   = sin(1)     = 0.841
PE[1,1] = cos(1 / 1)   = cos(1)     = 0.540
PE[1,2] = sin(1 / 100) = sin(0.01)  = 0.010
PE[1,3] = cos(1 / 100) = cos(0.01)  = 1.000

PE[1] = [0.841, 0.540, 0.010, 1.000]


POSITION 2 ("you"):
───────────────────

PE[2,0] = sin(2 / 1)   = sin(2)     = 0.909
PE[2,1] = cos(2 / 1)   = cos(2)     = -0.416
PE[2,2] = sin(2 / 100) = sin(0.02)  = 0.020
PE[2,3] = cos(2 / 100) = cos(0.02)  = 1.000

PE[2] = [0.909, -0.416, 0.020, 1.000]


SUMMARY OF POSITIONAL ENCODINGS:
────────────────────────────────

              d₀      d₁       d₂      d₃
          ┌───────┬────────┬───────┬───────┐
   PE[0]  │ 0.000 │  1.000 │ 0.000 │ 1.000 │  Position 0 ("I")
          ├───────┼────────┼───────┼───────┤
   PE[1]  │ 0.841 │  0.540 │ 0.010 │ 1.000 │  Position 1 ("love")
          ├───────┼────────┼───────┼───────┤
   PE[2]  │ 0.909 │ -0.416 │ 0.020 │ 1.000 │  Position 2 ("you")
          └───────┴────────┴───────┴───────┘

Notice: Each position has a UNIQUE pattern!
```

### Visualizing Positional Encoding Patterns

```
╔══════════════════════════════════════════════════════════════════════╗
║              POSITIONAL ENCODING PATTERNS                            ║
╚══════════════════════════════════════════════════════════════════════╝

Different dimensions oscillate at different frequencies:

Dimensions 0,1 (high frequency - changes rapidly):
Position:  0      1      2      3      4      5
    d₀:   0.00   0.84   0.91   0.14  -0.76  -0.96   (sin, fast)
    d₁:   1.00   0.54  -0.42  -0.99  -0.65   0.28   (cos, fast)

Dimensions 2,3 (low frequency - changes slowly):
Position:  0      1      2      3      4      5
    d₂:   0.00   0.01   0.02   0.03   0.04   0.05   (sin, slow)
    d₃:   1.00   1.00   1.00   1.00   1.00   1.00   (cos, slow)


Visual Pattern:
───────────────

         Position: 0    1    2    3    4    5    6    7    8    9
                   ─────────────────────────────────────────────────
    Dim 0 (fast):  ░░░░ ████ ████ ▒▒▒▒ ░░░░ ░░░░ ▒▒▒▒ ████ ████ ▒▒▒▒
    Dim 1 (fast):  ████ ▓▓▓▓ ░░░░ ░░░░ ░░░░ ▒▒▒▒ ████ ████ ▓▓▓▓ ░░░░
    Dim 2 (slow):  ░░░░ ░░░░ ░░░░ ░░░░ ░░░░ ░░░░ ░░░░ ░░░░ ░░░░ ░░░░
    Dim 3 (slow):  ████ ████ ████ ████ ████ ████ ████ ████ ████ ████

The combination of fast and slow patterns creates a UNIQUE "fingerprint"
for each position, like a binary number but continuous.
```

### Step-by-Step: Adding Positional Encoding to Embeddings

Now we add the positional encodings to our word embeddings:

```
╔══════════════════════════════════════════════════════════════════════╗
║      STEP-BY-STEP: COMBINING EMBEDDINGS + POSITIONAL ENCODING        ║
╚══════════════════════════════════════════════════════════════════════╝

Formula: Final Input = Embedding + Positional Encoding


POSITION 0: "I"
───────────────

Embedding("I")  = [0.200, 0.500, 0.100, 0.800]
PE[0]           = [0.000, 1.000, 0.000, 1.000]
                  ─────────────────────────────
Final[0]        = [0.200, 1.500, 0.100, 1.800]   (element-wise addition)


POSITION 1: "love"
──────────────────

Embedding("love") = [0.900, 0.100, 0.700, 0.300]
PE[1]             = [0.841, 0.540, 0.010, 1.000]
                    ─────────────────────────────
Final[1]          = [1.741, 0.640, 0.710, 1.300]


POSITION 2: "you"
─────────────────

Embedding("you") = [0.300,  0.800, 0.200, 0.600]
PE[2]            = [0.909, -0.416, 0.020, 1.000]
                   ─────────────────────────────
Final[2]         = [1.209,  0.384, 0.220, 1.600]


FINAL INPUT MATRIX X (with position information):
─────────────────────────────────────────────────

                       d₀      d₁      d₂      d₃
                   ┌───────┬───────┬───────┬───────┐
    Position 0: I  │ 0.200 │ 1.500 │ 0.100 │ 1.800 │
                   ├───────┼───────┼───────┼───────┤
    Position 1:    │ 1.741 │ 0.640 │ 0.710 │ 1.300 │
             love  ├───────┼───────┼───────┼───────┤
    Position 2:    │ 1.209 │ 0.384 │ 0.220 │ 1.600 │
              you  └───────┴───────┴───────┴───────┘

Now each vector contains BOTH:
  ✓ Word meaning (from embedding)
  ✓ Position information (from positional encoding)
```

### Before and After Comparison

```
╔══════════════════════════════════════════════════════════════════════╗
║              BEFORE vs AFTER POSITIONAL ENCODING                     ║
╚══════════════════════════════════════════════════════════════════════╝

BEFORE (Embeddings only):
─────────────────────────

"I love you":    [0.2, 0.5, 0.1, 0.8]  [0.9, 0.1, 0.7, 0.3]  [0.3, 0.8, 0.2, 0.6]
"you love I":    [0.3, 0.8, 0.2, 0.6]  [0.9, 0.1, 0.7, 0.3]  [0.2, 0.5, 0.1, 0.8]

Problem: Same vectors, just reordered. Model can't tell them apart!


AFTER (Embeddings + Positional Encoding):
─────────────────────────────────────────

"I love you":    [0.20, 1.50, 0.10, 1.80]  [1.74, 0.64, 0.71, 1.30]  [1.21, 0.38, 0.22, 1.60]
                        ↑                         ↑                         ↑
                   "I" at pos 0            "love" at pos 1           "you" at pos 2

"you love I":    [1.21, 0.38, 0.22, 1.60]  [1.74, 0.64, 0.71, 1.30]  [0.20, 1.50, 0.10, 1.80]
                        ↑                         ↑                         ↑
                  "you" at pos 0           "love" at pos 1            "I" at pos 2

Solution: Now the vectors are DIFFERENT because position info is baked in!
```

---

## Self-Attention

### The Core Question

Now that we have position-aware embeddings, we need each word to **understand its context** by looking at other words. Self-attention answers the question:

> "For each word, which other words should it pay attention to?"

```
╔══════════════════════════════════════════════════════════════════════╗
║                    SELF-ATTENTION INTUITION                          ║
╚══════════════════════════════════════════════════════════════════════╝

Sentence: "I love you"

When processing "love":
  - How relevant is "I"?     → Who is doing the loving?
  - How relevant is "love"?  → The word itself
  - How relevant is "you"?   → Who is being loved?

Self-attention computes these relevance scores automatically!

         "I"          "love"         "you"
          │             │              │
          └─────────────┼──────────────┘
                        │
              ┌─────────┼─────────┐
              │         │         │
              ▼         ▼         ▼
           [ 0.2  ,   0.5   ,   0.3  ]   ← Attention weights
           
"love" pays 20% attention to "I", 50% to itself, 30% to "you"
```

### Query, Key, Value: The Core Mechanism

Self-attention uses three concepts:

- **Query (Q)**: "What am I looking for?"
- **Key (K)**: "What do I contain?"
- **Value (V)**: "What information do I provide?"

```
╔══════════════════════════════════════════════════════════════════════╗
║                    QUERY, KEY, VALUE ANALOGY                         ║
╚══════════════════════════════════════════════════════════════════════╝

Analogy: Library Search
───────────────────────

You have a QUERY (what you're searching for):
    "I want to know about the subject of the sentence"

Each word has a KEY (its label/description):
    "I"    - Key: "pronoun, subject, first-person"
    "love" - Key: "verb, action, emotion"
    "you"  - Key: "pronoun, object, second-person"

Each word has a VALUE (the actual information):
    "I"    - Value: [rich representation of "I"]
    "love" - Value: [rich representation of "love"]
    "you"  - Value: [rich representation of "you"]

Process:
    1. Compare QUERY to each KEY → get relevance scores
    2. Use scores to weight the VALUES
    3. Return weighted combination of VALUES
```

### Creating Q, K, V from Input

We create Q, K, V by multiplying our input by learned weight matrices:

$$Q = XW^Q \quad\quad K = XW^K \quad\quad V = XW^V$$

**Symbol Legend:**
- $X$ = Input matrix (3×4 in our example)
- $W^Q, W^K, W^V$ = Learned weight matrices (4×4 each)
- $Q, K, V$ = Query, Key, Value matrices (3×4 each)

### Step-by-Step: Computing Q, K, V

Let's compute Q, K, V for "I love you":

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP-BY-STEP: COMPUTING Q, K, V                         ║
╚══════════════════════════════════════════════════════════════════════╝

INPUT (with positional encoding):

X = ┌                            ┐
    │ 0.200  1.500  0.100  1.800 │  ← "I"    (position 0)
    │ 1.741  0.640  0.710  1.300 │  ← "love" (position 1)
    │ 1.209  0.384  0.220  1.600 │  ← "you"  (position 2)
    └                            ┘


WEIGHT MATRICES (learned during training):
──────────────────────────────────────────

For simplicity, we'll use these example weight matrices:

W^Q = ┌                    ┐     W^K = ┌                    ┐
      │ 1.0  0.0  0.0  0.0 │           │ 0.0  1.0  0.0  0.0 │
      │ 0.0  1.0  0.0  0.0 │           │ 1.0  0.0  0.0  0.0 │
      │ 0.0  0.0  1.0  0.0 │           │ 0.0  0.0  0.0  1.0 │
      │ 0.0  0.0  0.0  1.0 │           │ 0.0  0.0  1.0  0.0 │
      └                    ┘           └                    ┘

W^V = ┌                    ┐
      │ 0.5  0.5  0.0  0.0 │
      │ 0.5  0.5  0.0  0.0 │
      │ 0.0  0.0  0.5  0.5 │
      │ 0.0  0.0  0.5  0.5 │
      └                    ┘


COMPUTE Q = X × W^Q:
────────────────────

Q = ┌                            ┐
    │ 0.200  1.500  0.100  1.800 │     q₀ (query for "I")
    │ 1.741  0.640  0.710  1.300 │     q₁ (query for "love")
    │ 1.209  0.384  0.220  1.600 │     q₂ (query for "you")
    └                            ┘


COMPUTE K = X × W^K:
────────────────────

K[0] = [0.200, 1.500, 0.100, 1.800] × W^K = [1.500, 0.200, 1.800, 0.100]
K[1] = [1.741, 0.640, 0.710, 1.300] × W^K = [0.640, 1.741, 1.300, 0.710]
K[2] = [1.209, 0.384, 0.220, 1.600] × W^K = [0.384, 1.209, 1.600, 0.220]

K = ┌                            ┐
    │ 1.500  0.200  1.800  0.100 │     k₀ (key for "I")
    │ 0.640  1.741  1.300  0.710 │     k₁ (key for "love")
    │ 0.384  1.209  1.600  0.220 │     k₂ (key for "you")
    └                            ┘


COMPUTE V = X × W^V:
────────────────────

V[0] = [0.200, 1.500, 0.100, 1.800] × W^V = [0.850, 0.850, 0.950, 0.950]
V[1] = [1.741, 0.640, 0.710, 1.300] × W^V = [1.191, 1.191, 1.005, 1.005]
V[2] = [1.209, 0.384, 0.220, 1.600] × W^V = [0.797, 0.797, 0.910, 0.910]

V = ┌                            ┐
    │ 0.850  0.850  0.950  0.950 │     v₀ (value for "I")
    │ 1.191  1.191  1.005  1.005 │     v₁ (value for "love")
    │ 0.797  0.797  0.910  0.910 │     v₂ (value for "you")
    └                            ┘
```

### Computing Attention Scores

Now we compute how much each word should attend to every other word:

$$\text{Scores} = QK^T$$

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP-BY-STEP: ATTENTION SCORES (QK^T)                   ║
╚══════════════════════════════════════════════════════════════════════╝

We compute the dot product between each query and each key:

Score(i,j) = qᵢ · kⱼ = "How much should word i attend to word j?"


MATRIX MULTIPLICATION QK^T:
───────────────────────────

         K^T (transposed)
              k₀      k₁      k₂
           ┌───────────────────────┐
      q₀   │ q₀·k₀   q₀·k₁   q₀·k₂ │     "I" attending to all words
Q          ├───────────────────────┤
      q₁   │ q₁·k₀   q₁·k₁   q₁·k₂ │     "love" attending to all words
           ├───────────────────────┤
      q₂   │ q₂·k₀   q₂·k₁   q₂·k₂ │     "you" attending to all words
           └───────────────────────┘


COMPUTING EACH DOT PRODUCT:
───────────────────────────

q₀ = [0.200, 1.500, 0.100, 1.800]
k₀ = [1.500, 0.200, 1.800, 0.100]
k₁ = [0.640, 1.741, 1.300, 0.710]
k₂ = [0.384, 1.209, 1.600, 0.220]

Score[0,0] = q₀ · k₀ = (0.2×1.5) + (1.5×0.2) + (0.1×1.8) + (1.8×0.1)
                     = 0.30 + 0.30 + 0.18 + 0.18 = 0.96

Score[0,1] = q₀ · k₁ = (0.2×0.64) + (1.5×1.74) + (0.1×1.3) + (1.8×0.71)
                     = 0.13 + 2.61 + 0.13 + 1.28 = 4.15

Score[0,2] = q₀ · k₂ = (0.2×0.38) + (1.5×1.21) + (0.1×1.6) + (1.8×0.22)
                     = 0.08 + 1.82 + 0.16 + 0.40 = 2.46

[Similar calculations for rows 1 and 2...]


ATTENTION SCORES MATRIX:
────────────────────────

                    Keys (what we attend TO)
                    "I"     "love"   "you"
                 ┌────────┬────────┬────────┐
Queries    "I"   │  0.96  │  4.15  │  2.46  │
(what is    ─────┼────────┼────────┼────────┤
attending) "love"│  3.21  │  5.89  │  4.12  │
            ─────┼────────┼────────┼────────┤
           "you" │  2.87  │  4.76  │  3.54  │
                 └────────┴────────┴────────┘

Higher scores = more attention!
"love" has highest score attending to "love" (5.89)
```

### Scaling the Scores

We divide by $\sqrt{d_k}$ to prevent large values that would make softmax too "peaked":

$$\text{Scaled Scores} = \frac{QK^T}{\sqrt{d_k}}$$

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP-BY-STEP: SCALING                                   ║
╚══════════════════════════════════════════════════════════════════════╝

d_k = 4 (dimension of keys)
√d_k = √4 = 2

Scaled Scores = Scores / 2

                    "I"     "love"   "you"
                 ┌────────┬────────┬────────┐
           "I"   │  0.48  │  2.08  │  1.23  │
                 ├────────┼────────┼────────┤
         "love"  │  1.61  │  2.95  │  2.06  │
                 ├────────┼────────┼────────┤
          "you"  │  1.44  │  2.38  │  1.77  │
                 └────────┴────────┴────────┘

Why scale?
─────────
Without scaling, large d_k leads to large dot products,
which makes softmax produce values very close to 0 and 1.
This causes vanishing gradients during training.
```

### Applying Softmax

Convert scores to probabilities (each row sums to 1):

$$\alpha = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)$$

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP-BY-STEP: SOFTMAX                                   ║
╚══════════════════════════════════════════════════════════════════════╝

Softmax formula: softmax(xᵢ) = exp(xᵢ) / Σⱼ exp(xⱼ)


ROW 0 ("I" attending to all words):
───────────────────────────────────

Scaled scores: [0.48, 2.08, 1.23]

exp(0.48) = 1.62
exp(2.08) = 8.00
exp(1.23) = 3.42

Sum = 1.62 + 8.00 + 3.42 = 13.04

α[0] = [1.62/13.04, 8.00/13.04, 3.42/13.04]
     = [0.12, 0.61, 0.26]

Check: 0.12 + 0.61 + 0.26 = 0.99 ≈ 1.0 ✓


ROW 1 ("love" attending to all words):
──────────────────────────────────────

Scaled scores: [1.61, 2.95, 2.06]

exp(1.61) = 5.00
exp(2.95) = 19.11
exp(2.06) = 7.85

Sum = 5.00 + 19.11 + 7.85 = 31.96

α[1] = [5.00/31.96, 19.11/31.96, 7.85/31.96]
     = [0.16, 0.60, 0.25]


ROW 2 ("you" attending to all words):
─────────────────────────────────────

Scaled scores: [1.44, 2.38, 1.77]

exp(1.44) = 4.22
exp(2.38) = 10.80
exp(1.77) = 5.87

Sum = 4.22 + 10.80 + 5.87 = 20.89

α[2] = [4.22/20.89, 10.80/20.89, 5.87/20.89]
     = [0.20, 0.52, 0.28]


ATTENTION WEIGHTS MATRIX:
─────────────────────────

                    "I"     "love"   "you"    Sum
                 ┌────────┬────────┬────────┬──────┐
           "I"   │  0.12  │  0.61  │  0.26  │ 1.00 │
                 ├────────┼────────┼────────┼──────┤
         "love"  │  0.16  │  0.60  │  0.25  │ 1.00 │
                 ├────────┼────────┼────────┼──────┤
          "you"  │  0.20  │  0.52  │  0.28  │ 1.00 │
                 └────────┴────────┴────────┴──────┘

Each row is a probability distribution!
```

### Visualizing Attention Weights

```
╔══════════════════════════════════════════════════════════════════════╗
║              ATTENTION WEIGHTS VISUALIZATION                         ║
╚══════════════════════════════════════════════════════════════════════╝

Attention Heatmap:
──────────────────

              Attending TO →
              "I"      "love"     "you"
           ┌─────────┬─────────┬─────────┐
     "I"   │ ░░ 12%  │ ████ 61%│ ▒▒ 26% │
           ├─────────┼─────────┼─────────┤
   "love"  │ ░░ 16%  │ ████ 60%│ ▒▒ 25% │
           ├─────────┼─────────┼─────────┤
    "you"  │ ▒▒ 20%  │ ████ 52%│ ▒▒ 28% │
           └─────────┴─────────┴─────────┘

Legend: ░░ = low (0-20%), ▒▒ = medium (20-40%), ████ = high (40%+)


Interpretation:
───────────────

• ALL words pay most attention to "love" (61%, 60%, 52%)
  → "love" is the semantic center of this sentence

• "I" pays 61% attention to "love"
  → Understanding "I" in context of loving

• "love" pays 60% attention to itself
  → Verb is self-reinforcing its meaning

• "you" pays 52% attention to "love"  
  → Understanding "you" as the object of love
```

### Computing the Output

Finally, we compute the weighted sum of Values:

$$\text{Output} = \alpha \cdot V$$

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP-BY-STEP: COMPUTING OUTPUT                          ║
╚══════════════════════════════════════════════════════════════════════╝

Output = Attention Weights × Values

Each output row is a weighted combination of ALL value vectors.


OUTPUT FOR "I" (position 0):
────────────────────────────

Weights: α[0] = [0.12, 0.61, 0.26]

Values:
  v₀ = [0.850, 0.850, 0.950, 0.950]  (weighted by 0.12)
  v₁ = [1.191, 1.191, 1.005, 1.005]  (weighted by 0.61)
  v₂ = [0.797, 0.797, 0.910, 0.910]  (weighted by 0.26)

Output[0] = 0.12 × v₀ + 0.61 × v₁ + 0.26 × v₂

          = 0.12 × [0.850, 0.850, 0.950, 0.950]
          + 0.61 × [1.191, 1.191, 1.005, 1.005]
          + 0.26 × [0.797, 0.797, 0.910, 0.910]

Dimension 0: 0.12×0.850 + 0.61×1.191 + 0.26×0.797 = 0.10 + 0.73 + 0.21 = 1.04
Dimension 1: 0.12×0.850 + 0.61×1.191 + 0.26×0.797 = 0.10 + 0.73 + 0.21 = 1.04
Dimension 2: 0.12×0.950 + 0.61×1.005 + 0.26×0.910 = 0.11 + 0.61 + 0.24 = 0.96
Dimension 3: 0.12×0.950 + 0.61×1.005 + 0.26×0.910 = 0.11 + 0.61 + 0.24 = 0.96

Output[0] = [1.04, 1.04, 0.96, 0.96]


OUTPUT FOR "love" (position 1):
───────────────────────────────

Weights: α[1] = [0.16, 0.60, 0.25]

Output[1] = 0.16 × v₀ + 0.60 × v₁ + 0.25 × v₂
          = [1.05, 1.05, 0.98, 0.98]


OUTPUT FOR "you" (position 2):
──────────────────────────────

Weights: α[2] = [0.20, 0.52, 0.28]

Output[2] = 0.20 × v₀ + 0.52 × v₁ + 0.28 × v₂
          = [1.01, 1.01, 0.95, 0.95]


FINAL SELF-ATTENTION OUTPUT:
────────────────────────────

                       d₀      d₁      d₂      d₃
                   ┌───────┬───────┬───────┬───────┐
    "I"      (new) │  1.04 │  1.04 │  0.96 │  0.96 │
                   ├───────┼───────┼───────┼───────┤
    "love"   (new) │  1.05 │  1.05 │  0.98 │  0.98 │
                   ├───────┼───────┼───────┼───────┤
    "you"    (new) │  1.01 │  1.01 │  0.95 │  0.95 │
                   └───────┴───────┴───────┴───────┘

These are CONTEXTUALIZED representations!
Each word's vector now contains information from ALL words.
```

### The Complete Self-Attention Formula

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

```
╔══════════════════════════════════════════════════════════════════════╗
║              SELF-ATTENTION: COMPLETE SUMMARY                        ║
╚══════════════════════════════════════════════════════════════════════╝

INPUT: X (embeddings + positional encoding)
       ┌                          ┐
       │ 0.200  1.500  0.100  1.800 │  "I"
       │ 1.741  0.640  0.710  1.300 │  "love"
       │ 1.209  0.384  0.220  1.600 │  "you"
       └                          ┘
                    │
                    │  × W^Q, W^K, W^V
                    ▼
              Q, K, V matrices
                    │
                    │  QK^T (dot products)
                    ▼
            Attention Scores
                    │
                    │  ÷ √d_k (scale)
                    ▼
            Scaled Scores
                    │
                    │  softmax (normalize)
                    ▼
           Attention Weights
       ┌                          ┐
       │  0.12    0.61    0.26    │  "I" → most attention on "love"
       │  0.16    0.60    0.25    │  "love" → most attention on "love"
       │  0.20    0.52    0.28    │  "you" → most attention on "love"
       └                          ┘
                    │
                    │  × V (weighted sum)
                    ▼
OUTPUT: Contextualized representations
       ┌                          ┐
       │  1.04    1.04    0.96    0.96 │  "I" (now knows about context)
       │  1.05    1.05    0.98    0.98 │  "love" (now knows about context)
       │  1.01    1.01    0.95    0.95 │  "you" (now knows about context)
       └                          ┘
```

---

## Building the Encoder

### Encoder Overview

The encoder processes our input "I love you" and creates rich, contextualized representations. It consists of **N stacked identical layers** (N=6 in the original Transformer).

Each encoder layer has:
1. **Multi-Head Self-Attention**
2. **Add & Norm** (residual connection + layer normalization)
3. **Feed-Forward Network**
4. **Add & Norm**

```
╔══════════════════════════════════════════════════════════════════════╗
║                    ENCODER ARCHITECTURE                              ║
╚══════════════════════════════════════════════════════════════════════╝

Input: "I love you" (with positional encoding)
              │
              ▼
    ╔═══════════════════════════════════════╗
    ║         ENCODER LAYER 1               ║
    ║  ┌─────────────────────────────────┐  ║
    ║  │    Multi-Head Self-Attention    │  ║
    ║  └───────────────┬─────────────────┘  ║
    ║                  ▼                    ║
    ║  ┌─────────────────────────────────┐  ║
    ║  │         Add & Norm              │◄─╫── (residual from input)
    ║  └───────────────┬─────────────────┘  ║
    ║                  ▼                    ║
    ║  ┌─────────────────────────────────┐  ║
    ║  │      Feed-Forward Network       │  ║
    ║  └───────────────┬─────────────────┘  ║
    ║                  ▼                    ║
    ║  ┌─────────────────────────────────┐  ║
    ║  │         Add & Norm              │◄─╫── (residual)
    ║  └───────────────┬─────────────────┘  ║
    ╚══════════════════╪════════════════════╝
                       │
                       ▼
    ╔═══════════════════════════════════════╗
    ║         ENCODER LAYER 2               ║
    ║              ...                      ║
    ╚══════════════════╪════════════════════╝
                       │
                       ▼
                      ...
                       │
                       ▼
    ╔═══════════════════════════════════════╗
    ║         ENCODER LAYER N               ║
    ╚══════════════════╪════════════════════╝
                       │
                       ▼
              ENCODER OUTPUT
    (3 context-rich vectors for "I", "love", "you")
```

### Multi-Head Attention: Multiple Perspectives

Instead of one attention computation, we use **multiple "heads"** that attend to different aspects:

```
╔══════════════════════════════════════════════════════════════════════╗
║                    MULTI-HEAD ATTENTION                              ║
╚══════════════════════════════════════════════════════════════════════╝

Why multiple heads?
───────────────────
Different heads can learn different relationships:

  Head 1: "Who is the subject?"     → "I" attends strongly to "love"
  Head 2: "Who is the object?"      → "you" attends strongly to "love"
  Head 3: "What's the action?"      → "love" attends to itself
  Head 4: "Positional patterns"     → Adjacent words attend to each other


How it works (h=4 heads, d=4, so d_k = d/h = 1 per head):
─────────────────────────────────────────────────────────

Input X
   │
   ├──► Head 1: Attention with W₁^Q, W₁^K, W₁^V ──► Output₁
   │
   ├──► Head 2: Attention with W₂^Q, W₂^K, W₂^V ──► Output₂
   │
   ├──► Head 3: Attention with W₃^Q, W₃^K, W₃^V ──► Output₃
   │
   └──► Head 4: Attention with W₄^Q, W₄^K, W₄^V ──► Output₄
                                                      │
                              ┌───────────────────────┘
                              ▼
                    Concatenate all outputs
                              │
                              ▼
                    Linear projection W^O
                              │
                              ▼
                      Final Output

Formula: MultiHead(Q,K,V) = Concat(head₁,...,headₕ)W^O
```

### Add & Norm: Residual Connections and Layer Normalization

```
╔══════════════════════════════════════════════════════════════════════╗
║                    ADD & NORM                                        ║
╚══════════════════════════════════════════════════════════════════════╝

Two crucial components that make training stable:


1. RESIDUAL CONNECTION (Add):
─────────────────────────────

Output = x + Sublayer(x)

    Input x ─────────────────────┐
        │                        │
        ▼                        │
   ┌──────────┐                  │
   │ Sublayer │                  │
   │(Attention│                  │
   │ or FFN)  │                  │
   └────┬─────┘                  │
        │                        │
        ▼                        │
       (+) ◄─────────────────────┘
        │
        ▼
     Output

Why? Gradients can flow directly through the skip connection,
     preventing vanishing gradients in deep networks.


2. LAYER NORMALIZATION (Norm):
──────────────────────────────

Normalize each vector to have mean=0, variance=1:

LayerNorm(x) = γ × (x - μ) / √(σ² + ε) + β

Where:
  μ = mean of x
  σ² = variance of x
  γ, β = learned parameters
  ε = small constant for stability

Why? Stabilizes training by keeping activations in a good range.


Complete Add & Norm:
────────────────────

Output = LayerNorm(x + Sublayer(x))
```

### Feed-Forward Network

A simple two-layer neural network applied to each position independently:

```
╔══════════════════════════════════════════════════════════════════════╗
║                    FEED-FORWARD NETWORK                              ║
╚══════════════════════════════════════════════════════════════════════╝

Formula: FFN(x) = W₂ · ReLU(W₁ · x + b₁) + b₂


Architecture:
─────────────

Input: x ∈ ℝ^d (d = 4 in our example)
        │
        ▼
   ┌──────────┐
   │  W₁, b₁  │  Expand: d → d_ff (e.g., 4 → 16)
   └────┬─────┘
        │
        ▼
   ┌──────────┐
   │   ReLU   │  Non-linearity: max(0, x)
   └────┬─────┘
        │
        ▼
   ┌──────────┐
   │  W₂, b₂  │  Contract: d_ff → d (e.g., 16 → 4)
   └────┬─────┘
        │
        ▼
Output: y ∈ ℝ^d (same size as input)


Why FFN?
────────
• Adds non-linearity (attention is mostly linear)
• Processes each position independently
• Expansion (d → d_ff) adds capacity for learning complex patterns
• Typical: d_ff = 4 × d
```

### What Happens in Each Layer

```
╔══════════════════════════════════════════════════════════════════════╗
║              WHAT EACH ENCODER LAYER LEARNS                          ║
╚══════════════════════════════════════════════════════════════════════╝

Layer 1 (Early): Basic patterns
───────────────────────────────
• Local word relationships
• Simple syntax (adjective-noun, subject-verb)
• "I" recognizes it's before a verb

Layer 2-3 (Middle): Building meaning
────────────────────────────────────
• Phrase-level understanding
• "I love" forms a subject-verb unit
• "love you" forms a verb-object unit

Layer 4-6 (Late): Deep understanding
────────────────────────────────────
• Full sentence semantics
• "I" knows it's the one loving
• "you" knows it's being loved
• "love" knows its subject and object


After 6 layers, our encoder output:
───────────────────────────────────

Original input:
    "I"    → [0.2, 1.5, 0.1, 1.8]
    "love" → [1.7, 0.6, 0.7, 1.3]
    "you"  → [1.2, 0.4, 0.2, 1.6]

Encoder output (6 layers of processing):
    "I"    → [rich contextual representation]
    "love" → [rich contextual representation]
    "you"  → [rich contextual representation]

These vectors now "understand" the full sentence!
```

---

## Parallel Computation

### The RNN Problem: Sequential Processing

Before Transformers, RNNs processed sequences one word at a time:

```
╔══════════════════════════════════════════════════════════════════════╗
║                    RNN: SEQUENTIAL PROCESSING                        ║
╚══════════════════════════════════════════════════════════════════════╝

Processing "I love you" with an RNN:

Time step 1: Process "I"
─────────────────────────
    "I" ──► [RNN] ──► h₁
    
Time step 2: Process "love" (must wait for h₁!)
──────────────────────────────────────────────
    "love" ──► [RNN] ──► h₂
                 ▲
                 │
                h₁ (from step 1)

Time step 3: Process "you" (must wait for h₂!)
──────────────────────────────────────────────
    "you" ──► [RNN] ──► h₃
                 ▲
                 │
                h₂ (from step 2)


Timeline:
─────────
    Time: ═══╡ Step 1 ╞═══╡ Step 2 ╞═══╡ Step 3 ╞═══►
              "I"          "love"        "you"

Total time: 3 sequential steps
Cannot parallelize: each step depends on the previous!
```

### The Transformer Advantage: Parallel Processing

Transformers process all words simultaneously:

```
╔══════════════════════════════════════════════════════════════════════╗
║              TRANSFORMER: PARALLEL PROCESSING                        ║
╚══════════════════════════════════════════════════════════════════════╝

Processing "I love you" with a Transformer:

All words processed at the SAME time:
─────────────────────────────────────

    "I"    ──►┐
              │     ┌───────────────────┐      ┌──► Output for "I"
    "love" ──►├────►│   Self-Attention  │─────►├──► Output for "love"
              │     │   (matrix mult)   │      └──► Output for "you"
    "you"  ──►┘     └───────────────────┘


Timeline:
─────────
    Time: ═══╡   Single Step   ╞═══►
              "I", "love", "you"
              (all at once!)

Total time: 1 parallel step


HOW IT'S PARALLEL:
──────────────────

Self-attention is matrix multiplication:

    Q × K^T = ┌─────────────────────┐
              │ q₀·k₀  q₀·k₁  q₀·k₂ │  ← All computed
              │ q₁·k₀  q₁·k₁  q₁·k₂ │    simultaneously
              │ q₂·k₀  q₂·k₁  q₂·k₂ │    on GPU!
              └─────────────────────┘

GPUs are optimized for matrix operations = massive parallelism!
```

### Speed Comparison

```
╔══════════════════════════════════════════════════════════════════════╗
║                    SPEED COMPARISON                                  ║
╚══════════════════════════════════════════════════════════════════════╝

For a sequence of length n:

┌─────────────────┬─────────────────┬─────────────────────────────────┐
│ Architecture    │ Time Complexity │ For n=1000                      │
├─────────────────┼─────────────────┼─────────────────────────────────┤
│ RNN             │ O(n) sequential │ 1000 sequential steps           │
│                 │                 │ (slow, can't parallelize)       │
├─────────────────┼─────────────────┼─────────────────────────────────┤
│ Transformer     │ O(1) parallel   │ 1 parallel step                 │
│                 │ (O(n²) compute) │ (fast on GPU, parallelized)     │
└─────────────────┴─────────────────┴─────────────────────────────────┘


Real-world impact:
──────────────────

Training time for same model quality:
  RNN-based translation:     2-3 weeks
  Transformer translation:   2-3 days

This speedup enabled training much larger models (GPT, BERT, etc.)!
```

---

## The Decoder and Decoder Self-Attention

### Decoder Overview

The decoder generates the output "Je t'aime" one word at a time, using the encoder's representation of "I love you."

```
╔══════════════════════════════════════════════════════════════════════╗
║                    DECODER'S JOB                                     ║
╚══════════════════════════════════════════════════════════════════════╝

Encoder output: Rich representations of "I love you"
                         │
                         ▼
              ┌─────────────────────┐
              │       DECODER       │
              │                     │
              │  Step 1: Generate   │───► "Je"
              │  Step 2: Generate   │───► "t'"
              │  Step 3: Generate   │───► "aime"
              │  Step 4: Generate   │───► <END>
              │                     │
              └─────────────────────┘

The decoder generates one token at a time,
feeding each output back as input for the next step.
```

### Decoder Input: Shifted Target Sequence

During **training**, we feed the target sequence shifted right:

```
╔══════════════════════════════════════════════════════════════════════╗
║                    DECODER INPUT (TRAINING)                          ║
╚══════════════════════════════════════════════════════════════════════╝

Target sequence: "Je t'aime <END>"

Shifted right (add <START>, remove last):
    Decoder input:  "<START>  Je   t'   aime"
    Expected output:   "Je    t'  aime  <END>"

Position:           0       1     2      3
                    │       │     │      │
                    ▼       ▼     ▼      ▼
Decoder input:  <START>    Je    t'   aime
                    │       │     │      │
                    ▼       ▼     ▼      ▼
Expected output:   Je      t'   aime   <END>


Why shift?
──────────
• At position 0: Given <START>, predict "Je"
• At position 1: Given <START> Je, predict "t'"
• At position 2: Given <START> Je t', predict "aime"
• At position 3: Given <START> Je t' aime, predict <END>
```

### The Problem: Preventing Cheating

During training, we feed all target words at once for parallel processing. But the model must NOT look at future words!

```
╔══════════════════════════════════════════════════════════════════════╗
║                    THE CHEATING PROBLEM                              ║
╚══════════════════════════════════════════════════════════════════════╝

WITHOUT masking (WRONG!):
─────────────────────────

When predicting position 1 ("t'"):
    Decoder can see: <START>, Je, t', aime
                              ↑   ↑    ↑
                           Current  Future tokens!
                           position

The model could just COPY "t'" from the input!
It learns nothing useful.


WITH masking (CORRECT):
───────────────────────

When predicting position 1 ("t'"):
    Decoder can see: <START>, Je, [MASKED], [MASKED]
                              ↑
                           Only past tokens!

The model must actually LEARN to predict "t'".
```

### The Causal Mask

We use a **causal mask** (also called "look-ahead mask") to block future positions:

```
╔══════════════════════════════════════════════════════════════════════╗
║                    CAUSAL MASK                                       ║
╚══════════════════════════════════════════════════════════════════════╝

Decoder input: "<START> Je t' aime" (4 positions)


MASK MATRIX:
────────────

                 Keys (positions we attend TO)
                 pos 0    pos 1    pos 2    pos 3
                <START>    Je       t'      aime
              ┌─────────┬─────────┬─────────┬─────────┐
    pos 0     │    0    │   -∞    │   -∞    │   -∞    │
   <START>    ├─────────┼─────────┼─────────┼─────────┤
Queries       │    0    │    0    │   -∞    │   -∞    │
(positions    pos 1 Je  ├─────────┼─────────┼─────────┼─────────┤
we attend     │    0    │    0    │    0    │   -∞    │
FROM)         pos 2 t'  ├─────────┼─────────┼─────────┼─────────┤
              │    0    │    0    │    0    │    0    │
    pos 3     └─────────┴─────────┴─────────┴─────────┘
    aime

Legend:
  0  = Can attend (allowed)
  -∞ = Cannot attend (blocked, set to negative infinity)


The mask is LOWER TRIANGULAR:
─────────────────────────────

    ┌─────────────────────────┐
    │ ✓   ✗   ✗   ✗         │
    │ ✓   ✓   ✗   ✗         │
    │ ✓   ✓   ✓   ✗         │
    │ ✓   ✓   ✓   ✓         │
    └─────────────────────────┘

Each position can only attend to itself and PREVIOUS positions.
```

### Step-by-Step: Applying the Mask

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP-BY-STEP: MASKED SELF-ATTENTION                     ║
╚══════════════════════════════════════════════════════════════════════╝

1. COMPUTE ATTENTION SCORES (QK^T / √d_k):
──────────────────────────────────────────

Scores = ┌─────────────────────────────────┐
         │  2.0    1.5    0.8    1.2       │  <START> attending
         │  1.8    2.5    1.1    0.9       │  Je attending
         │  1.2    1.9    2.3    1.4       │  t' attending
         │  1.0    1.6    1.8    2.1       │  aime attending
         └─────────────────────────────────┘


2. ADD MASK (add -∞ to blocked positions):
──────────────────────────────────────────

Masked = Scores + Mask

       = ┌─────────────────────────────────┐
         │  2.0    -∞     -∞     -∞        │
         │  1.8    2.5    -∞     -∞        │
         │  1.2    1.9    2.3    -∞        │
         │  1.0    1.6    1.8    2.1       │
         └─────────────────────────────────┘


3. APPLY SOFTMAX:
─────────────────

Remember: softmax(-∞) = 0

Row 0: softmax([2.0, -∞, -∞, -∞])
     = [1.00, 0.00, 0.00, 0.00]
     
     <START> pays 100% attention to itself only!

Row 1: softmax([1.8, 2.5, -∞, -∞])
     = [0.33, 0.67, 0.00, 0.00]
     
     "Je" attends to <START> (33%) and itself (67%)

Row 2: softmax([1.2, 1.9, 2.3, -∞])
     = [0.19, 0.38, 0.43, 0.00]
     
     "t'" attends to <START>, "Je", and itself

Row 3: softmax([1.0, 1.6, 1.8, 2.1])
     = [0.14, 0.25, 0.30, 0.31]
     
     "aime" attends to all positions


FINAL ATTENTION WEIGHTS:
────────────────────────

              <START>    Je       t'      aime
            ┌─────────┬─────────┬─────────┬─────────┐
   <START>  │  1.00   │  0.00   │  0.00   │  0.00   │
            ├─────────┼─────────┼─────────┼─────────┤
       Je   │  0.33   │  0.67   │  0.00   │  0.00   │
            ├─────────┼─────────┼─────────┼─────────┤
       t'   │  0.19   │  0.38   │  0.43   │  0.00   │
            ├─────────┼─────────┼─────────┼─────────┤
     aime   │  0.14   │  0.25   │  0.30   │  0.31   │
            └─────────┴─────────┴─────────┴─────────┘

Notice: Upper triangle is all ZEROS (blocked by mask)!
```

### Visualizing Masked vs Unmasked Attention

```
╔══════════════════════════════════════════════════════════════════════╗
║              MASKED vs UNMASKED ATTENTION                            ║
╚══════════════════════════════════════════════════════════════════════╝

ENCODER Self-Attention (UNMASKED):
──────────────────────────────────
Every word can attend to every other word.

         "I"     "love"   "you"
       ┌───────┬───────┬───────┐
  "I"  │ ████  │ ████  │ ████  │
       ├───────┼───────┼───────┤
"love" │ ████  │ ████  │ ████  │
       ├───────┼───────┼───────┤
 "you" │ ████  │ ████  │ ████  │
       └───────┴───────┴───────┘

All positions can see all positions ✓


DECODER Self-Attention (MASKED):
────────────────────────────────
Each word can only attend to previous words.

         <S>     "Je"    "t'"   "aime"
       ┌───────┬───────┬───────┬───────┐
  <S>  │ ████  │ ░░░░  │ ░░░░  │ ░░░░  │
       ├───────┼───────┼───────┼───────┤
 "Je"  │ ████  │ ████  │ ░░░░  │ ░░░░  │
       ├───────┼───────┼───────┼───────┤
 "t'"  │ ████  │ ████  │ ████  │ ░░░░  │
       ├───────┼───────┼───────┼───────┤
"aime" │ ████  │ ████  │ ████  │ ████  │
       └───────┴───────┴───────┴───────┘

████ = Can attend    ░░░░ = Blocked (cannot attend)

Lower triangular pattern!
```

---

## Encoder-Decoder Attention

### Connecting Encoder and Decoder

After decoder self-attention, the decoder needs to look at the **encoder's output** to understand the input sentence. This is called **cross-attention** or **encoder-decoder attention**.

```
╔══════════════════════════════════════════════════════════════════════╗
║                    ENCODER-DECODER ATTENTION                         ║
╚══════════════════════════════════════════════════════════════════════╝

The key insight:
────────────────

• QUERY: Comes from the DECODER
  "What French word should I generate?"

• KEY & VALUE: Come from the ENCODER
  "Here's the information about 'I love you'"


            ENCODER                           DECODER
         "I love you"                     "Je t' aime"
              │                                │
              ▼                                ▼
    ┌─────────────────┐              ┌─────────────────┐
    │ Encoder output  │              │ Decoder state   │
    │   K and V       │◄─────────────│   Q (query)     │
    └─────────────────┘              └─────────────────┘
              │                                │
              │         Cross-Attention        │
              └────────────────┬───────────────┘
                               │
                               ▼
                    Context from encoder
                    (relevant for current
                     decoder position)
```

### How Cross-Attention Works

```
╔══════════════════════════════════════════════════════════════════════╗
║              CROSS-ATTENTION MECHANISM                               ║
╚══════════════════════════════════════════════════════════════════════╝

Encoder output (from "I love you"):
    E = [e₀, e₁, e₂]  where e₀="I", e₁="love", e₂="you"

Decoder state (generating French):
    D = [d₀, d₁, d₂, d₃]  where d₀=<S>, d₁="Je", d₂="t'", d₃="aime"


Creating Q, K, V:
─────────────────

    Q = D × W^Q  (Queries from DECODER)
    K = E × W^K  (Keys from ENCODER)
    V = E × W^V  (Values from ENCODER)


Attention computation:
──────────────────────

    Scores = Q × K^T
    
                     Encoder (Keys)
                    "I"    "love"   "you"
                 ┌────────┬────────┬────────┐
    Decoder  <S> │ q₀·k₀  │ q₀·k₁  │ q₀·k₂  │
    (Queries)    ├────────┼────────┼────────┤
             Je  │ q₁·k₀  │ q₁·k₁  │ q₁·k₂  │
                 ├────────┼────────┼────────┤
             t'  │ q₂·k₀  │ q₂·k₁  │ q₂·k₂  │
                 ├────────┼────────┼────────┤
           aime  │ q₃·k₀  │ q₃·k₁  │ q₃·k₂  │
                 └────────┴────────┴────────┘

Each decoder position attends to ALL encoder positions!
(No masking in cross-attention)
```

### Step-by-Step: Cross-Attention Example

Let's see which English words each French word attends to:

```
╔══════════════════════════════════════════════════════════════════════╗
║         STEP-BY-STEP: CROSS-ATTENTION                                ║
╚══════════════════════════════════════════════════════════════════════╝

After computing attention scores and softmax:


CROSS-ATTENTION WEIGHTS:
────────────────────────

                 Encoder positions (English)
                  "I"      "love"    "you"
              ┌─────────┬─────────┬─────────┐
    <START>   │  0.33   │  0.34   │  0.33   │  (even attention)
              ├─────────┼─────────┼─────────┤
Decoder  Je   │  0.55   │  0.25   │  0.20   │  (mostly "I")
positions     ├─────────┼─────────┼─────────┤
         t'   │  0.20   │  0.25   │  0.55   │  (mostly "you")
              ├─────────┼─────────┼─────────┤
       aime   │  0.15   │  0.70   │  0.15   │  (mostly "love")
              └─────────┴─────────┴─────────┘


INTERPRETATION:
───────────────

"Je" (I in French):
    Attends 55% to "I" ← Makes sense! Same meaning
    
"t'" (you in French):
    Attends 55% to "you" ← Makes sense! Same meaning
    
"aime" (love in French):
    Attends 70% to "love" ← Makes sense! Same meaning


VISUALIZATION:
──────────────

    English:     "I"         "love"        "you"
                  │            │             │
                  │    ┌───────┼───────┐     │
                  │    │       │       │     │
                55%   25%     70%     55%   25%
                  │    │       │       │     │
                  ▼    ▼       ▼       ▼     ▼
    French:     "Je"  ───   "aime"   ───   "t'"

The model learns to ALIGN words across languages!
```

### Cross-Attention Heatmap

```
╔══════════════════════════════════════════════════════════════════════╗
║              CROSS-ATTENTION HEATMAP                                 ║
╚══════════════════════════════════════════════════════════════════════╝

Translation: "I love you" → "Je t'aime"

                        ENGLISH (Encoder)
                      "I"     "love"    "you"
                   ┌─────────┬─────────┬────────┐
          <START> │   ▒▒    │   ▒▒    │   ▒▒    │  33% 34% 33%
                  ├─────────┼─────────┼─────────┤
 FRENCH      Je   │  ████   │   ░░    │   ░░    │  55% 25% 20%
(Decoder)         ├─────────┼─────────┼─────────┤
             t'   │   ░░    │   ▒▒    │  ████   │  20% 25% 55%
                  ├─────────┼─────────┼─────────┤
           aime   │   ░░    │  ████   │   ░░    │  15% 70% 15%
                  └─────────┴─────────┴─────────┘

Legend: ░░ = low (0-25%), ▒▒ = medium (25-50%), ████ = high (50%+)


The diagonal-ish pattern shows word ALIGNMENT:
    "Je"   aligns with "I"
    "t'"   aligns with "you"  
    "aime" aligns with "love"

This is learned automatically during training!
```

---

## Decoding Numbers into Words

### The Final Layers

After the decoder processes the input, we need to convert the output vectors back into actual words. This requires two final steps:

```
╔══════════════════════════════════════════════════════════════════════╗
║              CONVERTING VECTORS TO WORDS                             ║
╚══════════════════════════════════════════════════════════════════════╝

Decoder output (vectors):
    Position 0: [1.2, 0.8, 0.5, 1.1]  ← predict what?
    Position 1: [0.9, 1.3, 0.7, 0.6]  ← predict what?
    Position 2: [1.0, 0.4, 1.2, 0.9]  ← predict what?
    Position 3: [0.3, 0.2, 0.1, 0.4]  ← predict what?

We need to convert each vector to a WORD from our vocabulary!


Step 1: Linear Projection
─────────────────────────
Project from d_model dimensions to vocabulary_size dimensions

    d_model = 4
    vocabulary_size = 10

    Linear layer: W ∈ ℝ^(4 × 10)
    
    Output = Decoder_output × W
           = [1.2, 0.8, 0.5, 1.1] × W
           = [score₀, score₁, ..., score₉]
           
    One score for each word in vocabulary!


Step 2: Softmax
───────────────
Convert scores to probabilities

    Probabilities = softmax([score₀, score₁, ..., score₉])
    
    Sum of all probabilities = 1.0
```

### Step-by-Step: Generating "Je"

Let's trace through generating the first output word:

```
╔══════════════════════════════════════════════════════════════════════╗
║         STEP-BY-STEP: GENERATING "Je"                                ║
╚══════════════════════════════════════════════════════════════════════╝

Input to decoder: <START> at position 0


1. DECODER PROCESSING:
──────────────────────

<START> goes through:
  • Embedding + Positional Encoding
  • Masked Self-Attention
  • Cross-Attention (attends to encoder output for "I love you")
  • Feed-Forward Network

Decoder output vector for position 0:
  d₀ = [1.2, 0.8, 0.5, 1.1]


2. LINEAR PROJECTION (to vocabulary):
─────────────────────────────────────

Vocabulary: <START>, I, love, you, Je, t', aime, hello, world, <END>
            (indices: 0,   1,  2,   3,  4,  5,   6,     7,     8,   9)

Projection weight matrix W (4 × 10):

d₀ × W = [1.2, 0.8, 0.5, 1.1] × W

       = [-0.5, 0.3, 0.8, 0.2, 2.5, 1.1, 0.9, -0.2, 0.1, -0.8]
          ↑     ↑    ↑    ↑    ↑    ↑    ↑     ↑     ↑     ↑
        <START> I  love  you  Je   t'  aime hello world <END>

"Je" has the highest score (2.5)!


3. SOFTMAX (to probabilities):
──────────────────────────────

Apply softmax to convert scores to probabilities:

scores = [-0.5, 0.3, 0.8, 0.2, 2.5, 1.1, 0.9, -0.2, 0.1, -0.8]

exp(scores) = [0.61, 1.35, 2.23, 1.22, 12.18, 3.00, 2.46, 0.82, 1.11, 0.45]

sum = 25.43

probabilities = exp(scores) / sum

┌─────────┬────────┬─────────────┐
│  Word   │ Score  │ Probability │
├─────────┼────────┼─────────────┤
│ <START> │ -0.5   │   0.024     │
│ I       │  0.3   │   0.053     │
│ love    │  0.8   │   0.088     │
│ you     │  0.2   │   0.048     │
│ Je      │  2.5   │   0.479  ← HIGHEST! │
│ t'      │  1.1   │   0.118     │
│ aime    │  0.9   │   0.097     │
│ hello   │ -0.2   │   0.032     │
│ world   │  0.1   │   0.044     │
│ <END>   │ -0.8   │   0.018     │
├─────────┼────────┼─────────────┤
│ TOTAL   │        │   1.000     │
└─────────┴────────┴─────────────┘


4. SELECT OUTPUT:
─────────────────

Greedy decoding: Pick the word with highest probability

    argmax(probabilities) = index 4 = "Je"

OUTPUT: "Je" ✓
```

### Probability Distribution Visualization

```
╔══════════════════════════════════════════════════════════════════════╗
║              PROBABILITY DISTRIBUTION FOR POSITION 0                 ║
╚══════════════════════════════════════════════════════════════════════╝

Given: <START>
Predict: ???

Probability:
    │
 50%│                   ████
    │                   ████
 40%│                   ████
    │                   ████
 30%│                   ████
    │                   ████
 20%│                   ████
    │              ▓▓▓▓ ████ ▓▓▓▓
 10%│         ▒▒▒▒ ▓▓▓▓ ████ ▓▓▓▓ ▒▒▒▒
    │    ░░░░ ▒▒▒▒ ▓▓▓▓ ████ ▓▓▓▓ ▒▒▒▒ ░░░░ ░░░░ ░░░░
  0%└────────────────────────────────────────────────────
       <S>   I   love  you  Je   t'  aime hello world <END>
                             ↑
                        SELECTED
                        (47.9%)

The model is 47.9% confident the first output word should be "Je"!
```

---

## Generating the Complete Translation

### Autoregressive Generation

The decoder generates words **one at a time**, feeding each output back as input for the next step:

```
╔══════════════════════════════════════════════════════════════════════╗
║              AUTOREGRESSIVE GENERATION                               ║
╚══════════════════════════════════════════════════════════════════════╝

The decoder generates the translation step by step:

Step 1:  Input: <START>           → Output: "Je"
Step 2:  Input: <START> Je        → Output: "t'"
Step 3:  Input: <START> Je t'     → Output: "aime"
Step 4:  Input: <START> Je t'aime → Output: <END>

Each step uses the previous output as part of the new input!
```

### Step 1: Generate "Je"

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP 1: GENERATE "Je"                                   ║
╚══════════════════════════════════════════════════════════════════════╝

INPUT:
──────
Decoder receives: <START>

                ┌─────────┐
                │ <START> │
                └────┬────┘
                     │
                     ▼
    ┌─────────────────────────────────┐
    │           DECODER               │
    │  • Embed + Position             │
    │  • Masked Self-Attention        │
    │  • Cross-Attention to Encoder   │
    │  • Feed-Forward                 │
    │  • Linear + Softmax             │
    └───────────────┬─────────────────┘
                    │
                    ▼
         Probabilities over vocabulary
         
         P("Je") = 0.479  ← HIGHEST
         P("I")  = 0.053
         P("t'") = 0.118
         ...

OUTPUT: "Je"
────────────

Next decoder input will be: <START> Je
```

### Step 2: Generate "t'"

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP 2: GENERATE "t'"                                   ║
╚══════════════════════════════════════════════════════════════════════╝

INPUT:
──────
Decoder receives: <START> Je

    ┌─────────┬─────────┐
    │ <START> │   Je    │
    └────┬────┴────┬────┘
         │         │
         ▼         ▼
    ┌─────────────────────────────────┐
    │           DECODER               │
    │                                 │
    │  Masked Self-Attention:         │
    │    <START> attends to: <START>  │
    │    Je attends to: <START>, Je   │
    │                                 │
    │  Cross-Attention:               │
    │    Je attends to: I (55%)       │
    │                                 │
    └───────────────┬─────────────────┘
                    │
                    ▼
         Probabilities for position 1:
         
         P("t'")   = 0.52  ← HIGHEST
         P("aime") = 0.21
         P("Je")   = 0.08
         ...

OUTPUT: "t'"
─────────────

Next decoder input will be: <START> Je t'
```

### Step 3: Generate "aime"

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP 3: GENERATE "aime"                                 ║
╚══════════════════════════════════════════════════════════════════════╝

INPUT:
──────
Decoder receives: <START> Je t'

    ┌─────────┬─────────┬─────────┐
    │ <START> │   Je    │   t'    │
    └────┬────┴────┬────┴────┬────┘
         │         │         │
         ▼         ▼         ▼
    ┌─────────────────────────────────┐
    │           DECODER               │
    │                                 │
    │  Masked Self-Attention:         │
    │    <START> attends to: <START>  │
    │    Je attends to: <START>, Je   │
    │    t' attends to: <START>,Je,t' │
    │                                 │
    │  Cross-Attention:               │
    │    t' attends to: you (55%)     │
    │                                 │
    └───────────────┬─────────────────┘
                    │
                    ▼
         Probabilities for position 2:
         
         P("aime") = 0.61  ← HIGHEST
         P("<END>")= 0.12
         P("Je")   = 0.05
         ...

OUTPUT: "aime"
──────────────

Next decoder input will be: <START> Je t' aime
```

### Step 4: Generate \<END\>

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP 4: GENERATE <END>                                  ║
╚══════════════════════════════════════════════════════════════════════╝

INPUT:
──────
Decoder receives: <START> Je t' aime

    ┌─────────┬─────────┬─────────┬─────────┐
    │ <START> │   Je    │   t'    │  aime   │
    └────┬────┴────┬────┴────┬────┴────┬────┘
         │         │         │         │
         ▼         ▼         ▼         ▼
    ┌─────────────────────────────────────────┐
    │              DECODER                    │
    │                                         │
    │  Masked Self-Attention:                 │
    │    aime attends to: <START>,Je,t',aime  │
    │                                         │
    │  Cross-Attention:                       │
    │    aime attends to: love (70%)          │
    │                                         │
    └───────────────────┬─────────────────────┘
                        │
                        ▼
         Probabilities for position 3:
         
         P("<END>")= 0.73  ← HIGHEST
         P("aime") = 0.08
         P("Je")   = 0.04
         ...

OUTPUT: <END> (STOP signal)
────────────────────────────

Translation complete!
```

### Complete Translation Summary

```
╔══════════════════════════════════════════════════════════════════════╗
║              COMPLETE TRANSLATION: "I love you" → "Je t'aime"        ║
╚══════════════════════════════════════════════════════════════════════╝


THE COMPLETE FLOW:
──────────────────

    INPUT: "I love you"
              │
              ▼
    ┌─────────────────────────────────────────────────────────────────┐
    │                         ENCODER                                 │
    │                                                                 │
    │  "I"    → Embed → +PE → Self-Attn → FFN → [context vector 0]   │
    │  "love" → Embed → +PE → Self-Attn → FFN → [context vector 1]   │
    │  "you"  → Embed → +PE → Self-Attn → FFN → [context vector 2]   │
    │                                                                 │
    └─────────────────────────────┬───────────────────────────────────┘
                                  │
                    Encoder Output (K, V for cross-attention)
                                  │
                                  ▼
    ┌─────────────────────────────────────────────────────────────────┐
    │                         DECODER                                 │
    │                                                                 │
    │  Step 1: <START>              → Masked Attn → Cross Attn → "Je"│
    │  Step 2: <START> Je           → Masked Attn → Cross Attn → "t'"│
    │  Step 3: <START> Je t'        → Masked Attn → Cross Attn →"aime"│
    │  Step 4: <START> Je t' aime   → Masked Attn → Cross Attn →<END>│
    │                                                                 │
    └─────────────────────────────────────────────────────────────────┘
              │
              ▼
    OUTPUT: "Je t'aime"


CROSS-ATTENTION ALIGNMENT:
──────────────────────────

         English: "I"    ─────────    "love"    ─────────    "you"
                    \                    │                    /
                     \                   │                   /
                      55%               70%               55%
                        \                │                /
                         \               │               /
         French:        "Je"   ────   "aime"   ────    "t'"
         
         
    "Je"   (I)     pays 55% attention to "I"
    "t'"   (you)   pays 55% attention to "you"  
    "aime" (love)  pays 70% attention to "love"


TRANSLATION COMPLETE! ✓
───────────────────────

    Input:  "I love you"
    Output: "Je t'aime"
```

### The Big Picture

```
╔══════════════════════════════════════════════════════════════════════╗
║              THE TRANSFORMER: PUTTING IT ALL TOGETHER                ║
╚══════════════════════════════════════════════════════════════════════╝


    "I love you"
         │
         │ 1. TOKENIZE
         ▼
    [1, 2, 3]
         │
         │ 2. EMBED (lookup in embedding matrix)
         ▼
    [[0.2,0.5,0.1,0.8], [0.9,0.1,0.7,0.3], [0.3,0.8,0.2,0.6]]
         │
         │ 3. ADD POSITIONAL ENCODING (inject position info)
         ▼
    [[0.2,1.5,0.1,1.8], [1.7,0.6,0.7,1.3], [1.2,0.4,0.2,1.6]]
         │
         │ 4. ENCODER (self-attention: each word sees all words)
         │    (feed-forward: process each position)
         │    (repeat N times)
         ▼
    [encoder_out_0, encoder_out_1, encoder_out_2]
         │
         │ 5. DECODER (for each output position):
         │    - Masked self-attention (see only past outputs)
         │    - Cross-attention (look at encoder output)
         │    - Feed-forward
         │    - Linear + Softmax → predict next word
         │
         │    <START>           → "Je"    (P=0.48)
         │    <START> Je        → "t'"    (P=0.52)
         │    <START> Je t'     → "aime"  (P=0.61)
         │    <START> Je t'aime → <END>   (P=0.73)
         │
         ▼
    "Je t'aime"


THE KEY INNOVATIONS:
────────────────────

1. SELF-ATTENTION: Every word can directly attend to every other word
   → Captures long-range dependencies
   → Parallel processing (fast on GPUs)

2. POSITIONAL ENCODING: Sine/cosine patterns encode position
   → Model knows word order without recurrence

3. MULTI-HEAD ATTENTION: Multiple attention "perspectives"
   → Learn different types of relationships

4. ENCODER-DECODER ATTENTION: Decoder attends to encoder
   → Learns word alignments across languages

5. MASKED ATTENTION: Decoder can't see future tokens
   → Enables autoregressive generation


This architecture powers GPT, BERT, Claude, and virtually all
modern language AI systems!
```
