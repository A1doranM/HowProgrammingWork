# Transformer Network

## Table of Contents
1. [Introduction to Transformers](#introduction-to-transformers)
2. [Problems with Previous Architectures](#problems-with-previous-architectures)
3. [Transformer Architecture Overview](#transformer-architecture-overview)
4. [Input Representation](#input-representation)
5. [The Encoder Deep Dive](#the-encoder-deep-dive)
6. [The Decoder Deep Dive](#the-decoder-deep-dive)
7. [Key Components Detailed Breakdown](#key-components-detailed-breakdown)
8. [Complete Forward Pass Example](#complete-forward-pass-example)
9. [Training vs Inference](#training-vs-inference)

---

## Introduction to Transformers

### What is a Transformer?

The **Transformer** is a neural network architecture that processes sequences using **self-attention** as its core mechanism, completely eliminating the need for recurrence (RNNs) or convolutions (CNNs). It was introduced in the landmark paper "Attention Is All You Need" (Vaswani et al., 2017).

**Core Innovation**: Instead of processing tokens sequentially (like RNNs), Transformers process all positions in parallel using attention mechanisms, allowing each position to directly attend to every other position.

**Key Insight**: The paper's title says it all — attention alone is sufficient to build state-of-the-art sequence models, without any recurrent or convolutional layers.

### The "Attention Is All You Need" Paper (2017)

```
╔══════════════════════════════════════════════════════════════════════╗
║              HISTORICAL CONTEXT                                      ║
╚══════════════════════════════════════════════════════════════════════╝

Before Transformers (2014-2017):
────────────────────────────────
• Seq2Seq with Attention (Bahdanau, 2014)
• Still relied on RNNs/LSTMs for sequence processing
• Attention was an "add-on" to help with long sequences
• Sequential processing was the bottleneck

The Transformer Paper (June 2017):
──────────────────────────────────
• Authors: Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin
• Key claim: Remove RNNs entirely, use ONLY attention
• Achieved state-of-the-art on machine translation
• Training was 10x faster than RNN-based models

Impact:
───────
• 2018: BERT (encoder-only Transformer)
• 2018-2020: GPT, GPT-2, GPT-3 (decoder-only Transformer)
• 2020+: T5, BART, Claude, Llama, PaLM, Gemini...
• Transformers now dominate NLP, vision, audio, and more

```

### Why Transformers Revolutionized AI

```
╔══════════════════════════════════════════════════════════════════════╗
║              WHY TRANSFORMERS CHANGED EVERYTHING                     ║
╚══════════════════════════════════════════════════════════════════════╝

1. PARALLELIZATION
─────────────────
RNN:  Process token 1 → then token 2 → then token 3 → ... (sequential)
Transformer: Process ALL tokens simultaneously (parallel)

Result: Massive speedup on GPUs/TPUs

2. LONG-RANGE DEPENDENCIES
──────────────────────────
RNN:  Token 1 must pass through ALL intermediate tokens to reach token 100
      Path length: O(n)
      
Transformer: Token 1 directly attends to token 100
             Path length: O(1)

Result: Much better at capturing long-range relationships

3. SCALABILITY
──────────────
• Transformers scale predictably with more data and parameters
• "Scaling laws" show consistent improvement
• Enabled training of 100B+ parameter models

4. TRANSFER LEARNING
────────────────────
• Pre-train once on massive data
• Fine-tune for specific tasks
• BERT, GPT showed this works incredibly well

5. FLEXIBILITY
──────────────
• Same architecture works for text, images, audio, video, proteins...
• Vision Transformer (ViT), Whisper, AlphaFold all use Transformers
```

---

## Problems with Previous Architectures

### RNN/LSTM Limitations

Before Transformers, **Recurrent Neural Networks (RNNs)** and their variants (LSTM, GRU) were the dominant architecture for sequence modeling.

```
╔══════════════════════════════════════════════════════════════════════╗
║              RNN ARCHITECTURE                                        ║
╚══════════════════════════════════════════════════════════════════════╝

Input: "The cat sat on the mat"

    x₁      x₂      x₃      x₄      x₅      x₆
   "The"   "cat"   "sat"   "on"    "the"   "mat"
     │       │       │       │       │       │
     ▼       ▼       ▼       ▼       ▼       ▼
   ┌───┐   ┌───┐   ┌───┐   ┌───┐   ┌───┐   ┌───┐
   │RNN│──►│RNN│──►│RNN│──►│RNN│──►│RNN│──►│RNN│
   └───┘   └───┘   └───┘   └───┘   └───┘   └───┘
     │       │       │       │       │       │
     ▼       ▼       ▼       ▼       ▼       ▼
    h₁      h₂      h₃      h₄      h₅      h₆

Key characteristic: SEQUENTIAL processing
• Must compute h₁ before h₂
• Must compute h₂ before h₃
• And so on...
```

#### Problem 1: Sequential Processing (No Parallelization)

```
╔══════════════════════════════════════════════════════════════════════╗
║              PROBLEM 1: SEQUENTIAL BOTTLENECK                        ║
╚══════════════════════════════════════════════════════════════════════╝

RNN Computation:
────────────────
Time step 1: Compute h₁ = f(x₁, h₀)
Time step 2: Compute h₂ = f(x₂, h₁)  ← Must wait for h₁!
Time step 3: Compute h₃ = f(x₃, h₂)  ← Must wait for h₂!
...
Time step n: Compute hₙ = f(xₙ, hₙ₋₁)

Total time: O(n) sequential steps

For a sequence of 1000 tokens:
• 1000 sequential operations
• Cannot parallelize across positions
• GPU utilization is poor

┌─────────────────────────────────────────────────────────────────────┐
│ GPU UTILIZATION                                                     │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│ RNN (sequential):                                                   │
│ Time: ══╡1╞══╡2╞══╡3╞══╡4╞══╡5╞══╡6╞══════════►                     │
│ GPU:  [█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]  ~10% utilized              │
│                                                                     │
│ Transformer (parallel):                                             │
│ Time: ══╡1,2,3,4,5,6╞══════════════════►                            │
│ GPU:  [██████████████████████████████]  ~90% utilized               │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

#### Problem 2: Vanishing/Exploding Gradients

```
╔══════════════════════════════════════════════════════════════════════╗
║              PROBLEM 2: GRADIENT FLOW                                ║
╚══════════════════════════════════════════════════════════════════════╝

During backpropagation, gradients must flow backward through time:

Forward:  h₁ ──► h₂ ──► h₃ ──► ... ──► h₉₉ ──► h₁₀₀
Backward: ∂L/∂h₁ ◄── ∂L/∂h₂ ◄── ... ◄── ∂L/∂h₉₉ ◄── ∂L/∂h₁₀₀

The gradient at h₁ involves multiplying many weight matrices:

∂L/∂h₁ = ∂L/∂h₁₀₀ × W × W × W × ... × W  (99 multiplications!)

If eigenvalues of W are:
• < 1: Gradient → 0 (VANISHING) — early tokens don't learn
• > 1: Gradient → ∞ (EXPLODING) — training becomes unstable

Visualization:
──────────────
                    Gradient magnitude
    │
100%│█
    │██
    │███
 50%│████
    │█████
    │██████
  1%│███████████████████████████████████████████████
    └────────────────────────────────────────────────►
     h₁₀₀  h₉₀   h₈₀   h₇₀   h₆₀  ...  h₁₀   h₁
     
     Recent tokens                    Early tokens
     (learn well)                     (gradients vanish)

LSTMs partially solve this with gates, but the problem persists for
very long sequences.
```

#### Problem 3: Long-Range Dependencies

```
╔══════════════════════════════════════════════════════════════════════╗
║              PROBLEM 3: LONG-RANGE DEPENDENCIES                      ║
╚══════════════════════════════════════════════════════════════════════╝

Example sentence:
"The cat, which was sitting on the mat that my grandmother gave me
last Christmas when she visited from Florida, was sleeping."

Question: What was sleeping?
Answer: "The cat" (100+ tokens away from "was sleeping")

RNN path from "cat" to "sleeping":
──────────────────────────────────

"cat" ─► h₂ ─► h₃ ─► h₄ ─► ... ─► h₄₈ ─► h₄₉ ─► h₅₀ ─► "sleeping"
          │     │     │            │      │      │
          ▼     ▼     ▼            ▼      ▼      ▼
      (information gets diluted at each step)

Path length: ~50 steps
Information must survive 50 transformations!

Transformer path from "cat" to "sleeping":
──────────────────────────────────────────

"cat" ════════════════════════════════════════════► "sleeping"
                    DIRECT ATTENTION
                    
Path length: 1 step
Direct connection via attention!

┌─────────────────────────────────────────────────────────────────────┐
│              PATH LENGTH COMPARISON                                 │
├─────────────────┬─────────────────────┬─────────────────────────────┤
│ Architecture    │ Path Length         │ Long-Range Dependencies     │
├─────────────────┼─────────────────────┼─────────────────────────────┤
│ RNN             │ O(n)                │ Difficult                   │
│ CNN             │ O(n/k) or O(logₖn)  │ Moderate                     │
│ Transformer     │ O(1)                │ Easy ✓                      │
└─────────────────┴─────────────────────┴─────────────────────────────┘
```

### How Transformers Solve Each Problem

```
╔══════════════════════════════════════════════════════════════════════╗
║              TRANSFORMER SOLUTIONS                                   ║
╚══════════════════════════════════════════════════════════════════════╝

┌─────────────────────────────────────────────────────────────────────┐
│ PROBLEM                    │ TRANSFORMER SOLUTION                   │
├─────────────────────────────────────────────────────────────────────┤
│                            │                                        │
│ Sequential processing      │ Self-attention processes ALL           │
│ (can't parallelize)        │ positions in PARALLEL                  │
│                            │                                        │
├─────────────────────────────────────────────────────────────────────┤
│                            │                                        │
│ Vanishing gradients        │ Residual connections provide           │
│ (early tokens don't learn) │ direct gradient paths                  │
│                            │                                        │
├─────────────────────────────────────────────────────────────────────┤
│                            │                                        │
│ Long-range dependencies    │ Attention connects ANY two             │
│ (info lost over distance)  │ positions directly (O(1) path)         │
│                            │                                        │
└─────────────────────────────────────────────────────────────────────┘
```

---

## Transformer Architecture Overview

### High-Level Structure

The original Transformer uses an **encoder-decoder** architecture:

```
╔══════════════════════════════════════════════════════════════════════╗
║              TRANSFORMER ARCHITECTURE (HIGH-LEVEL)                   ║
╚══════════════════════════════════════════════════════════════════════╝

                Input                             Output
            "I love you"                       "Je t'aime"
                 │                                  ▲
                 ▼                                  │
        ┌────────────────┐                  ┌───────┴────────┐
        │                │                  │                │
        │  INPUT EMBED   │                  │ OUTPUT LINEAR  │
        │  + POSITION    │                  │   + SOFTMAX    │
        │                │                  │                │
        └───────┬────────┘                  └───────┬────────┘
                │                                   │
                ▼                                   │
        ╔════════════════╗                  ╔═══════╧════════╗
        ║                ║                  ║                ║
        ║    ENCODER     ║                  ║    DECODER     ║
        ║                ║                  ║                ║
        ║  ┌──────────┐  ║                  ║  ┌──────────┐  ║
        ║  │ Layer N  │  ║                  ║  │ Layer N  │  ║
        ║  ├──────────┤  ║                  ║  ├──────────┤  ║
        ║  │   ...    │  ║ ════════════════►║  │   ...    │  ║
        ║  ├──────────┤  ║  (cross-attn)    ║  ├──────────┤  ║
        ║  │ Layer 2  │  ║                  ║  │ Layer 2  │  ║
        ║  ├──────────┤  ║                  ║  ├──────────┤  ║
        ║  │ Layer 1  │  ║                  ║  │ Layer 1  │  ║
        ║  └──────────┘  ║                  ║  └──────────┘  ║
        ║                ║                  ║                ║
        ╚════════════════╝                  ╚════════════════╝
                                                   ▲
                                                   │
                                           ┌───────┴────────┐
                                           │                │
                                           │ OUTPUT EMBED   │
                                           │ + POSITION     │
                                           │                │
                                           └───────┬────────┘
                                                   │
                                             "<START> Je t'"
                                            (shifted outputs)

Key Components:
───────────────
• Encoder: Processes input sequence, creates representations
• Decoder: Generates output sequence, attends to encoder
• Embeddings: Convert tokens to vectors
• Positional Encoding: Add position information
• N layers: Typically N=6 in original paper
```

### The Complete Architecture Diagram

```
╔══════════════════════════════════════════════════════════════════════════╗
║                    COMPLETE TRANSFORMER ARCHITECTURE                     ║
╚══════════════════════════════════════════════════════════════════════════╝


         ENCODER SIDE                              DECODER SIDE
         ════════════                              ════════════

          Input Text                             Output Text (shifted)
         "I love you"                            "<S> Je t'aime"
              │                                        │
              ▼                                        ▼
     ┌─────────────────┐                      ┌─────────────────┐
     │ Input Embedding │                      │Output Embedding │
     └────────┬────────┘                      └────────┬────────┘
              │                                        │
              ▼                                        ▼
     ┌─────────────────┐                      ┌─────────────────┐
     │   Positional    │                      │   Positional    │
     │    Encoding     │                      │    Encoding     │
     └────────┬────────┘                      └────────┬────────┘
              │                                        │
              ▼                                        ▼
     ┌────────┴────────┐                      ┌────────┴────────┐
     │      (+)        │                      │      (+)        │
     └────────┬────────┘                      └────────┬────────┘
              │                                        │
   ╔══════════╧══════════╗              ╔══════════════╧══════════════╗
   ║                     ║              ║                             ║
   ║   ENCODER BLOCK     ║              ║      DECODER BLOCK          ║
   ║       (×N)          ║              ║          (×N)               ║
   ║                     ║              ║                             ║
   ║ ┌─────────────────┐ ║              ║ ┌─────────────────────────┐ ║
   ║ │  Multi-Head     │ ║              ║ │  Masked Multi-Head      │ ║
   ║ │ Self-Attention  │ ║              ║ │    Self-Attention       │ ║
   ║ └────────┬────────┘ ║              ║ └────────────┬────────────┘ ║
   ║          ▼          ║              ║              ▼              ║
   ║ ┌─────────────────┐ ║              ║ ┌─────────────────────────┐ ║
   ║ │   Add & Norm    │ ║              ║ │       Add & Norm        │ ║
   ║ └────────┬────────┘ ║              ║ └────────────┬────────────┘ ║
   ║          ▼          ║              ║              ▼              ║
   ║ ┌─────────────────┐ ║              ║ ┌─────────────────────────┐ ║
   ║ │  Feed-Forward   │ ║    ┌─────────╫─┤  Multi-Head Cross-Attn  │ ║
   ║ │    Network      │ ║    │         ║ │  (Attend to Encoder)    │ ║
   ║ └────────┬────────┘ ║    │         ║ └────────────┬────────────┘ ║
   ║          ▼          ║    │         ║              ▼              ║
   ║ ┌─────────────────┐ ║    │         ║ ┌─────────────────────────┐ ║
   ║ │   Add & Norm    │ ║    │         ║ │       Add & Norm        │ ║
   ║ └────────┬────────┘ ║    │         ║ └────────────┬────────────┘ ║
   ║          │          ║    │         ║              ▼              ║
   ╚══════════╪══════════╝    │         ║ ┌─────────────────────────┐ ║
              │               │         ║ │     Feed-Forward        │ ║
              │               │         ║ │       Network           │ ║
              ▼               │         ║ └────────────┬────────────┘ ║
   ┌─────────────────────┐    │         ║              ▼              ║
   │   ENCODER OUTPUT    │────┘         ║ ┌─────────────────────────┐ ║
   │  (Context Vectors)  │              ║ │       Add & Norm        │ ║
   └─────────────────────┘              ║ └────────────┬────────────┘ ║
                                        ║              │              ║
                                        ╚══════════════╪══════════════╝
                                                       │
                                                       ▼
                                       ┌─────────────────────────────┐
                                       │     Linear Projection       │
                                       │   (to vocabulary size)      │
                                       └──────────────┬──────────────┘
                                                      │
                                                      ▼
                                       ┌─────────────────────────────┐
                                       │          Softmax            │
                                       └──────────────┬──────────────┘
                                                      │
                                                      ▼
                                              Output Probabilities
                                              P("Je"), P("t'"), ...


════════════════════════════════════════════════════════════════════════════
LEGEND:
────────
  ×N         = Stack of N identical layers (N=6 in original Transformer)
  Add & Norm = Residual connection followed by Layer Normalization
  (+)        = Element-wise addition
  ────►      = Data flow direction
════════════════════════════════════════════════════════════════════════════
```

### Component Inventory

```
╔══════════════════════════════════════════════════════════════════════╗
║              TRANSFORMER COMPONENTS                                  ║
╚══════════════════════════════════════════════════════════════════════╝

┌─────────────────────┬────────────────────────────────────────────────┐
│ Component           │ Purpose                                        │
├─────────────────────┼────────────────────────────────────────────────┤
│ Token Embedding     │ Convert token IDs to dense vectors             │
├─────────────────────┼────────────────────────────────────────────────┤
│ Positional Encoding │ Add position information (attention has none)  │
├─────────────────────┼────────────────────────────────────────────────┤
│ Multi-Head Attention│ Allow positions to attend to each other        │
├─────────────────────┼────────────────────────────────────────────────┤
│ Feed-Forward Network│ Add non-linearity and processing capacity      │
├─────────────────────┼────────────────────────────────────────────────┤
│ Residual Connection │ Enable gradient flow, stabilize training       │ 
├─────────────────────┼────────────────────────────────────────────────┤
│ Layer Normalization │ Normalize activations, stabilize training      │
├─────────────────────┼────────────────────────────────────────────────┤
│ Output Linear       │ Project to vocabulary size                     │
├─────────────────────┼────────────────────────────────────────────────┤
│ Softmax             │ Convert to probability distribution            │
└─────────────────────┴────────────────────────────────────────────────┘
```

---

## Input Representation

### Token Embeddings

Before processing, text must be converted to numerical vectors:

```
╔══════════════════════════════════════════════════════════════════════╗
║              TOKENIZATION AND EMBEDDING                              ║
╚══════════════════════════════════════════════════════════════════════╝

Step 1: TOKENIZATION (text → token IDs)
───────────────────────────────────────

Input text: "I love transformers"

Tokenization (subword, e.g., BPE):
  "I"           → token_id: 40
  " love"       → token_id: 765
  " transform"  → token_id: 6121
  "ers"         → token_id: 364

Token IDs: [40, 765, 6121, 364]


Step 2: EMBEDDING LOOKUP (token IDs → vectors)
──────────────────────────────────────────────

Embedding Matrix E ∈ ℝ^(V × d_model)
  V = vocabulary size (e.g., 50,000)
  d_model = embedding dimension (e.g., 512)

┌─────────────────────────────────────────────────────────────────────┐
│                    EMBEDDING MATRIX                                 │
│                                                                     │
│              d_model = 512 dimensions                               │
│         ┌─────────────────────────────────────┐                     │
│ token 0 │ 0.12  -0.34  0.56  ...  0.23  0.11  │                     │
│ token 1 │ 0.45   0.67 -0.12  ... -0.34  0.89  │                     │
│ token 2 │-0.23   0.11  0.78  ...  0.45 -0.56  │                     │
│   ...   │ ...    ...   ...   ...  ...   ...   │ V rows              │
│token 40 │ 0.31  -0.22  0.15  ...  0.67  0.43  │ ← "I"               │
│   ...   │ ...    ...   ...   ...  ...   ...   │                     │
│token765 │ 0.89   0.12 -0.45  ...  0.23 -0.11  │ ← " love"           │
│   ...   │ ...    ...   ...   ...  ...   ...   │                     │
│         └─────────────────────────────────────┘                     │
└─────────────────────────────────────────────────────────────────────┘

Lookup: E[40], E[765], E[6121], E[364]

Result: 4 vectors, each of dimension 512
```

### Positional Encoding

**Critical Problem**: Self-attention treats the input as a **set**, not a sequence. It has no inherent notion of position or order!

```
╔══════════════════════════════════════════════════════════════════════╗
║              WHY POSITIONAL ENCODING IS NEEDED                       ║
╚══════════════════════════════════════════════════════════════════════╝

Without positional information:

  "The cat sat on the mat"   →  Self-Attention  →  Same output as
  "mat the on sat cat The"   →  Self-Attention  →  this sentence!

Self-attention computes:
  Attention(Q, K, V) = softmax(QK^T / √d) V

The operation is PERMUTATION INVARIANT:
  • Shuffling input order doesn't change the attention computation
  • "cat" attends to "sat" the same way regardless of position

But word ORDER MATTERS in language!
  "Dog bites man" ≠ "Man bites dog"

Solution: ADD position information to embeddings BEFORE attention

  Input = TokenEmbedding + PositionalEncoding
```

### Sinusoidal Positional Encoding

The original Transformer uses **sinusoidal** positional encodings:

**Formulas**:

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

**Symbol Legend**:
- $pos$ = Position in the sequence (0, 1, 2, ...)
- $i$ = Dimension index (0, 1, 2, ..., d_model/2 - 1)
- $d_{model}$ = Model dimension (e.g., 512)
- $PE_{(pos, 2i)}$ = Positional encoding at position $pos$, even dimension $2i$
- $PE_{(pos, 2i+1)}$ = Positional encoding at position $pos$, odd dimension $2i+1$

```
╔══════════════════════════════════════════════════════════════════════╗
║              SINUSOIDAL POSITIONAL ENCODING                          ║
╚══════════════════════════════════════════════════════════════════════╝

For d_model = 512:
  • Dimensions 0, 2, 4, ..., 510 use SINE
  • Dimensions 1, 3, 5, ..., 511 use COSINE

Each dimension has a different frequency:
  • Low dimensions (i small): HIGH frequency (rapid oscillation)
  • High dimensions (i large): LOW frequency (slow oscillation)

Visualization (simplified, d_model = 8):
────────────────────────────────────────

Position:   0    1    2    3    4    5    6    7    8    9
         ────────────────────────────────────────────────────
dim 0 (sin)│████    ████    ████    ████    ████    │ High freq
dim 1 (cos)│████████        ████████        ████████│
dim 2 (sin)│████████████            ████████████    │
dim 3 (cos)│████████████████                ████████│
dim 4 (sin)│████████████████████████                │ 
dim 5 (cos)│████████████████████████████████        │
dim 6 (sin)│████████████████████████████████████████│ Low freq
dim 7 (cos)│████████████████████████████████████████│

Key insight: Position is encoded as a combination of frequencies,
like a Fourier series. This allows the model to:
  1. Distinguish any two positions
  2. Learn relative positions (PE[pos+k] is a linear function of PE[pos])
  3. Generalize to longer sequences than seen during training


Numerical Example (d_model = 4):
────────────────────────────────

Position 0:
  PE[0,0] = sin(0 / 10000^(0/4)) = sin(0) = 0.000
  PE[0,1] = cos(0 / 10000^(0/4)) = cos(0) = 1.000
  PE[0,2] = sin(0 / 10000^(2/4)) = sin(0) = 0.000
  PE[0,3] = cos(0 / 10000^(2/4)) = cos(0) = 1.000
  
  PE[0] = [0.000, 1.000, 0.000, 1.000]

Position 1:
  PE[1,0] = sin(1 / 10000^(0/4)) = sin(1) = 0.841
  PE[1,1] = cos(1 / 10000^(0/4)) = cos(1) = 0.540
  PE[1,2] = sin(1 / 10000^(2/4)) = sin(0.01) = 0.010
  PE[1,3] = cos(1 / 10000^(2/4)) = cos(0.01) = 1.000
  
  PE[1] = [0.841, 0.540, 0.010, 1.000]

Position 2:
  PE[2,0] = sin(2) = 0.909
  PE[2,1] = cos(2) = -0.416
  PE[2,2] = sin(0.02) = 0.020
  PE[2,3] = cos(0.02) = 1.000
  
  PE[2] = [0.909, -0.416, 0.020, 1.000]
```

### Combining Embeddings and Positions

```
╔══════════════════════════════════════════════════════════════════════╗
║              FINAL INPUT REPRESENTATION                              ║
╚══════════════════════════════════════════════════════════════════════╝

For input "I love you" (3 tokens):

Step 1: Token Embeddings
────────────────────────
  E["I"]    = [0.12, -0.34, 0.56, 0.23]  (from embedding matrix)
  E["love"] = [0.45,  0.67, -0.12, 0.89]
  E["you"]  = [-0.23, 0.11, 0.78, 0.45]


Step 2: Positional Encodings
────────────────────────────
  PE[0] = [0.000, 1.000, 0.000, 1.000]  (position 0: "I")
  PE[1] = [0.841, 0.540, 0.010, 1.000]  (position 1: "love")
  PE[2] = [0.909, -0.416, 0.020, 1.000] (position 2: "you")


Step 3: Add Together (element-wise)
───────────────────────────────────

  Input["I"]    = E["I"]    + PE[0]
                = [0.12, -0.34, 0.56, 0.23] + [0.000, 1.000, 0.000, 1.000]
                = [0.120, 0.660, 0.560, 1.230]

  Input["love"] = E["love"] + PE[1]
                = [0.45, 0.67, -0.12, 0.89] + [0.841, 0.540, 0.010, 1.000]
                = [1.291, 1.210, -0.110, 1.890]

  Input["you"]  = E["you"]  + PE[2]
                = [-0.23, 0.11, 0.78, 0.45] + [0.909, -0.416, 0.020, 1.000]
                = [0.679, -0.306, 0.800, 1.450]


Final Input Matrix X ∈ ℝ^(3 × 4):
─────────────────────────────────
         d₀      d₁      d₂      d₃
      ┌───────┬───────┬───────┬───────┐
  "I" │ 0.120 │ 0.660 │ 0.560 │ 1.230 │  pos 0
      ├───────┼───────┼───────┼───────┤
"love"│ 1.291 │ 1.210 │-0.110 │ 1.890 │  pos 1
      ├───────┼───────┼───────┼───────┤
 "you"│ 0.679 │-0.306 │ 0.800 │ 1.450 │  pos 2
      └───────┴───────┴───────┴───────┘

This matrix X is now ready to enter the encoder!
```

---

## The Encoder Deep Dive

### Encoder Stack Structure

The encoder consists of N identical layers stacked on top of each other:

```
╔══════════════════════════════════════════════════════════════════════╗
║              ENCODER STACK (N = 6 layers)                            ║
╚══════════════════════════════════════════════════════════════════════╝

Input X (with positional encoding)
        │
        ▼
┌───────────────────────────────────────────────────────────────────────┐
│                         ENCODER LAYER 1                               │
└───────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────┐
│                         ENCODER LAYER 2                               │
└───────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────┐
│                         ENCODER LAYER 3                               │
└───────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────┐
│                         ENCODER LAYER 4                               │
└───────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────┐
│                         ENCODER LAYER 5                               │
└───────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────┐
│                         ENCODER LAYER 6                               │
└───────────────────────────────────────────────────────────────────────┘
        │
        ▼
  Encoder Output
  (sent to decoder)

Each layer:
  • Has the SAME structure
  • Has DIFFERENT learned parameters
  • Input shape = Output shape (residual connections require this)
```

### Single Encoder Layer

```
╔══════════════════════════════════════════════════════════════════════╗
║              SINGLE ENCODER LAYER (Detailed)                         ║
╚══════════════════════════════════════════════════════════════════════╝

Input: X ∈ ℝ^(n × d_model)
        │
        │
        ▼
┌──────────────────────────────────────────────────────────────────────┐
│                                                                      │
│    ┌─────────────────────────────────────────────────────────────┐   │
│    │               MULTI-HEAD SELF-ATTENTION                     │   │
│    │                                                             │   │
│    │   Q = XW^Q    K = XW^K    V = XW^V                          │   │
│    │                                                             │   │
│    │   Attention(Q, K, V) = softmax(QK^T / √d_k) V               │   │
│    │                                                             │   │
│    └─────────────────────────┬───────────────────────────────────┘   │
│                              │                                       │
│    ┌─────────────────────────┼───────────────────────────────────┐   │
│    │         ADD & NORM      │                                   │   │
│    │                         ▼                                   │   │
│ ───┼──────────────────────► (+) ──► LayerNorm ───────────────────┼─► │
│    │    (residual)           │                                   │   │
│    └─────────────────────────┼───────────────────────────────────┘   │
│                              │                                       │
│                              ▼                                       │
│    ┌─────────────────────────────────────────────────────────────┐   │
│    │               FEED-FORWARD NETWORK                          │   │
│    │                                                             │   │
│    │   FFN(x) = W₂ · ReLU(W₁ · x + b₁) + b₂                      │   │
│    │                                                             │   │
│    │   W₁: d_model → d_ff (expand)                               │   │
│    │   W₂: d_ff → d_model (contract)                             │   │
│    │                                                             │   │
│    └─────────────────────────┬───────────────────────────────────┘   │
│                              │                                       │
│    ┌─────────────────────────┼───────────────────────────────────┐   │
│    │         ADD & NORM      │                                   │   │
│    │                         ▼                                   │   │
│ ───┼──────────────────────► (+) ──► LayerNorm ───────────────────┼─► │
│    │    (residual)           │                                   │   │
│    └─────────────────────────┼───────────────────────────────────┘   │
│                              │                                       │
└──────────────────────────────┼───────────────────────────────────────┘
                               │
                               ▼
                        Output: same shape as input
                        (n × d_model)


Formula Summary:
────────────────
1. Self-Attention output:   A = MultiHead(X, X, X)
2. First residual + norm:   X' = LayerNorm(X + A)
3. FFN output:              F = FFN(X')
4. Second residual + norm:  Output = LayerNorm(X' + F)
```

### Encoder Information Flow

```
╔══════════════════════════════════════════════════════════════════════╗
║              WHAT THE ENCODER DOES                                   ║
╚══════════════════════════════════════════════════════════════════════╝

Input: "The cat sat"

Layer 1 (Low-level patterns):
─────────────────────────────
• Each word looks at nearby words
• Learns basic syntactic patterns
• "cat" notices it follows "The" (determiner-noun)

    "The"  "cat"  "sat"
      │      │      │
      ▼      ▼      ▼
    [z₁¹]  [z₂¹]  [z₃¹]   ← Layer 1 outputs

Layer 2-3 (Mid-level patterns):
───────────────────────────────
• Builds phrase-level understanding
• "cat" integrates information from "The cat"
• "sat" understands its subject is "cat"

Layer 4-6 (High-level understanding):
─────────────────────────────────────
• Abstract semantic representations
• Each position contains contextual meaning
• "cat" representation includes: it's a noun, subject of "sat",
  modified by "The", represents an animal concept

Final encoder output:
─────────────────────
    "The"  "cat"  "sat"
      │      │      │
      ▼      ▼      ▼
    [z₁⁶]  [z₂⁶]  [z₃⁶]   ← Final representations

These are NOT just word embeddings anymore!
Each zᵢ⁶ is a CONTEXTUALIZED representation that "knows"
about the entire input sentence.

Example:
  "bank" in "river bank" → representation about nature, geography
  "bank" in "bank account" → representation about finance, money
  
  Same word, different contextual representations!
```

---

## The Decoder Deep Dive

### Decoder Stack Structure

```
╔══════════════════════════════════════════════════════════════════════╗
║              DECODER STACK (N = 6 layers)                            ║
╚══════════════════════════════════════════════════════════════════════╝

Output Embeddings (shifted right) + Positional Encoding
        │
        ▼
┌───────────────────────────────────────────────────────────────────────┐
│                         DECODER LAYER 1                         ◄════╬═══╗
└───────────────────────────────────────────────────────────────────────┘  ║
        │                                                                  ║
        ▼                                                                  ║
┌───────────────────────────────────────────────────────────────────────┐  ║
│                         DECODER LAYER 2                         ◄════╬═══╣
└───────────────────────────────────────────────────────────────────────┘  ║
        │                                                                  ║
        ▼                                                                  ║
       ...                                                                 ║
        │                                                                  ║
        ▼                                                                  ║
┌───────────────────────────────────────────────────────────────────────┐  ║
│                         DECODER LAYER 6                         ◄════╬═══╣
└───────────────────────────────────────────────────────────────────────┘  ║
        │                                                                  ║
        ▼                                                             ENCODER
   Linear Layer                                                       OUTPUT
        │                                                            (from
        ▼                                                            encoder)
   Softmax
        │
        ▼
 Output Probabilities
```

### Single Decoder Layer

```
╔══════════════════════════════════════════════════════════════════════╗
║              SINGLE DECODER LAYER (Detailed)                         ║
╚══════════════════════════════════════════════════════════════════════╝

Input: Y ∈ ℝ^(m × d_model)     Encoder Output: Z ∈ ℝ^(n × d_model)
        │                                │
        │                                │
        ▼                                │
┌───────────────────────────────────────────────────────────────────────┐
│                                                                       │
│    ┌──────────────────────────────────────────────────────────────┐   │
│    │          MASKED MULTI-HEAD SELF-ATTENTION                    │   │
│    │                                                              │   │
│    │   Q = YW^Q    K = YW^K    V = YW^V                           │   │
│    │                                                              │   │
│    │   Apply MASK to prevent attending to future positions        │   │
│    │   Attention(Q, K, V, Mask)                                   │   │
│    │                                                              │   │
│    └─────────────────────────┬────────────────────────────────────┘   │
│                              │                                        │
│    ┌─────────────────────────┼───────────────────────────────────┐    │
│    │         ADD & NORM      │                                   │    │
│ ───┼──────────────────────► (+) ──► LayerNorm ───────────────────┼─►  │
│    └─────────────────────────┼───────────────────────────────────┘    │
│                              │                                        │
│                              ▼                                        │
│    ┌─────────────────────────────────────────────────────────────┐    │
│    │          MULTI-HEAD CROSS-ATTENTION                     ◄───┼────┤
│    │                                                             │    │
│    │   Q = (from below)     K = ZW^K    V = ZW^V                 │    │
│    │        ↑                    ↑           ↑                   │    │
│    │   (decoder)            (encoder)   (encoder)                │    │
│    │                                                             │    │
│    │   Decoder queries attend to encoder keys/values             │    │
│    │                                                             │    │
│    └─────────────────────────┬───────────────────────────────────┘    │
│                              │                                        │
│    ┌─────────────────────────┼───────────────────────────────────┐    │
│    │         ADD & NORM      │                                   │    │
│ ───┼──────────────────────► (+) ──► LayerNorm ───────────────────┼─►  │
│    └─────────────────────────┼───────────────────────────────────┘    │
│                              │                                        │
│                              ▼                                        │
│    ┌──────────────────────────────────────────────────────────────┐   │
│    │               FEED-FORWARD NETWORK                           │   │
│    │                                                              │   │
│    │   FFN(x) = W₂ · ReLU(W₁ · x + b₁) + b₂                       │   │
│    │                                                              │   │
│    └─────────────────────────┬────────────────────────────────────┘   │
│                              │                                        │
│    ┌─────────────────────────┼───────────────────────────────────┐    │
│    │         ADD & NORM      │                                   │    │
│ ───┼──────────────────────► (+) ──► LayerNorm ───────────────────┼─►  │
│    └─────────────────────────┼───────────────────────────────────┘    │
│                              │                                        │
└──────────────────────────────┼────────────────────────────────────────┘
                               │
                               ▼
                        Output: (m × d_model)


Key Differences from Encoder:
─────────────────────────────
1. Self-attention is MASKED (can't see future)
2. Has CROSS-ATTENTION to encoder output (encoder doesn't have this)
3. Three sub-layers instead of two
```

### Masking Explained

**Why Masking?** During training, we feed the entire target sequence to the decoder at once. But the model should NOT be able to "cheat" by looking at future tokens!

```
╔══════════════════════════════════════════════════════════════════════╗
║              WHY MASKING IS NEEDED                                   ║
╚══════════════════════════════════════════════════════════════════════╝

Task: Translate "I love you" → "Je t'aime"

Training input to decoder: "<START> Je t' aime"
Expected output:           "Je t' aime <END>"

WITHOUT masking (CHEATING!):
────────────────────────────
When predicting position 1 ("Je"):
  Decoder can see: "<START>", "Je", "t'", "aime"
                              ↑    ↑     ↑
                        Future tokens visible!
  
  Model can just COPY the answer → learns nothing!

WITH masking (CORRECT):
───────────────────────
When predicting position 1 ("Je"):
  Decoder can see: "<START>" only
  
When predicting position 2 ("t'"):
  Decoder can see: "<START>", "Je"
  
When predicting position 3 ("aime"):
  Decoder can see: "<START>", "Je", "t'"
  
When predicting position 4 ("<END>"):
  Decoder can see: "<START>", "Je", "t'", "aime"

This mimics inference time when future tokens don't exist yet!
```

### Causal Mask Visualization

```
╔══════════════════════════════════════════════════════════════════════╗
║              CAUSAL (LOOK-AHEAD) MASK                                ║
╚══════════════════════════════════════════════════════════════════════╝

Sequence: "<S> Je t' aime" (4 positions)

Attention Score Matrix (before masking):
─────────────────────────────────────────

              Keys (what we attend TO)
              <S>    Je     t'    aime
           ┌───────┬───────┬───────┬───────┐
      <S>  │  1.2  │  0.5  │  0.3  │  0.1  │
           ├───────┼───────┼───────┼───────┤
Queries Je │  0.8  │  2.1  │  1.5  │  0.9  │
(what is   ├───────┼───────┼───────┼───────┤
attending) t'│  0.4  │  1.3  │  1.8  │  1.2  │
           ├───────┼───────┼───────┼───────┤
     aime  │  0.2  │  0.7  │  1.1  │  2.0  │
           └───────┴───────┴───────┴───────┘

Mask Matrix:
────────────

              <S>    Je     t'    aime
           ┌───────┬───────┬───────┬───────┐
      <S>  │   0   │  -∞   │  -∞   │  -∞   │  (can only see itself)
           ├───────┼───────┼───────┼───────┤
      Je   │   0   │   0   │  -∞   │  -∞   │  (can see <S>, Je)
           ├───────┼───────┼───────┼───────┤
      t'   │   0   │   0   │   0   │  -∞   │  (can see <S>, Je, t')
           ├───────┼───────┼───────┼───────┤
     aime  │   0   │   0   │   0   │   0   │  (can see all)
           └───────┴───────┴───────┴───────┘

Legend: 0 = allowed, -∞ = blocked


Masked Attention Scores (Score + Mask):
───────────────────────────────────────

              <S>    Je     t'    aime
           ┌───────┬───────┬───────┬───────┐
      <S>  │  1.2  │  -∞   │  -∞   │  -∞   │
           ├───────┼───────┼───────┼───────┤
      Je   │  0.8  │  2.1  │  -∞   │  -∞   │
           ├───────┼───────┼───────┼───────┤
      t'   │  0.4  │  1.3  │  1.8  │  -∞   │
           ├───────┼───────┼───────┼───────┤
     aime  │  0.2  │  0.7  │  1.1  │  2.0  │
           └───────┴───────┴───────┴───────┘


After Softmax:
──────────────
softmax(-∞) = 0  (blocked positions get ZERO attention)

              <S>    Je     t'    aime
           ┌───────┬───────┬───────┬───────┐
      <S>  │ 1.00  │ 0.00  │ 0.00  │ 0.00  │  sum = 1.0
           ├───────┼───────┼───────┼───────┤
      Je   │ 0.21  │ 0.79  │ 0.00  │ 0.00  │  sum = 1.0
           ├───────┼───────┼───────┼───────┤
      t'   │ 0.11  │ 0.27  │ 0.62  │ 0.00  │  sum = 1.0
           ├───────┼───────┼───────┼───────┤
     aime  │ 0.06  │ 0.10  │ 0.17  │ 0.67  │  sum = 1.0
           └───────┴───────┴───────┴───────┘

Visual pattern (lower triangular):

           ┌───────────────────────┐
           │ ██  ░░  ░░  ░░        │
           │ ██  ██  ░░  ░░        │
           │ ██  ██  ██  ░░        │
           │ ██  ██  ██  ██        │
           └───────────────────────┘
           
           ██ = can attend    ░░ = blocked (future)
```

### Autoregressive Generation

```
╔══════════════════════════════════════════════════════════════════════╗
║              AUTOREGRESSIVE GENERATION                               ║
╚══════════════════════════════════════════════════════════════════════╝

The decoder generates ONE token at a time, using previous outputs
as input for the next step.

Step 1: Generate first token
───────────────────────────────────────────────────────────────────────
Input:  ["<START>"]
Output: Predict → "Je" (highest probability)

Step 2: Generate second token
───────────────────────────────────────────────────────────────────────
Input:  ["<START>", "Je"]
Output: Predict → "t'" (highest probability)

Step 3: Generate third token
───────────────────────────────────────────────────────────────────────
Input:  ["<START>", "Je", "t'"]
Output: Predict → "aime" (highest probability)

Step 4: Generate fourth token
───────────────────────────────────────────────────────────────────────
Input:  ["<START>", "Je", "t'", "aime"]
Output: Predict → "<END>" (stop token)

Final output: "Je t'aime"


Visualization:
──────────────

  Step 1        Step 2          Step 3            Step 4
    │             │               │                 │
    ▼             ▼               ▼                 ▼
 ┌─────┐      ┌─────┐         ┌─────┐           ┌─────┐
 │<S>  │      │<S>  │         │<S>  │           │<S>  │
 │     │  →   │ Je  │    →    │ Je  │     →     │ Je  │
 │     │      │     │         │ t'  │           │ t'  │
 │     │      │     │         │     │           │aime │
 └──┬──┘      └──┬──┘         └──┬──┘           └──┬──┘
    │            │               │                 │
    ▼            ▼               ▼                 ▼
  "Je"         "t'"           "aime"            "<END>"

Each step feeds back into the next!
```

---

## Key Components Detailed Breakdown

### 7.1 Multi-Head Attention (Transformer-Specific)

```
╔══════════════════════════════════════════════════════════════════════╗
║              MULTI-HEAD ATTENTION IN TRANSFORMERS                    ║
╚══════════════════════════════════════════════════════════════════════╝

Formulas:
─────────

MultiHead(Q, K, V) = Concat(head₁, head₂, ..., headₕ) W^O

where headᵢ = Attention(QWᵢ^Q, KWᵢ^K, VWᵢ^V)

and Attention(Q, K, V) = softmax(QK^T / √dₖ) V


Dimensions (original Transformer):
──────────────────────────────────

d_model = 512      (model dimension)
h = 8              (number of heads)
d_k = d_v = 64     (dimension per head = d_model / h)

Parameter matrices:
  Wᵢ^Q ∈ ℝ^(d_model × d_k) = ℝ^(512 × 64)    (one per head)
  Wᵢ^K ∈ ℝ^(d_model × d_k) = ℝ^(512 × 64)    (one per head)
  Wᵢ^V ∈ ℝ^(d_model × d_v) = ℝ^(512 × 64)    (one per head)
  W^O ∈ ℝ^(h·d_v × d_model) = ℝ^(512 × 512)  (output projection)


Architecture Diagram:
─────────────────────

                    Input X ∈ ℝ^(n × 512)
                           │
        ┌──────────────────┼──────────────────┐
        │                  │                  │
        ▼                  ▼                  ▼
   ┌─────────┐        ┌─────────┐        ┌─────────┐
   │  W^Q    │        │  W^K    │        │  W^V    │
   │(512×512)│        │(512×512)│        │(512×512)│
   └────┬────┘        └────┬────┘        └────┬────┘
        │                  │                  │
        ▼                  ▼                  ▼
   Q (n×512)          K (n×512)          V (n×512)
        │                  │                  │
        │    ┌─────────────┼─────────────┐    │
        │    │   Split into h=8 heads    │    │
        │    └─────────────┼─────────────┘    │
        │                  │                  │
   ┌────┴────┬────┬────┬───┴───┬────┬────┬────┴────┐
   │         │    │    │       │    │    │         │
   ▼         ▼    ▼    ▼       ▼    ▼    ▼         ▼
 Q₁(n×64)  Q₂   Q₃   Q₄    ...                  Q₈(n×64)
 K₁(n×64)  K₂   K₃   K₄    ...                  K₈(n×64)
 V₁(n×64)  V₂   V₃   V₄    ...                  V₈(n×64)
   │         │    │    │       │    │    │         │
   ▼         ▼    ▼    ▼       ▼    ▼    ▼         ▼
┌─────┐  ┌─────┐    ┌─────┐        ┌─────┐     ┌─────┐
│Attn₁│  │Attn₂│    │Attn₃│  ...   │Attn₇│     │Attn₈│
│head │  │head │    │head │        │head │     │head │
└──┬──┘  └──┬──┘    └──┬──┘        └──┬──┘     └──┬──┘
   │        │          │              │           │
   ▼        ▼          ▼              ▼           ▼
 O₁(n×64) O₂(n×64)  O₃(n×64)  ...  O₇(n×64)  O₈(n×64)
   │        │          │              │           │
   └────────┴──────────┴──────────────┴───────────┘
                       │
                       ▼
              Concat: (n × 512)
                       │
                       ▼
                 ┌───────────┐
                 │    W^O    │
                 │ (512×512) │
                 └─────┬─────┘
                       │
                       ▼
               Output (n × 512)
```

### 7.2 Feed-Forward Network (FFN)

```
╔══════════════════════════════════════════════════════════════════════╗
║              FEED-FORWARD NETWORK                                    ║
╚══════════════════════════════════════════════════════════════════════╝

Formula:
────────

FFN(x) = W₂ · ReLU(W₁ · x + b₁) + b₂

Alternative (modern Transformers use GELU):
FFN(x) = W₂ · GELU(W₁ · x + b₁) + b₂


Dimensions:
───────────

d_model = 512      (input/output dimension)
d_ff = 2048        (inner dimension, typically 4 × d_model)

W₁ ∈ ℝ^(d_model × d_ff) = ℝ^(512 × 2048)   (expand)
b₁ ∈ ℝ^(d_ff) = ℝ^(2048)
W₂ ∈ ℝ^(d_ff × d_model) = ℝ^(2048 × 512)   (contract)
b₂ ∈ ℝ^(d_model) = ℝ^(512)


Architecture:
─────────────

Input x ∈ ℝ^(512)
       │
       ▼
  ┌─────────┐
  │   W₁    │  (512 × 2048)
  │   + b₁  │
  └────┬────┘
       │
       ▼
  ┌─────────┐
  │  ReLU   │  or GELU
  │         │
  └────┬────┘
       │
       ▼
  Hidden ∈ ℝ^(2048)   ← EXPANDED (4× larger)
       │
       ▼
  ┌─────────┐
  │   W₂    │  (2048 × 512)
  │   + b₂  │
  └────┬────┘
       │
       ▼
Output ∈ ℝ^(512)      ← Back to original size


Why FFN is needed:
──────────────────

1. Attention is LINEAR in values (weighted sum)
   → Need non-linearity for complex functions

2. FFN adds "processing capacity"
   → Each position is independently transformed

3. Expansion-contraction pattern
   → Allows learning complex transformations
   → Inner dimension d_ff > d_model gives more capacity


Applied position-wise:
──────────────────────

For sequence of n positions, FFN is applied INDEPENDENTLY to each:

  Position 1: FFN(x₁) → y₁
  Position 2: FFN(x₂) → y₂
  Position 3: FFN(x₃) → y₃
  ...
  Position n: FFN(xₙ) → yₙ

Same weights W₁, W₂ for all positions (parameter sharing)
```

### 7.3 Residual Connections

```
╔══════════════════════════════════════════════════════════════════════╗
║              RESIDUAL CONNECTIONS                                    ║
╚══════════════════════════════════════════════════════════════════════╝

Formula:
────────

Output = LayerNorm(x + Sublayer(x))

where Sublayer is either:
  • Multi-Head Attention
  • Feed-Forward Network


Diagram:
────────

        x (input)
        │
    ────┼──────────────────────┐
        │                      │
        ▼                      │
  ┌───────────┐                │
  │ Sublayer  │                │
  │ (Attn or  │                │
  │   FFN)    │                │
  └─────┬─────┘                │
        │                      │
        ▼                      │
       (+) ◄───────────────────┘  (RESIDUAL: add input directly)
        │
        ▼
  ┌───────────┐
  │ LayerNorm │
  └─────┬─────┘
        │
        ▼
     Output


Why Residuals Help:
───────────────────

1. GRADIENT FLOW
   Without residual: gradient must pass through every sublayer
   With residual: gradient has a "highway" that bypasses sublayers

   ∂L/∂x = ∂L/∂output × (1 + ∂Sublayer/∂x)
                         ↑
           This "1" ensures gradient flows even if sublayer gradient is small

2. TRAINING STABILITY
   • Deep networks (N=6+) are hard to train
   • Residuals prevent gradient vanishing
   • Each layer learns a "delta" to add to input

3. IDENTITY INITIALIZATION
   • If Sublayer outputs 0, output = x (identity function)
   • Network can start as identity and gradually learn
   • Makes optimization landscape smoother


Visualization of Gradient Highway:
──────────────────────────────────

WITHOUT residuals:
  Input → Layer1 → Layer2 → Layer3 → ... → Layer6 → Output
            ↓        ↓        ↓              ↓
          (gradient must pass through all layers)

WITH residuals:
  Input ═══════════════════════════════════════════════► Output
          ↓        ↓        ↓              ↓
        Layer1   Layer2   Layer3   ...   Layer6
        
  Gradient can flow directly OR through layers!
```

### 7.4 Layer Normalization

```
╔══════════════════════════════════════════════════════════════════════╗
║              LAYER NORMALIZATION                                     ║
╚══════════════════════════════════════════════════════════════════════╝

Formula:
────────

LayerNorm(x) = γ ⊙ (x - μ) / √(σ² + ε) + β

where:
  μ = (1/d) Σᵢ xᵢ           (mean over features)
  σ² = (1/d) Σᵢ (xᵢ - μ)²  (variance over features)


Symbol Legend:
──────────────
x ∈ ℝ^d        = Input vector (d = d_model)
μ ∈ ℝ          = Mean of x
σ² ∈ ℝ         = Variance of x
ε              = Small constant for numerical stability (e.g., 10⁻⁶)
γ ∈ ℝ^d        = Learned scale parameter
β ∈ ℝ^d        = Learned shift parameter
⊙              = Element-wise multiplication


Numerical Example:
──────────────────

Input: x = [2.0, 4.0, 6.0, 8.0]  (d = 4)

Step 1: Compute mean
  μ = (2.0 + 4.0 + 6.0 + 8.0) / 4 = 20.0 / 4 = 5.0

Step 2: Compute variance
  σ² = [(2-5)² + (4-5)² + (6-5)² + (8-5)²] / 4
     = [9 + 1 + 1 + 9] / 4
     = 20 / 4 = 5.0

Step 3: Normalize
  x_norm = (x - μ) / √(σ² + ε)
         = ([2,4,6,8] - 5) / √5.0
         = [-3, -1, 1, 3] / 2.236
         = [-1.34, -0.45, 0.45, 1.34]

Step 4: Scale and shift (assume γ = [1,1,1,1], β = [0,0,0,0])
  output = γ ⊙ x_norm + β
         = [-1.34, -0.45, 0.45, 1.34]


LayerNorm vs BatchNorm:
───────────────────────

┌─────────────────────────────────────────────────────────────────────┐
│              NORMALIZATION COMPARISON                               │
├─────────────────┬─────────────────────┬─────────────────────────────┤
│ Property        │ BatchNorm           │ LayerNorm                   │
├─────────────────┼─────────────────────┼─────────────────────────────┤
│ Normalize over  │ Batch dimension     │ Feature dimension           │
│ Statistics      │ Across samples      │ Within each sample          │
│ Batch size dep. │ Yes (needs batches) │ No (works with batch=1)     │
│ Sequence models │ Problematic         │ Works well ✓                │
│ Used in         │ CNNs, older models  │ Transformers, RNNs          │
└─────────────────┴─────────────────────┴─────────────────────────────┘


Why LayerNorm for Transformers:
───────────────────────────────

1. Works with variable-length sequences
2. No batch statistics needed (good for inference)
3. Normalizes each position independently
4. Stabilizes activations during training
```

---

## Complete Forward Pass Example

### Setup

**Task**: Translate "I love" → "Je t'aime"

**Model Configuration**:
- d_model = 4 (simplified for illustration)
- h = 2 heads
- d_k = d_v = 2
- d_ff = 8
- N = 1 layer (simplified)

### Step 1: Tokenize and Embed

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP 1: INPUT EMBEDDING                                 ║
╚══════════════════════════════════════════════════════════════════════╝

Input: "I love"

Tokenization:
  "I"    → token_id 1
  "love" → token_id 2

Embedding lookup (d_model = 4):
  E[1] = [0.5, -0.3, 0.2, 0.8]   ("I")
  E[2] = [0.1, 0.7, -0.4, 0.3]   ("love")

Positional Encoding (simplified):
  PE[0] = [0.0, 1.0, 0.0, 1.0]   (position 0)
  PE[1] = [0.84, 0.54, 0.01, 1.0] (position 1)

Input = Embedding + Position:
  X[0] = [0.5, -0.3, 0.2, 0.8] + [0.0, 1.0, 0.0, 1.0] = [0.5, 0.7, 0.2, 1.8]
  X[1] = [0.1, 0.7, -0.4, 0.3] + [0.84, 0.54, 0.01, 1.0] = [0.94, 1.24, -0.39, 1.3]

Input Matrix X:
┌───────────────────────────────────┐
│  0.50   0.70   0.20   1.80  │ "I"
│  0.94   1.24  -0.39   1.30  │ "love"
└───────────────────────────────────┘
```

### Step 2: Encoder Self-Attention

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP 2: ENCODER SELF-ATTENTION                          ║
╚══════════════════════════════════════════════════════════════════════╝

Simplified: Using single-head attention for clarity

Weight matrices (learned parameters):
  W^Q = [[0.5, 0.1],      W^K = [[0.3, 0.2],      W^V = [[0.4, 0.1],
         [-0.2, 0.4],            [0.1, 0.5],             [0.2, 0.3],
         [0.3, -0.1],            [-0.1, 0.3],            [-0.1, 0.4],
         [0.1, 0.2]]             [0.4, 0.1]]             [0.3, 0.2]]

Compute Q = X W^Q:
  Q[0] = [0.5, 0.7, 0.2, 1.8] × W^Q = [0.56, 0.65]
  Q[1] = [0.94, 1.24, -0.39, 1.3] × W^Q = [0.47, 0.84]

  Q = [[0.56, 0.65],
       [0.47, 0.84]]

Compute K = X W^K:
  K = [[0.95, 0.75],
       [0.89, 1.05]]

Compute V = X W^V:
  V = [[0.69, 0.60],
       [0.75, 0.55]]


Attention Scores: QK^T / √d_k
─────────────────────────────

QK^T = [[0.56×0.95 + 0.65×0.75,  0.56×0.89 + 0.65×1.05],
        [0.47×0.95 + 0.84×0.75,  0.47×0.89 + 0.84×1.05]]
     
     = [[1.02, 1.18],
        [1.08, 1.30]]

Scaled (÷ √2 ≈ 1.41):
     = [[0.72, 0.84],
        [0.77, 0.92]]


Softmax (row-wise):
───────────────────

Row 0: softmax([0.72, 0.84]) = [0.47, 0.53]
Row 1: softmax([0.77, 0.92]) = [0.46, 0.54]

Attention weights:
┌─────────────────┐
│  0.47    0.53   │  "I" attends to "I" and "love"
│  0.46    0.54   │  "love" attends to "I" and "love"
└─────────────────┘


Context = Attention × V:
────────────────────────

Context[0] = 0.47×[0.69, 0.60] + 0.53×[0.75, 0.55] = [0.72, 0.57]
Context[1] = 0.46×[0.69, 0.60] + 0.54×[0.75, 0.55] = [0.72, 0.57]

(Note: similar because weights are similar - both words attend similarly)
```

### Step 3: Encoder Add & Norm, FFN

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP 3: RESIDUAL, LAYERNORM, FFN                        ║
╚══════════════════════════════════════════════════════════════════════╝

After projecting attention output back to d_model = 4:
  Attn_out = [[0.72, 0.57, 0.45, 0.63],
              [0.72, 0.57, 0.48, 0.61]]

Residual connection:
  X' = X + Attn_out
     = [[0.5, 0.7, 0.2, 1.8],     +  [[0.72, 0.57, 0.45, 0.63],
        [0.94, 1.24, -0.39, 1.3]]     [0.72, 0.57, 0.48, 0.61]]
     
     = [[1.22, 1.27, 0.65, 2.43],
        [1.66, 1.81, 0.09, 1.91]]

Layer Normalization:
  X'_norm = LayerNorm(X')
          = [[-0.71, -0.61, -1.15, 0.88],
             [-0.23, 0.10, -1.40, 0.55]]  (normalized per row)


Feed-Forward Network:
─────────────────────

FFN(x) = W₂ × ReLU(W₁ × x + b₁) + b₂

After FFN:
  FFN_out = [[0.45, 0.32, 0.21, 0.56],
             [0.38, 0.41, 0.18, 0.49]]


Final Residual + LayerNorm:
───────────────────────────

Encoder_output = LayerNorm(X'_norm + FFN_out)
               = [[-0.52, -0.43, -0.89, 0.72],
                  [-0.31, 0.28, -1.02, 0.61]]

This is sent to the decoder!
```

### Step 4: Decoder (Generating "Je")

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP 4: DECODER - GENERATING FIRST TOKEN                ║
╚══════════════════════════════════════════════════════════════════════╝

Decoder input: ["<START>"]
  Y = [[0.1, 0.2, 0.3, 0.4]]  (embedding of <START>)


MASKED SELF-ATTENTION:
──────────────────────
Only one token, so no masking effect yet.
Y attends to itself → gets its own representation.

Self_attn_out = [[0.15, 0.25, 0.35, 0.45]]


CROSS-ATTENTION TO ENCODER:
───────────────────────────

Query from decoder:
  Q_dec = Y × W^Q_cross = [[0.42, 0.38]]

Keys/Values from encoder output:
  K_enc = Encoder_output × W^K_cross
  V_enc = Encoder_output × W^V_cross

Attention scores:
  Scores = Q_dec × K_enc^T / √d_k
  
Softmax:
  α = [0.45, 0.55]  (attends to both "I" and "love")

Context from encoder:
  c = 0.45 × V_enc[0] + 0.55 × V_enc[1]


FEED-FORWARD + OUTPUT:
──────────────────────

After FFN and final projection to vocabulary:

  logits = Linear(decoder_output)  → [vocab_size] scores
  probs = Softmax(logits)

  probs["Je"] = 0.85  ← HIGHEST
  probs["I"]  = 0.05
  probs["Tu"] = 0.03
  ...

Output: "Je" ✓
```

### Step 5: Continue Generation

```
╔══════════════════════════════════════════════════════════════════════╗
║              STEP 5: CONTINUE GENERATING                             ║
╚══════════════════════════════════════════════════════════════════════╝

Iteration 2: Generate "t'"
──────────────────────────
Decoder input: ["<START>", "Je"]
  • Masked self-attention: "Je" can only see "<START>" and itself
  • Cross-attention: attend to encoder output
  • Output: "t'" (highest probability)


Iteration 3: Generate "aime"
────────────────────────────
Decoder input: ["<START>", "Je", "t'"]
  • Masked self-attention: "aime" can see all previous tokens
  • Cross-attention: attend to encoder output
  • Output: "aime" (highest probability)


Iteration 4: Generate "<END>"
─────────────────────────────
Decoder input: ["<START>", "Je", "t'", "aime"]
  • Output: "<END>" (stop token)


COMPLETE GENERATION:
────────────────────

    Step 1        Step 2        Step 3        Step 4
       │             │             │             │
       ▼             ▼             ▼             ▼
    ┌─────┐      ┌─────┐      ┌─────┐      ┌─────┐
    │<S>  │      │<S>  │      │<S>  │      │<S>  │
    │     │  →   │ Je  │  →   │ Je  │  →   │ Je  │
    │     │      │     │      │ t'  │      │ t'  │
    │     │      │     │      │     │      │aime │
    └──┬──┘      └──┬──┘      └──┬──┘      └──┬──┘
       │            │             │             │
       ▼            ▼             ▼             ▼
     "Je"         "t'"        "aime"       "<END>"

Final output: "Je t'aime" ✓

Input: "I love"
Output: "Je t'aime"
Translation successful!
```

---

## Training vs Inference

### Training Mode

```
╔══════════════════════════════════════════════════════════════════════╗
║              TRAINING MODE                                           ║
╚══════════════════════════════════════════════════════════════════════╝

TEACHER FORCING:
────────────────
During training, we feed the GROUND TRUTH target sequence to the decoder,
not the model's own predictions.

Example:
  Source: "I love you"
  Target: "<S> Je t'aime <E>"

  Decoder input:  ["<S>", "Je", "t'", "aime"]    (ground truth, shifted)
  Expected output: ["Je", "t'", "aime", "<E>"]   (ground truth)


Why Teacher Forcing?
────────────────────
• PARALLELIZATION: All positions computed at once!
• STABLE TRAINING: Model always sees correct history
• FASTER CONVERGENCE: Direct supervision signal


Training Forward Pass:
──────────────────────

      Encoder Input          Decoder Input (ground truth)
      "I love you"           "<S> Je t' aime"
           │                       │
           ▼                       ▼
      ┌─────────┐             ┌─────────┐
      │ ENCODER │────────────►│ DECODER │
      └─────────┘             └────┬────┘
                                   │
                                   ▼ (ALL positions at once!)
                            ┌─────────────┐
                            │   Output    │
                            │ Predictions │
                            └──────┬──────┘
                                   │
                    ┌──────────────┼──────────────┐
                    │              │              │
                    ▼              ▼              ▼
                P("Je")      P("t'")       P("aime")    P("<E>")
                   │              │              │          │
                   ▼              ▼              ▼          ▼
              Compare to    Compare to    Compare to   Compare to
               "Je"          "t'"         "aime"       "<E>"
                   │              │              │          │
                   └──────────────┴──────────────┴──────────┘
                                   │
                                   ▼
                            Cross-Entropy Loss
                                   │
                                   ▼
                             Backpropagation


Loss Calculation:
─────────────────

Loss = -Σₜ log P(yₜ | y<ₜ, x)

For each position t:
  • Model predicts probability distribution over vocabulary
  • Loss = -log(probability of correct token)
  • Sum over all positions

Example:
  Position 1: P("Je") = 0.8    → loss = -log(0.8) = 0.22
  Position 2: P("t'") = 0.7    → loss = -log(0.7) = 0.36
  Position 3: P("aime") = 0.9  → loss = -log(0.9) = 0.11
  Position 4: P("<E>") = 0.85  → loss = -log(0.85) = 0.16
  
  Total loss = 0.22 + 0.36 + 0.11 + 0.16 = 0.85
```

### Inference Mode

```
╔══════════════════════════════════════════════════════════════════════╗
║              INFERENCE MODE                                          ║
╚══════════════════════════════════════════════════════════════════════╝

AUTOREGRESSIVE GENERATION:
──────────────────────────
During inference, we don't have the target sequence.
Must generate ONE token at a time, feeding each output back as input.


Inference Forward Pass:
───────────────────────

Step 1:
      "I love you"           "<S>"
           │                   │
           ▼                   ▼
      ┌─────────┐         ┌─────────┐
      │ ENCODER │────────►│ DECODER │───► "Je"
      └─────────┘         └─────────┘
      
Step 2:
      (cached)              "<S> Je"
                              │
                              ▼
                         ┌─────────┐
      ─────────────────►│ DECODER │───► "t'"
                         └─────────┘
      
Step 3:
      (cached)            "<S> Je t'"
                              │
                              ▼
                         ┌─────────┐
      ─────────────────►│ DECODER │───► "aime"
                         └─────────┘

Step 4:
      (cached)          "<S> Je t' aime"
                              │
                              ▼
                         ┌─────────┐
      ─────────────────►│ DECODER │───► "<E>" (STOP)
                         └─────────┘


KV-CACHING:
───────────
Problem: At step t, we recompute attention for ALL previous positions
Solution: Cache the Key and Value matrices from previous steps

Without cache:
  Step 3: Compute attention for "<S>", "Je", "t'" (all 3)
  
With cache:
  Step 3: Reuse K,V for "<S>", "Je" from cache
          Only compute new K,V for "t'"
          
Speedup: O(t²) → O(t) per step


DECODING STRATEGIES:
────────────────────

1. GREEDY: Pick argmax at each step
   Simple, fast, but can be suboptimal

2. BEAM SEARCH: Keep top-B candidates (see Beam Search document)
   Better quality, more computation

3. SAMPLING: Sample from probability distribution
   More diverse, used for creative tasks
   
4. TOP-K / TOP-P: Sample from truncated distribution
   Balance between greedy and full sampling
```

### Training vs Inference Comparison

```
╔══════════════════════════════════════════════════════════════════════╗
║              TRAINING vs INFERENCE COMPARISON                        ║
╚══════════════════════════════════════════════════════════════════════╝

┌─────────────────────┬─────────────────────────┬──────────────────────┐
│ Aspect              │ Training                │ Inference            │
├─────────────────────┼─────────────────────────┼──────────────────────┤
│ Decoder input       │ Ground truth (teacher   │ Model's own outputs  │
│                     │ forcing)                │ (autoregressive)     │
├─────────────────────┼─────────────────────────┼──────────────────────┤
│ Parallelization     │ All positions at once   │ One position at a    │
│                     │ (parallel)              │ time (sequential)    │
├─────────────────────┼─────────────────────────┼──────────────────────┤
│ Masking             │ Causal mask applied     │ Natural causality    │
│                     │ (prevent cheating)      │ (future doesn't exist)│
├─────────────────────┼─────────────────────────┼──────────────────────┤
│ Encoder runs        │ Once per example        │ Once per example     │
├─────────────────────┼─────────────────────────┼──────────────────────┤
│ Decoder runs        │ Once (all positions)    │ T times (T = output  │
│                     │                         │ length)              │
├─────────────────────┼─────────────────────────┼──────────────────────┤
│ KV-Cache            │ Not needed              │ Critical for speed   │
├─────────────────────┼─────────────────────────┼──────────────────────┤
│ Speed               │ Fast (parallel)         │ Slow (sequential)    │
├─────────────────────┼─────────────────────────┼──────────────────────┤
│ Goal                │ Minimize loss           │ Generate good output │
└─────────────────────┴─────────────────────────┴──────────────────────┘


Visual Summary:
───────────────

TRAINING:
                    ┌───────────────────────────────────────┐
Input: x ──────────►│           TRANSFORMER                 │
                    │                                       │
Target: y ─────────►│   (process ALL of y in PARALLEL)      │
(ground truth)      │                                       │
                    └───────────────────┬───────────────────┘
                                        │
                                        ▼
                              Predictions for ALL positions
                                        │
                                        ▼
                                      LOSS


INFERENCE:
                    ┌───────────────────────────────────────┐
Input: x ──────────►│           TRANSFORMER                 │
                    │                                       │
"<S>" ─────────────►│   → "word₁"                           │
"<S> word₁" ───────►│   → "word₂"                           │
"<S> w₁ w₂" ───────►│   → "word₃"                           │
       ...          │      ...                              │
                    └───────────────────────────────────────┘
                                        │
                                        ▼
                              Generate SEQUENTIALLY
                                        │
                                        ▼
                              "word₁ word₂ word₃ ..."
```
