{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "xg_boost.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# What is XGBoost (Extreme Gradient Boosting)?\n\nA high‑performance gradient boosting library for **tree‑based** models (classification/regression). Strong default for **tabular** data.\n\nRule of thumb: “Add small decision trees one‑by‑one; each new tree corrects errors of the previous ones using gradients.”\n\n---\n\n# Core idea and notation\n\n- At iteration t, the model is $$\\hat{y}^{(t)}(x) = \\hat{y}^{(t-1)}(x) + f_t(x)$$, where $$f_t$$ is a new regression tree.\n- Objective to minimize:\n$$\n\\mathcal{L}^{(t)} = \\sum_{i=1}^N l\\big(y_i, \\hat{y}^{(t-1)}_i + f_t(x_i)\\big) \\, + \\, \\Omega(f_t)\n$$\n- Using Taylor expansion around $$\\hat{y}^{(t-1)}$$:\n  - First derivative (gradient): $$g_i = \\partial_{\\hat{y}} l(y_i, \\hat{y}^{(t-1)}_i)$$\n  - Second derivative (Hessian): $$h_i = \\partial^2_{\\hat{y}} l(y_i, \\hat{y}^{(t-1)}_i)$$\n- For each leaf j with index set $$I_j$$, define $$G_j = \\sum_{i\\in I_j} g_i$$, $$H_j = \\sum_{i\\in I_j} h_i$$.\n- The optimal leaf weight is approx:\n$$\nw_j^* = -\\frac{G_j}{H_j + \\lambda}\n$$\n- The split **gain** when splitting a node into left/right:\n$$\n\\text{Gain} = \\frac{1}{2}\\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G^2}{H + \\lambda} \\right) - \\gamma\n$$\n($$\\lambda$$: L2 regularization; $$\\gamma$$: min loss reduction to make a split)\n\n---\n\n# Important hyperparameters (and roles)\n\n- `eta` (learning rate): 0.01–0.3; smaller needs more trees, can generalize better.\n- `n_estimators` (num_boost_round): number of trees; pair with early stopping.\n- `max_depth` / `max_leaves`: tree complexity; deeper can overfit.\n- `min_child_weight`: larger → more conservative splits.\n- `subsample`, `colsample_bytree`: row/feature subsampling for regularization and speed.\n- `reg_alpha` (L1), `reg_lambda` (L2): regularize leaf weights.\n- `gamma`: required loss reduction to split (tree-specific penalty).\n- Imbalance: `scale_pos_weight` ≈ (neg/pos ratio) for binary classification.\n\n---\n\n# Step‑by‑step (paper‑and‑pencil)\n\n1) Initialize $$\\hat{y}^{(0)}$$ (e.g., constant prediction).\n2) For t = 1..T:\n   - Compute gradients $$g_i$$ and Hessians $$h_i$$ for current predictions.\n   - Build a new tree by choosing splits that maximize **Gain**.\n   - Add the scaled tree to the model: $$\\hat{y}^{(t)} = \\hat{y}^{(t-1)} + \\eta f_t(x)$$.\n   - Optionally use early stopping based on validation set.\n\n---\n\n# Pseudocode\n\n```\ny_hat = init_constant()\nfor t in 1..T:\n  g, h = grad_hess(loss, y, y_hat)\n  tree = build_tree_by_gain(X, g, h, lambda, gamma, max_depth, min_child_weight)\n  y_hat = y_hat + eta * tree.predict(X)\n  if early_stopping and no_improvement: break\n```\n\n---\n\n# Minimal Python (sklearn API)\n\n```python\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\nxgb = XGBClassifier(\n    n_estimators=1000, learning_rate=0.05, max_depth=4,\n    subsample=0.8, colsample_bytree=0.8,\n    reg_lambda=1.0, reg_alpha=0.0, gamma=0.0,\n    eval_metric=\"auc\", n_jobs=-1, random_state=42\n)\nxgb.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False, early_stopping_rounds=50)\nprint(\"Best iteration:\", xgb.best_iteration)\nprint(\"Val AUC:\", xgb.best_score)\n```\n\n---\n\n# Practical tips, pitfalls, and variants\n\n- Always monitor a **validation** metric with **early stopping** to avoid overfitting.\n- Tune `eta` jointly with `n_estimators` (smaller eta → more trees).\n- Keep trees shallow to start (max_depth 3–6).\n- Use CV/Grid/RandomizedSearch to tune; mind interaction of hyperparameters.\n- Be careful with leakage (all preprocessing fit on train only).\n- Interpretability: feature importances can be biased; consider permutation importance/SHAP.\n\n---\n\n# Quick cheat sheet\n\n- Additive trees: each corrects previous errors using gradients\n- Gain formula drives splits; regularize with $$\\lambda,\\gamma$$\n- Use early stopping; shallow trees; tune subsampling\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FLud1n-3pVm",
    "colab_type": "text"
   },
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO8VPU6n3vES",
    "colab_type": "text"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "clDSsF7P33NU",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGpwK5XD386E",
    "colab_type": "text"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zcksk88u4Ae8",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNn2RnST6_Q-",
    "colab_type": "text"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ajhBL-er7Gry",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Y89ctGZ7Mcx",
    "colab_type": "text"
   },
   "source": [
    "## Training XGBoost on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ude1J0E47SKN",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivqmubzW7dFJ",
    "colab_type": "text"
   },
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SUSZ3zm_7gRD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnbCjHgQ8XPn",
    "colab_type": "text"
   },
   "source": [
    "## Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yYbfiITD8ZAz",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}
