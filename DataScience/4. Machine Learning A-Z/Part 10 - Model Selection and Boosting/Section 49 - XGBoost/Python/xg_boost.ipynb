{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "xg_boost.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Theory — XGBoost (Extreme Gradient Boosting)\n\nWhat it solves\n- High‑performance tree‑based gradient boosting for classification/regression; strong tabular baseline.\n\nPlain intuition\n- Build trees sequentially; each new tree fits the gradient of the loss (errors) made so far.\n- Many small improvements (weak learners) add up to a strong model.\n\nCore math (sketch)\n- Objective at iteration t: L = Σ_i l(y_i, ŷ_i^{(t-1)} + f_t(x_i)) + Ω(f_t)\n  • l: loss (e.g., logistic), f_t: new tree, Ω: regularization (L1/L2 + tree complexity)\n- Add tree f_t that best reduces L using a second‑order Taylor approximation (gradient + Hessian).\n\nImportant hyperparameters\n- Learning rate (eta): 0.01–0.3; smaller eta → more trees needed, better generalization.\n- n_estimators (num_boost_round): total boosting rounds; use early stopping with a validation set.\n- max_depth / max_leaves: tree complexity; deeper trees can overfit.\n- min_child_weight: minimum sum of instance weight in a leaf; larger → conservative.\n- subsample / colsample_bytree: row/feature subsampling for regularization and speed.\n- reg_alpha (L1), reg_lambda (L2): regularize leaf weights.\n- gamma: minimum loss reduction to make a split.\n- scale_pos_weight: handle class imbalance (= negative/positive ratio).\n\nTraining tips\n- Always monitor validation metric with early_stopping_rounds (e.g., 50).\n- Tune eta with n_estimators jointly; use smaller eta with more rounds.\n- Start with shallow trees (max_depth 3–6) and moderate subsampling.\n\nTiny example (concept)\n- Begin with constant prediction (e.g., average log‑odds).\n- Tree 1 fits residuals; add scaled by eta; repeat → decision function improves iteratively.\n\nPros, cons, pitfalls\n- Pros: Excellent accuracy on tabular data; handles missing values; many regularization knobs.\n- Cons: Many hyperparameters; can overfit if unchecked; longer training on very large datasets.\n- Pitfalls: No early stopping; too deep trees; leakage in preprocessing; not using proper metric for imbalance.\n\nHow this notebook implements it\n- Dataset: Data.csv / Churn_Modelling.csv depending on variant.\n- Steps: split → build DMatrix (or sklearn API) → fit with eval_set and early stopping → evaluate.\n- Tip: Use sklearn API (XGBClassifier/XGBRegressor) for easy Pipeline/CV integration.\n\nQuick checklist\n- Use validation set + early stopping.\n- Tune depth, learning rate, rounds, and subsampling.\n- Check feature importance cautiously; validate with CV.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FLud1n-3pVm",
    "colab_type": "text"
   },
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO8VPU6n3vES",
    "colab_type": "text"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "clDSsF7P33NU",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGpwK5XD386E",
    "colab_type": "text"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zcksk88u4Ae8",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNn2RnST6_Q-",
    "colab_type": "text"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ajhBL-er7Gry",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Y89ctGZ7Mcx",
    "colab_type": "text"
   },
   "source": [
    "## Training XGBoost on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ude1J0E47SKN",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivqmubzW7dFJ",
    "colab_type": "text"
   },
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SUSZ3zm_7gRD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnbCjHgQ8XPn",
    "colab_type": "text"
   },
   "source": [
    "## Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yYbfiITD8ZAz",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}
