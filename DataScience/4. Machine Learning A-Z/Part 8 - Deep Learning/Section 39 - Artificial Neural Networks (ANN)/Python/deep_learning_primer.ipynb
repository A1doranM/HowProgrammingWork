{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d691ebf9",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning & Artificial Neural Networks — A Hands‑On Primer\n",
    "\n",
    "**Format:** Notebook with math in $$...$$ for easy copy into Markdown cells.  \n",
    "**What you’ll learn:**  \n",
    "1) CNN layers (convolution, ReLU, pooling, flattening)  \n",
    "2) Neurons & synapses (weights & biases)  \n",
    "3) Activation functions (what/why/derivatives)  \n",
    "4) Gradient descent vs brute‑force optimization  \n",
    "5) Full training loop of a tiny Neural Network (from scratch, NumPy)\n",
    "\n",
    "> Tip: Run each code cell in order. The math blocks are self-contained and can be pasted into Markdown cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cec11",
   "metadata": {},
   "source": [
    "\n",
    "## 0) What is Deep Learning and an Artificial Neural Network (ANN)?\n",
    "\n",
    "**Deep Learning** uses **stacked layers** of simple units (**neurons**) to learn complex functions from data.  \n",
    "Each neuron computes a **weighted sum** of its inputs, adds a **bias**, then applies a **nonlinear activation**.\n",
    "\n",
    "$$\n",
    "\\text{Given input vector } x \\in \\mathbb{R}^{d}, \\quad\n",
    "z = w^\\top x + b, \\quad\n",
    "a = \\phi(z).\n",
    "$$\n",
    "\n",
    "**Symbols:**  \n",
    "- $$x \\in \\mathbb{R}^d $$ — input (features).  \n",
    "- $$w \\in \\mathbb{R}^d$$ — weights (strength of each input connection).  \n",
    "- $$b \\in \\mathbb{R}$$ — bias (threshold).  \n",
    "- $$z \\in \\mathbb{R}$$ — pre-activation (linear response).  \n",
    "- $$a = \\phi(z)$$ — post-activation (nonlinear output).\n",
    "\n",
    "A **layer** bundles many neurons in parallel. If the previous layer output is $$a^{(l-1)} \\in \\mathbb{R}^{n_{l-1}}$$ and the current has $$n_l$$ neurons, then\n",
    "\n",
    "$$\n",
    "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}, \\qquad\n",
    "a^{(l)} = \\phi\\!\\left(z^{(l)}\\right).\n",
    "$$\n",
    "\n",
    "**Shapes:**  \n",
    "- $$W^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$$, $$b^{(l)} \\in \\mathbb{R}^{n_l}$$, $$z^{(l)}, a^{(l)} \\in \\mathbb{R}^{n_l}$$.\n",
    "\n",
    "Stacking layers gives a **deep** network: a function composition that can model rich patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a9cc3",
   "metadata": {},
   "source": [
    "\n",
    "## 1) CNN Layers: Convolution, ReLU, Pooling, Flattening\n",
    "\n",
    "We use a **shop image** analogy: imagine an input image (e.g., a product photo). CNNs scan it with small **filters** to detect edges/textures.\n",
    "\n",
    "### 1.1 Convolution (2D, single channel)\n",
    "\n",
    "Given an input image $$X \\in \\mathbb{R}^{H \\times W}$$ and a filter/kernel $$K \\in \\mathbb{R}^{k_h \\times k_w}$$, **valid** convolution at location $$(i,j)$$ (top-left index) is\n",
    "\n",
    "$$\n",
    "Y_{i,j} = \\sum_{u=0}^{k_h-1} \\sum_{v=0}^{k_w-1} X_{i+u,\\; j+v} \\; K_{u,v}.\n",
    "$$\n",
    "\n",
    "**With stride $$s$$ and zero-padding $$p$$:** output size is  \n",
    "$$\n",
    "H_{\\text{out}} = \\left\\lfloor \\frac{H - k_h + 2p}{s} \\right\\rfloor + 1,\\quad\n",
    "W_{\\text{out}} = \\left\\lfloor \\frac{W - k_w + 2p}{s} \\right\\rfloor + 1.\n",
    "$$\n",
    "\n",
    "**Multi-channel (e.g., RGB):** for input depth $$C$$ and filter $$K \\in \\mathbb{R}^{C \\times k_h \\times k_w}$$,\n",
    "$$\n",
    "Y_{i,j} = \\sum_{c=1}^{C} \\sum_{u=0}^{k_h-1} \\sum_{v=0}^{k_w-1} X_{c,i+u,\\; j+v} \\; K_{c,u,v} + b.\n",
    "$$\n",
    "\n",
    "**Symbols:**  \n",
    "- $$X$$ — input image/feature map, $$K$$ — kernel/filter, $$b$$ — per-filter bias, $$Y$$ — output feature map.  \n",
    "- $$s$$ — stride (step size), $$p$$ — padding (border zeros), $$(i,j)$$ — output spatial indices.\n",
    "\n",
    "**Intuition:** kernels detect **local patterns** (edges/corners). Stacking many filters yields multiple feature maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Tiny numeric example: 4x4 image, 3x3 kernel, stride=1, no padding\n",
    "X = np.array([\n",
    "    [1, 2, 0, 1],\n",
    "    [0, 1, 3, 2],\n",
    "    [2, 1, 0, 1],\n",
    "    [1, 0, 2, 3]\n",
    "], dtype=float)\n",
    "\n",
    "K = np.array([\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1]\n",
    "], dtype=float)  # simple \"vertical edge\" detector (Sobel-like simplified)\n",
    "\n",
    "def conv2d_valid(X, K):\n",
    "    H, W = X.shape\n",
    "    kh, kw = K.shape\n",
    "    Hout, Wout = H - kh + 1, W - kw + 1\n",
    "    Y = np.zeros((Hout, Wout), dtype=float)\n",
    "    for i in range(Hout):\n",
    "        for j in range(Wout):\n",
    "            window = X[i:i+kh, j:j+kw]\n",
    "            Y[i, j] = np.sum(window * K)\n",
    "    return Y\n",
    "\n",
    "Y = conv2d_valid(X, K)\n",
    "print(\"Input X:\\n\", X)\n",
    "print(\"\\nKernel K:\\n\", K)\n",
    "print(\"\\nConvolution Y:\\n\", Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0c375",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 ReLU (Rectified Linear Unit)\n",
    "\n",
    "Makes features **nonlinear** and keeps positives, zeroes out negatives:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z), \\qquad\n",
    "\\frac{d}{dz}\\text{ReLU}(z) = \\begin{cases}1 & z>0 \\\\ 0 & z\\le 0 \\end{cases}.\n",
    "$$\n",
    "\n",
    "**Why:** nonlinearity lets networks compose features into richer patterns; it’s cheap and reduces vanishing gradients compared to sigmoids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d44994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def relu(X):\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "# Apply ReLU to the previous convolution output\n",
    "Y_relu = relu(Y)\n",
    "print(\"ReLU(Y):\\n\", Y_relu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3529d6f4",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 Pooling (Downsampling)\n",
    "\n",
    "**Max pooling** keeps the strongest activation in a window; **average pooling** averages.  \n",
    "For window size $$p_h \\times p_w$$, stride $$s$$:\n",
    "\n",
    "- Max pooling:\n",
    "$$\n",
    "Y_{i,j} = \\max_{0 \\le u < p_h,\\; 0 \\le v < p_w} X_{i\\cdot s + u,\\; j\\cdot s + v}.\n",
    "$$\n",
    "\n",
    "- Average pooling:\n",
    "$$\n",
    "Y_{i,j} = \\frac{1}{p_h p_w} \\sum_{u=0}^{p_h-1} \\sum_{v=0}^{p_w-1} X_{i\\cdot s + u,\\; j\\cdot s + v}.\n",
    "$$\n",
    "\n",
    "**Why:** reduces spatial size, creates local invariance, and saves parameters/computation later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a804fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def max_pool2d(X, pool=(2,2), stride=2):\n",
    "    ph, pw = pool\n",
    "    H, W = X.shape\n",
    "    Hout = (H - ph)//stride + 1\n",
    "    Wout = (W - pw)//stride + 1\n",
    "    Y = np.zeros((Hout, Wout), dtype=float)\n",
    "    for i in range(Hout):\n",
    "        for j in range(Wout):\n",
    "            window = X[i*stride:i*stride+ph, j*stride:j*stride+pw]\n",
    "            Y[i, j] = np.max(window)\n",
    "    return Y\n",
    "\n",
    "pooled = max_pool2d(Y_relu, pool=(2,2), stride=2)\n",
    "print(\"MaxPool(ReLU(conv)):\\n\", pooled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55873007",
   "metadata": {},
   "source": [
    "\n",
    "### 1.4 Flattening\n",
    "\n",
    "Converts a multi-dimensional feature map into a **1‑D vector** to feed **fully connected** layers.\n",
    "\n",
    "If the feature map is $$X \\in \\mathbb{R}^{C \\times H \\times W}$$, **flatten** reshapes it into $$x \\in \\mathbb{R}^{CHW}$$ without changing values.\n",
    "\n",
    "**Why:** Dense layers expect vectors; flatten bridges CNN features to classifiers/regressors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecd1d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example flatten\n",
    "C, H, W = 1, pooled.shape[0], pooled.shape[1]\n",
    "feat_map = pooled.reshape(C, H, W)\n",
    "flat = feat_map.reshape(-1)  # CHW\n",
    "print(\"Feature map shape:\", feat_map.shape, \"-> Flattened vector length:\", flat.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf89b1c",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Neurons and Synapses (Weights)\n",
    "\n",
    "A **neuron** computes a weighted sum (synapses = weighted connections) plus bias, then applies a nonlinearity:\n",
    "\n",
    "$$\n",
    "z = w^\\top x + b, \\qquad a = \\phi(z).\n",
    "$$\n",
    "\n",
    "In a **layer** with $$n_l$$ neurons, compact form:\n",
    "\n",
    "$$\n",
    "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}, \\qquad a^{(l)} = \\phi\\!\\left(z^{(l)}\\right).\n",
    "$$\n",
    "\n",
    "**Connections = parameters:** every edge (synapse) has a weight. The goal of training is to **learn** these weights and biases to minimize a **loss**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee6a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tiny dense layer forward example (from scratch):\n",
    "rng = np.random.default_rng(42)\n",
    "x = flat  # vector from CNN toy pathway\n",
    "in_dim = x.size\n",
    "out_dim = 3\n",
    "W = rng.normal(scale=0.1, size=(out_dim, in_dim))\n",
    "b = np.zeros(out_dim)\n",
    "z = W @ x + b\n",
    "a = np.maximum(0, z)  # ReLU\n",
    "print(\"Dense layer output (3 units):\", a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8a638",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Activation Functions\n",
    "\n",
    "Nonlinearities let networks approximate complex functions. Common choices:\n",
    "\n",
    "**Sigmoid** (good for probabilities in [0,1] but can saturate):\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad \\sigma'(z) = \\sigma(z)(1-\\sigma(z)).\n",
    "$$\n",
    "\n",
    "**Tanh** (zero-centered; still can saturate):\n",
    "$$\n",
    "\\tanh(z) = \\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}, \\quad \\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z).\n",
    "$$\n",
    "\n",
    "**ReLU** (sparse activations; simple; popular):\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0,z), \\quad \\text{ReLU}'(z) = \\begin{cases}1 & z>0\\\\ 0 & z\\le 0\\end{cases}.\n",
    "$$\n",
    "\n",
    "**Leaky ReLU** (fixes “dead” ReLUs):\n",
    "$$\n",
    "\\text{LeakyReLU}_\\alpha(z) = \\begin{cases}z & z\\ge 0\\\\ \\alpha z & z<0 \\end{cases}, \\quad \\alpha \\in (0,1).\n",
    "$$\n",
    "\n",
    "**Softmax** (multi-class output; converts scores to a probability simplex):\n",
    "$$\n",
    "\\text{softmax}(z)_k = \\frac{e^{z_k}}{\\sum_{j} e^{z_j}}.\n",
    "$$\n",
    "\n",
    "**Notes:**  \n",
    "- Use **ReLU-family** in hidden layers typically.  \n",
    "- Use **sigmoid** for binary output and **softmax** for multi-class output (with cross-entropy loss).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e3f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize activations (one plot per function, as required)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = np.linspace(-5, 5, 400)\n",
    "\n",
    "def sigmoid(z): return 1/(1+np.exp(-z))\n",
    "def tanh(z): return np.tanh(z)\n",
    "def relu(z): return np.maximum(0,z)\n",
    "def leaky_relu(z, alpha=0.1): return np.where(z>0, z, alpha*z)\n",
    "\n",
    "for name, f in [(\"Sigmoid\", sigmoid), (\"Tanh\", tanh), (\"ReLU\", relu)]:\n",
    "    plt.figure()\n",
    "    plt.plot(xs, f(xs))\n",
    "    plt.title(name)\n",
    "    plt.xlabel(\"z\"); plt.ylabel(f\"{name}(z)\")\n",
    "    plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xs, leaky_relu(xs, 0.1))\n",
    "plt.title(\"Leaky ReLU (alpha=0.1)\")\n",
    "plt.xlabel(\"z\"); plt.ylabel(\"LeakyReLU(z)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d7a04",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Gradient Descent vs Brute Force Optimization\n",
    "\n",
    "We want to minimize a **loss** $$L(\\theta)$$ over parameters $$\\theta$$.  \n",
    "**Gradient Descent (GD)** updates iteratively in the negative gradient direction:\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\, \\nabla_\\theta L\\!\\left(\\theta^{(t)}\\right).\n",
    "$$\n",
    "\n",
    "**Symbols:**  \n",
    "- $$\\theta$$ — parameters (weights/biases).  \n",
    "- $$\\eta>0$$ — learning rate (step size).  \n",
    "- $$\\nabla_\\theta L$$ — gradient (vector of partial derivatives).\n",
    "\n",
    "**Why not brute force?** Trying every parameter setting is **exponential/infinite**. Even for one parameter we could grid search, but for millions (typical NN) it’s impossible.\n",
    "\n",
    "### Toy 1‑D example\n",
    "Fit $$w$$ to minimize mean squared error on pairs $$(x_i,y_i)$$ with model $$\\hat{y}_i = w x_i$$:\n",
    "\n",
    "$$\n",
    "L(w) = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n",
    "= \\frac{1}{N}\\sum_{i=1}^N (w x_i - y_i)^2, \\quad\n",
    "\\frac{dL}{dw} = \\frac{2}{N}\\sum_{i=1}^N x_i (w x_i - y_i).\n",
    "$$\n",
    "\n",
    "We’ll compare **GD** vs **brute-force grid** on $$w \\in [-5,5]$$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "N = 60\n",
    "x = rng.uniform(-2, 2, size=N)\n",
    "true_w = 1.7\n",
    "y = true_w * x + rng.normal(scale=0.2, size=N)\n",
    "\n",
    "def loss(w):\n",
    "    return np.mean((w*x - y)**2)\n",
    "\n",
    "def grad(w):\n",
    "    return (2.0/len(x)) * np.sum(x * (w*x - y))\n",
    "\n",
    "# Brute-force grid (for 1D only, feasible here)\n",
    "ws = np.linspace(-5,5,1000)\n",
    "Ls = np.array([loss(w) for w in ws])\n",
    "w_star_grid = ws[np.argmin(Ls)]\n",
    "\n",
    "# Gradient Descent\n",
    "w = -4.0\n",
    "eta = 0.05\n",
    "hist = []\n",
    "for t in range(200):\n",
    "    g = grad(w)\n",
    "    w = w - eta * g\n",
    "    hist.append((t, w, loss(w)))\n",
    "\n",
    "w_gd = w\n",
    "\n",
    "# Plot loss landscape and GD path\n",
    "plt.figure()\n",
    "plt.plot(ws, Ls)\n",
    "plt.title(\"1D Loss Landscape (grid)\")\n",
    "plt.xlabel(\"w\"); plt.ylabel(\"L(w)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([h[0] for h in hist], [h[2] for h in hist])\n",
    "plt.title(\"Gradient Descent Loss over Iterations\")\n",
    "plt.xlabel(\"iteration\"); plt.ylabel(\"L(w)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"True w ~ {true_w:.3f}, Grid argmin ~ {w_star_grid:.3f}, GD final ~ {w_gd:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15c536c",
   "metadata": {},
   "source": [
    "\n",
    "**Takeaways:**  \n",
    "- In low dimensions, a grid can find the minimum, but scales horribly.  \n",
    "- **GD** works in high dimensions by following the slope (gradient).  \n",
    "- In practice, we use **mini‑batch SGD**, **Momentum**, **Adam**, etc., but the core idea is the same: move parameters downhill.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cacaa44",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Training a Neural Network — Step by Step (from scratch)\n",
    "\n",
    "We’ll train a tiny **2‑layer** network on a simple 2D dataset.\n",
    "\n",
    "### 5.1 Forward pass\n",
    "\n",
    "Layer 1 (hidden, ReLU):\n",
    "$$\n",
    "Z^{[1]} = X W^{[1]} + \\mathbf{1} b^{[1]\\top}, \\quad A^{[1]} = \\text{ReLU}(Z^{[1]}).\n",
    "$$\n",
    "\n",
    "Layer 2 (output, binary logistic):\n",
    "$$\n",
    "Z^{[2]} = A^{[1]} W^{[2]} + \\mathbf{1} b^{[2]\\top}, \\quad \\hat{y} = \\sigma(Z^{[2]}) = \\frac{1}{1+e^{-Z^{[2]}}}.\n",
    "$$\n",
    "\n",
    "**Shapes:**  \n",
    "- $$X \\in \\mathbb{R}^{N \\times d}$$, $$W^{[1]} \\in \\mathbb{R}^{d \\times h}$$, $$b^{[1]} \\in \\mathbb{R}^{h}$$.  \n",
    "- $$A^{[1]} \\in \\mathbb{R}^{N \\times h}$$, $$W^{[2]} \\in \\mathbb{R}^{h \\times 1}$$, $$b^{[2]} \\in \\mathbb{R}^{1}$$, $$\\hat{y} \\in \\mathbb{R}^{N \\times 1}$$.\n",
    "\n",
    "### 5.2 Loss (binary cross-entropy)\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{N}\\sum_{i=1}^N \\left[ y_i \\log \\hat{y}_i + (1-y_i)\\log(1-\\hat{y}_i) \\right].\n",
    "$$\n",
    "\n",
    "### 5.3 Backpropagation (gradients via chain rule)\n",
    "\n",
    "For sigmoid + cross-entropy, the output derivative simplifies:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z^{[2]}} = \\frac{1}{N}(\\hat{y} - y).\n",
    "$$\n",
    "\n",
    "Then propagate back:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[2]}} = A^{[1]\\top} \\frac{\\partial L}{\\partial Z^{[2]}}, \\quad\n",
    "\\frac{\\partial L}{\\partial b^{[2]}} = \\sum_{i=1}^N \\left(\\frac{\\partial L}{\\partial Z^{[2]}}\\right)_i.\n",
    "$$\n",
    "\n",
    "For ReLU:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z^{[1]}} = \\left(\\frac{\\partial L}{\\partial A^{[1]}}\\right) \\odot \\mathbf{1}[Z^{[1]} > 0], \\quad\n",
    "\\frac{\\partial L}{\\partial A^{[1]}} = \\frac{\\partial L}{\\partial Z^{[2]}} W^{[2]\\top}.\n",
    "$$\n",
    "\n",
    "Finally:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[1]}} = X^\\top \\frac{\\partial L}{\\partial Z^{[1]}}, \\quad\n",
    "\\frac{\\partial L}{\\partial b^{[1]}} = \\sum_{i=1}^N \\left(\\frac{\\partial L}{\\partial Z^{[1]}}\\right)_i.\n",
    "$$\n",
    "\n",
    "**Symbols:** $$\\odot$$ is elementwise product; $$\\mathbf{1}[\\,\\cdot\\,]$$ is an indicator mask (1 if condition true else 0).\n",
    "\n",
    "### 5.4 Update (gradient descent)\n",
    "\n",
    "With learning rate $$\\eta$$:\n",
    "$$\n",
    "\\Theta \\leftarrow \\Theta - \\eta \\nabla_\\Theta L, \\quad \\Theta \\in \\{W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\\}.\n",
    "$$\n",
    "\n",
    "We’ll code all of this and plot the training loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5188983",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "# Create a toy 2D binary dataset (two moons-like by manual shift/rotation)\n",
    "N = 400\n",
    "# Class 0\n",
    "theta0 = rng.uniform(0, np.pi, N//2)\n",
    "r0 = 1.0 + 0.05*rng.standard_normal(N//2)\n",
    "x0 = np.c_[r0*np.cos(theta0), r0*np.sin(theta0)] + np.array([0.0, 0.0])\n",
    "# Class 1\n",
    "theta1 = rng.uniform(0, np.pi, N//2)\n",
    "r1 = 1.0 + 0.05*rng.standard_normal(N//2)\n",
    "x1 = np.c_[r1*np.cos(theta1), -r1*np.sin(theta1)] + np.array([0.8, 0.2])\n",
    "\n",
    "X = np.vstack([x0, x1])\n",
    "y = np.vstack([np.zeros((N//2,1)), np.ones((N//2,1))])\n",
    "\n",
    "# Initialize 2-layer NN\n",
    "d = 2       # input dimension\n",
    "h = 8       # hidden units\n",
    "W1 = rng.normal(scale=0.7, size=(d, h))\n",
    "b1 = np.zeros((1, h))\n",
    "W2 = rng.normal(scale=0.7, size=(h, 1))\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "def relu(Z): return np.maximum(0, Z)\n",
    "def relu_grad(Z): return (Z > 0).astype(Z.dtype)\n",
    "def sigmoid(Z): return 1/(1+np.exp(-Z))\n",
    "\n",
    "def forward(X):\n",
    "    Z1 = X @ W1 + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = A1 @ W2 + b2\n",
    "    Yhat = sigmoid(Z2)\n",
    "    return Z1, A1, Z2, Yhat\n",
    "\n",
    "def loss(Yhat, y):\n",
    "    eps = 1e-8\n",
    "    return -np.mean(y*np.log(Yhat+eps) + (1-y)*np.log(1-Yhat+eps))\n",
    "\n",
    "eta = 0.1\n",
    "epochs = 4000\n",
    "loss_hist = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    # Forward\n",
    "    Z1, A1, Z2, Yhat = forward(X)\n",
    "    L = loss(Yhat, y)\n",
    "    loss_hist.append(L)\n",
    "\n",
    "    # Backward\n",
    "    dZ2 = (Yhat - y)/len(X)         # (N,1)\n",
    "    dW2 = A1.T @ dZ2                # (h,1)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "    dA1 = dZ2 @ W2.T                # (N,h)\n",
    "    dZ1 = dA1 * relu_grad(Z1)       # (N,h)\n",
    "    dW1 = X.T @ dZ1                 # (d,h)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update\n",
    "    W2 -= eta * dW2; b2 -= eta * db2\n",
    "    W1 -= eta * dW1; b1 -= eta * db1\n",
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "plt.plot(loss_hist)\n",
    "plt.title(\"Training Loss (2-layer NN)\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"cross-entropy\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate accuracy\n",
    "_, A1, _, Yhat = forward(X)\n",
    "pred = (Yhat >= 0.5).astype(int)\n",
    "acc = (pred == y).mean()\n",
    "print(f\"Training accuracy: {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06323a",
   "metadata": {},
   "source": [
    "\n",
    "### 5.5 How pieces connect\n",
    "\n",
    "1. **Convolution** extracts local patterns from images (edges, textures).  \n",
    "2. **ReLU** injects nonlinearity so stacked layers can model complex functions.  \n",
    "3. **Pooling** compresses spatial info for invariance & efficiency.  \n",
    "4. **Flatten** reshapes feature maps into vectors for dense layers.  \n",
    "5. **Neurons & synapses** are the parameters we optimize.  \n",
    "6. **Activations** shape signal flow and gradients.  \n",
    "7. **Gradient descent** (via backprop) updates weights to reduce loss.  \n",
    "8. Repeat forward→loss→backward→update over many **epochs** until convergence.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
