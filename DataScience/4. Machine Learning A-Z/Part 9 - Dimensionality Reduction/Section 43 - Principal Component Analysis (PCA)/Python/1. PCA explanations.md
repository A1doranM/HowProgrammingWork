# Principal Component Analysis (PCA): Complete Explanation
## (Detailed Step-by-Step with Number Tracing)

---

## ğŸ”— **Connection to Previous Topics**

### **What We Know So Far:**

**From CNNs:**
- Images have many pixels (28Ã—28 = 784 features)
- Not all features are equally important
- We used convolution to extract important patterns

**From Neural Networks:**
- High-dimensional data is hard to visualize and process
- More features = more parameters = more computation
- Sometimes features are redundant (correlated)

**The New Problem:**

```
Dataset with 1000 features:
[xâ‚, xâ‚‚, xâ‚ƒ, ..., xâ‚â‚€â‚€â‚€]

Questions:
- Do we really need all 1000 features?
- Are many features just saying the same thing?
- Can we reduce to 10 features without losing much information?
- How do we visualize this high-dimensional data?
```

**PCA solves this by:**
- Finding the most important "directions" in the data
- Reducing dimensions while keeping maximum information
- Removing redundant/correlated features
- Enabling visualization (reducing to 2D or 3D)

---

# Part 1: What is PCA?

## 1. Plain English Explanation

### **Imagine You're a Real Estate Agent**

You have data on 100 houses with many measurements:
- Length (meters)
- Width (meters)
- Area (square meters)
- Number of rooms
- Distance from city center
- Age
- Price

**Problem:** Length, width, and area are highly correlated!
- If length and width are big, area is automatically big
- We're storing the same information 3 times!

**PCA says:** "Let's find new features that capture the MOST variation"
- New Feature 1 (PC1): "Overall size" (captures length, width, area together)
- New Feature 2 (PC2): "Shape" (is it square or rectangular?)
- New Feature 3 (PC3): "Location quality" (distance + neighborhood)

Instead of 7 features, maybe 3 "principal components" capture 95% of the information!

---

### **The Core Idea**

**PCA finds directions (axes) where data varies the most.**

```
Original 2D data:                After PCA rotation:

    â”‚                               â”‚ PC2 (low variance)
    â”‚   â—                          â”€â”¼â”€
    â”‚ â—   â—                     â—   â”‚   â—
    â”‚  â— â—                     â—  â— â”‚ â— â—
â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€                 â”€â”€â”€â”€â”€â—â—â—â—â—â”€â”€â”€â”€
    â”‚    â—â—â—                       â—â”‚  â—
    â”‚      â—                        â”‚
    â”‚                               PC1 (high variance)

Data spreads mainly diagonally    Rotate axes to align with spread
                                   Now PC1 captures most variation!
```

**Key Concepts:**

1. **Principal Components (PCs):** New axes aligned with maximum variance
2. **Variance:** How spread out the data is (information content)
3. **Dimensionality Reduction:** Keep top PCs, drop others
4. **Orthogonal:** PCs are perpendicular (uncorrelated with each other)

---

## 2. Step-by-Step Real World Walkthrough

Let's work through a **tiny, complete example** where we can trace every number!

### **Scenario: Student Study Habits**

We surveyed 5 students on 2 aspects:
- **Hours studied per week** (xâ‚)
- **Hours of sleep per week** (xâ‚‚)

**The Data:**

| Student | Study Hours (xâ‚) | Sleep Hours (xâ‚‚) |
|---------|------------------|------------------|
| Alice   | 5                | 10               |
| Bob     | 10               | 8                |
| Carol   | 8                | 6                |
| Dave    | 12               | 4                |
| Eve     | 15               | 2                |

```
Dataset matrix X (5 students Ã— 2 features):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5   10  â”‚  â† Alice
â”‚ 10    8  â”‚  â† Bob
â”‚  8    6  â”‚  â† Carol
â”‚ 12    4  â”‚  â† Dave
â”‚ 15    2  â”‚  â† Eve
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Observation:** More study hours â†’ less sleep hours (negative correlation!)

---

### **STEP 1: Center the Data (Remove Mean)**

**Why?** PCA finds directions of maximum variance around the origin. We need to center data at (0,0).

**1a. Calculate mean of each feature:**

```
Mean of xâ‚ (study hours):
Î¼â‚ = (5 + 10 + 8 + 12 + 15) / 5 = 50 / 5 = 10

Mean of xâ‚‚ (sleep hours):
Î¼â‚‚ = (10 + 8 + 6 + 4 + 2) / 5 = 30 / 5 = 6
```

**1b. Subtract mean from each data point:**

```
Alice:  [5, 10]  - [10, 6]  = [-5,  4]
Bob:    [10, 8]  - [10, 6]  = [ 0,  2]
Carol:  [8, 6]   - [10, 6]  = [-2,  0]
Dave:   [12, 4]  - [10, 6]  = [ 2, -2]
Eve:    [15, 2]  - [10, 6]  = [ 5, -4]
```

**Centered data matrix X_centered:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -5    4  â”‚  â† Alice (studies 5 hrs less than average, sleeps 4 hrs more)
â”‚  0    2  â”‚  â† Bob (average study, sleeps 2 hrs more)
â”‚ -2    0  â”‚  â† Carol (studies 2 hrs less, average sleep)
â”‚  2   -2  â”‚  â† Dave (studies 2 hrs more, sleeps 2 hrs less)
â”‚  5   -4  â”‚  â† Eve (studies 5 hrs more, sleeps 4 hrs less)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Visual representation:**

```
    Sleep (xâ‚‚)
      â†‘
    4 â”‚ â— Alice
    2 â”‚   â— Bob
    0 â”‚â”€â—â”€Carol
   -2 â”‚     â— Dave
   -4 â”‚         â— Eve
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Study (xâ‚)
       -5  -2  0  2  5

Data now centered at origin (0, 0)
Clear negative correlation visible!
```

---

### **STEP 2: Calculate Covariance Matrix**

**What is covariance?** Measures how two variables change together.
- Positive covariance: both increase together
- Negative covariance: one increases, other decreases
- Zero covariance: independent

**Formula for covariance matrix:**
$$C = \frac{1}{n-1} X^T X$$

Where:
- X is centered data (n Ã— d matrix)
- n = number of samples (5 students)
- d = number of features (2)

**2a. Compute X^T (transpose):**

```
X_centered:              X^T:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -5    4  â”‚            â”‚ -5   0  -2   2   5â”‚
â”‚  0    2  â”‚            â”‚  4   2   0  -2  -4â”‚
â”‚ -2    0  â”‚     â†’      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  2   -2  â”‚            (2 rows Ã— 5 columns)
â”‚  5   -4  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
(5 rows Ã— 2 columns)
```

**2b. Multiply X^T Ã— X:**

This gives us a 2Ã—2 covariance matrix.

**Element [0,0]: Variance of xâ‚ (study hours)**

```
= [(-5)Ã—(-5) + 0Ã—0 + (-2)Ã—(-2) + 2Ã—2 + 5Ã—5] / (5-1)
= [25 + 0 + 4 + 4 + 25] / 4
= 58 / 4
= 14.5
```

**Element [0,1] and [1,0]: Covariance between xâ‚ and xâ‚‚**

```
= [(-5)Ã—4 + 0Ã—2 + (-2)Ã—0 + 2Ã—(-2) + 5Ã—(-4)] / 4
= [-20 + 0 + 0 - 4 - 20] / 4
= -44 / 4
= -11.0
```

**Element [1,1]: Variance of xâ‚‚ (sleep hours)**

```
= [4Ã—4 + 2Ã—2 + 0Ã—0 + (-2)Ã—(-2) + (-4)Ã—(-4)] / 4
= [16 + 4 + 0 + 4 + 16] / 4
= 40 / 4
= 10.0
```

**Covariance Matrix C:**
```
C = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 14.5  -11.0 â”‚  â† variance(xâ‚),  cov(xâ‚,xâ‚‚)
    â”‚-11.0   10.0 â”‚  â† cov(xâ‚,xâ‚‚),    variance(xâ‚‚)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What this tells us:**
- Variance of study hours: 14.5 (high spread)
- Variance of sleep hours: 10.0 (moderate spread)
- Covariance: -11.0 (strong negative correlation!)
- When study increases, sleep decreases

---

### **STEP 3: Find Eigenvalues and Eigenvectors**

**This is the heart of PCA!**

**Eigenvalues:** Tell us how much variance each principal component captures
**Eigenvectors:** Tell us the direction of each principal component

**Mathematical problem:** Find Î» (eigenvalue) and v (eigenvector) such that:
$$Cv = \lambda v$$

**In plain English:** "Find directions where the covariance matrix just stretches the vector (doesn't rotate it)"

---

#### **Finding Eigenvalues**

**Solve:** det(C - Î»I) = 0

```
C - Î»I = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ 14.5-Î»    -11.0  â”‚
         â”‚ -11.0    10.0-Î»  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Determinant:
det(C - Î»I) = (14.5-Î»)(10.0-Î») - (-11.0)(-11.0)
            = (14.5-Î»)(10.0-Î») - 121
            = 145 - 14.5Î» - 10Î» + Î»Â² - 121
            = Î»Â² - 24.5Î» + 24

Set to zero:
Î»Â² - 24.5Î» + 24 = 0

Using quadratic formula: Î» = [24.5 Â± âˆš(24.5Â² - 4Ã—24)] / 2
                            = [24.5 Â± âˆš(600.25 - 96)] / 2
                            = [24.5 Â± âˆš504.25] / 2
                            = [24.5 Â± 22.46] / 2

Î»â‚ = (24.5 + 22.46) / 2 = 46.96 / 2 = 23.48
Î»â‚‚ = (24.5 - 22.46) / 2 = 2.04 / 2 = 1.02
```

**Eigenvalues:**
```
Î»â‚ = 23.48  (First principal component - captures most variance!)
Î»â‚‚ = 1.02   (Second principal component - captures remaining variance)

Total variance = Î»â‚ + Î»â‚‚ = 23.48 + 1.02 = 24.5 âœ“
(This equals trace of covariance matrix: 14.5 + 10.0 = 24.5 âœ“)
```

---

#### **Finding Eigenvectors**

**For Î»â‚ = 23.48:**

Solve: (C - Î»â‚I)vâ‚ = 0

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”
â”‚ 14.5-23.48   -11.0   â”‚ Ã— â”‚vâ‚â‚â”‚ = â”‚ 0 â”‚
â”‚   -11.0    10.0-23.48â”‚   â”‚vâ‚â‚‚â”‚   â”‚ 0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”
â”‚ -8.98  -11.0 â”‚ Ã— â”‚vâ‚â‚â”‚ = â”‚ 0 â”‚
â”‚ -11.0  -13.48â”‚   â”‚vâ‚â‚‚â”‚   â”‚ 0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜

From first equation:
-8.98vâ‚â‚ - 11.0vâ‚â‚‚ = 0
-8.98vâ‚â‚ = 11.0vâ‚â‚‚
vâ‚â‚ = -(11.0/8.98)vâ‚â‚‚ = -1.225vâ‚â‚‚

Let vâ‚â‚‚ = -1 (we can choose any scale, will normalize later)
Then vâ‚â‚ = 1.225

Eigenvector 1 (unnormalized):
vâ‚ = [1.225, -1]
```

**Normalize vâ‚ (make length = 1):**

```
Length = âˆš(1.225Â² + (-1)Â²) = âˆš(1.501 + 1) = âˆš2.501 = 1.581

vâ‚_normalized = [1.225/1.581, -1/1.581]
              = [0.775, -0.632]
```

**For Î»â‚‚ = 1.02:**

```
Following same process:
(C - Î»â‚‚I)vâ‚‚ = 0

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”
â”‚ 13.48  -11.0 â”‚ Ã— â”‚vâ‚‚â‚â”‚ = â”‚ 0 â”‚
â”‚ -11.0   8.98 â”‚   â”‚vâ‚‚â‚‚â”‚   â”‚ 0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜

From first equation:
13.48vâ‚‚â‚ - 11.0vâ‚‚â‚‚ = 0
vâ‚‚â‚ = (11.0/13.48)vâ‚‚â‚‚ = 0.816vâ‚‚â‚‚

Let vâ‚‚â‚‚ = 1
Then vâ‚‚â‚ = 0.816

Eigenvector 2 (unnormalized):
vâ‚‚ = [0.816, 1]

Normalize:
Length = âˆš(0.816Â² + 1Â²) = âˆš1.666 + 1 = âˆš2.666 = 1.633

vâ‚‚_normalized = [0.816/1.633, 1/1.633]
              = [0.500, 0.612]

Wait, let me recalculate to ensure it's perpendicular to vâ‚...
Actually, vâ‚‚ should be [0.632, 0.775] to be perpendicular to vâ‚
```

**Corrected eigenvectors (perpendicular to each other):**

```
PC1 (First Principal Component):
vâ‚ = [0.775, -0.632]

PC2 (Second Principal Component):
vâ‚‚ = [0.632, 0.775]

Check: vâ‚ Â· vâ‚‚ = 0.775Ã—0.632 + (-0.632)Ã—0.775 = 0.490 - 0.490 = 0 âœ“
(Perpendicular!)
```

---

### **STEP 4: Interpret Principal Components**

**PC1 = [0.775, -0.632]**

```
Meaning: 0.775Ã—(study hours) - 0.632Ã—(sleep hours)

Interpretation:
- When study increases by 1 unit â†’ PC1 increases by 0.775
- When sleep increases by 1 unit â†’ PC1 decreases by 0.632
- PC1 represents: "Study intensity" (more study, less sleep)
- Captures 23.48/24.5 = 95.8% of variance!
```

**PC2 = [0.632, 0.775]**

```
Meaning: 0.632Ã—(study hours) + 0.775Ã—(sleep hours)

Interpretation:
- When both study AND sleep increase â†’ PC2 increases
- PC2 represents: "Overall time spent" (total hours allocated)
- Captures 1.02/24.5 = 4.2% of variance
- Much less important!
```

**Visualize the new axes:**

```
    Original axes:           Principal Component axes:
    
    Sleep                    
      â†‘                          â•± PC2 (4.2% variance)
    4 â”‚ â—                      â•±
    2 â”‚   â—                  â•±
    0 â”‚â”€â—â”€â”€â”€â—â”€â”€â†’ Study      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PC1 (95.8% variance)
   -2 â”‚       â—           â•±
   -4 â”‚                 â•±
      
Data spreads mostly       PC1 aligned with
along diagonal           maximum spread!
```

---

### **STEP 5: Project Data onto Principal Components**

**Transform from original features to PC features:**

**Formula:** Z = X_centered Ã— V

Where:
- Z = transformed data (n Ã— k matrix)
- X_centered = centered original data (n Ã— d)
- V = eigenvector matrix (d Ã— k)

**Eigenvector matrix V (2 features â†’ 2 PCs):**
```
V = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 0.775  0.632 â”‚  â† [vâ‚â‚, vâ‚‚â‚] (contributions from xâ‚)
    â”‚-0.632  0.775 â”‚  â† [vâ‚â‚‚, vâ‚‚â‚‚] (contributions from xâ‚‚)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Transform each student:**

**Alice: [-5, 4]**
```
PC1 = 0.775Ã—(-5) + (-0.632)Ã—4
    = -3.875 - 2.528
    = -6.403

PC2 = 0.632Ã—(-5) + 0.775Ã—4
    = -3.160 + 3.100
    = -0.060

Alice in PC space: [-6.403, -0.060]
```

**Bob: [0, 2]**
```
PC1 = 0.775Ã—0 + (-0.632)Ã—2
    = 0 - 1.264
    = -1.264

PC2 = 0.632Ã—0 + 0.775Ã—2
    = 0 + 1.550
    = 1.550

Bob in PC space: [-1.264, 1.550]
```

**Carol: [-2, 0]**
```
PC1 = 0.775Ã—(-2) + (-0.632)Ã—0
    = -1.550 + 0
    = -1.550

PC2 = 0.632Ã—(-2) + 0.775Ã—0
    = -1.264 + 0
    = -1.264

Carol in PC space: [-1.550, -1.264]
```

**Dave: [2, -2]**
```
PC1 = 0.775Ã—2 + (-0.632)Ã—(-2)
    = 1.550 + 1.264
    = 2.814

PC2 = 0.632Ã—2 + 0.775Ã—(-2)
    = 1.264 - 1.550
    = -0.286

Dave in PC space: [2.814, -0.286]
```

**Eve: [5, -4]**
```
PC1 = 0.775Ã—5 + (-0.632)Ã—(-4)
    = 3.875 + 2.528
    = 6.403

PC2 = 0.632Ã—5 + 0.775Ã—(-4)
    = 3.160 - 3.100
    = 0.060

Eve in PC space: [6.403, 0.060]
```

**Complete transformation:**

```
Original data:           PC data:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -5    4  â”‚            â”‚ -6.403 -0.060â”‚  â† Alice
â”‚  0    2  â”‚            â”‚ -1.264  1.550â”‚  â† Bob
â”‚ -2    0  â”‚     â†’      â”‚ -1.550 -1.264â”‚  â† Carol
â”‚  2   -2  â”‚            â”‚  2.814 -0.286â”‚  â† Dave
â”‚  5   -4  â”‚            â”‚  6.403  0.060â”‚  â† Eve
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **STEP 6: Dimensionality Reduction**

**Key insight:** PC2 only captures 4.2% of variance. Can we drop it?

**Keep only PC1:**

```
Reduced data (5 students Ã— 1 feature):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -6.403   â”‚  â† Alice (low study intensity)
â”‚ -1.264   â”‚  â† Bob
â”‚ -1.550   â”‚  â† Carol
â”‚  2.814   â”‚  â† Dave
â”‚  6.403   â”‚  â† Eve (high study intensity)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

We went from 2 features â†’ 1 feature
Kept 95.8% of information!
```

**Visualize:**

```
Original 2D data:         Reduced to 1D (PC1 only):

    Sleep                      
      â†‘                    â—â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â—
    4 â”‚ â—                 Alice  Carol Bob    Dave      Eve
    2 â”‚   â—              -6.4    -1.6  -1.3   2.8       6.4
    0 â”‚â”€â—â”€â”€â”€â—â”€â”€â†’ Study    
   -2 â”‚       â—          All variation captured in one number!
   -4 â”‚                  
```

**Information preserved:**

```
Variance in original data:
- xâ‚ variance: 14.5
- xâ‚‚ variance: 10.0
- Total: 24.5

Variance in PC data:
- PC1 variance: 23.48 (95.8%)
- PC2 variance: 1.02 (4.2%)
- Total: 24.5 âœ“ (same!)

By keeping only PC1:
Information kept: 23.48/24.5 = 95.8%
Information lost: 1.02/24.5 = 4.2%
```

---

### **STEP 7: Reconstruct Original Data (Approximate)**

**Can we go back from PC space to original space?**

**Formula:** X_reconstructed = Z Ã— V^T

**If we use only PC1:**

```
For Alice:
PC1 = -6.403, PC2 = 0 (we dropped it)

xâ‚_reconstructed = PC1Ã—vâ‚â‚ + PC2Ã—vâ‚‚â‚
                 = -6.403Ã—0.775 + 0Ã—0.632
                 = -4.962

xâ‚‚_reconstructed = PC1Ã—vâ‚â‚‚ + PC2Ã—vâ‚‚â‚‚
                 = -6.403Ã—(-0.632) + 0Ã—0.775
                 = 4.047

Alice reconstructed: [-4.962, 4.047]
Alice original (centered): [-5, 4]
Error: [0.038, 0.047] (tiny!)
```

**All students reconstructed from PC1 only:**

```
Original (centered):     Reconstructed:         Error:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -5    4  â”‚            â”‚-4.96  4.05â”‚          â”‚-0.04  0.05â”‚
â”‚  0    2  â”‚            â”‚-0.98  0.80â”‚          â”‚ 0.98  1.20â”‚
â”‚ -2    0  â”‚     â‰ˆ      â”‚-1.20  0.98â”‚          â”‚-0.80 -0.98â”‚
â”‚  2   -2  â”‚            â”‚ 2.18 -1.78â”‚          â”‚-0.18 -0.22â”‚
â”‚  5   -4  â”‚            â”‚ 4.96 -4.05â”‚          â”‚ 0.04  0.05â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Average error per value: ~0.6
We reduced from 2 features to 1 feature
With only 4.2% information loss!
```

---

## 3. Formula Legend

### Core Symbols
| Symbol | Name | Meaning |
|--------|------|---------|
| **X** | Data matrix | n samples Ã— d features |
| **n** | Number of samples | How many data points (5 students) |
| **d** | Number of features | Dimensions (2: study & sleep) |
| **Î¼** | Mean vector | Average of each feature |
| **X_c** | Centered data | X with means removed |

### Covariance Components
| Symbol | Name | Meaning |
|--------|------|---------|
| **C** | Covariance matrix | dÃ—d matrix showing feature relationships |
| **ÏƒÂ²** | Variance | Spread of a single feature |
| **cov(x,y)** | Covariance | How two features vary together |
| **X^T** | Transpose | Flip rows and columns |

### Eigendecomposition
| Symbol | Name | Meaning |
|--------|------|---------|
| **Î»** (lambda) | Eigenvalue | Amount of variance in that direction |
| **v** | Eigenvector | Direction of principal component |
| **V** | Eigenvector matrix | All eigenvectors as columns |
| **I** | Identity matrix | Diagonal matrix of ones |
| **det()** | Determinant | Scalar value from matrix |

### PCA Transformation
| Symbol | Name | Meaning |
|--------|------|---------|
| **Z** | Transformed data | Data in PC space (n Ã— k) |
| **k** | Number of PCs kept | How many components to keep |
| **PC_i** | Principal Component i | The i-th new feature |

---

## 4. The Formulas

### **Step 1: Center the Data**

$$X_c = X - \mu$$

Where $\mu_j = \frac{1}{n}\sum_{i=1}^{n} X_{ij}$ for each feature j

**In matrix form:**
$$X_c = X - \mathbf{1}\mu^T$$

---

### **Step 2: Compute Covariance Matrix**

$$C = \frac{1}{n-1}X_c^T X_c$$

**Expanded form (for 2 features):**
$$C = \begin{bmatrix} 
\text{var}(x_1) & \text{cov}(x_1,x_2) \\
\text{cov}(x_1,x_2) & \text{var}(x_2)
\end{bmatrix}$$

**Where:**
$$\text{var}(x_j) = \frac{1}{n-1}\sum_{i=1}^{n}(x_{ij} - \mu_j)^2$$

$$\text{cov}(x_j, x_k) = \frac{1}{n-1}\sum_{i=1}^{n}(x_{ij} - \mu_j)(x_{ik} - \mu_k)$$

---

### **Step 3: Eigendecomposition**

**Find eigenvalues and eigenvectors:**
$$Cv = \lambda v$$

**Characteristic equation:**
$$\det(C - \lambda I) = 0$$

**For 2Ã—2 matrix:**
$$\det\begin{bmatrix} c_{11}-\lambda & c_{12} \\ c_{21} & c_{22}-\lambda \end{bmatrix} = 0$$

$$(c_{11}-\lambda)(c_{22}-\lambda) - c_{12}c_{21} = 0$$

---

### **Step 4: Sort by Eigenvalue**

**Order eigenvalues:** $\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_d$

**Corresponding eigenvectors:** $v_1, v_2, ..., v_d$

**Eigenvector matrix:**
$$V = [v_1 | v_2 | ... | v_k]$$

Where k â‰¤ d is the number of PCs to keep

---

### **Step 5: Transform Data**

$$Z = X_c V$$

**For each sample i:**
$$z_i = X_{c,i} \cdot V$$

**In components:**
$$\text{PC}_j^{(i)} = \sum_{d=1}^{D} x_{c,i,d} \cdot v_{j,d}$$

---

### **Step 6: Variance Explained**

**Proportion of variance by PC j:**
$$\text{Explained Variance}_j = \frac{\lambda_j}{\sum_{i=1}^{d}\lambda_i}$$

**Cumulative variance:**
$$\text{Cumulative}_k = \frac{\sum_{j=1}^{k}\lambda_j}{\sum_{i=1}^{d}\lambda_i}$$

---

### **Step 7: Reconstruction**

**From PC space back to original space:**
$$X_{reconstructed} = ZV^T + \mu$$

**With k components (dimensionality reduction):**
$$X_{approx} = Z_k V_k^T + \mu$$

Where $Z_k$ has only first k PCs, $V_k$ has only first k eigenvectors

**Reconstruction error:**
$$\text{Error} = ||X - X_{approx}||^2 = \sum_{j=k+1}^{d}\lambda_j$$

---

## 5. Complete PCA Algorithm

```
INPUT: Data matrix X (n Ã— d)
OUTPUT: Transformed data Z (n Ã— k), where k â‰¤ d

1. CENTER DATA:
   Î¼ = mean(X, axis=0)
   X_c = X - Î¼

2. COMPUTE COVARIANCE:
   C = (1/(n-1)) Ã— X_c^T Ã— X_c

3. EIGENDECOMPOSITION:
   Solve: Cv = Î»v
   Get: eigenvalues Î»â‚,...,Î»_d
        eigenvectors vâ‚,...,v_d

4. SORT BY EIGENVALUE:
   Order: Î»â‚ â‰¥ Î»â‚‚ â‰¥ ... â‰¥ Î»_d
   Corresponding vâ‚, vâ‚‚, ..., v_d

5. SELECT TOP k COMPONENTS:
   V = [vâ‚ | vâ‚‚ | ... | v_k]

6. TRANSFORM:
   Z = X_c Ã— V

7. (OPTIONAL) RECONSTRUCT:
   X_approx = Z Ã— V^T + Î¼
```

---

## 6. Visual Understanding

### **What PCA Does Geometrically**

```
BEFORE PCA (Original Features):

    xâ‚‚ (Sleep)
      â†‘
    4 â”‚ â—                     Points scattered
    2 â”‚   â—                   in 2D space
    0 â”‚â”€â—â”€â”€â”€â—â”€â”€â”€â”€â†’ xâ‚ (Study)
   -2 â”‚       â—
   -4 â”‚

Covariance matrix tells us:
- Data spreads more along diagonal
- xâ‚ and xâ‚‚ are negatively correlated


AFTER PCA (Rotated to PCs):

    PC2 (4.2% variance)
      â†‘
    2 â”‚  â—                    Same points,
    0 â”œâ”€â”€â—â—â”€â”€â”€â”€â†’ PC1 (95.8% variance)  rotated axes
   -2 â”‚    â—
      
New axes aligned with spread!
- PC1: main direction of variation
- PC2: perpendicular, captures residual
```

---

### **The Projection Process**

```
STEP BY STEP:

1. Original point: Alice [-5, 4]

2. Draw onto PC1 axis:
   
      xâ‚‚                     Drop perpendicular
       â†‘                     to PC1 axis
     4 â”‚ â— Alice                    â•±
       â”‚  â•²                        â•±
       â”‚   â•²                     â•±
     0 â”œâ”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â†’ xâ‚       â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€ PC1
       â”‚   Alice'             Alice' on PC1
      -5              
      
   Alice' = -6.403 on PC1

3. Information lost:
   - Original: 2 numbers [-5, 4]
   - Reduced: 1 number [-6.403]
   - Lost: tiny variation perpendicular to PC1
```

---

### **Variance Explained Visualization**

```
Original features (14.5 + 10.0 = 24.5 total variance):

xâ‚: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 14.5 (59%)
xâ‚‚: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 10.0 (41%)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Total: 24.5 (100%)


After PCA (same 24.5 total, redistributed):

PC1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 23.48 (95.8%)
PC2: â–ˆâ–ˆ 1.02 (4.2%)
     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     Total: 24.5 (100%)

PC1 captures almost ALL variation!
We can drop PC2 with minimal loss.
```

---

## 7. Practical Applications

### **Use Case 1: Image Compression**

```
Original image: 100Ã—100 pixels = 10,000 dimensions
â†“
Apply PCA
â†“
Keep top 50 PCs (captures 95% of variance)
â†“
Compressed: 50 dimensions
â†“
Storage: 200Ã— smaller!
Can reconstruct image with only 5% error
```

### **Use Case 2: Visualization**

```
Dataset: 1,000 features per person
Problem: Can't visualize 1,000-dimensional space!

Solution:
1. Apply PCA
2. Keep top 2 or 3 PCs
3. Plot in 2D or 3D
4. See clusters, patterns, outliers

Example: Gene expression data (20,000 genes)
â†’ PCA to 2D
â†’ See cancer types cluster separately
```

### **Use Case 3: Noise Reduction**

```
Signal: 100 features, noisy measurements
PCA shows:
- PC1-PC10: Real signal (80% variance)
- PC11-PC100: Mostly noise (20% variance)

Keep only PC1-PC10 â†’ Denoise signal!
```

### **Use Case 4: Feature Engineering for ML**

```
Before PCA:
- 1000 correlated features
- Neural network: millions of parameters
- Training: slow, overfitting

After PCA:
- 50 uncorrelated PCs
- Neural network: fewer parameters
- Training: fast, better generalization
```

---

## 8. Connection to Machine Learning

### **PCA in the ML Pipeline**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Machine Learning Workflow        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RAW DATA (high-dimensional, correlated)
        â†“
    [PCA Preprocessing]
        â†“
REDUCED DATA (low-dimensional, uncorrelated)
        â†“
    [Train Model]
    - Neural Network
    - SVM
    - Random Forest
        â†“
    PREDICTIONS
```

### **Why PCA Helps Neural Networks**

**Before PCA:**
```
Input layer: 1000 neurons
Hidden layer: 500 neurons
Connections: 1000 Ã— 500 = 500,000 weights
Problem: Overfitting, slow training
```

**After PCA (keep 50 PCs):**
```
Input layer: 50 neurons
Hidden layer: 500 neurons
Connections: 50 Ã— 500 = 25,000 weights
Benefit: 20Ã— fewer parameters, faster, less overfitting!
```

---

## 9. Key Takeaways

### **What PCA Does:**

1. **Finds directions of maximum variance** (principal components)
2. **Removes correlation** (PCs are orthogonal/independent)
3. **Ranks features by importance** (via eigenvalues)
4. **Enables dimensionality reduction** (keep top PCs)
5. **Preserves information** (keeps maximum variance)

### **When to Use PCA:**

âœ… **Use PCA when:**
- You have many correlated features
- You need to visualize high-dimensional data
- Training is slow due to high dimensionality
- You want to denoise data
- Features are continuous (not categorical)

âŒ **Don't use PCA when:**
- Features are already uncorrelated
- You need to interpret individual features (PCs are combinations!)
- Data is non-linear (PCA is linear) â†’ use t-SNE or UMAP instead
- You have categorical features

### **The Math Behind the Magic:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PCA = Rotation to Maximize Variance          â”‚
â”‚                                               â”‚
â”‚  Eigenvalues = Amount of variance            â”‚
â”‚  Eigenvectors = Direction of variance         â”‚
â”‚                                               â”‚
â”‚  Keep top k eigenvectors                     â”‚
â”‚  â†’ Optimal k-dimensional representation       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. Summary: Our Student Example

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          COMPLETE PCA JOURNEY                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STARTED WITH:
5 students Ã— 2 features (study, sleep)

DISCOVERED:
- Strong negative correlation (-11.0)
- Study varies more than sleep (14.5 vs 10.0)

FOUND PRINCIPAL COMPONENTS:
- PC1 [0.775, -0.632]: "Study intensity"
  Captures 95.8% of variance
  
- PC2 [0.632, 0.775]: "Total time"
  Captures 4.2% of variance

REDUCED DIMENSIONS:
2 features â†’ 1 feature (PC1 only)
Kept 95.8% of information!

LEARNED:
Students vary mainly in study intensity
(tradeoff between study and sleep)
Not much variation in total time spent
```
