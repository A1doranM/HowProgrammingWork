# Linear Discriminant Analysis (LDA): Complete Explanation
## (Detailed Step-by-Step with Number Tracing)

---

## ğŸ”— **Connection to PCA**

### **What We Learned from PCA:**

```
PCA:
- Found directions of maximum variance
- Unsupervised (ignored labels)
- Goal: Compress data, reduce dimensions

Example from PCA:
Students: [study hours, sleep hours]
PC1: Direction where data spreads most
NO consideration of any categories!
```

### **The Problem with PCA for Classification:**

```
Dataset with 2 classes:

    xâ‚‚                         PCA says:
     â†‘                         "Project here!" â†“
   5 â”‚ â—‹ â—‹ â—‹                        PC1
   4 â”‚ â—‹ â—‹ â—‹                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
   3 â”‚                         (max variance)
   2 â”‚   â— â— â—
   1 â”‚   â— â— â—
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚
     
After PCA projection:
â”€â—‹â—‹â—‹â—â—â—â”€â”€â”€â”€â”€â”€â”€â”€â†’ PC1

Classes overlap! Bad for classification!
PCA found variance, but ignored class labels.
```

**What we really want:**

```
    xâ‚‚                         LDA says:
     â†‘                         "Project here!" â†“
   5 â”‚ â—‹ â—‹ â—‹                        LD1
   4 â”‚ â—‹ â—‹ â—‹                          â†‘
   3 â”‚                                â”‚
   2 â”‚   â— â— â—                        â”‚
   1 â”‚   â— â— â—
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚
     
After LDA projection:
         â—‹â—‹â—‹
         â†‘
    (Class 1)
    
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
         â—â—â—
         â†“
    (Class 2)

Classes separated! Great for classification!
LDA maximizes separation between classes.
```

---

## **The Core Difference:**

| Aspect | PCA | LDA |
|--------|-----|-----|
| **Type** | Unsupervised | Supervised |
| **Uses labels?** | No | Yes |
| **Maximizes** | Total variance | Class separation |
| **Goal** | Dimensionality reduction | Classification |
| **Output** | Principal Components | Linear Discriminants |
| **Best for** | Data visualization, compression | Classification tasks |

---

# Part 1: What is LDA?

## 1. Plain English Explanation

### **The Intuition**

Imagine you're a doctor trying to diagnose patients:
- You measure: height, weight, blood pressure, age
- You have two groups: Healthy and Sick

**PCA would say:** "Let's find the direction where people vary most (probably height)"
**LDA would say:** "Let's find the direction that best separates Healthy from Sick!"

### **LDA's Two Goals (Must Balance Both):**

1. **Maximize between-class variance:** Push class means far apart
2. **Minimize within-class variance:** Keep each class tightly clustered

```
BAD projection (like PCA might give):
Class 1: â—‹ â—‹   â—‹ â—‹     â—‹
Class 2:   â— â—   â— â— â—
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         Overlapping mess!

GOOD projection (what LDA finds):
Class 1: â—‹â—‹â—‹â—‹â—‹
              
              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              
Class 2:           â—â—â—â—â—
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         Clean separation!
```

### **The Fisher Criterion**

LDA uses "Fisher's Linear Discriminant" which maximizes:

$$J(w) = \frac{\text{between-class variance}}{\text{within-class variance}}$$

**In words:** "Find the direction where classes are far apart AND each class is tight."

---

## 2. Step-by-Step Real World Walkthrough

Let's work through a **complete numerical example** with every calculation!

### **Scenario: Flower Species Classification**

We have measurements of two iris species:
- **Setosa** (Class 1): Small flowers
- **Versicolor** (Class 2): Large flowers

**Features:**
- xâ‚: Petal length (cm)
- xâ‚‚: Petal width (cm)

**The Data (6 flowers total):**

| Flower | Species | Petal Length (xâ‚) | Petal Width (xâ‚‚) |
|--------|---------|-------------------|------------------|
| 1 | Setosa | 1.4 | 0.2 |
| 2 | Setosa | 1.3 | 0.2 |
| 3 | Setosa | 1.5 | 0.4 |
| 4 | Versicolor | 4.7 | 1.4 |
| 5 | Versicolor | 4.5 | 1.5 |
| 6 | Versicolor | 4.6 | 1.3 |

```
Dataset matrix:
        xâ‚   xâ‚‚   Class
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    1 â”‚ 1.4  0.2 â”‚ S â”‚
    2 â”‚ 1.3  0.2 â”‚ S â”‚
    3 â”‚ 1.5  0.4 â”‚ S â”‚
    4 â”‚ 4.7  1.4 â”‚ V â”‚
    5 â”‚ 4.5  1.5 â”‚ V â”‚
    6 â”‚ 4.6  1.3 â”‚ V â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

S = Setosa (Class 1)
V = Versicolor (Class 2)
```

**Visual representation:**

```
    xâ‚‚ (width)
     â†‘
  1.5â”‚         â—        Class 2: Versicolor
  1.4â”‚      â—    â—      (Large petals)
  1.3â”‚
  1.2â”‚
     â”‚
  0.4â”‚  â—‹
  0.2â”‚â—‹   â—‹              Class 1: Setosa
  0.0â”‚                   (Small petals)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚ (length)
      0  1  2  3  4  5

Clear visual separation!
Let's find the best projection mathematically.
```

---

### **STEP 1: Separate Data by Class**

**Class 1 (Setosa): nâ‚ = 3 samples**
```
Xâ‚ = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ 1.4  0.2 â”‚
     â”‚ 1.3  0.2 â”‚
     â”‚ 1.5  0.4 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Class 2 (Versicolor): nâ‚‚ = 3 samples**
```
Xâ‚‚ = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ 4.7  1.4 â”‚
     â”‚ 4.5  1.5 â”‚
     â”‚ 4.6  1.3 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **STEP 2: Calculate Class Means (Centers)**

**Mean of Class 1: Î¼â‚**

```
Î¼â‚[xâ‚] = (1.4 + 1.3 + 1.5) / 3 = 4.2 / 3 = 1.4
Î¼â‚[xâ‚‚] = (0.2 + 0.2 + 0.4) / 3 = 0.8 / 3 = 0.267

Î¼â‚ = [1.4, 0.267]
```

**Mean of Class 2: Î¼â‚‚**

```
Î¼â‚‚[xâ‚] = (4.7 + 4.5 + 4.6) / 3 = 13.8 / 3 = 4.6
Î¼â‚‚[xâ‚‚] = (1.4 + 1.5 + 1.3) / 3 = 4.2 / 3 = 1.4

Î¼â‚‚ = [4.6, 1.4]
```

**Overall mean: Î¼ (all 6 samples)**

```
Î¼[xâ‚] = (1.4 + 1.3 + 1.5 + 4.7 + 4.5 + 4.6) / 6 = 18.0 / 6 = 3.0
Î¼[xâ‚‚] = (0.2 + 0.2 + 0.4 + 1.4 + 1.5 + 1.3) / 6 = 5.0 / 6 = 0.833

Î¼ = [3.0, 0.833]
```

**Visualize means:**

```
    xâ‚‚
     â†‘
  1.5â”‚         â—
  1.4â”‚      â—  â˜…â‚‚ â—     â˜…â‚‚ = Î¼â‚‚ [4.6, 1.4] (Versicolor mean)
  1.3â”‚
     â”‚
  0.4â”‚  â—‹
  0.2â”‚â—‹ â˜…â‚â—‹             â˜…â‚ = Î¼â‚ [1.4, 0.267] (Setosa mean)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚
      0  1  2  3  4  5
```

---

### **STEP 3: Calculate Within-Class Scatter Matrix (S_W)**

**What is this?** Measures how spread out each class is around its own mean.

**Formula:**
$$S_W = S_1 + S_2$$

Where for each class:
$$S_i = \sum_{x \in \text{Class } i} (x - \mu_i)(x - \mu_i)^T$$

---

#### **For Class 1 (Setosa):**

**Sample 1: [1.4, 0.2] - Î¼â‚ = [1.4, 0.267]**

```
x - Î¼â‚ = [1.4, 0.2] - [1.4, 0.267]
       = [0.0, -0.067]

Outer product: (x - Î¼â‚)(x - Î¼â‚)^T
â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  0.0 â”‚ Ã— â”‚ 0.0 -0.067â”‚ = â”‚  0.000  -0.000 â”‚
â”‚-0.067â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ -0.000   0.004 â”‚
â””â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Sample 2: [1.3, 0.2] - Î¼â‚ = [1.4, 0.267]**

```
x - Î¼â‚ = [1.3, 0.2] - [1.4, 0.267]
       = [-0.1, -0.067]

Outer product:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -0.1  â”‚ Ã— â”‚ -0.1  -0.067 â”‚ = â”‚  0.010   0.007 â”‚
â”‚-0.067 â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  0.007   0.004 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Sample 3: [1.5, 0.4] - Î¼â‚ = [1.4, 0.267]**

```
x - Î¼â‚ = [1.5, 0.4] - [1.4, 0.267]
       = [0.1, 0.133]

Outer product:
â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0.1  â”‚ Ã— â”‚ 0.1   0.133 â”‚ = â”‚  0.010   0.013 â”‚
â”‚0.133 â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  0.013   0.018 â”‚
â””â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Sum for Class 1:**

```
Sâ‚ = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  0.000  -0.000 â”‚   â”‚  0.010   0.007 â”‚   â”‚  0.010   0.013 â”‚
     â”‚ -0.000   0.004 â”‚ + â”‚  0.007   0.004 â”‚ + â”‚  0.013   0.018 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sâ‚ = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  0.020   0.020 â”‚
     â”‚  0.020   0.027 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### **For Class 2 (Versicolor):**

**Sample 4: [4.7, 1.4] - Î¼â‚‚ = [4.6, 1.4]**

```
x - Î¼â‚‚ = [4.7, 1.4] - [4.6, 1.4]
       = [0.1, 0.0]

Outer product:
â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0.1 â”‚ Ã— â”‚ 0.1  0.0â”‚ = â”‚  0.010   0.000 â”‚
â”‚ 0.0 â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  0.000   0.000 â”‚
â””â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Sample 5: [4.5, 1.5] - Î¼â‚‚ = [4.6, 1.4]**

```
x - Î¼â‚‚ = [4.5, 1.5] - [4.6, 1.4]
       = [-0.1, 0.1]

Outer product:
â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -0.1 â”‚ Ã— â”‚ -0.1  0.1â”‚ = â”‚  0.010  -0.010 â”‚
â”‚  0.1 â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ -0.010   0.010 â”‚
â””â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Sample 6: [4.6, 1.3] - Î¼â‚‚ = [4.6, 1.4]**

```
x - Î¼â‚‚ = [4.6, 1.3] - [4.6, 1.4]
       = [0.0, -0.1]

Outer product:
â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  0.0 â”‚ Ã— â”‚ 0.0 -0.1 â”‚ = â”‚  0.000   0.000 â”‚
â”‚ -0.1 â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  0.000   0.010 â”‚
â””â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Sum for Class 2:**

```
Sâ‚‚ = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  0.010   0.000 â”‚   â”‚  0.010  -0.010 â”‚   â”‚  0.000   0.000 â”‚
     â”‚  0.000   0.000 â”‚ + â”‚ -0.010   0.010 â”‚ + â”‚  0.000   0.010 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sâ‚‚ = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  0.020  -0.010 â”‚
     â”‚ -0.010   0.020 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### **Total Within-Class Scatter:**

```
S_W = Sâ‚ + Sâ‚‚

S_W = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  0.020   0.020 â”‚   â”‚  0.020  -0.010 â”‚
      â”‚  0.020   0.027 â”‚ + â”‚ -0.010   0.020 â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

S_W = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  0.040   0.010 â”‚
      â”‚  0.010   0.047 â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What does S_W tell us?**
- Diagonal elements: variance within each class for each feature
- Off-diagonal: covariance within classes
- We want to minimize this (tight clusters)

---

### **STEP 4: Calculate Between-Class Scatter Matrix (S_B)**

**What is this?** Measures how far apart the class means are.

**Formula:**
$$S_B = \sum_{i=1}^{c} n_i (\mu_i - \mu)(\mu_i - \mu)^T$$

Where:
- c = number of classes (2)
- n_i = number of samples in class i
- Î¼_i = mean of class i
- Î¼ = overall mean

---

#### **For Class 1:**

```
Î¼â‚ - Î¼ = [1.4, 0.267] - [3.0, 0.833]
       = [-1.6, -0.567]

Outer product:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -1.6   â”‚ Ã— â”‚ -1.6    -0.567   â”‚ = â”‚  2.560    0.907   â”‚
â”‚ -0.567 â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  0.907    0.321   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Multiply by nâ‚ = 3:
3 Ã— â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  2.560    0.907   â”‚ = â”‚  7.680    2.720   â”‚
    â”‚  0.907    0.321   â”‚   â”‚  2.720    0.963   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **For Class 2:**

```
Î¼â‚‚ - Î¼ = [4.6, 1.4] - [3.0, 0.833]
       = [1.6, 0.567]

Outer product:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1.6  â”‚ Ã— â”‚  1.6    0.567   â”‚ = â”‚  2.560    0.907   â”‚
â”‚ 0.567 â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  0.907    0.321   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Multiply by nâ‚‚ = 3:
3 Ã— â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  2.560    0.907   â”‚ = â”‚  7.680    2.720   â”‚
    â”‚  0.907    0.321   â”‚   â”‚  2.720    0.963   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Total Between-Class Scatter:**

```
S_B = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  7.680    2.720   â”‚   â”‚  7.680    2.720   â”‚
      â”‚  2.720    0.963   â”‚ + â”‚  2.720    0.963   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

S_B = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  15.360    5.440   â”‚
      â”‚   5.440    1.927   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What does S_B tell us?**
- Measures separation between class means
- Larger values = classes farther apart
- We want to maximize this (separate clusters)

---

### **STEP 5: Solve the Generalized Eigenvalue Problem**

**The LDA objective:** Find direction w that maximizes:

$$J(w) = \frac{w^T S_B w}{w^T S_W w}$$

**This is equivalent to solving:**
$$S_W^{-1} S_B w = \lambda w$$

We need to:
1. Compute S_W^(-1) (inverse of within-class scatter)
2. Compute S_W^(-1) Ã— S_B
3. Find eigenvalues and eigenvectors

---

#### **Step 5a: Invert S_W**

**S_W:**
```
S_W = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  0.040   0.010 â”‚
      â”‚  0.010   0.047 â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Formula for 2Ã—2 matrix inverse:**
$$\begin{bmatrix} a & b \\ c & d \end{bmatrix}^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$$

**Calculate determinant:**
```
det(S_W) = (0.040)(0.047) - (0.010)(0.010)
         = 0.00188 - 0.0001
         = 0.00178
```

**Calculate inverse:**
```
S_W^(-1) = (1/0.00178) Ã— â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  0.047  -0.010 â”‚
                         â”‚ -0.010   0.040 â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

S_W^(-1) = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  26.404  -5.618  â”‚
           â”‚  -5.618  22.472  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### **Step 5b: Compute S_W^(-1) Ã— S_B**

```
S_W^(-1) Ã— S_B = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  26.404  -5.618  â”‚ Ã— â”‚  15.360    5.440   â”‚
                 â”‚  -5.618  22.472  â”‚   â”‚   5.440    1.927   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Element [0,0]:**
```
(26.404)(15.360) + (-5.618)(5.440)
= 405.645 - 30.562
= 375.083
```

**Element [0,1]:**
```
(26.404)(5.440) + (-5.618)(1.927)
= 143.638 - 10.825
= 132.813
```

**Element [1,0]:**
```
(-5.618)(15.360) + (22.472)(5.440)
= -86.293 + 122.248
= 35.955
```

**Element [1,1]:**
```
(-5.618)(5.440) + (22.472)(1.927)
= -30.562 + 43.303
= 12.741
```

**Result:**
```
S_W^(-1) S_B = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  375.083   132.813  â”‚
               â”‚   35.955    12.741  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### **Step 5c: Find Eigenvalues**

**Characteristic equation:** det(S_W^(-1) S_B - Î»I) = 0

```
â”‚375.083-Î»   132.813  â”‚
â”‚  35.955    12.741-Î» â”‚ = 0

(375.083-Î»)(12.741-Î») - (132.813)(35.955) = 0
4,779.182 - 375.083Î» - 12.741Î» + Î»Â² - 4,774.601 = 0
Î»Â² - 387.824Î» + 4.581 = 0
```

**Using quadratic formula:**
```
Î» = [387.824 Â± âˆš(387.824Â² - 4Ã—4.581)] / 2
  = [387.824 Â± âˆš(150,407.052 - 18.324)] / 2
  = [387.824 Â± âˆš150,388.728] / 2
  = [387.824 Â± 387.801] / 2

Î»â‚ = (387.824 + 387.801) / 2 = 387.813
Î»â‚‚ = (387.824 - 387.801) / 2 = 0.012
```

**Eigenvalues:**
```
Î»â‚ = 387.813  (Primary discriminant - captures all separation!)
Î»â‚‚ = 0.012    (Negligible - almost zero)
```

**Note:** For 2-class LDA, we get maximum of 1 useful discriminant (c-1 = 2-1 = 1).
Î»â‚‚ is essentially zero, confirming this!

---

#### **Step 5d: Find Eigenvector for Î»â‚**

**Solve:** (S_W^(-1) S_B - Î»â‚I)w = 0

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”
â”‚  375.083-387.813      132.813    â”‚ Ã— â”‚wâ‚ â”‚ = â”‚ 0 â”‚
â”‚     35.955         12.741-387.813â”‚   â”‚wâ‚‚ â”‚   â”‚ 0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”
â”‚ -12.730  132.813â”‚ Ã— â”‚wâ‚ â”‚ = â”‚ 0 â”‚
â”‚  35.955 -375.072â”‚   â”‚wâ‚‚ â”‚   â”‚ 0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜

From first equation:
-12.730wâ‚ + 132.813wâ‚‚ = 0
12.730wâ‚ = 132.813wâ‚‚
wâ‚ = (132.813/12.730)wâ‚‚
wâ‚ = 10.433wâ‚‚

Let wâ‚‚ = 1
Then wâ‚ = 10.433

Eigenvector (unnormalized):
w = [10.433, 1]
```

**Normalize:**
```
Length = âˆš(10.433Â² + 1Â²) = âˆš(108.847 + 1) = âˆš109.847 = 10.481

w_normalized = [10.433/10.481, 1/10.481]
             = [0.995, 0.095]
```

**LDA Direction (Linear Discriminant):**
```
w = [0.995, 0.095]

Interpretation:
LDâ‚ = 0.995Ã—(petal length) + 0.095Ã—(petal width)

Petal length is ~10Ã— more important than width
for separating Setosa from Versicolor!
```

---

### **STEP 6: Project Data onto Linear Discriminant**

**Formula:** For each sample x:
$$z = w^T x = w_1 x_1 + w_2 x_2$$

---

#### **Class 1 (Setosa):**

**Sample 1: [1.4, 0.2]**
```
z = 0.995Ã—1.4 + 0.095Ã—0.2
  = 1.393 + 0.019
  = 1.412
```

**Sample 2: [1.3, 0.2]**
```
z = 0.995Ã—1.3 + 0.095Ã—0.2
  = 1.294 + 0.019
  = 1.313
```

**Sample 3: [1.5, 0.4]**
```
z = 0.995Ã—1.5 + 0.095Ã—0.4
  = 1.493 + 0.038
  = 1.531
```

**Setosa projections:** [1.412, 1.313, 1.531]

---

#### **Class 2 (Versicolor):**

**Sample 4: [4.7, 1.4]**
```
z = 0.995Ã—4.7 + 0.095Ã—1.4
  = 4.677 + 0.133
  = 4.810
```

**Sample 5: [4.5, 1.5]**
```
z = 0.995Ã—4.5 + 0.095Ã—1.5
  = 4.478 + 0.143
  = 4.621
```

**Sample 6: [4.6, 1.3]**
```
z = 0.995Ã—4.6 + 0.095Ã—1.3
  = 4.577 + 0.124
  = 4.701
```

**Versicolor projections:** [4.810, 4.621, 4.701]

---

### **STEP 7: Visualize the Results**

**Before LDA (Original 2D space):**

```
    xâ‚‚ (width)
     â†‘
  1.5â”‚         â—5
  1.4â”‚      â—4   â—6
  1.3â”‚
     â”‚
  0.4â”‚  â—‹3
  0.2â”‚â—‹1  â—‹2
  0.0â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚ (length)
      0  1  2  3  4  5

Classes separated, but using 2 dimensions
```

**After LDA (Projected onto 1D line):**

```
              LDA Direction â†’
              w = [0.995, 0.095]

Original space:            Projection on LDâ‚:

    xâ‚‚                     Class 1      Class 2
     â†‘                     â—‹â—‹â—‹          â—â—â—
  1.5â”‚    â—â—â—              â†“            â†“
  1.0â”‚  LDâ‚ â†’          â”€â”€â”€1.4â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€4.7â”€â”€â”€â”€â”€
  0.5â”‚  â•±                 Setosa     Versicolor
  0.2â”‚â—‹â—‹â—‹
     â””â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚

Perfect separation in 1D!

Setosa:      1.313  1.412  1.531     (mean â‰ˆ 1.42)
                   â†•
           Gap = 3.09 standard deviations!
                   â†•
Versicolor:  4.621  4.701  4.810     (mean â‰ˆ 4.71)
```

**Quantify the separation:**

```
Setosa mean: Î¼â‚ = (1.412 + 1.313 + 1.531) / 3 = 1.419
Versicolor mean: Î¼â‚‚ = (4.810 + 4.621 + 4.701) / 3 = 4.711

Distance between means: 4.711 - 1.419 = 3.292

Setosa variance: 
  = [(1.412-1.419)Â² + (1.313-1.419)Â² + (1.531-1.419)Â²] / 2
  = [0.000049 + 0.011236 + 0.012544] / 2
  = 0.011914

Versicolor variance:
  = [(4.810-4.711)Â² + (4.621-4.711)Â² + (4.701-4.711)Â²] / 2
  = [0.009801 + 0.008100 + 0.000100] / 2
  = 0.009001

Fisher criterion:
J = (Î¼â‚‚ - Î¼â‚)Â² / (Ïƒâ‚Â² + Ïƒâ‚‚Â²)
  = (3.292)Â² / (0.011914 + 0.009001)
  = 10.836 / 0.020915
  = 518.2

Extremely high separation! Classes perfectly discriminated!
```

---

### **STEP 8: Classification Decision Boundary**

**How to classify a new flower?**

**Decision rule:** Project onto LDâ‚, compare to threshold

**Threshold (midpoint between class means):**
```
threshold = (Î¼â‚ + Î¼â‚‚) / 2
          = (1.419 + 4.711) / 2
          = 3.065

Decision rule:
If z < 3.065 â†’ Setosa
If z â‰¥ 3.065 â†’ Versicolor
```

**Example: New flower with [3.0, 0.8]**

```
z = 0.995Ã—3.0 + 0.095Ã—0.8
  = 2.985 + 0.076
  = 3.061

Compare: 3.061 < 3.065
Prediction: Setosa (just barely!)
```

**Visual decision boundary in original space:**

```
    xâ‚‚
     â†‘
  1.5â”‚         â—â—â—
  1.0â”‚   â•± Decision boundary
  0.5â”‚  â•± (perpendicular to LDâ‚)
  0.2â”‚â—‹â—‹â—‹
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚

The boundary is a line perpendicular to w
passing through point [3.065/0.995, 0] â‰ˆ [3.08, 0]
```

---

## 3. Formula Legend

### Data Components
| Symbol | Name | Meaning |
|--------|------|---------|
| **X** | Data matrix | n samples Ã— d features |
| **X_i** | Class i data | Samples belonging to class i |
| **n** | Total samples | Total number of data points |
| **n_i** | Class i samples | Number of samples in class i |
| **c** | Number of classes | How many categories (usually 2) |
| **d** | Dimensions | Number of features |

### Mean Vectors
| Symbol | Name | Meaning |
|--------|------|---------|
| **Î¼** | Overall mean | Mean of all samples (dÃ—1 vector) |
| **Î¼_i** | Class i mean | Mean of class i samples (dÃ—1 vector) |

### Scatter Matrices
| Symbol | Name | Meaning |
|--------|------|---------|
| **S_W** | Within-class scatter | Variance within each class (dÃ—d matrix) |
| **S_B** | Between-class scatter | Variance between class means (dÃ—d matrix) |
| **S_i** | Scatter for class i | Contribution of class i to S_W |

### LDA Components
| Symbol | Name | Meaning |
|--------|------|---------|
| **w** | Projection vector | Direction of linear discriminant (dÃ—1) |
| **Î»** | Eigenvalue | Fisher criterion value (separation score) |
| **z** | Projected value | Sample projected onto discriminant (scalar) |
| **LD** | Linear Discriminant | The projection direction found by LDA |
| **J(w)** | Fisher criterion | Objective function to maximize |

### Classification
| Symbol | Name | Meaning |
|--------|------|---------|
| **Ï„** (tau) | Threshold | Decision boundary in projected space |
| **y** | Class label | Predicted or true class (0, 1, 2, ...) |

---

## 4. The Formulas

### **Step 1: Compute Class Means**

**Overall mean:**
$$\mu = \frac{1}{n}\sum_{i=1}^{n} x_i$$

**Class k mean:**
$$\mu_k = \frac{1}{n_k}\sum_{x_i \in C_k} x_i$$

---

### **Step 2: Within-Class Scatter Matrix**

**For each class:**
$$S_k = \sum_{x_i \in C_k}(x_i - \mu_k)(x_i - \mu_k)^T$$

**Total within-class scatter:**
$$S_W = \sum_{k=1}^{c} S_k = \sum_{k=1}^{c}\sum_{x_i \in C_k}(x_i - \mu_k)(x_i - \mu_k)^T$$

**In words:** Sum of covariance matrices for each class

---

### **Step 3: Between-Class Scatter Matrix**

$$S_B = \sum_{k=1}^{c} n_k(\mu_k - \mu)(\mu_k - \mu)^T$$

**For 2 classes (binary classification):**
$$S_B = n_1(\mu_1 - \mu)(\mu_1 - \mu)^T + n_2(\mu_2 - \mu)(\mu_2 - \mu)^T$$

**Alternative formulation:**
$$S_B = (\mu_2 - \mu_1)(\mu_2 - \mu_1)^T \quad \text{(for 2 classes)}$$

---

### **Step 4: Fisher's Linear Discriminant**

**Objective:** Maximize the ratio:
$$J(w) = \frac{w^T S_B w}{w^T S_W w}$$

**Numerator:** Between-class variance in projected space
**Denominator:** Within-class variance in projected space

**Solution:** Solve the generalized eigenvalue problem:
$$S_W^{-1}S_B w = \lambda w$$

Or equivalently:
$$S_B w = \lambda S_W w$$

---

### **Step 5: Projection**

**Project sample x onto discriminant w:**
$$z = w^T x = \sum_{j=1}^{d} w_j x_j$$

**For k discriminants (matrix form):**
$$Z = XW$$

Where W is (d Ã— k) matrix of eigenvectors

---

### **Step 6: Classification**

**Two-class case:**

Project to 1D: $z = w^T x$

**Threshold (at midpoint):**
$$\tau = \frac{w^T\mu_1 + w^T\mu_2}{2}$$

**Decision rule:**
$$\hat{y} = \begin{cases}
\text{Class 1} & \text{if } z < \tau \\
\text{Class 2} & \text{if } z \geq \tau
\end{cases}$$

**Multi-class case:**

Use nearest class mean in projected space, or train classifiers on projected features.

---

### **Step 7: Number of Discriminants**

Maximum number of meaningful discriminants:
$$k = \min(c-1, d)$$

Where:
- c = number of classes
- d = number of features

**Examples:**
- 2 classes, 10 features â†’ 1 discriminant (c-1 = 1)
- 10 classes, 2 features â†’ 2 discriminants (d = 2)
- 5 classes, 100 features â†’ 4 discriminants (c-1 = 4)

---

## 5. Complete LDA Algorithm

```
INPUT: 
  X: data matrix (n Ã— d)
  y: class labels (n Ã— 1)
  
OUTPUT:
  W: projection matrix (d Ã— k)
  
STEPS:

1. COMPUTE MEANS:
   For each class k:
     Î¼_k = mean of samples in class k
   Î¼ = overall mean

2. COMPUTE WITHIN-CLASS SCATTER:
   S_W = 0
   For each class k:
     For each sample x in class k:
       S_W += (x - Î¼_k)(x - Î¼_k)^T

3. COMPUTE BETWEEN-CLASS SCATTER:
   S_B = 0
   For each class k:
     S_B += n_k Ã— (Î¼_k - Î¼)(Î¼_k - Î¼)^T

4. SOLVE EIGENVALUE PROBLEM:
   Compute S_W^(-1) Ã— S_B
   Find eigenvalues Î» and eigenvectors w
   Sort by eigenvalue: Î»â‚ â‰¥ Î»â‚‚ â‰¥ ... â‰¥ Î»_d

5. SELECT TOP k EIGENVECTORS:
   W = [wâ‚ | wâ‚‚ | ... | w_k]
   Where k = min(c-1, desired_dimensions)

6. TRANSFORM DATA:
   Z = X Ã— W

7. CLASSIFY (for new point x_new):
   z_new = W^T Ã— x_new
   Assign to nearest class mean in Z-space
```

---

## 6. LDA vs PCA: Side-by-Side Comparison

### **Same Dataset, Different Results:**

```
Original Data:
    xâ‚‚
     â†‘
  1.5â”‚         â—â—â—  (Versicolor)
  1.0â”‚
  0.5â”‚
  0.2â”‚â—‹â—‹â—‹          (Setosa)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚
```

---

### **PCA Result:**

```
PCA finds direction of maximum variance:

    xâ‚‚
     â†‘
  1.5â”‚         â—â—â—
  1.0â”‚    PC1 â•±
  0.5â”‚      â•±  (diagonal direction)
  0.2â”‚â—‹â—‹â—‹ â•±
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚

PC1 â‰ˆ [0.707, 0.707] (45Â° diagonal)

After projection:
â—‹â—‹â—‹â”€â”€â”€â”€â—â—â—â”€â”€â”€â”€â”€â”€â”€â”€â†’ PC1
   â†‘    â†‘
Setosa overlap! (some separation, but not optimal)

PCA doesn't use class labels,
so it doesn't optimize for separation!
```

---

### **LDA Result:**

```
LDA finds direction of maximum class separation:

    xâ‚‚
     â†‘
  1.5â”‚         â—â—â—
  1.0â”‚   LD1 â”€â”€
  0.5â”‚      â”€â”€  (nearly horizontal)
  0.2â”‚â—‹â—‹â—‹ â”€â”€
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚

LD1 â‰ˆ [0.995, 0.095] (almost horizontal)

After projection:
â—‹â—‹â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â—â—â†’ LD1
    â†‘              â†‘
 Setosa      Versicolor
 
Perfect separation!

LDA uses class labels,
so it finds optimal separation direction!
```

---

### **Comparison Table:**

| Aspect | PCA | LDA |
|--------|-----|-----|
| **Supervision** | Unsupervised | Supervised |
| **Objective** | Max total variance | Max class separation |
| **Uses labels?** | No | Yes |
| **Output** | Principal Components | Linear Discriminants |
| **Max components** | min(n, d) | min(c-1, d) |
| **For 2 classes** | Can give d components | Gives 1 discriminant |
| **Best for** | Visualization, compression | Classification |
| **Scatter matrices** | Total scatter (S_T) | S_W and S_B |
| **Solves** | Sv = Î»v | S_W^(-1)S_B w = Î»w |

---

### **When to Use Which:**

**Use PCA when:**
- âœ“ No class labels available
- âœ“ Goal is dimensionality reduction
- âœ“ Want to visualize data structure
- âœ“ Need to compress data
- âœ“ Reducing noise

**Use LDA when:**
- âœ“ Have class labels
- âœ“ Goal is classification
- âœ“ Want to maximize class separation
- âœ“ Need discriminative features
- âœ“ Building a classifier

**Use Both (PCA then LDA):**
- Very high dimensions (d >> n)
- Computational constraints
- First PCA to reduce to reasonable size
- Then LDA for classification

---

## 7. Practical Example: Multi-Class LDA

### **Scenario: Iris 3-Species Classification**

```
Dataset: 3 species (c=3)
Features: 2 (petal length, width)

Setosa:      â—‹â—‹â—‹  (small petals)
Versicolor:  â—â—â—  (medium petals)
Virginica:   Ã—Ã—Ã—  (large petals)

With c=3 classes, we get k = c-1 = 2 discriminants!
```

**Visualization:**

```
Original 2D space:        After LDA (2D â†’ 2D):

    xâ‚‚                         LD2 â†‘
     â†‘                            â”‚  Ã—Ã—Ã—
   2 â”‚     Ã—Ã—Ã—                    â”‚  (Virginica)
   1 â”‚   â—â—â—                      â”‚
   0 â”‚ â—‹â—‹â—‹                   â—â—â—â”€â”€â”¼â”€â”€â†’ LD1
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚         (Versicolor) â”‚
                                      â—‹â—‹â—‹
                                   (Setosa)

LD1: Separates Setosa from others
LD2: Separates Versicolor from Virginica

Both discriminants needed for 3-class separation!
```

**Algorithm same as before, but:**
- Compute S_W summing over 3 classes
- Compute S_B summing over 3 classes
- Get 2 eigenvectors (top 2 by eigenvalue)
- Project onto 2D LDA space

---

## 8. Mathematical Insights

### **Why Does LDA Work?**

**The Fisher Criterion intuition:**

$$J(w) = \frac{\text{between-class variance}}{\text{within-class variance}}$$

```
GOOD projection (high J):

Class 1: â—â—â—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â—â—â— Class 2
         â†‘                        â†‘
      tight cluster          tight cluster
         
         â†â”€â”€â”€â”€ large gap â”€â”€â”€â”€â†’
         
Between-class: LARGE (means far apart)
Within-class: SMALL (tight clusters)
J = LARGE / SMALL = HUGE! âœ“


BAD projection (low J):

Class 1: â—â”â—â”â”â”â—â”â—â”â”â—â”â—â”â”â—â—â”â”â—â— Class 2
         
         Overlapping mess!
         
Between-class: SMALL (means close)
Within-class: LARGE (spread out)
J = SMALL / LARGE = tiny âœ—
```

---

### **Why S_W^(-1) S_B?**

**Without going through full derivation:**

1. We want to maximize: $J(w) = \frac{w^T S_B w}{w^T S_W w}$

2. Take derivative and set to zero (calculus of variations)

3. This gives: $S_B w = \lambda S_W w$

4. Multiply both sides by $S_W^{-1}$: $S_W^{-1} S_B w = \lambda w$

5. This is a standard eigenvalue problem!

6. Eigenvectors = optimal projection directions
7. Eigenvalues = separation quality (larger = better)

---

### **Connection to Mahalanobis Distance**

LDA is related to Mahalanobis distance:

**Euclidean distance:** $d = ||x - y||$
**Mahalanobis distance:** $d = \sqrt{(x-y)^T S^{-1} (x-y)}$

LDA finds directions where Mahalanobis distance between classes is maximized!

---

## 9. Limitations and Assumptions

### **Assumptions LDA Makes:**

1. **Normal distributions:** Each class is normally distributed
2. **Equal covariance:** All classes have same covariance matrix (S_W)
3. **Linear boundaries:** Classes separated by linear decision boundary

**When assumptions violated:**

```
ASSUMPTION OK:              ASSUMPTION VIOLATED:

  â—â—â—                        â—â—â—â—â—â—
  â—â—â—  â—‹â—‹â—‹                  â—â—â—â—â—â—‹â—‹â—‹â—
  â—â—â—  â—‹â—‹â—‹                  â—â—â—‹â—‹â—‹â—‹â—‹â—â—
       â—‹â—‹â—‹                  â—â—â—â—â—â—â—â—

Linear boundary works      Curved boundary needed
â†’ Use LDA âœ“               â†’ Use QDA or kernel methods
```

---

### **Limitations:**

**1. Small Sample Size Problem:**
```
If n < d (samples < features):
  S_W is singular (can't invert!)
  
Solution: Use PCA first to reduce dimensions
```

**2. Outlier Sensitivity:**
```
LDA uses means and covariances
â†’ Sensitive to outliers

    â—â—â—
    â—â—â—  â—‹â—‹â—‹â—‹
    â—â—â—  â—‹
         â—‹  â— (outlier!)
         
Outlier pulls mean and increases S_W
â†’ Worse separation

Solution: Remove outliers first, or use robust methods
```

**3. Non-linear Separability:**
```
    â—â—â—
  â—‹â—‹â—â—â—â—‹â—‹
    â—â—â—
    
No linear line separates these!
LDA will fail.

Solution: Use Kernel LDA, or non-linear classifiers
```

---

## 10. Variations of LDA

### **Quadratic Discriminant Analysis (QDA)**

**When:** Classes have different covariances

```
LDA (shared covariance):    QDA (separate covariances):

  â—â—â—                          â—â—â—
  â—â—â—  â—‹â—‹â—‹                    â—â—â—  â—‹
  â—â—â—  â—‹â—‹â—‹                    â—â—â—  â—‹â—‹
       â—‹â—‹â—‹                         â—‹â—‹â—‹

Linear boundary             Curved boundary
Assumes same spread         Allows different spreads
```

**Formula:** Each class k gets its own covariance matrix S_k

Decision: Assign to class with highest discriminant score:
$$\delta_k(x) = -\frac{1}{2}\log|S_k| - \frac{1}{2}(x-\mu_k)^T S_k^{-1}(x-\mu_k) + \log P(C_k)$$

---

### **Regularized LDA**

**When:** Small sample size, or ill-conditioned S_W

**Approach:** Add small value to diagonal:
$$S_W^{reg} = S_W + \lambda I$$

Where Î» is regularization parameter (e.g., 0.01)

**Effect:** Makes S_W invertible, reduces overfitting

---

### **Kernel LDA**

**When:** Classes not linearly separable

**Idea:** Map to higher dimension where they ARE linearly separable

```
Original space:              Kernel space:
  â—â—â—                         â—â—â—â—â—
â—‹â—‹â—â—â—â—‹â—‹                      â—â—â—â—â—
  â—â—â—                   â—‹â—‹â—‹â—‹       â—‹â—‹â—‹â—‹

Can't separate             Can separate with
with line                  hyperplane!
```

**Uses kernel trick:** $K(x, y) = \phi(x)^T\phi(y)$

Common kernels: RBF, polynomial

---

## 11. Complete Worked Example Summary

### **Our Iris Example:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LDA: START TO FINISH                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT:
6 flowers, 2 features, 2 classes
Setosa: [1.4,0.2], [1.3,0.2], [1.5,0.4]
Versicolor: [4.7,1.4], [4.5,1.5], [4.6,1.3]

STEP 1: Compute means
Î¼â‚ = [1.4, 0.267]    (Setosa mean)
Î¼â‚‚ = [4.6, 1.4]      (Versicolor mean)

STEP 2: Compute S_W (within-class scatter)
S_W = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ 0.040 0.010â”‚
      â”‚ 0.010 0.047â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      
STEP 3: Compute S_B (between-class scatter)
S_B = â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ 15.36  5.44 â”‚
      â”‚  5.44  1.93 â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 4: Solve S_W^(-1) S_B w = Î»w
Î»â‚ = 387.813 (huge eigenvalue!)
w = [0.995, 0.095] (nearly horizontal)

STEP 5: Project data
Setosa â†’ [1.31, 1.41, 1.53]
Versicolor â†’ [4.62, 4.70, 4.81]

RESULT:
Perfect separation in 1D!
Fisher criterion J = 518.2 (excellent!)
Classification threshold: 3.065
```

---

## 12. Key Takeaways

### **What LDA Does:**

1. **Finds optimal separation** between classes
2. **Supervised method** (uses labels)
3. **Linear projection** to lower dimensions
4. **Maximizes Fisher criterion** (between/within variance ratio)
5. **Gives at most c-1 discriminants** for c classes

### **The Three Core Matrices:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ S_W: Within-class scatter            â”‚
â”‚      Measures: Spread within classes â”‚
â”‚      Goal: MINIMIZE this             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ S_B: Between-class scatter           â”‚
â”‚      Measures: Separation of means   â”‚
â”‚      Goal: MAXIMIZE this             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ S_W^(-1) S_B: Combined objective     â”‚
â”‚      Eigenvectors: Discriminants     â”‚
â”‚      Eigenvalues: Separation quality â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **When to Use LDA:**

âœ… **Use LDA when:**
- You have labeled data
- Classes are (roughly) normally distributed
- Classes have similar covariances
- You need interpretable linear boundaries
- You want to reduce dimensions for classification

âŒ **Don't use LDA when:**
- No labels available â†’ use PCA
- Non-linear boundaries â†’ use kernel methods
- Very different class spreads â†’ use QDA
- Fewer samples than features (n < d) â†’ regularize or PCA first
