# Linear Discriminant Analysis (LDA): Complete Explanation
## (Detailed Step-by-Step with Number Tracing)

---

## 🔗 **Connection to PCA**

### **What We Learned from PCA:**

```
PCA:
- Found directions of maximum variance
- Unsupervised (ignored labels)
- Goal: Compress data, reduce dimensions

Example from PCA:
Students: [study hours, sleep hours]
PC1: Direction where data spreads most
NO consideration of any categories!
```

### **The Problem with PCA for Classification:**

```
Dataset with 2 classes:

    x₂                         PCA says:
     ↑                         "Project here!" ↓
   5 │ ○ ○ ○                        PC1
   4 │ ○ ○ ○                    ─────────→
   3 │                         (max variance)
   2 │   ● ● ●
   1 │   ● ● ●
     └───────────→ x₁
     
After PCA projection:
─○○○●●●────────→ PC1

Classes overlap! Bad for classification!
PCA found variance, but ignored class labels.
```

**What we really want:**

```
    x₂                         LDA says:
     ↑                         "Project here!" ↓
   5 │ ○ ○ ○                        LD1
   4 │ ○ ○ ○                          ↑
   3 │                                │
   2 │   ● ● ●                        │
   1 │   ● ● ●
     └───────────→ x₁
     
After LDA projection:
         ○○○
         ↑
    (Class 1)
    
    ───────────
    
         ●●●
         ↓
    (Class 2)

Classes separated! Great for classification!
LDA maximizes separation between classes.
```

---

## **The Core Difference:**

| Aspect | PCA | LDA |
|--------|-----|-----|
| **Type** | Unsupervised | Supervised |
| **Uses labels?** | No | Yes |
| **Maximizes** | Total variance | Class separation |
| **Goal** | Dimensionality reduction | Classification |
| **Output** | Principal Components | Linear Discriminants |
| **Best for** | Data visualization, compression | Classification tasks |

---

# Part 1: What is LDA?

## 1. Plain English Explanation

### **The Intuition**

Imagine you're a doctor trying to diagnose patients:
- You measure: height, weight, blood pressure, age
- You have two groups: Healthy and Sick

**PCA would say:** "Let's find the direction where people vary most (probably height)"
**LDA would say:** "Let's find the direction that best separates Healthy from Sick!"

### **LDA's Two Goals (Must Balance Both):**

1. **Maximize between-class variance:** Push class means far apart
2. **Minimize within-class variance:** Keep each class tightly clustered

```
BAD projection (like PCA might give):
Class 1: ○ ○   ○ ○     ○
Class 2:   ● ●   ● ● ●
         ─────────────────
         Overlapping mess!

GOOD projection (what LDA finds):
Class 1: ○○○○○
              
              ───────────
              
Class 2:           ●●●●●
         ─────────────────
         Clean separation!
```

### **The Fisher Criterion**

LDA uses "Fisher's Linear Discriminant" which maximizes:

$$J(w) = \frac{\text{between-class variance}}{\text{within-class variance}}$$

**In words:** "Find the direction where classes are far apart AND each class is tight."

---

## 2. Step-by-Step Real World Walkthrough

Let's work through a **complete numerical example** with every calculation!

### **Scenario: Flower Species Classification**

We have measurements of two iris species:
- **Setosa** (Class 1): Small flowers
- **Versicolor** (Class 2): Large flowers

**Features:**
- x₁: Petal length (cm)
- x₂: Petal width (cm)

**The Data (6 flowers total):**

| Flower | Species | Petal Length (x₁) | Petal Width (x₂) |
|--------|---------|-------------------|------------------|
| 1 | Setosa | 1.4 | 0.2 |
| 2 | Setosa | 1.3 | 0.2 |
| 3 | Setosa | 1.5 | 0.4 |
| 4 | Versicolor | 4.7 | 1.4 |
| 5 | Versicolor | 4.5 | 1.5 |
| 6 | Versicolor | 4.6 | 1.3 |

```
Dataset matrix:
        x₁   x₂   Class
      ┌──────────────┐
    1 │ 1.4  0.2 │ S │
    2 │ 1.3  0.2 │ S │
    3 │ 1.5  0.4 │ S │
    4 │ 4.7  1.4 │ V │
    5 │ 4.5  1.5 │ V │
    6 │ 4.6  1.3 │ V │
      └──────────────┘

S = Setosa (Class 1)
V = Versicolor (Class 2)
```

**Visual representation:**

```
    x₂ (width)
     ↑
  1.5│         ●        Class 2: Versicolor
  1.4│      ●    ●      (Large petals)
  1.3│
  1.2│
     │
  0.4│  ○
  0.2│○   ○              Class 1: Setosa
  0.0│                   (Small petals)
     └──────────────────→ x₁ (length)
      0  1  2  3  4  5

Clear visual separation!
Let's find the best projection mathematically.
```

---

### **STEP 1: Separate Data by Class**

**Class 1 (Setosa): n₁ = 3 samples**
```
X₁ = ┌──────────┐
     │ 1.4  0.2 │
     │ 1.3  0.2 │
     │ 1.5  0.4 │
     └──────────┘
```

**Class 2 (Versicolor): n₂ = 3 samples**
```
X₂ = ┌──────────┐
     │ 4.7  1.4 │
     │ 4.5  1.5 │
     │ 4.6  1.3 │
     └──────────┘
```

---

### **STEP 2: Calculate Class Means (Centers)**

**Mean of Class 1: μ₁**

```
μ₁[x₁] = (1.4 + 1.3 + 1.5) / 3 = 4.2 / 3 = 1.4
μ₁[x₂] = (0.2 + 0.2 + 0.4) / 3 = 0.8 / 3 = 0.267

μ₁ = [1.4, 0.267]
```

**Mean of Class 2: μ₂**

```
μ₂[x₁] = (4.7 + 4.5 + 4.6) / 3 = 13.8 / 3 = 4.6
μ₂[x₂] = (1.4 + 1.5 + 1.3) / 3 = 4.2 / 3 = 1.4

μ₂ = [4.6, 1.4]
```

**Overall mean: μ (all 6 samples)**

```
μ[x₁] = (1.4 + 1.3 + 1.5 + 4.7 + 4.5 + 4.6) / 6 = 18.0 / 6 = 3.0
μ[x₂] = (0.2 + 0.2 + 0.4 + 1.4 + 1.5 + 1.3) / 6 = 5.0 / 6 = 0.833

μ = [3.0, 0.833]
```

**Visualize means:**

```
    x₂
     ↑
  1.5│         ●
  1.4│      ●  ★₂ ●     ★₂ = μ₂ [4.6, 1.4] (Versicolor mean)
  1.3│
     │
  0.4│  ○
  0.2│○ ★₁○             ★₁ = μ₁ [1.4, 0.267] (Setosa mean)
     └──────────────────→ x₁
      0  1  2  3  4  5
```

---

### **STEP 3: Calculate Within-Class Scatter Matrix (S_W)**

**What is this?** Measures how spread out each class is around its own mean.

**Formula:**
$$S_W = S_1 + S_2$$

Where for each class:
$$S_i = \sum_{x \in \text{Class } i} (x - \mu_i)(x - \mu_i)^T$$

---

#### **For Class 1 (Setosa):**

**Sample 1: [1.4, 0.2] - μ₁ = [1.4, 0.267]**

```
x - μ₁ = [1.4, 0.2] - [1.4, 0.267]
       = [0.0, -0.067]

Outer product: (x - μ₁)(x - μ₁)^T
┌──────┐   ┌──────────┐   ┌────────────────┐
│  0.0 │ × │ 0.0 -0.067│ = │  0.000  -0.000 │
│-0.067│   └──────────┘   │ -0.000   0.004 │
└──────┘                  └────────────────┘
```

**Sample 2: [1.3, 0.2] - μ₁ = [1.4, 0.267]**

```
x - μ₁ = [1.3, 0.2] - [1.4, 0.267]
       = [-0.1, -0.067]

Outer product:
┌───────┐   ┌──────────────┐   ┌────────────────┐
│ -0.1  │ × │ -0.1  -0.067 │ = │  0.010   0.007 │
│-0.067 │   └──────────────┘   │  0.007   0.004 │
└───────┘                      └────────────────┘
```

**Sample 3: [1.5, 0.4] - μ₁ = [1.4, 0.267]**

```
x - μ₁ = [1.5, 0.4] - [1.4, 0.267]
       = [0.1, 0.133]

Outer product:
┌──────┐   ┌─────────────┐   ┌────────────────┐
│ 0.1  │ × │ 0.1   0.133 │ = │  0.010   0.013 │
│0.133 │   └─────────────┘   │  0.013   0.018 │
└──────┘                     └────────────────┘
```

**Sum for Class 1:**

```
S₁ = ┌────────────────┐   ┌────────────────┐   ┌────────────────┐
     │  0.000  -0.000 │   │  0.010   0.007 │   │  0.010   0.013 │
     │ -0.000   0.004 │ + │  0.007   0.004 │ + │  0.013   0.018 │
     └────────────────┘   └────────────────┘   └────────────────┘

S₁ = ┌────────────────┐
     │  0.020   0.020 │
     │  0.020   0.027 │
     └────────────────┘
```

---

#### **For Class 2 (Versicolor):**

**Sample 4: [4.7, 1.4] - μ₂ = [4.6, 1.4]**

```
x - μ₂ = [4.7, 1.4] - [4.6, 1.4]
       = [0.1, 0.0]

Outer product:
┌─────┐   ┌─────────┐   ┌────────────────┐
│ 0.1 │ × │ 0.1  0.0│ = │  0.010   0.000 │
│ 0.0 │   └─────────┘   │  0.000   0.000 │
└─────┘                 └────────────────┘
```

**Sample 5: [4.5, 1.5] - μ₂ = [4.6, 1.4]**

```
x - μ₂ = [4.5, 1.5] - [4.6, 1.4]
       = [-0.1, 0.1]

Outer product:
┌──────┐   ┌──────────┐   ┌────────────────┐
│ -0.1 │ × │ -0.1  0.1│ = │  0.010  -0.010 │
│  0.1 │   └──────────┘   │ -0.010   0.010 │
└──────┘                  └────────────────┘
```

**Sample 6: [4.6, 1.3] - μ₂ = [4.6, 1.4]**

```
x - μ₂ = [4.6, 1.3] - [4.6, 1.4]
       = [0.0, -0.1]

Outer product:
┌──────┐   ┌──────────┐   ┌────────────────┐
│  0.0 │ × │ 0.0 -0.1 │ = │  0.000   0.000 │
│ -0.1 │   └──────────┘   │  0.000   0.010 │
└──────┘                  └────────────────┘
```

**Sum for Class 2:**

```
S₂ = ┌────────────────┐   ┌────────────────┐   ┌────────────────┐
     │  0.010   0.000 │   │  0.010  -0.010 │   │  0.000   0.000 │
     │  0.000   0.000 │ + │ -0.010   0.010 │ + │  0.000   0.010 │
     └────────────────┘   └────────────────┘   └────────────────┘

S₂ = ┌────────────────┐
     │  0.020  -0.010 │
     │ -0.010   0.020 │
     └────────────────┘
```

---

#### **Total Within-Class Scatter:**

```
S_W = S₁ + S₂

S_W = ┌────────────────┐   ┌────────────────┐
      │  0.020   0.020 │   │  0.020  -0.010 │
      │  0.020   0.027 │ + │ -0.010   0.020 │
      └────────────────┘   └────────────────┘

S_W = ┌────────────────┐
      │  0.040   0.010 │
      │  0.010   0.047 │
      └────────────────┘
```

**What does S_W tell us?**
- Diagonal elements: variance within each class for each feature
- Off-diagonal: covariance within classes
- We want to minimize this (tight clusters)

---

### **STEP 4: Calculate Between-Class Scatter Matrix (S_B)**

**What is this?** Measures how far apart the class means are.

**Formula:**
$$S_B = \sum_{i=1}^{c} n_i (\mu_i - \mu)(\mu_i - \mu)^T$$

Where:
- c = number of classes (2)
- n_i = number of samples in class i
- μ_i = mean of class i
- μ = overall mean

---

#### **For Class 1:**

```
μ₁ - μ = [1.4, 0.267] - [3.0, 0.833]
       = [-1.6, -0.567]

Outer product:
┌────────┐   ┌──────────────────┐   ┌────────────────────┐
│ -1.6   │ × │ -1.6    -0.567   │ = │  2.560    0.907   │
│ -0.567 │   └──────────────────┘   │  0.907    0.321   │
└────────┘                          └────────────────────┘

Multiply by n₁ = 3:
3 × ┌────────────────────┐   ┌────────────────────┐
    │  2.560    0.907   │ = │  7.680    2.720   │
    │  0.907    0.321   │   │  2.720    0.963   │
    └────────────────────┘   └────────────────────┘
```

#### **For Class 2:**

```
μ₂ - μ = [4.6, 1.4] - [3.0, 0.833]
       = [1.6, 0.567]

Outer product:
┌───────┐   ┌─────────────────┐   ┌────────────────────┐
│  1.6  │ × │  1.6    0.567   │ = │  2.560    0.907   │
│ 0.567 │   └─────────────────┘   │  0.907    0.321   │
└───────┘                         └────────────────────┘

Multiply by n₂ = 3:
3 × ┌────────────────────┐   ┌────────────────────┐
    │  2.560    0.907   │ = │  7.680    2.720   │
    │  0.907    0.321   │   │  2.720    0.963   │
    └────────────────────┘   └────────────────────┘
```

#### **Total Between-Class Scatter:**

```
S_B = ┌────────────────────┐   ┌────────────────────┐
      │  7.680    2.720   │   │  7.680    2.720   │
      │  2.720    0.963   │ + │  2.720    0.963   │
      └────────────────────┘   └────────────────────┘

S_B = ┌─────────────────────┐
      │  15.360    5.440   │
      │   5.440    1.927   │
      └─────────────────────┘
```

**What does S_B tell us?**
- Measures separation between class means
- Larger values = classes farther apart
- We want to maximize this (separate clusters)

---

### **STEP 5: Solve the Generalized Eigenvalue Problem**

**The LDA objective:** Find direction w that maximizes:

$$J(w) = \frac{w^T S_B w}{w^T S_W w}$$

**This is equivalent to solving:**
$$S_W^{-1} S_B w = \lambda w$$

We need to:
1. Compute S_W^(-1) (inverse of within-class scatter)
2. Compute S_W^(-1) × S_B
3. Find eigenvalues and eigenvectors

---

#### **Step 5a: Invert S_W**

**S_W:**
```
S_W = ┌────────────────┐
      │  0.040   0.010 │
      │  0.010   0.047 │
      └────────────────┘
```

**Formula for 2×2 matrix inverse:**
$$\begin{bmatrix} a & b \\ c & d \end{bmatrix}^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$$

**Calculate determinant:**
```
det(S_W) = (0.040)(0.047) - (0.010)(0.010)
         = 0.00188 - 0.0001
         = 0.00178
```

**Calculate inverse:**
```
S_W^(-1) = (1/0.00178) × ┌────────────────┐
                         │  0.047  -0.010 │
                         │ -0.010   0.040 │
                         └────────────────┘

S_W^(-1) = ┌──────────────────┐
           │  26.404  -5.618  │
           │  -5.618  22.472  │
           └──────────────────┘
```

---

#### **Step 5b: Compute S_W^(-1) × S_B**

```
S_W^(-1) × S_B = ┌──────────────────┐   ┌─────────────────────┐
                 │  26.404  -5.618  │ × │  15.360    5.440   │
                 │  -5.618  22.472  │   │   5.440    1.927   │
                 └──────────────────┘   └─────────────────────┘
```

**Element [0,0]:**
```
(26.404)(15.360) + (-5.618)(5.440)
= 405.645 - 30.562
= 375.083
```

**Element [0,1]:**
```
(26.404)(5.440) + (-5.618)(1.927)
= 143.638 - 10.825
= 132.813
```

**Element [1,0]:**
```
(-5.618)(15.360) + (22.472)(5.440)
= -86.293 + 122.248
= 35.955
```

**Element [1,1]:**
```
(-5.618)(5.440) + (22.472)(1.927)
= -30.562 + 43.303
= 12.741
```

**Result:**
```
S_W^(-1) S_B = ┌─────────────────────┐
               │  375.083   132.813  │
               │   35.955    12.741  │
               └─────────────────────┘
```

---

#### **Step 5c: Find Eigenvalues**

**Characteristic equation:** det(S_W^(-1) S_B - λI) = 0

```
│375.083-λ   132.813  │
│  35.955    12.741-λ │ = 0

(375.083-λ)(12.741-λ) - (132.813)(35.955) = 0
4,779.182 - 375.083λ - 12.741λ + λ² - 4,774.601 = 0
λ² - 387.824λ + 4.581 = 0
```

**Using quadratic formula:**
```
λ = [387.824 ± √(387.824² - 4×4.581)] / 2
  = [387.824 ± √(150,407.052 - 18.324)] / 2
  = [387.824 ± √150,388.728] / 2
  = [387.824 ± 387.801] / 2

λ₁ = (387.824 + 387.801) / 2 = 387.813
λ₂ = (387.824 - 387.801) / 2 = 0.012
```

**Eigenvalues:**
```
λ₁ = 387.813  (Primary discriminant - captures all separation!)
λ₂ = 0.012    (Negligible - almost zero)
```

**Note:** For 2-class LDA, we get maximum of 1 useful discriminant (c-1 = 2-1 = 1).
λ₂ is essentially zero, confirming this!

---

#### **Step 5d: Find Eigenvector for λ₁**

**Solve:** (S_W^(-1) S_B - λ₁I)w = 0

```
┌──────────────────────────────────┐   ┌───┐   ┌───┐
│  375.083-387.813      132.813    │ × │w₁ │ = │ 0 │
│     35.955         12.741-387.813│   │w₂ │   │ 0 │
└──────────────────────────────────┘   └───┘   └───┘

┌─────────────────┐   ┌───┐   ┌───┐
│ -12.730  132.813│ × │w₁ │ = │ 0 │
│  35.955 -375.072│   │w₂ │   │ 0 │
└─────────────────┘   └───┘   └───┘

From first equation:
-12.730w₁ + 132.813w₂ = 0
12.730w₁ = 132.813w₂
w₁ = (132.813/12.730)w₂
w₁ = 10.433w₂

Let w₂ = 1
Then w₁ = 10.433

Eigenvector (unnormalized):
w = [10.433, 1]
```

**Normalize:**
```
Length = √(10.433² + 1²) = √(108.847 + 1) = √109.847 = 10.481

w_normalized = [10.433/10.481, 1/10.481]
             = [0.995, 0.095]
```

**LDA Direction (Linear Discriminant):**
```
w = [0.995, 0.095]

Interpretation:
LD₁ = 0.995×(petal length) + 0.095×(petal width)

Petal length is ~10× more important than width
for separating Setosa from Versicolor!
```

---

### **STEP 6: Project Data onto Linear Discriminant**

**Formula:** For each sample x:
$$z = w^T x = w_1 x_1 + w_2 x_2$$

---

#### **Class 1 (Setosa):**

**Sample 1: [1.4, 0.2]**
```
z = 0.995×1.4 + 0.095×0.2
  = 1.393 + 0.019
  = 1.412
```

**Sample 2: [1.3, 0.2]**
```
z = 0.995×1.3 + 0.095×0.2
  = 1.294 + 0.019
  = 1.313
```

**Sample 3: [1.5, 0.4]**
```
z = 0.995×1.5 + 0.095×0.4
  = 1.493 + 0.038
  = 1.531
```

**Setosa projections:** [1.412, 1.313, 1.531]

---

#### **Class 2 (Versicolor):**

**Sample 4: [4.7, 1.4]**
```
z = 0.995×4.7 + 0.095×1.4
  = 4.677 + 0.133
  = 4.810
```

**Sample 5: [4.5, 1.5]**
```
z = 0.995×4.5 + 0.095×1.5
  = 4.478 + 0.143
  = 4.621
```

**Sample 6: [4.6, 1.3]**
```
z = 0.995×4.6 + 0.095×1.3
  = 4.577 + 0.124
  = 4.701
```

**Versicolor projections:** [4.810, 4.621, 4.701]

---

### **STEP 7: Visualize the Results**

**Before LDA (Original 2D space):**

```
    x₂ (width)
     ↑
  1.5│         ●5
  1.4│      ●4   ●6
  1.3│
     │
  0.4│  ○3
  0.2│○1  ○2
  0.0│
     └──────────────────→ x₁ (length)
      0  1  2  3  4  5

Classes separated, but using 2 dimensions
```

**After LDA (Projected onto 1D line):**

```
              LDA Direction →
              w = [0.995, 0.095]

Original space:            Projection on LD₁:

    x₂                     Class 1      Class 2
     ↑                     ○○○          ●●●
  1.5│    ●●●              ↓            ↓
  1.0│  LD₁ →          ───1.4──────────4.7─────
  0.5│  ╱                 Setosa     Versicolor
  0.2│○○○
     └────────→ x₁

Perfect separation in 1D!

Setosa:      1.313  1.412  1.531     (mean ≈ 1.42)
                   ↕
           Gap = 3.09 standard deviations!
                   ↕
Versicolor:  4.621  4.701  4.810     (mean ≈ 4.71)
```

**Quantify the separation:**

```
Setosa mean: μ₁ = (1.412 + 1.313 + 1.531) / 3 = 1.419
Versicolor mean: μ₂ = (4.810 + 4.621 + 4.701) / 3 = 4.711

Distance between means: 4.711 - 1.419 = 3.292

Setosa variance: 
  = [(1.412-1.419)² + (1.313-1.419)² + (1.531-1.419)²] / 2
  = [0.000049 + 0.011236 + 0.012544] / 2
  = 0.011914

Versicolor variance:
  = [(4.810-4.711)² + (4.621-4.711)² + (4.701-4.711)²] / 2
  = [0.009801 + 0.008100 + 0.000100] / 2
  = 0.009001

Fisher criterion:
J = (μ₂ - μ₁)² / (σ₁² + σ₂²)
  = (3.292)² / (0.011914 + 0.009001)
  = 10.836 / 0.020915
  = 518.2

Extremely high separation! Classes perfectly discriminated!
```

---

### **STEP 8: Classification Decision Boundary**

**How to classify a new flower?**

**Decision rule:** Project onto LD₁, compare to threshold

**Threshold (midpoint between class means):**
```
threshold = (μ₁ + μ₂) / 2
          = (1.419 + 4.711) / 2
          = 3.065

Decision rule:
If z < 3.065 → Setosa
If z ≥ 3.065 → Versicolor
```

**Example: New flower with [3.0, 0.8]**

```
z = 0.995×3.0 + 0.095×0.8
  = 2.985 + 0.076
  = 3.061

Compare: 3.061 < 3.065
Prediction: Setosa (just barely!)
```

**Visual decision boundary in original space:**

```
    x₂
     ↑
  1.5│         ●●●
  1.0│   ╱ Decision boundary
  0.5│  ╱ (perpendicular to LD₁)
  0.2│○○○
     └──────────────→ x₁

The boundary is a line perpendicular to w
passing through point [3.065/0.995, 0] ≈ [3.08, 0]
```

---

## 3. Formula Legend

### Data Components
| Symbol | Name | Meaning |
|--------|------|---------|
| **X** | Data matrix | n samples × d features |
| **X_i** | Class i data | Samples belonging to class i |
| **n** | Total samples | Total number of data points |
| **n_i** | Class i samples | Number of samples in class i |
| **c** | Number of classes | How many categories (usually 2) |
| **d** | Dimensions | Number of features |

### Mean Vectors
| Symbol | Name | Meaning |
|--------|------|---------|
| **μ** | Overall mean | Mean of all samples (d×1 vector) |
| **μ_i** | Class i mean | Mean of class i samples (d×1 vector) |

### Scatter Matrices
| Symbol | Name | Meaning |
|--------|------|---------|
| **S_W** | Within-class scatter | Variance within each class (d×d matrix) |
| **S_B** | Between-class scatter | Variance between class means (d×d matrix) |
| **S_i** | Scatter for class i | Contribution of class i to S_W |

### LDA Components
| Symbol | Name | Meaning |
|--------|------|---------|
| **w** | Projection vector | Direction of linear discriminant (d×1) |
| **λ** | Eigenvalue | Fisher criterion value (separation score) |
| **z** | Projected value | Sample projected onto discriminant (scalar) |
| **LD** | Linear Discriminant | The projection direction found by LDA |
| **J(w)** | Fisher criterion | Objective function to maximize |

### Classification
| Symbol | Name | Meaning |
|--------|------|---------|
| **τ** (tau) | Threshold | Decision boundary in projected space |
| **y** | Class label | Predicted or true class (0, 1, 2, ...) |

---

## 4. The Formulas

### **Step 1: Compute Class Means**

**Overall mean:**
$$\mu = \frac{1}{n}\sum_{i=1}^{n} x_i$$

**Class k mean:**
$$\mu_k = \frac{1}{n_k}\sum_{x_i \in C_k} x_i$$

---

### **Step 2: Within-Class Scatter Matrix**

**For each class:**
$$S_k = \sum_{x_i \in C_k}(x_i - \mu_k)(x_i - \mu_k)^T$$

**Total within-class scatter:**
$$S_W = \sum_{k=1}^{c} S_k = \sum_{k=1}^{c}\sum_{x_i \in C_k}(x_i - \mu_k)(x_i - \mu_k)^T$$

**In words:** Sum of covariance matrices for each class

---

### **Step 3: Between-Class Scatter Matrix**

$$S_B = \sum_{k=1}^{c} n_k(\mu_k - \mu)(\mu_k - \mu)^T$$

**For 2 classes (binary classification):**
$$S_B = n_1(\mu_1 - \mu)(\mu_1 - \mu)^T + n_2(\mu_2 - \mu)(\mu_2 - \mu)^T$$

**Alternative formulation:**
$$S_B = (\mu_2 - \mu_1)(\mu_2 - \mu_1)^T \quad \text{(for 2 classes)}$$

---

### **Step 4: Fisher's Linear Discriminant**

**Objective:** Maximize the ratio:
$$J(w) = \frac{w^T S_B w}{w^T S_W w}$$

**Numerator:** Between-class variance in projected space
**Denominator:** Within-class variance in projected space

**Solution:** Solve the generalized eigenvalue problem:
$$S_W^{-1}S_B w = \lambda w$$

Or equivalently:
$$S_B w = \lambda S_W w$$

---

### **Step 5: Projection**

**Project sample x onto discriminant w:**
$$z = w^T x = \sum_{j=1}^{d} w_j x_j$$

**For k discriminants (matrix form):**
$$Z = XW$$

Where W is (d × k) matrix of eigenvectors

---

### **Step 6: Classification**

**Two-class case:**

Project to 1D: $z = w^T x$

**Threshold (at midpoint):**
$$\tau = \frac{w^T\mu_1 + w^T\mu_2}{2}$$

**Decision rule:**
$$\hat{y} = \begin{cases}
\text{Class 1} & \text{if } z < \tau \\
\text{Class 2} & \text{if } z \geq \tau
\end{cases}$$

**Multi-class case:**

Use nearest class mean in projected space, or train classifiers on projected features.

---

### **Step 7: Number of Discriminants**

Maximum number of meaningful discriminants:
$$k = \min(c-1, d)$$

Where:
- c = number of classes
- d = number of features

**Examples:**
- 2 classes, 10 features → 1 discriminant (c-1 = 1)
- 10 classes, 2 features → 2 discriminants (d = 2)
- 5 classes, 100 features → 4 discriminants (c-1 = 4)

---

## 5. Complete LDA Algorithm

```
INPUT: 
  X: data matrix (n × d)
  y: class labels (n × 1)
  
OUTPUT:
  W: projection matrix (d × k)
  
STEPS:

1. COMPUTE MEANS:
   For each class k:
     μ_k = mean of samples in class k
   μ = overall mean

2. COMPUTE WITHIN-CLASS SCATTER:
   S_W = 0
   For each class k:
     For each sample x in class k:
       S_W += (x - μ_k)(x - μ_k)^T

3. COMPUTE BETWEEN-CLASS SCATTER:
   S_B = 0
   For each class k:
     S_B += n_k × (μ_k - μ)(μ_k - μ)^T

4. SOLVE EIGENVALUE PROBLEM:
   Compute S_W^(-1) × S_B
   Find eigenvalues λ and eigenvectors w
   Sort by eigenvalue: λ₁ ≥ λ₂ ≥ ... ≥ λ_d

5. SELECT TOP k EIGENVECTORS:
   W = [w₁ | w₂ | ... | w_k]
   Where k = min(c-1, desired_dimensions)

6. TRANSFORM DATA:
   Z = X × W

7. CLASSIFY (for new point x_new):
   z_new = W^T × x_new
   Assign to nearest class mean in Z-space
```

---

## 6. LDA vs PCA: Side-by-Side Comparison

### **Same Dataset, Different Results:**

```
Original Data:
    x₂
     ↑
  1.5│         ●●●  (Versicolor)
  1.0│
  0.5│
  0.2│○○○          (Setosa)
     └──────────────→ x₁
```

---

### **PCA Result:**

```
PCA finds direction of maximum variance:

    x₂
     ↑
  1.5│         ●●●
  1.0│    PC1 ╱
  0.5│      ╱  (diagonal direction)
  0.2│○○○ ╱
     └──────────────→ x₁

PC1 ≈ [0.707, 0.707] (45° diagonal)

After projection:
○○○────●●●────────→ PC1
   ↑    ↑
Setosa overlap! (some separation, but not optimal)

PCA doesn't use class labels,
so it doesn't optimize for separation!
```

---

### **LDA Result:**

```
LDA finds direction of maximum class separation:

    x₂
     ↑
  1.5│         ●●●
  1.0│   LD1 ──
  0.5│      ──  (nearly horizontal)
  0.2│○○○ ──
     └──────────────→ x₁

LD1 ≈ [0.995, 0.095] (almost horizontal)

After projection:
○○○────────────────●●●→ LD1
    ↑              ↑
 Setosa      Versicolor
 
Perfect separation!

LDA uses class labels,
so it finds optimal separation direction!
```

---

### **Comparison Table:**

| Aspect | PCA | LDA |
|--------|-----|-----|
| **Supervision** | Unsupervised | Supervised |
| **Objective** | Max total variance | Max class separation |
| **Uses labels?** | No | Yes |
| **Output** | Principal Components | Linear Discriminants |
| **Max components** | min(n, d) | min(c-1, d) |
| **For 2 classes** | Can give d components | Gives 1 discriminant |
| **Best for** | Visualization, compression | Classification |
| **Scatter matrices** | Total scatter (S_T) | S_W and S_B |
| **Solves** | Sv = λv | S_W^(-1)S_B w = λw |

---

### **When to Use Which:**

**Use PCA when:**
- ✓ No class labels available
- ✓ Goal is dimensionality reduction
- ✓ Want to visualize data structure
- ✓ Need to compress data
- ✓ Reducing noise

**Use LDA when:**
- ✓ Have class labels
- ✓ Goal is classification
- ✓ Want to maximize class separation
- ✓ Need discriminative features
- ✓ Building a classifier

**Use Both (PCA then LDA):**
- Very high dimensions (d >> n)
- Computational constraints
- First PCA to reduce to reasonable size
- Then LDA for classification

---

## 7. Practical Example: Multi-Class LDA

### **Scenario: Iris 3-Species Classification**

```
Dataset: 3 species (c=3)
Features: 2 (petal length, width)

Setosa:      ○○○  (small petals)
Versicolor:  ●●●  (medium petals)
Virginica:   ×××  (large petals)

With c=3 classes, we get k = c-1 = 2 discriminants!
```

**Visualization:**

```
Original 2D space:        After LDA (2D → 2D):

    x₂                         LD2 ↑
     ↑                            │  ×××
   2 │     ×××                    │  (Virginica)
   1 │   ●●●                      │
   0 │ ○○○                   ●●●──┼──→ LD1
     └──────────→ x₁         (Versicolor) │
                                      ○○○
                                   (Setosa)

LD1: Separates Setosa from others
LD2: Separates Versicolor from Virginica

Both discriminants needed for 3-class separation!
```

**Algorithm same as before, but:**
- Compute S_W summing over 3 classes
- Compute S_B summing over 3 classes
- Get 2 eigenvectors (top 2 by eigenvalue)
- Project onto 2D LDA space

---

## 8. Mathematical Insights

### **Why Does LDA Work?**

**The Fisher Criterion intuition:**

$$J(w) = \frac{\text{between-class variance}}{\text{within-class variance}}$$

```
GOOD projection (high J):

Class 1: ●●●━━━━━━━━━━━━━━━━━━━━━●●● Class 2
         ↑                        ↑
      tight cluster          tight cluster
         
         ←──── large gap ────→
         
Between-class: LARGE (means far apart)
Within-class: SMALL (tight clusters)
J = LARGE / SMALL = HUGE! ✓


BAD projection (low J):

Class 1: ●━●━━━●━●━━●━●━━●●━━●● Class 2
         
         Overlapping mess!
         
Between-class: SMALL (means close)
Within-class: LARGE (spread out)
J = SMALL / LARGE = tiny ✗
```

---

### **Why S_W^(-1) S_B?**

**Without going through full derivation:**

1. We want to maximize: $J(w) = \frac{w^T S_B w}{w^T S_W w}$

2. Take derivative and set to zero (calculus of variations)

3. This gives: $S_B w = \lambda S_W w$

4. Multiply both sides by $S_W^{-1}$: $S_W^{-1} S_B w = \lambda w$

5. This is a standard eigenvalue problem!

6. Eigenvectors = optimal projection directions
7. Eigenvalues = separation quality (larger = better)

---

### **Connection to Mahalanobis Distance**

LDA is related to Mahalanobis distance:

**Euclidean distance:** $d = ||x - y||$
**Mahalanobis distance:** $d = \sqrt{(x-y)^T S^{-1} (x-y)}$

LDA finds directions where Mahalanobis distance between classes is maximized!

---

## 9. Limitations and Assumptions

### **Assumptions LDA Makes:**

1. **Normal distributions:** Each class is normally distributed
2. **Equal covariance:** All classes have same covariance matrix (S_W)
3. **Linear boundaries:** Classes separated by linear decision boundary

**When assumptions violated:**

```
ASSUMPTION OK:              ASSUMPTION VIOLATED:

  ●●●                        ●●●●●●
  ●●●  ○○○                  ●●●●●○○○●
  ●●●  ○○○                  ●●○○○○○●●
       ○○○                  ●●●●●●●●

Linear boundary works      Curved boundary needed
→ Use LDA ✓               → Use QDA or kernel methods
```

---

### **Limitations:**

**1. Small Sample Size Problem:**
```
If n < d (samples < features):
  S_W is singular (can't invert!)
  
Solution: Use PCA first to reduce dimensions
```

**2. Outlier Sensitivity:**
```
LDA uses means and covariances
→ Sensitive to outliers

    ●●●
    ●●●  ○○○○
    ●●●  ○
         ○  ● (outlier!)
         
Outlier pulls mean and increases S_W
→ Worse separation

Solution: Remove outliers first, or use robust methods
```

**3. Non-linear Separability:**
```
    ●●●
  ○○●●●○○
    ●●●
    
No linear line separates these!
LDA will fail.

Solution: Use Kernel LDA, or non-linear classifiers
```

---

## 10. Variations of LDA

### **Quadratic Discriminant Analysis (QDA)**

**When:** Classes have different covariances

```
LDA (shared covariance):    QDA (separate covariances):

  ●●●                          ●●●
  ●●●  ○○○                    ●●●  ○
  ●●●  ○○○                    ●●●  ○○
       ○○○                         ○○○

Linear boundary             Curved boundary
Assumes same spread         Allows different spreads
```

**Formula:** Each class k gets its own covariance matrix S_k

Decision: Assign to class with highest discriminant score:
$$\delta_k(x) = -\frac{1}{2}\log|S_k| - \frac{1}{2}(x-\mu_k)^T S_k^{-1}(x-\mu_k) + \log P(C_k)$$

---

### **Regularized LDA**

**When:** Small sample size, or ill-conditioned S_W

**Approach:** Add small value to diagonal:
$$S_W^{reg} = S_W + \lambda I$$

Where λ is regularization parameter (e.g., 0.01)

**Effect:** Makes S_W invertible, reduces overfitting

---

### **Kernel LDA**

**When:** Classes not linearly separable

**Idea:** Map to higher dimension where they ARE linearly separable

```
Original space:              Kernel space:
  ●●●                         ●●●●●
○○●●●○○                      ●●●●●
  ●●●                   ○○○○       ○○○○

Can't separate             Can separate with
with line                  hyperplane!
```

**Uses kernel trick:** $K(x, y) = \phi(x)^T\phi(y)$

Common kernels: RBF, polynomial

---

## 11. Complete Worked Example Summary

### **Our Iris Example:**

```
┌─────────────────────────────────────────────┐
│         LDA: START TO FINISH                 │
└─────────────────────────────────────────────┘

INPUT:
6 flowers, 2 features, 2 classes
Setosa: [1.4,0.2], [1.3,0.2], [1.5,0.4]
Versicolor: [4.7,1.4], [4.5,1.5], [4.6,1.3]

STEP 1: Compute means
μ₁ = [1.4, 0.267]    (Setosa mean)
μ₂ = [4.6, 1.4]      (Versicolor mean)

STEP 2: Compute S_W (within-class scatter)
S_W = ┌────────────┐
      │ 0.040 0.010│
      │ 0.010 0.047│
      └────────────┘
      
STEP 3: Compute S_B (between-class scatter)
S_B = ┌─────────────┐
      │ 15.36  5.44 │
      │  5.44  1.93 │
      └─────────────┘

STEP 4: Solve S_W^(-1) S_B w = λw
λ₁ = 387.813 (huge eigenvalue!)
w = [0.995, 0.095] (nearly horizontal)

STEP 5: Project data
Setosa → [1.31, 1.41, 1.53]
Versicolor → [4.62, 4.70, 4.81]

RESULT:
Perfect separation in 1D!
Fisher criterion J = 518.2 (excellent!)
Classification threshold: 3.065
```

---

## 12. Key Takeaways

### **What LDA Does:**

1. **Finds optimal separation** between classes
2. **Supervised method** (uses labels)
3. **Linear projection** to lower dimensions
4. **Maximizes Fisher criterion** (between/within variance ratio)
5. **Gives at most c-1 discriminants** for c classes

### **The Three Core Matrices:**

```
┌──────────────────────────────────────┐
│ S_W: Within-class scatter            │
│      Measures: Spread within classes │
│      Goal: MINIMIZE this             │
└──────────────────────────────────────┘

┌──────────────────────────────────────┐
│ S_B: Between-class scatter           │
│      Measures: Separation of means   │
│      Goal: MAXIMIZE this             │
└──────────────────────────────────────┘

┌──────────────────────────────────────┐
│ S_W^(-1) S_B: Combined objective     │
│      Eigenvectors: Discriminants     │
│      Eigenvalues: Separation quality │
└──────────────────────────────────────┘
```

### **When to Use LDA:**

✅ **Use LDA when:**
- You have labeled data
- Classes are (roughly) normally distributed
- Classes have similar covariances
- You need interpretable linear boundaries
- You want to reduce dimensions for classification

❌ **Don't use LDA when:**
- No labels available → use PCA
- Non-linear boundaries → use kernel methods
- Very different class spreads → use QDA
- Fewer samples than features (n < d) → regularize or PCA first
