{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Theory — Data Preprocessing Pipeline\n\nWhy preprocessing matters\n- Real-world data is messy: missing values, inconsistent categories, different scales, outliers, skewed distributions.\n- Good models rely on consistent inputs. Preprocessing turns raw tables into clean numeric matrices that models can learn from.\n- Key principle: avoid data leakage — compute any statistics (means, scalers, encoders) on training data only and apply to validation/test data later.\n\nTypical pipeline (plain-English and order of operations)\n1) Define the problem and the `target` (y). Is it regression (numeric y) or classification (categorical y)?\n2) Split early: create train/test (and optionally validation) splits BEFORE computing any statistics.\n3) Handle missing values:\n   - Numeric: mean/median imputation; optionally add a “was_missing” indicator column.\n   - Categorical: most frequent category (mode); optionally add explicit “Missing” category.\n   - Advanced: KNNImputer, MICE/IterativeImputer for richer patterns.\n4) Encode categorical features:\n   - One-Hot Encoding (OHE): safe default for unordered categories (country, color).\n   - Ordinal Encoding: only if categories have an inherent order (low < medium < high).\n   - High cardinality: consider hashing or target encoding (with care to avoid leakage).\n5) Scale/normalize numeric features:\n   - StandardScaler (zero mean, unit variance): default for many models (SVM, KNN, linear/logistic regression).\n   - MinMaxScaler [0,1]: preserves shape; often used for neural nets or bounded features.\n   - RobustScaler: resilient to outliers by using median/IQR.\n   - Trees/forests/boosting usually do NOT need scaling.\n6) Optional feature engineering:\n   - Date/time decomposition (year, month, dow), interactions, binning, log/Box–Cox/Yeo–Johnson transformations.\n   - Text: Bag-of-Words/TF‑IDF; Images: normalization/augmentation (outside this course’s scope).\n7) Fit model on the preprocessed training matrix. Use cross‑validation with the entire pipeline to choose hyperparameters.\n8) Evaluate on held‑out test data. Inspect residuals (regression), confusion matrices/ROC/PR (classification).\n\nEssential scikit‑learn tools\n- SimpleImputer, OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, RobustScaler\n- ColumnTransformer: apply different transforms to numeric vs categorical columns.\n- Pipeline: chain transforms and estimator as a single object.\n- train_test_split, cross_val_score/GridSearchCV: split and evaluate without leakage.\n\nTiny worked example (paper‑and‑pencil)\nDataset (Country, Age, Salary, Purchased):\n- Row1: France, 44, 72000, Yes\n- Row2: Spain, 27, 48000, No\n- Row3: Germany, 30, NaN, Yes\n- Row4: Spain, 38, 61000, No\n- Row5: France, 40,  NaN, Yes\n\nStep‑by‑step:\n- Train/test split first (e.g., 80/20).\n- Impute missing Salary (numeric): median of train salaries; add indicator if desired.\n- Encode Country with OHE: France→[1,0,0], Germany→[0,1,0], Spain→[0,0,1].\n- Scale numeric features (Age, Salary) with StandardScaler fitted on train only; transform test with the same scaler.\n- Target Purchased (Yes/No) becomes y (e.g., 1/0).\n\nLeakage watchlist\n- Never fit imputer/scaler/encoder on full data before splitting.\n- Do not target‑encode using the entire y; use CV folds or leave‑one‑out schemes.\n- Avoid peeking at test data during feature selection.\n\nPros, cons, pitfalls\n- Pros: Cleaner, more stable models; reproducibility; fewer surprises at deployment.\n- Cons: More steps; need careful orchestration.\n- Pitfalls: Fitting transforms on full data; mismatched columns between train/test; unseen categories at prediction time (set OneHotEncoder(handle_unknown=\"ignore\")).\n\nQuick checklist\n- Identify target and task type.\n- Split early (train/test).\n- Impute → Encode → Scale in a ColumnTransformer + Pipeline.\n- Validate via CV; keep the pipeline intact.\n- Export the fitted pipeline for production use.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WOw8yMd1VlnD"
   },
   "source": [
    "# Data Preprocessing Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvUGC8QQV6bV"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfFEXZC0WS-V"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhYaZ-ENV_c5"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqHTg9bxWT_u"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3abSxRqvWEIB"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hm48sif-WWsh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOD2/gZgY69JdiiGJVNfu7s",
   "collapsed_sections": [],
   "name": "data_preprocessing_template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
