{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upgrading our MNIST Network"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![](./images/img_4.png)\n",
    "\n",
    "![](./images/img_5.png)\n",
    "\n",
    "![](./images/img_6.png)\n",
    "\n",
    "![](./images/img_7.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:32:03.321931Z",
     "start_time": "2025-03-21T08:43:26.631082Z"
    }
   },
   "source": [
    "import numpy as np, sys\n",
    "np.random.seed(1)  # Fix the random seed for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1) Load and prepare the MNIST data\n",
    "# --------------------------------------------------------------------\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# 'x_train' and 'x_test' each are arrays of shape (N, 28, 28),\n",
    "# containing the pixel data for the images (0..255).\n",
    "# 'y_train' and 'y_test' each are arrays of shape (N,),\n",
    "# containing integer labels (0..9) for each image.\n",
    "\n",
    "# We limit ourselves to the first 1000 training samples to speed up this demo\n",
    "images = x_train[0:1000].reshape(1000, 28*28) / 255.0\n",
    "# 1) .reshape(1000, 28*28): Flatten each 28x28 image into 784 pixels per row.\n",
    "# 2) / 255.0: Scale pixel values to the range [0..1].\n",
    "\n",
    "labels = y_train[0:1000]\n",
    "# 'labels' are still integers in [0..9].\n",
    "\n",
    "# Convert these integer labels to one-hot vectors of length 10.\n",
    "one_hot_labels = np.zeros((len(labels), 10))\n",
    "for i, l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "# Now 'labels' is shape (1000, 10). Each row might look like [0,0,0,0,1,0,0,0,0,0] for digit '4'.\n",
    "\n",
    "# Prepare the entire test set in the same way: flatten, scale, one-hot\n",
    "test_images = x_test.reshape(len(x_test), 28*28) / 255.0\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i, l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2) Define Activation Functions\n",
    "# --------------------------------------------------------------------\n",
    "def tanh(x):\n",
    "    # Hyperbolic tangent: maps real numbers to (-1, 1)\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    # Derivative of tanh: given output = tanh(x),\n",
    "    # derivative = 1 - (tanh(x))^2 = 1 - output^2\n",
    "    return 1 - (output ** 2)\n",
    "\n",
    "def softmax(x):\n",
    "    # Softmax turns a vector of numbers into a probability distribution.\n",
    "    # 1) exponentiate each number\n",
    "    # 2) divide by sum of all exponents\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3) Hyperparameters and Dimensions\n",
    "# --------------------------------------------------------------------\n",
    "alpha = 2          # learning rate\n",
    "iterations = 300   # total number of epochs (full passes over the data)\n",
    "pixels_per_image = 784  # each image is 28x28\n",
    "num_labels = 10         # digits 0..9\n",
    "batch_size = 128        # mini-batch size\n",
    "\n",
    "# We'll do a \"manual convolution\" with 3x3 kernels on 28x28 images\n",
    "input_rows = 28\n",
    "input_cols = 28\n",
    "kernel_rows = 3\n",
    "kernel_cols = 3\n",
    "num_kernels = 16\n",
    "# We have 16 different kernels, each of size 3x3.\n",
    "# This means each kernel has 9 weights.\n",
    "\n",
    "# \"hidden_size\" is how large the hidden layer is after we convolve\n",
    "#   - For each image, we can slide a 3x3 patch across 26 steps horizontally\n",
    "#     and 26 steps vertically (since 28-3=25 +1 => 26 possible positions).\n",
    "#   - That means 26 * 26 = 676 patches per image.\n",
    "#   - Each patch is processed by 16 kernels => total 676*16 = 10816 \"neuron inputs.\"\n",
    "hidden_size = ((input_rows - kernel_rows) *\n",
    "               (input_cols - kernel_cols)) * num_kernels\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4) Initialize Weights (Kernels and Final Layer)\n",
    "# --------------------------------------------------------------------\n",
    "# 'kernels': shape (9, 16) => 9 weights per kernel, 16 kernels\n",
    "kernels = 0.02 * np.random.random((kernel_rows * kernel_cols, num_kernels)) - 0.01\n",
    "\n",
    "# 'weights_1_2': shape (hidden_size, 10)\n",
    "# connects the flatten \"convolution output\" to the 10 output classes\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5) Helper Function to Extract a Section of an Image\n",
    "# --------------------------------------------------------------------\n",
    "def get_image_section(layer, row_from, row_to, col_from, col_to):\n",
    "    \"\"\"\n",
    "    Given 'layer' of shape (batch_size, 28, 28),\n",
    "    extract a slice from row_from..row_to, col_from..col_to for each image,\n",
    "    returning shape (batch_size, 1, (row_to-row_from), (col_to-col_from)).\n",
    "    This is used to gather 3x3 patches from each image.\n",
    "    \"\"\"\n",
    "    section = layer[:, row_from:row_to, col_from:col_to]\n",
    "    return section.reshape(-1, 1, (row_to - row_from), (col_to - col_from))\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 6) Main Training Loop\n",
    "# --------------------------------------------------------------------\n",
    "for j in range(iterations):  # Repeat for 'iterations' epochs\n",
    "    correct_cnt = 0  # Track how many training samples we classify correctly this epoch\n",
    "\n",
    "    # We'll split the data into mini-batches\n",
    "    # The number of mini-batches = len(images)/batch_size\n",
    "    for i in range(int(len(images) / batch_size)):\n",
    "\n",
    "        # Identify the start and end indices of this batch\n",
    "        batch_start = i * batch_size\n",
    "        batch_end   = (i + 1) * batch_size\n",
    "\n",
    "        # 1) Reshape from (batch_size, 784) to (batch_size, 28, 28)\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0], 28, 28)\n",
    "\n",
    "        # 2) For each image, extract all 3x3 \"patches\"\n",
    "        # We'll store them in 'sects'\n",
    "        sects = []\n",
    "        for row_start in range(layer_0.shape[1] - kernel_rows):\n",
    "            for col_start in range(layer_0.shape[2] - kernel_cols):\n",
    "                # get_image_section returns shape (batch_size, 1, 3, 3)\n",
    "                sect = get_image_section(layer_0,\n",
    "                                         row_start,\n",
    "                                         row_start + kernel_rows,\n",
    "                                         col_start,\n",
    "                                         col_start + kernel_cols)\n",
    "                sects.append(sect)\n",
    "\n",
    "        # 3) Concatenate these patches along the second dimension\n",
    "        # so we get shape (batch_size, number_of_patches, 3, 3)\n",
    "        # number_of_patches = 26*26=676 if 28x28 -> 3x3\n",
    "        expanded_input = np.concatenate(sects, axis=1)\n",
    "        es = expanded_input.shape\n",
    "        # expanded_input: (batch_size, 676, 3, 3)\n",
    "\n",
    "        # 4) Flatten each 3x3 patch into a length-9 vector\n",
    "        #    So final shape is (batch_size * 676, 9)\n",
    "        flattened_input = expanded_input.reshape(es[0]*es[1], -1)\n",
    "\n",
    "        # 5) Multiply these 9-element patches by our kernel matrix (shape (9,16))\n",
    "        #    => kernel_output: (batch_size*676, 16)\n",
    "        kernel_output = flattened_input.dot(kernels)\n",
    "\n",
    "        # 6) Activation (tanh) + Dropout\n",
    "        #    Reshape from (batch_size*676, 16) to (batch_size, 676*16)\n",
    "        layer_1 = tanh(kernel_output.reshape(es[0], -1))\n",
    "        \n",
    "        # Apply dropout: random 0 or 1, then multiply by 2 to keep same scale\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "\n",
    "        # 7) Final layer: multiply by weights_1_2 and apply softmax\n",
    "        layer_2 = softmax(np.dot(layer_1, weights_1_2))\n",
    "        # layer_2 now has shape (batch_size, 10), representing class probabilities\n",
    "\n",
    "        # 8) Calculate training accuracy: how many predictions are correct\n",
    "        for k in range(batch_size):\n",
    "            # 'labelset' is the true one-hot label for this sample\n",
    "            labelset = labels[batch_start + k : batch_start + k + 1]\n",
    "            # Compare argmax\n",
    "            _inc = int(np.argmax(layer_2[k : k+1]) == np.argmax(labelset))\n",
    "            correct_cnt += _inc\n",
    "\n",
    "        # 9) Backpropagation\n",
    "        #    a) Output layer delta:\n",
    "        #       'layer_2_delta' = (true_label - prediction)\n",
    "        #       We divide by (batch_size * layer_2.shape[0]) to average the gradient\n",
    "        layer_2_delta = (labels[batch_start:batch_end] - layer_2) / (batch_size * layer_2.shape[0])\n",
    "\n",
    "        #    b) Hidden layer delta:\n",
    "        #       push the error back through 'weights_1_2',\n",
    "        #       then multiply by derivative of tanh\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tanh2deriv(layer_1)\n",
    "\n",
    "        #    c) Zero out deltas for neurons that were dropped (0) in dropout\n",
    "        layer_1_delta *= dropout_mask\n",
    "\n",
    "        # 10) Weight Updates\n",
    "        #     a) hidden -> output\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "\n",
    "        #     b) kernels\n",
    "        #        reshape 'layer_1_delta' back to (batch_size*676, 16),\n",
    "        #        same shape as kernel_output,\n",
    "        #        so we can multiply with 'flattened_input' to get gradient wrt. kernels\n",
    "        l1d_reshape = layer_1_delta.reshape(kernel_output.shape)\n",
    "        k_update = flattened_input.T.dot(l1d_reshape)\n",
    "        # We do 'kernels -= alpha * k_update' (a gradient descent step)\n",
    "        kernels -= alpha * k_update\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # After each epoch, test on the entire test set (no dropout here)\n",
    "    # ----------------------------------------------------------------\n",
    "    test_correct_cnt = 0\n",
    "    for i in range(len(test_images)):\n",
    "        layer_0 = test_images[i : i+1]\n",
    "        # reshape from (1, 784) to (1, 28, 28)\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0], 28, 28)\n",
    "        \n",
    "        # Extract 3x3 patches from this single image\n",
    "        sects = []\n",
    "        for row_start in range(layer_0.shape[1] - kernel_rows):\n",
    "            for col_start in range(layer_0.shape[2] - kernel_cols):\n",
    "                sect = get_image_section(layer_0,\n",
    "                                         row_start,\n",
    "                                         row_start + kernel_rows,\n",
    "                                         col_start,\n",
    "                                         col_start + kernel_cols)\n",
    "                sects.append(sect)\n",
    "\n",
    "        expanded_input = np.concatenate(sects, axis=1)\n",
    "        es = expanded_input.shape\n",
    "\n",
    "        # Flatten from (1, #patches, 3, 3) to (#patches, 9)\n",
    "        flattened_input = expanded_input.reshape(es[0]*es[1], -1)\n",
    "\n",
    "        # Dot with 'kernels' => shape (#patches, 16)\n",
    "        kernel_output = flattened_input.dot(kernels)\n",
    "        # Then reshape to (1, #patches*16)\n",
    "        layer_1 = tanh(kernel_output.reshape(es[0], -1))\n",
    "\n",
    "        # final layer (no dropout in testing)\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "        # Check if predicted class == true label\n",
    "        test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i : i+1]))\n",
    "\n",
    "    # Print results every epoch\n",
    "    sys.stdout.write(\n",
    "        \"\\n\"\n",
    "        + \"I:\" + str(j)  # which epoch\n",
    "        + \" Test-Acc:\" + str(test_correct_cnt / float(len(test_images)))\n",
    "        + \" Train-Acc:\" + str(correct_cnt / float(len(images)))\n",
    "    )\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 10:43:27.641652: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-21 10:43:27.655793: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-21 10:43:27.750387: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-21 10:43:27.837043: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742546607.947245  128400 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742546607.973683  128400 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742546608.184023  128400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742546608.184068  128400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742546608.184071  128400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742546608.184073  128400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-21 10:43:28.206295: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Acc:0.0288 Train-Acc:0.055\n",
      "I:1 Test-Acc:0.0273 Train-Acc:0.037\n",
      "I:2 Test-Acc:0.028 Train-Acc:0.037\n",
      "I:3 Test-Acc:0.0292 Train-Acc:0.04\n",
      "I:4 Test-Acc:0.0339 Train-Acc:0.046\n",
      "I:5 Test-Acc:0.0478 Train-Acc:0.068\n",
      "I:6 Test-Acc:0.076 Train-Acc:0.083\n",
      "I:7 Test-Acc:0.1316 Train-Acc:0.096\n",
      "I:8 Test-Acc:0.2137 Train-Acc:0.127\n",
      "I:9 Test-Acc:0.2941 Train-Acc:0.148\n",
      "I:10 Test-Acc:0.3563 Train-Acc:0.181\n",
      "I:11 Test-Acc:0.4023 Train-Acc:0.209\n",
      "I:12 Test-Acc:0.4358 Train-Acc:0.238\n",
      "I:13 Test-Acc:0.4473 Train-Acc:0.286\n",
      "I:14 Test-Acc:0.4389 Train-Acc:0.274\n",
      "I:15 Test-Acc:0.3951 Train-Acc:0.257\n",
      "I:16 Test-Acc:0.2222 Train-Acc:0.243\n",
      "I:17 Test-Acc:0.0613 Train-Acc:0.112\n",
      "I:18 Test-Acc:0.0266 Train-Acc:0.035\n",
      "I:19 Test-Acc:0.0127 Train-Acc:0.026\n",
      "I:20 Test-Acc:0.0133 Train-Acc:0.022\n",
      "I:21 Test-Acc:0.0185 Train-Acc:0.038\n",
      "I:22 Test-Acc:0.0363 Train-Acc:0.038\n",
      "I:23 Test-Acc:0.0928 Train-Acc:0.067\n",
      "I:24 Test-Acc:0.1994 Train-Acc:0.081\n",
      "I:25 Test-Acc:0.3086 Train-Acc:0.154\n",
      "I:26 Test-Acc:0.4276 Train-Acc:0.204\n",
      "I:27 Test-Acc:0.5323 Train-Acc:0.256\n",
      "I:28 Test-Acc:0.5919 Train-Acc:0.305\n",
      "I:29 Test-Acc:0.6324 Train-Acc:0.341\n",
      "I:30 Test-Acc:0.6608 Train-Acc:0.426\n",
      "I:31 Test-Acc:0.6815 Train-Acc:0.439\n",
      "I:32 Test-Acc:0.7048 Train-Acc:0.462\n",
      "I:33 Test-Acc:0.7171 Train-Acc:0.484\n",
      "I:34 Test-Acc:0.7313 Train-Acc:0.505\n",
      "I:35 Test-Acc:0.7355 Train-Acc:0.53\n",
      "I:36 Test-Acc:0.7417 Train-Acc:0.548\n",
      "I:37 Test-Acc:0.747 Train-Acc:0.534\n",
      "I:38 Test-Acc:0.7491 Train-Acc:0.55\n",
      "I:39 Test-Acc:0.7459 Train-Acc:0.562\n",
      "I:40 Test-Acc:0.7352 Train-Acc:0.54\n",
      "I:41 Test-Acc:0.7082 Train-Acc:0.496\n",
      "I:42 Test-Acc:0.6487 Train-Acc:0.456\n",
      "I:43 Test-Acc:0.5209 Train-Acc:0.353\n",
      "I:44 Test-Acc:0.3305 Train-Acc:0.234\n",
      "I:45 Test-Acc:0.2052 Train-Acc:0.174\n",
      "I:46 Test-Acc:0.2149 Train-Acc:0.136\n",
      "I:47 Test-Acc:0.2679 Train-Acc:0.171\n",
      "I:48 Test-Acc:0.3237 Train-Acc:0.172\n",
      "I:49 Test-Acc:0.3581 Train-Acc:0.186\n",
      "I:50 Test-Acc:0.4202 Train-Acc:0.21\n",
      "I:51 Test-Acc:0.5165 Train-Acc:0.223\n",
      "I:52 Test-Acc:0.6007 Train-Acc:0.262\n",
      "I:53 Test-Acc:0.6476 Train-Acc:0.308\n",
      "I:54 Test-Acc:0.676 Train-Acc:0.363\n",
      "I:55 Test-Acc:0.696 Train-Acc:0.402\n",
      "I:56 Test-Acc:0.7077 Train-Acc:0.434\n",
      "I:57 Test-Acc:0.7204 Train-Acc:0.441\n",
      "I:58 Test-Acc:0.7303 Train-Acc:0.475\n",
      "I:59 Test-Acc:0.7359 Train-Acc:0.475\n",
      "I:60 Test-Acc:0.7401 Train-Acc:0.525\n",
      "I:61 Test-Acc:0.7493 Train-Acc:0.517\n",
      "I:62 Test-Acc:0.7533 Train-Acc:0.517\n",
      "I:63 Test-Acc:0.7606 Train-Acc:0.538\n",
      "I:64 Test-Acc:0.7644 Train-Acc:0.554\n",
      "I:65 Test-Acc:0.7724 Train-Acc:0.57\n",
      "I:66 Test-Acc:0.7788 Train-Acc:0.586\n",
      "I:67 Test-Acc:0.7855 Train-Acc:0.595\n",
      "I:68 Test-Acc:0.7853 Train-Acc:0.591\n",
      "I:69 Test-Acc:0.7925 Train-Acc:0.605\n",
      "I:70 Test-Acc:0.7973 Train-Acc:0.64\n",
      "I:71 Test-Acc:0.8013 Train-Acc:0.621\n",
      "I:72 Test-Acc:0.8029 Train-Acc:0.626\n",
      "I:73 Test-Acc:0.8092 Train-Acc:0.631\n",
      "I:74 Test-Acc:0.8099 Train-Acc:0.638\n",
      "I:75 Test-Acc:0.8156 Train-Acc:0.661\n",
      "I:76 Test-Acc:0.8156 Train-Acc:0.639\n",
      "I:77 Test-Acc:0.8184 Train-Acc:0.65\n",
      "I:78 Test-Acc:0.8216 Train-Acc:0.67\n",
      "I:79 Test-Acc:0.8246 Train-Acc:0.675\n",
      "I:80 Test-Acc:0.8237 Train-Acc:0.666\n",
      "I:81 Test-Acc:0.8273 Train-Acc:0.673\n",
      "I:82 Test-Acc:0.8273 Train-Acc:0.704\n",
      "I:83 Test-Acc:0.8314 Train-Acc:0.674\n",
      "I:84 Test-Acc:0.8292 Train-Acc:0.686\n",
      "I:85 Test-Acc:0.8335 Train-Acc:0.699\n",
      "I:86 Test-Acc:0.8359 Train-Acc:0.694\n",
      "I:87 Test-Acc:0.8375 Train-Acc:0.704\n",
      "I:88 Test-Acc:0.8373 Train-Acc:0.697\n",
      "I:89 Test-Acc:0.8398 Train-Acc:0.704\n",
      "I:90 Test-Acc:0.8393 Train-Acc:0.687\n",
      "I:91 Test-Acc:0.8436 Train-Acc:0.705\n",
      "I:92 Test-Acc:0.8437 Train-Acc:0.711\n",
      "I:93 Test-Acc:0.8446 Train-Acc:0.721\n",
      "I:94 Test-Acc:0.845 Train-Acc:0.719\n",
      "I:95 Test-Acc:0.8469 Train-Acc:0.724\n",
      "I:96 Test-Acc:0.8476 Train-Acc:0.726\n",
      "I:97 Test-Acc:0.848 Train-Acc:0.718\n",
      "I:98 Test-Acc:0.8496 Train-Acc:0.719\n",
      "I:99 Test-Acc:0.85 Train-Acc:0.73\n",
      "I:100 Test-Acc:0.8511 Train-Acc:0.737\n",
      "I:101 Test-Acc:0.8503 Train-Acc:0.73\n",
      "I:102 Test-Acc:0.8504 Train-Acc:0.717\n",
      "I:103 Test-Acc:0.8528 Train-Acc:0.74\n",
      "I:104 Test-Acc:0.8532 Train-Acc:0.733\n",
      "I:105 Test-Acc:0.8537 Train-Acc:0.73\n",
      "I:106 Test-Acc:0.8568 Train-Acc:0.721\n",
      "I:107 Test-Acc:0.857 Train-Acc:0.75\n",
      "I:108 Test-Acc:0.8558 Train-Acc:0.731\n",
      "I:109 Test-Acc:0.8578 Train-Acc:0.744\n",
      "I:110 Test-Acc:0.8588 Train-Acc:0.754\n",
      "I:111 Test-Acc:0.8579 Train-Acc:0.732\n",
      "I:112 Test-Acc:0.8582 Train-Acc:0.747\n",
      "I:113 Test-Acc:0.8593 Train-Acc:0.747\n",
      "I:114 Test-Acc:0.8598 Train-Acc:0.751\n",
      "I:115 Test-Acc:0.8603 Train-Acc:0.74\n",
      "I:116 Test-Acc:0.86 Train-Acc:0.753\n",
      "I:117 Test-Acc:0.8588 Train-Acc:0.746\n",
      "I:118 Test-Acc:0.861 Train-Acc:0.741\n",
      "I:119 Test-Acc:0.8616 Train-Acc:0.731\n",
      "I:120 Test-Acc:0.8629 Train-Acc:0.753\n",
      "I:121 Test-Acc:0.8609 Train-Acc:0.743\n",
      "I:122 Test-Acc:0.8627 Train-Acc:0.752\n",
      "I:123 Test-Acc:0.8646 Train-Acc:0.76\n",
      "I:124 Test-Acc:0.8649 Train-Acc:0.766\n",
      "I:125 Test-Acc:0.8659 Train-Acc:0.752\n",
      "I:126 Test-Acc:0.868 Train-Acc:0.756\n",
      "I:127 Test-Acc:0.8648 Train-Acc:0.767\n",
      "I:128 Test-Acc:0.8662 Train-Acc:0.747\n",
      "I:129 Test-Acc:0.8669 Train-Acc:0.753\n",
      "I:130 Test-Acc:0.8694 Train-Acc:0.753\n",
      "I:131 Test-Acc:0.8692 Train-Acc:0.76\n",
      "I:132 Test-Acc:0.8658 Train-Acc:0.756\n",
      "I:133 Test-Acc:0.8666 Train-Acc:0.769\n",
      "I:134 Test-Acc:0.8692 Train-Acc:0.77\n",
      "I:135 Test-Acc:0.8681 Train-Acc:0.757\n",
      "I:136 Test-Acc:0.8705 Train-Acc:0.77\n",
      "I:137 Test-Acc:0.8706 Train-Acc:0.77\n",
      "I:138 Test-Acc:0.8684 Train-Acc:0.768\n",
      "I:139 Test-Acc:0.8664 Train-Acc:0.774\n",
      "I:140 Test-Acc:0.8666 Train-Acc:0.756\n",
      "I:141 Test-Acc:0.8705 Train-Acc:0.783\n",
      "I:142 Test-Acc:0.87 Train-Acc:0.775\n",
      "I:143 Test-Acc:0.8729 Train-Acc:0.769\n",
      "I:144 Test-Acc:0.8725 Train-Acc:0.776\n",
      "I:145 Test-Acc:0.8721 Train-Acc:0.772\n",
      "I:146 Test-Acc:0.8718 Train-Acc:0.765\n",
      "I:147 Test-Acc:0.8746 Train-Acc:0.777\n",
      "I:148 Test-Acc:0.8746 Train-Acc:0.77\n",
      "I:149 Test-Acc:0.8734 Train-Acc:0.778\n",
      "I:150 Test-Acc:0.873 Train-Acc:0.785\n",
      "I:151 Test-Acc:0.8732 Train-Acc:0.76\n",
      "I:152 Test-Acc:0.8727 Train-Acc:0.779\n",
      "I:153 Test-Acc:0.8754 Train-Acc:0.772\n",
      "I:154 Test-Acc:0.8729 Train-Acc:0.773\n",
      "I:155 Test-Acc:0.8758 Train-Acc:0.784\n",
      "I:156 Test-Acc:0.8732 Train-Acc:0.774\n",
      "I:157 Test-Acc:0.8743 Train-Acc:0.782\n",
      "I:158 Test-Acc:0.8762 Train-Acc:0.772\n",
      "I:159 Test-Acc:0.8755 Train-Acc:0.79\n",
      "I:160 Test-Acc:0.8751 Train-Acc:0.774\n",
      "I:161 Test-Acc:0.8749 Train-Acc:0.782\n",
      "I:162 Test-Acc:0.8744 Train-Acc:0.78\n",
      "I:163 Test-Acc:0.8765 Train-Acc:0.782\n",
      "I:164 Test-Acc:0.8738 Train-Acc:0.796\n",
      "I:165 Test-Acc:0.8753 Train-Acc:0.798\n",
      "I:166 Test-Acc:0.8767 Train-Acc:0.794\n",
      "I:167 Test-Acc:0.8746 Train-Acc:0.784\n",
      "I:168 Test-Acc:0.8769 Train-Acc:0.796\n",
      "I:169 Test-Acc:0.8758 Train-Acc:0.789\n",
      "I:170 Test-Acc:0.8764 Train-Acc:0.79\n",
      "I:171 Test-Acc:0.873 Train-Acc:0.791\n",
      "I:172 Test-Acc:0.8765 Train-Acc:0.797\n",
      "I:173 Test-Acc:0.8772 Train-Acc:0.789\n",
      "I:174 Test-Acc:0.8778 Train-Acc:0.781\n",
      "I:175 Test-Acc:0.8758 Train-Acc:0.799\n",
      "I:176 Test-Acc:0.8773 Train-Acc:0.785\n",
      "I:177 Test-Acc:0.8766 Train-Acc:0.796\n",
      "I:178 Test-Acc:0.8782 Train-Acc:0.803\n",
      "I:179 Test-Acc:0.8789 Train-Acc:0.794\n",
      "I:180 Test-Acc:0.8778 Train-Acc:0.794\n",
      "I:181 Test-Acc:0.8778 Train-Acc:0.8\n",
      "I:182 Test-Acc:0.8785 Train-Acc:0.791\n",
      "I:183 Test-Acc:0.8777 Train-Acc:0.787\n",
      "I:184 Test-Acc:0.8769 Train-Acc:0.781"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 199\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m row_start \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(layer_0\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m kernel_rows):\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m col_start \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(layer_0\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m-\u001B[39m kernel_cols):\n\u001B[0;32m--> 199\u001B[0m         sect \u001B[38;5;241m=\u001B[39m \u001B[43mget_image_section\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlayer_0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrow_start\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrow_start\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mkernel_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcol_start\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcol_start\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mkernel_cols\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m         sects\u001B[38;5;241m.\u001B[39mappend(sect)\n\u001B[1;32m    208\u001B[0m expanded_input \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(sects, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "Cell \u001B[0;32mIn[1], line 83\u001B[0m, in \u001B[0;36mget_image_section\u001B[0;34m(layer, row_from, row_to, col_from, col_to)\u001B[0m\n\u001B[1;32m     78\u001B[0m weights_1_2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.2\u001B[39m\u001B[38;5;241m*\u001B[39mnp\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandom((hidden_size, num_labels)) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m0.1\u001B[39m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# --------------------------------------------------------------------\u001B[39;00m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;66;03m# 5) Helper Function to Extract a Section of an Image\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;66;03m# --------------------------------------------------------------------\u001B[39;00m\n\u001B[0;32m---> 83\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_image_section\u001B[39m(layer, row_from, row_to, col_from, col_to):\n\u001B[1;32m     84\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;124;03m    Extract a 2D slice from the images in 'layer' along\u001B[39;00m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;124;03m    the specified rows and columns.\u001B[39;00m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;124;03m    Then reshape it to (batch_size, 1, kernel_rows, kernel_cols)\u001B[39;00m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;124;03m    for easier manipulation.\u001B[39;00m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     90\u001B[0m     section \u001B[38;5;241m=\u001B[39m layer[:, row_from:row_to, col_from:col_to]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
