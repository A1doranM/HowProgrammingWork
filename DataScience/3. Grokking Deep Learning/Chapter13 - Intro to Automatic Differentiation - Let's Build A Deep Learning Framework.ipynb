{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image%20copy%2014.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n",
      "[ 2  4  6  8 10]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \"\"\"\n",
    "    A simple wrapper around a NumPy array to illustrate\n",
    "    how a custom tensor class might behave in a tiny NN framework.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        # Store the data as a NumPy array, regardless of the initial input type (e.g., list).\n",
    "        self.data = np.array(data)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Overload the '+' operator so that you can do:\n",
    "            Tensor(...) + Tensor(...)\n",
    "        This performs elementwise addition of the internal NumPy arrays,\n",
    "        and returns a new Tensor with the result.\n",
    "        \"\"\"\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Define a 'representation' for debugging or interactive sessions.\n",
    "        We simply return the string representation of the underlying NumPy array.\n",
    "        \"\"\"\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Define how the Tensor prints (e.g., with print(x)).\n",
    "        It returns the string form of the NumPy array data.\n",
    "        \"\"\"\n",
    "        return str(self.data.__str__())\n",
    "\n",
    "# Create a Tensor 'x' from a Python list\n",
    "x = Tensor([1, 2, 3, 4, 5])\n",
    "print(x)  # This uses x.__str__ and should print \"[1 2 3 4 5]\"\n",
    "\n",
    "# Add 'x' to itself using our overloaded '+' operator.\n",
    "y = x + x\n",
    "print(y)  # Prints the result of elementwise addition \"[ 2  4  6  8 10]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Introduction to Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image%20copy%2013.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \"\"\"\n",
    "    A simple Tensor class that supports basic \"add\" operation\n",
    "    and can backpropagate (compute gradients) for the 'add' op.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, creators=None, creation_op=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : array-like\n",
    "            The raw numerical values that this Tensor holds.\n",
    "        creators : list of Tensors or None\n",
    "            References to any Tensor objects used to create this Tensor\n",
    "            (i.e., the \"parents\" in a computation graph).\n",
    "        creation_op : str or None\n",
    "            The operation that created this Tensor (e.g., \"add\").\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        self.data : ndarray\n",
    "            NumPy array holding the actual numerical values.\n",
    "        self.creation_op : str\n",
    "            Indicates which operation led to this Tensor.\n",
    "        self.creators : list of Tensors\n",
    "            The Tensor objects that were inputs to the creation_op.\n",
    "        self.grad : None or ndarray\n",
    "            Will hold the gradient (partial derivatives) once backprop is called.\n",
    "        \"\"\"\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None   # Will store gradient when 'backward' is called.\n",
    "\n",
    "    def backward(self, grad):\n",
    "        \"\"\"\n",
    "        Perform backpropagation from this Tensor, distributing 'grad'\n",
    "        to its parents (creators) based on the creation_op.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        grad : Tensor\n",
    "            The gradient from the next level in the graph, \n",
    "            telling us how the final output changes \n",
    "            w.r.t. this Tensor's data.\n",
    "        \"\"\"\n",
    "        # Store the gradient received\n",
    "        self.grad = grad\n",
    "\n",
    "        # If this Tensor was created by addition,\n",
    "        # then both of the input Tensors contributed equally (chain rule).\n",
    "        # So we pass the same gradient on to each creator.\n",
    "        if self.creation_op == \"add\":\n",
    "            # self.creators[0] and self.creators[1] are the two Tensors that were added\n",
    "            self.creators[0].backward(grad)\n",
    "            self.creators[1].backward(grad)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Overload the '+' operator to return a new Tensor.\n",
    "        'creators' is a list containing the two Tensors involved,\n",
    "        'creation_op' is set to \"add\" so we know how to backprop later.\n",
    "        \"\"\"\n",
    "        return Tensor(self.data + other.data,\n",
    "                      creators=[self, other],\n",
    "                      creation_op=\"add\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        # For debugging: returns a string representation of the underlying numpy array\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    def __str__(self):\n",
    "        # For printing: returns a nicer string version of the numpy array\n",
    "        return str(self.data.__str__())\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "x = Tensor([1, 2, 3, 4, 5])\n",
    "y = Tensor([2, 2, 2, 2, 2])\n",
    "\n",
    "# 'z' is a new Tensor resulting from x + y\n",
    "z = x + y\n",
    "\n",
    "# We call 'backward' on 'z', providing a gradient to \"kick off\" backprop.\n",
    "# Suppose the gradient is Tensor([1,1,1,1,1]) for demonstration.\n",
    "z.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "\n",
    "# If you now inspect x.grad or y.grad, you'd see that each \n",
    "# has the gradient passed on from z (in this case, [1,1,1,1,1]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[array([1, 2, 3, 4, 5]), array([2, 2, 2, 2, 2])]\n",
      "add\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.creators)\n",
    "print(z.creation_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5])\n",
    "b = Tensor([2,2,2,2,2])\n",
    "c = Tensor([5,4,3,2,1])\n",
    "d = Tensor([-1,-2,-3,-4,-5])\n",
    "\n",
    "e = a + b\n",
    "f = c + d\n",
    "g = e + f\n",
    "\n",
    "g.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Tensors That Are Used Multiple Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5])\n",
    "b = Tensor([2,2,2,2,2])\n",
    "c = Tensor([5,4,3,2,1])\n",
    "\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "\n",
    "b.grad.data == np.array([2,2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Upgrading Autograd to Support Multiple Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image%20copy%2012.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array-like\n",
    "            Numerical data for this tensor.\n",
    "        autograd : bool\n",
    "            Whether this tensor requires gradient tracking.\n",
    "        creators : list or None\n",
    "            If this tensor was created by an operation involving other tensors,\n",
    "            this is the list of 'parent' Tensors.\n",
    "        creation_op : str or None\n",
    "            The operation ('add', 'mul', etc.) that created this Tensor.\n",
    "        id : int or None\n",
    "            Unique identifier; if not provided, a random int is assigned.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        data : np.ndarray\n",
    "            The numerical data stored for this Tensor.\n",
    "        autograd : bool\n",
    "            Whether this Tensor tracks gradients.\n",
    "        grad : Tensor or None\n",
    "            Accumulated gradient for backpropagation.\n",
    "        creators : list of Tensors or None\n",
    "            Parent Tensors that were involved in creating this Tensor.\n",
    "        creation_op : str\n",
    "            The name of the operation that created this Tensor.\n",
    "        children : dict\n",
    "            Map of child IDs to a counter; helps manage gradient flow\n",
    "            so we know when all children have backpropagated.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "\n",
    "        # A unique ID for tracking children/parents in the graph\n",
    "        if id is None:\n",
    "            self.id = np.random.randint(0, 100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}  # Maps child tensor IDs -> how many grads we expect\n",
    "\n",
    "        # If this tensor has creators, register as a 'child' in each creator\n",
    "        if creators is not None:\n",
    "            for c in creators:\n",
    "                if self.id not in c.children:\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        \"\"\"\n",
    "        Check if all expected gradient flows from the child Tensors\n",
    "        have been received. If every child ID is at 0, \n",
    "        it means we've accounted for all backprop calls from them.\n",
    "        \"\"\"\n",
    "        for id, cnt in self.children.items():\n",
    "            if cnt != 0:\n",
    "                return False\n",
    "        return True        \n",
    "\n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        \"\"\"\n",
    "        Backpropagate from this Tensor, accumulating gradients and \n",
    "        passing them to parent Tensors if needed.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grad : Tensor or None\n",
    "            Gradient coming in from a child or final output.\n",
    "            If None, we assume ones-like this Tensor's shape for the gradient.\n",
    "        grad_origin : Tensor or None\n",
    "            The child Tensor from which this backward call originated.\n",
    "            Helps manage how many times each parent is called.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            # If no gradient was provided, default to a Tensor of ones\n",
    "            if grad is None:\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            # If we know which child is passing the gradient,\n",
    "            # decrement the count of expected gradients from that child.\n",
    "            if grad_origin is not None:\n",
    "                if self.children[grad_origin.id] == 0:\n",
    "                    raise Exception(\"Cannot backprop more than once from the same child.\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            # Accumulate this gradient into self.grad\n",
    "            if self.grad is None:\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # Assert that grad does not itself require autograd\n",
    "            # (we expect a leaf gradient, not a nested one)\n",
    "            assert grad.autograd == False\n",
    "\n",
    "            # Only continue backprop if this Tensor has parents (creators)\n",
    "            # and all children have backprop'd, or if this is the final call \n",
    "            # (grad_origin is None => we called backward on this Tensor directly).\n",
    "            if (self.creators is not None \n",
    "                and (self.all_children_grads_accounted_for() \n",
    "                     or grad_origin is None)):\n",
    "\n",
    "                # Depending on creation_op, distribute gradient to parents\n",
    "                if self.creation_op == \"add\":\n",
    "                    # In an 'add' operation, the gradient flows equally \n",
    "                    # to both parent Tensors.\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Overload the '+' operator. \n",
    "        If both Tensors require autograd, we track the parents/operation.\n",
    "        Otherwise, just return a new Tensor with the sum of data.\n",
    "        \"\"\"\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Demonstration:\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Create three Tensors that require gradients\n",
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([5,4,3,2,1], autograd=True)\n",
    "\n",
    "# Perform some additions\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e  # f = (a + b) + (b + c) = a + 2b + c\n",
    "\n",
    "# Backprop from f with an incoming gradient of [1,1,1,1,1].\n",
    "# This simulates \"df/dx = 1\" for each element, to see how \n",
    "# the partial derivatives distribute.\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "\n",
    "# At this point, a.grad, b.grad, c.grad should be populated \n",
    "# with the correct gradients from the chain rule. \n",
    "# We check b.grad against an expected value [2,2,2,2,2].\n",
    "print(b.grad.data == np.array([2,2,2,2,2]))\n",
    "# This should print an array of True values or a single bool True \n",
    "# if everything matches elementwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Add Support for Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image%20copy%2011.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \"\"\"\n",
    "        A simple Tensor class supporting basic operations (add, neg) \n",
    "        and backpropagation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array-like\n",
    "            Numerical data for this tensor, stored as a NumPy array.\n",
    "        autograd : bool\n",
    "            If True, this Tensor will participate in gradient tracking.\n",
    "        creators : list of Tensors or None\n",
    "            Parent Tensors that created this Tensor (if any).\n",
    "        creation_op : str or None\n",
    "            The operation used to create this Tensor (\"add\", \"neg\", etc.).\n",
    "        id : int or None\n",
    "            Unique identifier for this Tensor. If None, a random ID is generated.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert input data to a NumPy array\n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "\n",
    "        # Assign or generate an ID\n",
    "        if id is None:\n",
    "            self.id = np.random.randint(0, 100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators      # Tensors that were used to create this one\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}            # Dict to track how many times each child backprop is expected\n",
    "\n",
    "        # If this Tensor was created from other Tensors, register it as a child in those parent Tensors\n",
    "        if creators is not None:\n",
    "            for c in creators:\n",
    "                if self.id not in c.children:\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        \"\"\"\n",
    "        Check if all expected gradients from child Tensors have arrived.\n",
    "        If any child's count is not 0, we're still waiting for that child's gradient.\n",
    "        \"\"\"\n",
    "        for id, cnt in self.children.items():\n",
    "            if cnt != 0:\n",
    "                return False\n",
    "        return True        \n",
    "        \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        \"\"\"\n",
    "        Backpropagate from this Tensor, distributing gradients to its parents.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        grad : Tensor or None\n",
    "            The incoming gradient from the 'child' or from a final output.\n",
    "            If None, a default gradient of ones is used (like \"d(output)/d(this) = 1\").\n",
    "        grad_origin : Tensor or None\n",
    "            Which child Tensor is sending this gradient. This helps ensure\n",
    "            we only backprop once per child.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            # If no gradient is provided, assume a gradient of 1s for the entire shape\n",
    "            if grad is None:\n",
    "                grad = FloatTensor(np.ones_like(self.data))\n",
    "            \n",
    "            # If we're receiving a gradient from a specific child, decrement the child's counter\n",
    "            if grad_origin is not None:\n",
    "                if self.children[grad_origin.id] == 0:\n",
    "                    raise Exception(\"cannot backprop more than once from the same child\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            # Accumulate the incoming gradient into self.grad\n",
    "            if self.grad is None:\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # The incoming gradient must not require its own grad (i.e., a leaf grad)\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # If this Tensor has parents, and we've accounted for all child grads, or this is a direct call:\n",
    "            if (self.creators is not None \n",
    "                and (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "\n",
    "                # Backprop logic depending on the operation that created this Tensor\n",
    "                if self.creation_op == \"add\":\n",
    "                    # In 'add', gradient is passed to both parents unchanged\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if self.creation_op == \"neg\":\n",
    "                    # In 'neg', if z = -x, then dz/dx = -1, \n",
    "                    # so we pass the negative of self.grad to the parent\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Overload the '+' operator.\n",
    "        If both Tensors track gradients, create a new Tensor with 'creation_op=\"add\"'\n",
    "        and reference them as creators. Otherwise, just do raw NumPy addition.\n",
    "        \"\"\"\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"\n",
    "        Overload the unary negation operator (i.e., -self).\n",
    "        If autograd is True, record 'neg' as the creation_op.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1) \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# DEMONSTRATION\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Create a few Tensors that require gradient tracking\n",
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([5,4,3,2,1], autograd=True)\n",
    "\n",
    "# Use negation to build an expression\n",
    "d = a + (-b)  # => a - b\n",
    "e = (-b) + c  # => c - b\n",
    "f = d + e     # => (a - b) + (c - b) = a + c - 2b\n",
    "\n",
    "# Now backprop from f with an incoming gradient of [1,1,1,1,1]\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "\n",
    "# Check that b's gradient is -2 across all elements (since f w.r.t b is -2)\n",
    "print(b.grad.data == np.array([-2,-2,-2,-2,-2]))\n",
    "# This should print a boolean array or a single boolean True if all match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Add Support for Additional Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image%20copy%2010.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \"\"\"\n",
    "        A more complete Tensor class that supports:\n",
    "          - Basic elementwise operations (+, -, *)\n",
    "          - Matrix multiplication (mm)\n",
    "          - Summation along a dimension\n",
    "          - Transposition\n",
    "          - \"Expand\" operation to replicate data\n",
    "          - Negation\n",
    "        and an auto-differentiation mechanism for them.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : array-like\n",
    "            The underlying numerical data (stored as NumPy array).\n",
    "        autograd : bool\n",
    "            Whether this Tensor tracks gradients.\n",
    "        creators : list of Tensors or None\n",
    "            Parent Tensors that created this Tensor through an operation.\n",
    "        creation_op : str or None\n",
    "            The name of the operation that created this Tensor.\n",
    "        id : int or None\n",
    "            An optional unique ID. If None, a random ID is assigned.\n",
    "\n",
    "        Attributes:\n",
    "        -----------\n",
    "        data : np.ndarray\n",
    "            The numerical values for this Tensor.\n",
    "        autograd : bool\n",
    "            Whether gradient tracking is active.\n",
    "        grad : Tensor or None\n",
    "            Accumulated gradient.\n",
    "        creators : list or None\n",
    "            Tensors that were used in creating this one (parents).\n",
    "        creation_op : str\n",
    "            Operation name (\"add\", \"sub\", \"mul\", \"mm\", etc.).\n",
    "        children : dict\n",
    "            A map {child_id: count}, tracking how many times each child \n",
    "            needs to backprop for this Tensor.\n",
    "        \"\"\"\n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "\n",
    "        # Assign an ID for this Tensor\n",
    "        if id is None:\n",
    "            self.id = np.random.randint(0, 100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}  # child_id -> how many backprops expected\n",
    "\n",
    "        # If there are parent Tensors, register this as a child\n",
    "        if creators is not None:\n",
    "            for c in creators:\n",
    "                if self.id not in c.children:\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        \"\"\"\n",
    "        Check if all children have backpropagated.\n",
    "        Returns True if every child's count is zero.\n",
    "        \"\"\"\n",
    "        for child_id, cnt in self.children.items():\n",
    "            if cnt != 0:\n",
    "                return False\n",
    "        return True \n",
    "\n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        \"\"\"\n",
    "        Backpropagate through this Tensor, accumulating gradient and \n",
    "        passing it on to its parents based on creation_op rules.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        grad : Tensor or None\n",
    "            The gradient from the next level in the graph. If None, \n",
    "            defaults to a Tensor of ones shaped like self.data.\n",
    "        grad_origin : Tensor or None\n",
    "            The child Tensor sending this gradient; helps avoid double backprop.\n",
    "        \"\"\"\n",
    "        # Only do anything if autograd is True\n",
    "        if self.autograd:\n",
    "\n",
    "            # If no gradient given, assume an array of ones\n",
    "            if grad is None:\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            # Decrement the child's expected gradient count\n",
    "            if grad_origin is not None:\n",
    "                if self.children[grad_origin.id] == 0:\n",
    "                    raise Exception(\"cannot backprop more than once from the same child\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            # Accumulate gradient in self.grad\n",
    "            if self.grad is None:\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # The incoming grad must not require grad\n",
    "            assert grad.autograd == False\n",
    "\n",
    "            # If there are parent Tensors and we've received all child grads OR \n",
    "            # grad_origin is None (meaning we called .backward() directly):\n",
    "            if (self.creators is not None and\n",
    "               (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "\n",
    "                # Distribute gradients to parents based on the operation\n",
    "\n",
    "                if self.creation_op == \"add\":\n",
    "                    # x + y => gradient passes unchanged to both x and y\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                elif self.creation_op == \"sub\":\n",
    "                    # x - y => gradient to x is +grad, gradient to y is -grad\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(-self.grad.data), self)\n",
    "\n",
    "                elif self.creation_op == \"mul\":\n",
    "                    # x * y => grad w.r.t x is grad * y, w.r.t y is grad * x\n",
    "                    new = self.grad * self.creators[1]  # replicate shape if needed\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)\n",
    "\n",
    "                elif self.creation_op == \"mm\":\n",
    "                    # Matrix multiply\n",
    "                    # if z = x.mm(y), then dz/dx = grad.mm(y^T), dz/dy = x^T.mm(grad)\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "\n",
    "                    # grad for x\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new, self)\n",
    "\n",
    "                    # grad for y\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new, self)\n",
    "\n",
    "                elif self.creation_op == \"transpose\":\n",
    "                    # If z = x.transpose(), the gradient is just grad.transpose()\n",
    "                    self.creators[0].backward(self.grad.transpose(), self)\n",
    "\n",
    "                elif \"sum\" in self.creation_op:\n",
    "                    # Sums along a dimension\n",
    "                    # e.g. \"sum_0\" => summation along dim=0\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    # Expand the gradient back to the original shape\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                            self.creators[0].data.shape[dim]), self)\n",
    "\n",
    "                elif \"expand\" in self.creation_op:\n",
    "                    # The inverse of 'expand' is summation along the dimension that was expanded\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim), self)\n",
    "                    \n",
    "                elif self.creation_op == \"neg\":\n",
    "                    # If z = -x => grad for x is -self.grad\n",
    "                    self.creators[0].backward(self.grad.__neg__(), self)\n",
    "                    \n",
    "    # ----------------------\n",
    "    # Overloaded Operators\n",
    "    # ----------------------\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        x + y => if both autograd, track the operation,\n",
    "        otherwise just do np addition.\n",
    "        \"\"\"\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"\n",
    "        -x => if autograd is True, store op=\"neg\".\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        \"\"\"\n",
    "        x - y => if autograd, creation_op=\"sub\", else just subtract data.\n",
    "        \"\"\"\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        x * y => if autograd, creation_op=\"mul\", else just multiply data.\n",
    "        \"\"\"\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "\n",
    "    # ----------------------\n",
    "    # Additional Methods\n",
    "    # ----------------------\n",
    "    \n",
    "    def sum(self, dim):\n",
    "        \"\"\"\n",
    "        Summation along a specified dimension.\n",
    "        creation_op=\"sum_<dim>\" so we know how to backprop properly.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim, copies):\n",
    "        \"\"\"\n",
    "        'expand' to replicate data along a given dimension 'copies' times.\n",
    "        Example: If shape is (2,3) and expand(0,4) => shape (4,2,3).\n",
    "        We record creation_op for correct backprop.\n",
    "        \"\"\"\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        # Insert the new axis at 'dim'\n",
    "        trans_cmd.insert(dim, len(self.data.shape))\n",
    "        \n",
    "        # Repeat the data 'copies' times, then reshape and transpose\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if self.autograd:\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        \"\"\"\n",
    "        If autograd, keep track with creation_op=\"transpose\".\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        \"\"\"\n",
    "        Matrix multiplication (dot product).\n",
    "        If autograd, record creation_op=\"mm\".\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self, x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    # ----------------------\n",
    "    # Utility / Display\n",
    "    # ----------------------\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# DEMONSTRATION OF BASIC USAGE\n",
    "# ------------------------------------------------------------------------------\n",
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([5,4,3,2,1], autograd=True)\n",
    "\n",
    "# Some operations\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e  # effectively: (a+b) + (b+c) = a + 2b + c\n",
    "\n",
    "# Backprop from f with gradient [1,1,1,1,1]\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "\n",
    "# Now b's gradient is from the 2*b in that expression => [2,2,2,2,2]\n",
    "print(b.grad.data == np.array([2,2,2,2,2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few Notes on Sum and Expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor(np.array([[1,2,3],\n",
    "                     [4,5,6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 15])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1, 1],\n",
       "        [2, 2, 2, 2],\n",
       "        [3, 3, 3, 3]],\n",
       "\n",
       "       [[4, 4, 4, 4],\n",
       "        [5, 5, 5, 5],\n",
       "        [6, 6, 6, 6]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand(dim=2, copies=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Use Autograd to Train a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previously we would train a model like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.066439994622396\n",
      "0.4959907791902341\n",
      "0.4180671892167177\n",
      "0.35298133007809646\n",
      "0.2972549636567376\n",
      "0.24923260381633278\n",
      "0.20785392075862477\n",
      "0.17231260916265181\n",
      "0.14193744536652994\n",
      "0.11613979792168387\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "target = np.array([[0],[1],[0],[1]])\n",
    "\n",
    "weights_0_1 = np.random.rand(2,3)\n",
    "weights_1_2 = np.random.rand(3,1)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # Predict\n",
    "    layer_1 = data.dot(weights_0_1)\n",
    "    layer_2 = layer_1.dot(weights_1_2)\n",
    "    \n",
    "    # Compare\n",
    "    diff = (layer_2 - target)\n",
    "    sqdiff = (diff * diff)\n",
    "    loss = sqdiff.sum(0) # mean squared error loss\n",
    "\n",
    "    # Learn: this is the backpropagation piece\n",
    "    layer_1_grad = diff.dot(weights_1_2.transpose())\n",
    "    weight_1_2_update = layer_1.transpose().dot(diff)\n",
    "    weight_0_1_update = data.transpose().dot(layer_1_grad)\n",
    "    \n",
    "    weights_1_2 -= weight_1_2_update * 0.1\n",
    "    weights_0_1 -= weight_0_1_update * 0.1\n",
    "    print(loss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image%20copy%209.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58128304]\n",
      "[0.48988149]\n",
      "[0.41375111]\n",
      "[0.34489412]\n",
      "[0.28210124]\n",
      "[0.2254484]\n",
      "[0.17538853]\n",
      "[0.1324231]\n",
      "[0.09682769]\n",
      "[0.06849361]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# 1) Prepare Data\n",
    "#    Here, 'data' is a 4x2 matrix representing the inputs:\n",
    "#        [0,0], [0,1], [1,0], [1,1]\n",
    "#    and 'target' is a 4x1 matrix of desired outputs:\n",
    "#        0, 1, 0, 1\n",
    "data = Tensor(np.array([[0,0],\n",
    "                        [0,1],\n",
    "                        [1,0],\n",
    "                        [1,1]]),\n",
    "              autograd=True)\n",
    "\n",
    "target = Tensor(np.array([[0],\n",
    "                          [1],\n",
    "                          [0],\n",
    "                          [1]]),\n",
    "                autograd=True)\n",
    "\n",
    "# 2) Initialize Weights\n",
    "#    We create a list 'w' holding two weight matrices:\n",
    "#    w[0]: shape (2,3)\n",
    "#    w[1]: shape (3,1)\n",
    "#    This effectively creates a small 2-layer network:\n",
    "#         input -> (2x3) -> hidden -> (3x1) -> output\n",
    "w = list()\n",
    "w.append(Tensor(np.random.rand(2,3), autograd=True))\n",
    "w.append(Tensor(np.random.rand(3,1), autograd=True))\n",
    "\n",
    "# 3) Training Loop\n",
    "for i in range(10):  # We'll do 10 training iterations\n",
    "\n",
    "    # a) Forward pass\n",
    "    #    data.mm(w[0]) => shape (4,3)\n",
    "    #    .mm(w[1]) => shape (4,1)\n",
    "    #    This is our network's prediction for each of the 4 samples\n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "    \n",
    "    # b) Compute Loss\n",
    "    #    Here, we use MSE-like loss: (pred - target)^2, then sum across all samples\n",
    "    #    shape of pred and target => (4,1)\n",
    "    #    (pred - target)*(pred - target) => elementwise square\n",
    "    #    sum(0) => summation across the \"batch\" dimension\n",
    "    loss = ((pred - target) * (pred - target)).sum(0)\n",
    "    \n",
    "    # c) Backpropagate\n",
    "    #    We call .backward(...) on the loss. Because the loss is shape (1,),\n",
    "    #    we pass a gradient of 1 to backprop for that single scalar.\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "\n",
    "    # d) Gradient Descent Updates\n",
    "    #    For each weight matrix w_ in w, we do:\n",
    "    #        w_.data -= (learning_rate * w_.grad.data)\n",
    "    #    Then set w_.grad.data to zero to clear out old gradients before next iteration\n",
    "    for w_ in w:\n",
    "        w_.data -= w_.grad.data * 0.1\n",
    "        w_.grad.data *= 0  # reset gradient to zero\n",
    "\n",
    "    # e) Print the current loss for monitoring\n",
    "    #    'loss' is a Tensor of shape (1,)\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Adding Automatic Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    \n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "        \n",
    "    def step(self, zero=True):\n",
    "        \n",
    "        for p in self.parameters:\n",
    "            \n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            \n",
    "            if(zero):\n",
    "                p.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58128304]\n",
      "[0.48988149]\n",
      "[0.41375111]\n",
      "[0.34489412]\n",
      "[0.28210124]\n",
      "[0.2254484]\n",
      "[0.17538853]\n",
      "[0.1324231]\n",
      "[0.09682769]\n",
      "[0.06849361]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "w = list()\n",
    "w.append(Tensor(np.random.rand(2,3), autograd=True))\n",
    "w.append(Tensor(np.random.rand(3,1), autograd=True))\n",
    "\n",
    "optim = SGD(parameters=w, alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    # Predict\n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "    \n",
    "    # Compare\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    \n",
    "    # Learn\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: Adding Support for Layer Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image%20copy%208.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"\n",
    "    A base Layer class that any specific layer (e.g., Linear) can inherit from.\n",
    "    Typically, it keeps track of a list of parameters,\n",
    "    and has a method to fetch them for optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # We'll store layer parameters (weights, biases, etc.) in a list\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Return the list of parameters for this layer.\n",
    "        This can be used by an optimizer to update them.\n",
    "        \"\"\"\n",
    "        return self.parameters\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    \"\"\"\n",
    "    A 'Linear' layer (also called a fully connected or dense layer).\n",
    "    y = xW + b\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_inputs : int\n",
    "            The dimensionality of the input features.\n",
    "        n_outputs : int\n",
    "            The number of output features for this linear transform.\n",
    "\n",
    "        We'll initialize:\n",
    "          - self.weight: a Tensor of shape (n_inputs, n_outputs)\n",
    "          - self.bias  : a Tensor of shape (n_outputs,)\n",
    "        \"\"\"\n",
    "        super().__init__()  # Initialize the base Layer\n",
    "\n",
    "        # Using a random initialization:\n",
    "        # We scale by sqrt(2/n_inputs) (Kaiming-like init) to help with stable training\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0 / n_inputs)\n",
    "        \n",
    "        # Turn these NumPy arrays into Tensors that track gradients\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        # Register these Tensors as parameters of this layer\n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass for a linear layer:\n",
    "        output = input.mm(self.weight) + self.bias\n",
    "\n",
    "        We expand the bias across dimension 0 to match the batch size\n",
    "        if input is (batch_size, n_inputs).\n",
    "        \"\"\"\n",
    "        return input.mm(self.weight) + self.bias.expand(0, len(input.data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 10: Layers Which Contain Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image%20copy%207.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.33428272]\n",
      "[0.06743796]\n",
      "[0.0521849]\n",
      "[0.04079507]\n",
      "[0.03184365]\n",
      "[0.02479336]\n",
      "[0.01925443]\n",
      "[0.01491699]\n",
      "[0.01153118]\n",
      "[0.00889602]\n"
     ]
    }
   ],
   "source": [
    "class Sequential(Layer):\n",
    "    \"\"\"\n",
    "    A container that holds multiple layers in sequence.\n",
    "    When you call 'forward', it feeds data through each layer in turn.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        self.layers = layers  # A list of Layer objects (e.g., Linear, etc.)\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\"\n",
    "        Append another layer to the end of the sequence.\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Pass the 'input' through each layer in self.layers sequentially.\n",
    "        The output of each layer becomes the input to the next.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Gather parameters from each layer and return them in a single list.\n",
    "        This allows an optimizer to easily update all layer parameters together.\n",
    "        \"\"\"\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()  # extend the list\n",
    "        return params\n",
    "    \n",
    "\n",
    "# ------------------------\n",
    "# DEMO TRAINING SCRIPT\n",
    "# ------------------------\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# 1) Define the dataset\n",
    "#    data: 4 samples with 2 features each\n",
    "#    target: 4 samples with 1 label each\n",
    "data = Tensor(np.array([[0,0],\n",
    "                        [0,1],\n",
    "                        [1,0],\n",
    "                        [1,1]]),\n",
    "              autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]),\n",
    "                autograd=True)\n",
    "\n",
    "# 2) Create a model\n",
    "#    'Sequential' of two 'Linear' layers:\n",
    "#     - first takes 2 -> 3\n",
    "#     - second takes 3 -> 1\n",
    "model = Sequential([\n",
    "    Linear(2,3),\n",
    "    Linear(3,1)\n",
    "])\n",
    "\n",
    "# 3) Create an optimizer (like SGD).\n",
    "#    'model.get_parameters()' returns all layer parameters (weights, biases).\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
    "\n",
    "# 4) Training loop\n",
    "for i in range(10):\n",
    "    \n",
    "    # a) Forward pass: pass 'data' through the model\n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    # b) Compute loss (Mean Squared Error):\n",
    "    #    (pred - target)^2, then sum across all samples\n",
    "    loss = ((pred - target) * (pred - target)).sum(0)\n",
    "    \n",
    "    # c) Backprop: compute gradients w.r.t. all model parameters\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    \n",
    "    # d) Update parameters using the optimizer\n",
    "    optim.step()\n",
    "    \n",
    "    # e) Print the loss after each iteration\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 11: Loss Function Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image%20copy%206.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.33428272]\n",
      "[0.06743796]\n",
      "[0.0521849]\n",
      "[0.04079507]\n",
      "[0.03184365]\n",
      "[0.02479336]\n",
      "[0.01925443]\n",
      "[0.01491699]\n",
      "[0.01153118]\n",
      "[0.00889602]\n"
     ]
    }
   ],
   "source": [
    "class MSELoss(Layer):\n",
    "    \"\"\"\n",
    "    A layer that computes the Mean Squared Error (MSE) loss\n",
    "    between predictions (pred) and targets (target).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()  # We don't have any parameters in this layer.\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Given predicted values 'pred' and true values 'target',\n",
    "        return the sum of squared errors: ((pred - target)^2).sum(0)\n",
    "\n",
    "        shape of pred, target => e.g. (batch_size, num_outputs)\n",
    "        we sum across the batch dimension (dim=0) here.\n",
    "        \"\"\"\n",
    "        return ((pred - target) * (pred - target)).sum(0)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# 1) Create a small dataset for a simple binary classification-like task\n",
    "#    data is 4 samples of dimension 2 (like an XOR pattern),\n",
    "#    target is 4 samples of dimension 1 indicating the label\n",
    "data = Tensor(np.array([[0,0],\n",
    "                        [0,1],\n",
    "                        [1,0],\n",
    "                        [1,1]]),\n",
    "              autograd=True)\n",
    "target = Tensor(np.array([[0],\n",
    "                          [1],\n",
    "                          [0],\n",
    "                          [1]]),\n",
    "                autograd=True)\n",
    "\n",
    "# 2) Build a model using Sequential:\n",
    "#    - first layer: Linear(2 -> 3)\n",
    "#    - second layer: Linear(3 -> 1)\n",
    "model = Sequential([\n",
    "    Linear(2,3),\n",
    "    Linear(3,1)\n",
    "])\n",
    "\n",
    "# 3) Define the loss function\n",
    "criterion = MSELoss()\n",
    "\n",
    "# 4) Define an optimizer (e.g., SGD) to update model's parameters\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
    "\n",
    "# 5) Training loop\n",
    "for i in range(10):\n",
    "    \n",
    "    # a) Forward pass: get model predictions\n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    # b) Compute loss via the MSELoss layer\n",
    "    loss = criterion.forward(pred, target)\n",
    "    \n",
    "    # c) Backprop: compute gradients w.r.t. model parameters\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    \n",
    "    # d) Update parameters in the model\n",
    "    optim.step()\n",
    "    \n",
    "    # e) Print the current loss\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 12: Non-linearity Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image%20copy%205.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \"\"\"\n",
    "    A Tensor class supporting automatic differentiation\n",
    "    for various operations: +, -, *, matrix multiply (mm),\n",
    "    sum/expand, transpose, negation, and the nonlinearities\n",
    "    'sigmoid' and 'tanh'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array-like\n",
    "            NumPy array holding the numerical data.\n",
    "        autograd : bool\n",
    "            If True, we track gradients.\n",
    "        creators : list of Tensor or None\n",
    "            Parent Tensors in the computation graph.\n",
    "        creation_op : str or None\n",
    "            The name of the operation that created this Tensor\n",
    "            (e.g., 'add', 'neg', 'sigmoid', etc.).\n",
    "        id : int or None\n",
    "            Unique ID for this Tensor. If None, a random one is assigned.\n",
    "        \"\"\"\n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "\n",
    "        if id is None:\n",
    "            self.id = np.random.randint(0, 100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}  # track how many gradients to expect from child Tensors\n",
    "        \n",
    "        # If this Tensor has parents, register it as a child in those parent Tensors\n",
    "        if creators is not None:\n",
    "            for c in creators:\n",
    "                if self.id not in c.children:\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        \"\"\"\n",
    "        Check if all child Tensors have sent their gradients to this Tensor.\n",
    "        If any child's count is not zero, we're still waiting for that child's gradient.\n",
    "        \"\"\"\n",
    "        for id, cnt in self.children.items():\n",
    "            if cnt != 0:\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        \"\"\"\n",
    "        Backpropagate from this Tensor, distributing gradients \n",
    "        to its parents based on the operation that created it.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        grad : Tensor or None\n",
    "            The incoming gradient from a child or final output.\n",
    "            If None, defaults to a Tensor of ones with the same shape as self.\n",
    "        grad_origin : Tensor or None\n",
    "            Which child Tensor is sending this gradient (helps detect double backprop).\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            # If no incoming gradient, assume it's ones for each element\n",
    "            if grad is None:\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            # Decrement the child's expected gradient count\n",
    "            if grad_origin is not None:\n",
    "                if self.children[grad_origin.id] == 0:\n",
    "                    raise Exception(\"Cannot backprop more than once from the same child.\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            # Accumulate gradient in self.grad\n",
    "            if self.grad is None:\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # The incoming gradient must not have its own gradient\n",
    "            # i.e., it should be a leaf gradient in the context of our system\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # If all children have sent gradients (or grad_origin is None => direct call),\n",
    "            # we proceed to backprop to our parents\n",
    "            if (self.creators is not None and\n",
    "                (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "\n",
    "                # Depending on which operation created this Tensor,\n",
    "                # apply the chain rule to pass gradients back to parents.\n",
    "                \n",
    "                if self.creation_op == \"add\":\n",
    "                    # x + y => partial derivative wrt x, y is just 1\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                elif self.creation_op == \"sub\":\n",
    "                    # x - y => partial wrt x is +1, wrt y is -1\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor((-self.grad).data), self)\n",
    "\n",
    "                elif self.creation_op == \"mul\":\n",
    "                    # x * y => grad wrt x is grad * y, wrt y is grad * x\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                elif self.creation_op == \"mm\":\n",
    "                    # matrix multiply\n",
    "                    # if z = x.mm(y), then dz/dx = grad.mm(y^T), dz/dy = x^T.mm(grad)\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                elif self.creation_op == \"transpose\":\n",
    "                    # if z = x.transpose(), then grad wrt x is grad.transpose()\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                elif \"sum\" in self.creation_op:\n",
    "                    # sum_<dim>, e.g. sum_0\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    shape_dim = self.creators[0].data.shape[dim]\n",
    "                    # expand the gradient back to the shape of the original\n",
    "                    self.creators[0].backward(self.grad.expand(dim, shape_dim))\n",
    "\n",
    "                elif \"expand\" in self.creation_op:\n",
    "                    # If we expanded along a dimension, the inverse is sum along that dimension\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                elif self.creation_op == \"neg\":\n",
    "                    # z = -x => derivative wrt x is -1\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                elif self.creation_op == \"sigmoid\":\n",
    "                    # z = sigmoid(x) => dz/dx = z*(1-z)\n",
    "                    # chain rule => grad * z*(1-z)\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                elif self.creation_op == \"tanh\":\n",
    "                    # z = tanh(x) => dz/dx = 1 - z^2\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                    \n",
    "    # ----------------------\n",
    "    # Overloaded Operators\n",
    "    # ----------------------\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(-self.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(-self.data)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "\n",
    "    # -----------\n",
    "    # Sum, Expand\n",
    "    # -----------\n",
    "\n",
    "    def sum(self, dim):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\" + str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim, copies):\n",
    "        \"\"\"\n",
    "        Expand the Tensor along a given dimension 'copies' times.\n",
    "        \"\"\"\n",
    "        trans_cmd = list(range(0, len(self.data.shape)))\n",
    "        # Insert new axis at the 'dim' index\n",
    "        trans_cmd.insert(dim, len(self.data.shape))\n",
    "\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies])\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if self.autograd:\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\" + str(dim))\n",
    "        return Tensor(new_data)\n",
    "\n",
    "    # -----------\n",
    "    # Transpose, Matrix Multiply\n",
    "    # -----------\n",
    "\n",
    "    def transpose(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        \"\"\"\n",
    "        Matrix multiply: self.data.dot(x.data)\n",
    "        If autograd is True, record creation_op=\"mm\" and remember both parents\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self, x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    # -----------\n",
    "    # Nonlinearities\n",
    "    # -----------\n",
    "\n",
    "    def sigmoid(self):\n",
    "        \"\"\"\n",
    "        z = 1/(1 + e^-x)\n",
    "        creation_op=\"sigmoid\", so we can do chain rule in backward.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        \"\"\"\n",
    "        z = tanh(x), creation_op=\"tanh\".\n",
    "        We'll do chain rule: dz/dx = 1 - z^2 in backprop.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    # -----------\n",
    "    # Printing / Debug\n",
    "    # -----------\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "\n",
    "\n",
    "# -----------\n",
    "# Nonlinearity Layers\n",
    "# -----------\n",
    "class Tanh(Layer):\n",
    "    \"\"\"\n",
    "    A layer that applies tanh to its input via input.tanh().\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"\n",
    "    A layer that applies sigmoid to its input via input.sigmoid().\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.06372865]\n",
      "[0.75148144]\n",
      "[0.57384259]\n",
      "[0.39574294]\n",
      "[0.2482279]\n",
      "[0.15515294]\n",
      "[0.10423398]\n",
      "[0.07571169]\n",
      "[0.05837623]\n",
      "[0.04700013]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Linear(2,3), Tanh(), Linear(3,1), Sigmoid()])\n",
    "criterion = MSELoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=1)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    # Compare\n",
    "    loss = criterion.forward(pred, target)\n",
    "    \n",
    "    # Learn\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 13: The Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        # this random initialiation style is just a convention from word2vec\n",
    "        self.weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 14: Add Indexing to Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image%20copy%204.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \"\"\"\n",
    "    A Tensor class supporting auto-differentiation for various operations:\n",
    "    +, -, *, matrix multiply (mm), sum/expand, transpose, neg, sigmoid, tanh,\n",
    "    and now index_select for slicing by index while maintaining a computation graph.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array-like\n",
    "            The underlying NumPy array data.\n",
    "        autograd : bool\n",
    "            Whether to track gradients for this Tensor.\n",
    "        creators : list of Tensors or None\n",
    "            Parent Tensors in the computation graph (used in backprop).\n",
    "        creation_op : str or None\n",
    "            The operation name that created this Tensor (e.g. 'add', 'mm', 'index_select').\n",
    "        id : int or None\n",
    "            Unique identifier for this Tensor (optional; random if not provided).\n",
    "        \"\"\"\n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        \n",
    "        # Assign or generate a unique ID\n",
    "        if id is None:\n",
    "            self.id = np.random.randint(0, 100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        \n",
    "        # Dictionary to track how many gradient \"signals\" this Tensor\n",
    "        # should receive from child nodes in the graph.\n",
    "        self.children = {}\n",
    "        \n",
    "        # If we have creators, register this Tensor as a child for them\n",
    "        if creators is not None:\n",
    "            for c in creators:\n",
    "                if self.id not in c.children:\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        \"\"\"\n",
    "        Check if all expected gradients from child Tensors have arrived.\n",
    "        Returns True if each child's count is 0 (i.e., no more grads pending).\n",
    "        \"\"\"\n",
    "        for id, cnt in self.children.items():\n",
    "            if cnt != 0:\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        \"\"\"\n",
    "        Backpropagate from this Tensor, distributing gradients\n",
    "        to its parents according to the operation that created it.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grad : Tensor or None\n",
    "            The gradient from further down the graph. If None, we default to\n",
    "            a Tensor of ones matching self.data's shape.\n",
    "        grad_origin : Tensor or None\n",
    "            Which child Tensor the gradient is coming from (helps prevent repeated backprops).\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "\n",
    "            # If no gradient provided, assume a Tensor of ones\n",
    "            if grad is None:\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            # Decrement the child's expected gradient count\n",
    "            if grad_origin is not None:\n",
    "                if self.children[grad_origin.id] == 0:\n",
    "                    raise Exception(\"cannot backprop more than once from the same child\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            # Accumulate the incoming gradient in self.grad\n",
    "            if self.grad is None:\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # The incoming gradient must be a leaf (i.e., not requiring autograd itself)\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # If all children have sent gradients (or grad_origin is None => direct call),\n",
    "            # we can continue backprop to our parent(s).\n",
    "            if (self.creators is not None and\n",
    "                (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "\n",
    "                # Branch logic depending on the operation type\n",
    "                if self.creation_op == \"add\":\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if self.creation_op == \"sub\":\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor((-self.grad).data), self)\n",
    "\n",
    "                if self.creation_op == \"mul\":\n",
    "                    # x * y => grad wrt x is grad*y, wrt y is grad*x\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if self.creation_op == \"mm\":\n",
    "                    # If z = x.mm(y), then:\n",
    "                    # dz/dx = grad.mm(y^T), dz/dy = x^T.mm(grad)\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if self.creation_op == \"transpose\":\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if (\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(\n",
    "                        self.grad.expand(dim, self.creators[0].data.shape[dim])\n",
    "                    )\n",
    "\n",
    "                if (\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if self.creation_op == \"neg\":\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if self.creation_op == \"sigmoid\":\n",
    "                    # z = sigmoid(x) => dz/dx = z*(1 - z)\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                if self.creation_op == \"tanh\":\n",
    "                    # z = tanh(x) => dz/dx = 1 - z^2\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "                if self.creation_op == \"index_select\":\n",
    "                    # If y = x.index_select(indices),\n",
    "                    # the gradient is placed back into x at those specific indices.\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "\n",
    "    # ----------------------\n",
    "    # Overloaded operators\n",
    "    # ----------------------\n",
    "    def __add__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(-self.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(-self.data)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "\n",
    "    # ----------------------\n",
    "    # Summation / Expand\n",
    "    # ----------------------\n",
    "    def sum(self, dim):\n",
    "        \"\"\"\n",
    "        Sum along the specified dimension 'dim'.\n",
    "        creation_op = \"sum_<dim>\" so we can reconstruct shapes in backprop.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\" + str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim, copies):\n",
    "        \"\"\"\n",
    "        Expand (repeat) the data along dimension 'dim' 'copies' times.\n",
    "        Used in e.g. broadcasting or matching shapes.\n",
    "        \"\"\"\n",
    "        trans_cmd = list(range(len(self.data.shape)))\n",
    "        trans_cmd.insert(dim, len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies])\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if self.autograd:\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\" + str(dim))\n",
    "        return Tensor(new_data)\n",
    "\n",
    "    # ----------------------\n",
    "    # Transpose / MatMul\n",
    "    # ----------------------\n",
    "    def transpose(self):\n",
    "        \"\"\"\n",
    "        Returns a transposed version of this Tensor's data.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        \"\"\"\n",
    "        Matrix multiply (dot product).\n",
    "        If autograd, creation_op='mm' to handle backprop correctly.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self, x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "\n",
    "    # ----------------------\n",
    "    # Nonlinearities\n",
    "    # ----------------------\n",
    "    def sigmoid(self):\n",
    "        \"\"\"\n",
    "        Sigmoid activation => 1/(1 + e^-data).\n",
    "        creation_op='sigmoid' for chain rule derivative = z*(1 - z).\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        \"\"\"\n",
    "        Tanh activation => np.tanh(data).\n",
    "        creation_op='tanh' so derivative can be handled => 1 - z^2.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "\n",
    "    # ----------------------\n",
    "    # Indexing\n",
    "    # ----------------------\n",
    "    def index_select(self, indices):\n",
    "        \"\"\"\n",
    "        Return a new Tensor by indexing this Tensor at the specified 'indices'.\n",
    "        If autograd is enabled, record 'index_select' so we can route gradient\n",
    "        back to the correct positions in the original Tensor.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "\n",
    "    # ----------------------\n",
    "    # String/Debug\n",
    "    # ----------------------\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "\n",
    "\n",
    "# Example Nonlinear Layers (for completeness)\n",
    "class Tanh(Layer):\n",
    "    \"\"\"\n",
    "    A layer that applies tanh() to its input.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"\n",
    "    A layer that applies sigmoid() to its input.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(np.eye(5), autograd=True)\n",
    "x.index_select(Tensor([[1,2,3],[2,3,4]])).backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 15: The Embedding Layer (revisited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        # this random initialiation style is just a convention from word2vec\n",
    "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98874126]\n",
      "[0.6658868]\n",
      "[0.45639889]\n",
      "[0.31608168]\n",
      "[0.2260925]\n",
      "[0.16877423]\n",
      "[0.13120515]\n",
      "[0.10555487]\n",
      "[0.08731868]\n",
      "[0.07387834]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "embed = Embedding(5,3)\n",
    "model = Sequential([embed, Tanh(), Linear(3,1), Sigmoid()])\n",
    "criterion = MSELoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.5)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    # Compare\n",
    "    loss = criterion.forward(pred, target)\n",
    "    \n",
    "    # Learn\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 16: The Cross Entropy Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image%20copy%203.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \"\"\"\n",
    "    A Tensor class supporting auto-differentiation for various operations:\n",
    "    +, -, *, matrix multiply (mm), sum/expand, transpose, neg, sigmoid, tanh,\n",
    "    index_select, and now cross_entropy for classification tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array-like\n",
    "            NumPy array storing numerical values for this Tensor.\n",
    "        autograd : bool\n",
    "            If True, we track gradients (auto-diff).\n",
    "        creators : list or None\n",
    "            Parent Tensors involved in creating this one.\n",
    "        creation_op : str or None\n",
    "            The operation name that led to this Tensor's creation (e.g., \"add\", \"cross_entropy\").\n",
    "        id : int or None\n",
    "            Unique ID for the Tensor. If None, a random ID is assigned.\n",
    "        \"\"\"\n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "\n",
    "        # Assign or generate an ID\n",
    "        if id is None:\n",
    "            self.id = np.random.randint(0, 100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        # children[id] = how many grads we expect from that child\n",
    "        self.children = {}\n",
    "        \n",
    "        # If we have parent Tensors, register this as a child to them\n",
    "        if creators is not None:\n",
    "            for c in creators:\n",
    "                if self.id not in c.children:\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        \"\"\"\n",
    "        Check if we've received gradients from all child Tensors.\n",
    "        Returns True if none are pending.\n",
    "        \"\"\"\n",
    "        for id, cnt in self.children.items():\n",
    "            if cnt != 0:\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        \"\"\"\n",
    "        The main backprop method. Takes an incoming gradient (grad) from a child,\n",
    "        accumulates it in self.grad, and then, if all child grads are in,\n",
    "        applies chain rule logic to pass gradients back to parent Tensors.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "\n",
    "            # Default to a Tensor of ones if no grad is provided\n",
    "            if grad is None:\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            # Decrement child's expected gradient count\n",
    "            if grad_origin is not None:\n",
    "                if self.children[grad_origin.id] == 0:\n",
    "                    raise Exception(\"cannot backprop more than once from the same child\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            # Accumulate gradient in self.grad\n",
    "            if self.grad is None:\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # The incoming gradient should not require grad\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # If all child gradients are accounted for or there's a direct call:\n",
    "            if (self.creators is not None\n",
    "                and (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "\n",
    "                # Apply chain rule depending on creation_op\n",
    "                if self.creation_op == \"add\":\n",
    "                    # grad splits to both parents unchanged\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                elif self.creation_op == \"sub\":\n",
    "                    # x - y => partial wrt x is +grad, wrt y is -grad\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor((-self.grad).data), self)\n",
    "\n",
    "                elif self.creation_op == \"mul\":\n",
    "                    # x * y => partial wrt x is grad * y, wrt y is grad * x\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                elif self.creation_op == \"mm\":\n",
    "                    # z = x.mm(y)\n",
    "                    # partial wrt x => grad.mm(y^T), wrt y => x^T.mm(grad)\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                elif self.creation_op == \"transpose\":\n",
    "                    # partial wrt x => grad.transpose()\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                elif (\"sum\" in self.creation_op):\n",
    "                    # sum_<dim>\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    expanded = self.grad.expand(dim, self.creators[0].data.shape[dim])\n",
    "                    self.creators[0].backward(expanded)\n",
    "\n",
    "                elif (\"expand\" in self.creation_op):\n",
    "                    # partial wrt x => grad.sum(dim)\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                elif self.creation_op == \"neg\":\n",
    "                    # partial wrt x => -grad\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                elif self.creation_op == \"sigmoid\":\n",
    "                    # z = sigmoid(x), partial wrt x => grad * z*(1-z)\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                elif self.creation_op == \"tanh\":\n",
    "                    # z = tanh(x), partial wrt x => grad * (1 - z^2)\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "                elif self.creation_op == \"index_select\":\n",
    "                    # route grad back to correct indices\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = self.grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                elif self.creation_op == \"cross_entropy\":\n",
    "                    # Cross entropy with softmax:\n",
    "                    # partial wrt input => (softmax_output - target_dist)\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "\n",
    "    # ----------------------\n",
    "    # Overloaded Operators\n",
    "    # ----------------------\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(-self.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(-self.data)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "\n",
    "    # ----------------------\n",
    "    # Summation / Expand\n",
    "    # ----------------------\n",
    "    def sum(self, dim):\n",
    "        \"\"\"\n",
    "        Summation along dimension 'dim'. We'll store 'sum_<dim>' in creation_op\n",
    "        to handle backprop shape expansions.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\" + str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim, copies):\n",
    "        \"\"\"\n",
    "        Expand (repeat) data along dimension 'dim' 'copies' times,\n",
    "        helpful for broadcasting-like operations.\n",
    "        \"\"\"\n",
    "        trans_cmd = list(range(len(self.data.shape)))\n",
    "        trans_cmd.insert(dim, len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies])\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if self.autograd:\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\" + str(dim))\n",
    "        return Tensor(new_data)\n",
    "\n",
    "    # ----------------------\n",
    "    # Transpose / MatMul\n",
    "    # ----------------------\n",
    "    def transpose(self):\n",
    "        \"\"\"\n",
    "        Return a transposed view of this Tensor's data.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        \"\"\"\n",
    "        Matrix multiply: self.data.dot(x.data).\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self, x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "\n",
    "    # ----------------------\n",
    "    # Nonlinearities\n",
    "    # ----------------------\n",
    "    def sigmoid(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    # ----------------------\n",
    "    # Indexing\n",
    "    # ----------------------\n",
    "    def index_select(self, indices):\n",
    "        \"\"\"\n",
    "        Return a new Tensor by indexing self at positions given by 'indices'.\n",
    "        \"\"\"\n",
    "        if self.autograd:\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "\n",
    "    # ----------------------\n",
    "    # Cross Entropy\n",
    "    # ----------------------\n",
    "    def cross_entropy(self, target_indices):\n",
    "        \"\"\"\n",
    "        Cross entropy loss with softmax:\n",
    "        1) Compute softmax of self.data\n",
    "        2) Gather targets from 'target_indices'\n",
    "        3) Compute loss = - log(prob_of_true_class), averaged over batch\n",
    "        4) In backprop, we do (softmax_output - one_hot_targets)\n",
    "        \"\"\"\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape) - 1,\n",
    "                                       keepdims=True)\n",
    "        \n",
    "        # Flattening target indices for simple indexing\n",
    "        t = target_indices.data.flatten()\n",
    "        # Reshape the softmax output to match: (batch_size, num_classes)\n",
    "        p = softmax_output.reshape(len(t), -1)\n",
    "        # Build one-hot target distribution\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        \n",
    "        # Cross entropy: -sum( log(prob_of_true_class) ), average over batch\n",
    "        loss = -(np.log(p) * target_dist).sum(1).mean()\n",
    "    \n",
    "        if self.autograd:\n",
    "            out = Tensor(loss,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"cross_entropy\")\n",
    "            # Store the softmax distribution and the target distribution\n",
    "            # so we can do backprop: grad = (softmax_output - target_dist)\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "\n",
    "        # If not autograd, return a plain Tensor with no backprop references\n",
    "        return Tensor(loss)\n",
    "        \n",
    "    # ----------------------\n",
    "    # String / Debug\n",
    "    # ----------------------\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "\n",
    "\n",
    "# Optional Nonlinear Layers (if needed)\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3885032434928422\n",
      "0.9558181509266036\n",
      "0.6823083585795604\n",
      "0.509525996749312\n",
      "0.39574491472895856\n",
      "0.3175252728534828\n",
      "0.26172228619642157\n",
      "0.22061283923954234\n",
      "0.18946427334830074\n",
      "0.16527389263866676\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "# data indices\n",
    "data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "\n",
    "# target indices\n",
    "target = Tensor(np.array([0,1,0,1]), autograd=True)\n",
    "\n",
    "model = Sequential([Embedding(3,3), Tanh(), Linear(3,4)])\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    # Compare\n",
    "    loss = criterion.forward(pred, target)\n",
    "    \n",
    "    # Learn\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 17: The Recurrent Neural Network Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image%20copy%202.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(Layer):\n",
    "    \"\"\"\n",
    "    A single RNN cell that processes one time step of the data.\n",
    "    It takes the current input and the previous hidden state,\n",
    "    and outputs the new hidden state along with an output vector.\n",
    "    \n",
    "    Formally:\n",
    "      hidden_t = activation( input_t * W_ih + hidden_(t-1) * W_hh )\n",
    "      output_t = hidden_t * W_ho\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_inputs : int\n",
    "            Dimensionality of each input vector.\n",
    "        n_hidden : int\n",
    "            Size of the hidden state (number of hidden units).\n",
    "        n_output : int\n",
    "            Size of the output vector for each time step.\n",
    "        activation : str\n",
    "            Which nonlinear function to apply in the hidden update ('sigmoid' or 'tanh').\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        # Choose either Sigmoid or Tanh for the hidden state activation\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found. Use 'sigmoid' or 'tanh'.\")\n",
    "\n",
    "        # Linear transformations:\n",
    "        # w_ih: transforms input vector to hidden dimension\n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        # w_hh: transforms previous hidden to next hidden\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        # w_ho: transforms hidden state to output dimension\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        # Add all parameters (weights and biases) to self.parameters for easy access\n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass for one time step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : Tensor\n",
    "            The input vector for this time step (batch_size, n_inputs).\n",
    "        hidden : Tensor\n",
    "            The previous hidden state (batch_size, n_hidden).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        output : Tensor\n",
    "            The output vector for this time step (batch_size, n_output).\n",
    "        new_hidden : Tensor\n",
    "            The new hidden state (batch_size, n_hidden).\n",
    "        \"\"\"\n",
    "        # Transform the previous hidden state: hidden_(t-1) => next hidden\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        \n",
    "        # Transform the input: input_t => hidden dimension\n",
    "        from_input = self.w_ih.forward(input)\n",
    "        \n",
    "        # Combine them by addition, then apply the activation\n",
    "        combined = from_input + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "\n",
    "        # Finally, transform hidden => output dimension\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        \n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        \"\"\"\n",
    "        Initialize a zero hidden state for a given batch size.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hidden : Tensor\n",
    "            A (batch_size, n_hidden)-shaped Tensor of zeros.\n",
    "        \"\"\"\n",
    "        return Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/image.png)\n",
    "\n",
    "![](./images/image%20copy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4767529043186908 % Correct: 0.0\n",
      "Loss: 0.17431028405025537 % Correct: 0.27\n",
      "Loss: 0.1614921436370119 % Correct: 0.33\n",
      "Loss: 0.14318897302930267 % Correct: 0.34\n",
      "Loss: 0.1368414224697062 % Correct: 0.37\n",
      "Context: - mary moved to the \n",
      "True: bathroom.\n",
      "Pred: garden.\n"
     ]
    }
   ],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1) DATA LOADING AND PREPARATION\n",
    "# --------------------------------------------------------------------\n",
    "f = open('tasksv11/en/qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# 'raw' contains lines of text in a bAbI format.\n",
    "# We take only the first 1000 lines for demonstration.\n",
    "\n",
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    # Lowercase each line, strip newline, split on spaces.\n",
    "    # Then skip the first token (the bAbI line number) with [1:].\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
    "\n",
    "# 'tokens' is now a list of lists of words.\n",
    "\n",
    "# We'll ensure each line has length 6 (arbitrary cutoff),\n",
    "# padding with '-' if it's shorter.\n",
    "new_tokens = list()\n",
    "for line in tokens:\n",
    "    new_tokens.append(['-'] * (6 - len(line)) + line)\n",
    "tokens = new_tokens\n",
    "\n",
    "# 2) BUILD VOCABULARY\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# Build a mapping from word -> integer index\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "# A small helper function to convert words to indices\n",
    "def words2indices(sentence):\n",
    "    return [word2index[w] for w in sentence]\n",
    "\n",
    "# Convert every line in 'tokens' to a list of indices\n",
    "indices = []\n",
    "for line in tokens:\n",
    "    idx = [word2index[w] for w in line]\n",
    "    indices.append(idx)\n",
    "\n",
    "# Convert to a NumPy array for easy slicing\n",
    "data = np.array(indices)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3) BUILD MODEL COMPONENTS (Embedding + RNN + Criterion)\n",
    "# --------------------------------------------------------------------\n",
    "# We'll embed each token index into a 16-dimensional vector\n",
    "embed = Embedding(vocab_size=len(vocab), dim=16)\n",
    "\n",
    "# RNNCell: input dimension 16, hidden dimension 16, output dimension = vocab size\n",
    "model = RNNCell(n_inputs=16, n_hidden=16, n_output=len(vocab))\n",
    "\n",
    "# Cross-entropy loss for classification\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Define an optimizer (Stochastic Gradient Descent)\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(),\n",
    "            alpha=0.05)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4) TRAINING LOOP\n",
    "# --------------------------------------------------------------------\n",
    "for iter in range(1000):\n",
    "    batch_size = 100\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Initialize hidden state to zeros for batch_size=100\n",
    "    hidden = model.init_hidden(batch_size=batch_size)\n",
    "\n",
    "    # We'll process the first 5 tokens in each line, step by step\n",
    "    for t in range(5):\n",
    "        # Take token at position t for each line => shape (batch_size,)\n",
    "        input = Tensor(data[0:batch_size, t], autograd=True)\n",
    "        \n",
    "        # Convert these indices into embeddings => shape (batch_size, 16)\n",
    "        rnn_input = embed.forward(input=input)\n",
    "        \n",
    "        # RNN forward: produce an output and the new hidden state\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "    # After feeding 5 tokens, we want to predict the 6th (t+1).\n",
    "    # So the target is the token at column 't+1'\n",
    "    target = Tensor(data[0:batch_size, t+1], autograd=True)\n",
    "    \n",
    "    # Compute cross-entropy loss (with softmax inside cross_entropy)\n",
    "    loss = criterion.forward(output, target)\n",
    "    \n",
    "    # Backprop: This calls .backward() all the way through the RNN + embedding\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters with a step of gradient descent\n",
    "    optim.step()\n",
    "    \n",
    "    total_loss += loss.data\n",
    "\n",
    "    # Periodically print training stats\n",
    "    if (iter % 200) == 0:\n",
    "        # 'p_correct' = average fraction of correct predictions in this batch\n",
    "        p_correct = (target.data == np.argmax(output.data, axis=1)).mean()\n",
    "        print(\"Loss:\", total_loss / (len(data)/batch_size), \"% Correct:\", p_correct)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5) TEST / DEMO\n",
    "# --------------------------------------------------------------------\n",
    "batch_size = 1\n",
    "hidden = model.init_hidden(batch_size=batch_size)\n",
    "\n",
    "# We'll feed the first 5 tokens of the line (data[0]) one by one\n",
    "for t in range(5):\n",
    "    input = Tensor(data[0:batch_size, t], autograd=True)\n",
    "    rnn_input = embed.forward(input=input)\n",
    "    output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "# The last token in that line is the target\n",
    "target = Tensor(data[0:batch_size, t+1], autograd=True)\n",
    "loss = criterion.forward(output, target)\n",
    "\n",
    "# Print out the context, the true next token, and the predicted token\n",
    "ctx = \"\"\n",
    "for idx in data[0:batch_size][0][0:-1]:\n",
    "    ctx += vocab[idx] + \" \"\n",
    "\n",
    "print(\"Context:\", ctx)\n",
    "print(\"True:\", vocab[target.data[0]])\n",
    "print(\"Pred:\", vocab[output.data.argmax()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
