{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download & Preprocess the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download reviews.txt and labels.txt from here: \n",
    "# https://github.com/udacity/deep-learning/tree/master/sentiment-network\n",
    "\n",
    "def pretty_print_review_and_label(i):\n",
    "   # A helper function to quickly print a specific review\n",
    "   # along with its label for a quick preview.\n",
    "   print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
    "\n",
    "# 1) READ REVIEWS\n",
    "g = open('reviews.txt','r') \n",
    "reviews = list(map(lambda x: x[:-1], g.readlines()))\n",
    "g.close()\n",
    "# - We open 'reviews.txt' and read each line into 'reviews'.\n",
    "# - 'x[:-1]' removes the newline character from the end of each line.\n",
    "# - 'reviews' is now a list of strings, each string is one full movie review.\n",
    "\n",
    "# 2) READ LABELS\n",
    "g = open('labels.txt','r')\n",
    "labels = list(map(lambda x: x[:-1].upper(), g.readlines()))\n",
    "g.close()\n",
    "# - We open 'labels.txt' and read each line into 'labels'.\n",
    "# - 'x[:-1]' removes the newline character from each line.\n",
    "# - '.upper()' turns each label into uppercase (e.g., \"positive\" -> \"POSITIVE\").\n",
    "# - 'labels' is now a list of strings (\"POSITIVE\" or \"NEGATIVE\").\n",
    "\n",
    "# You can optionally view a sample review with its label:\n",
    "# pretty_print_review_and_label(0)\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# PREPROCESS DATASET (Alternative method)\n",
    "# -------------------------------------------\n",
    "\n",
    "import sys\n",
    "\n",
    "# 1) Read the entire 'reviews.txt' into raw_reviews\n",
    "f = open('reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# 2) Read the entire 'labels.txt' into raw_labels\n",
    "f = open('labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# 3) TOKENIZE REVIEWS\n",
    "# Convert each line into a set of words split on spaces.\n",
    "# Using a set removes duplicate words in each review.\n",
    "tokens = list(map(lambda x: set(x.split(\" \")), raw_reviews))\n",
    "\n",
    "# 4) BUILD A GLOBAL VOCABULARY\n",
    "#  - We'll gather every unique word across all reviews.\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if len(word) > 0:\n",
    "            vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)\n",
    "# Convert from a set to a list so we can index each word.\n",
    "\n",
    "# 5) CREATE A MAPPING FROM WORD -> UNIQUE ID (INDEX)\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "# Now each word is assigned a fixed integer ID.\n",
    "\n",
    "# 6) CONVERT REVIEWS INTO LISTS OF WORD INDICES\n",
    "#    We'll remove duplicates by storing indices in a set, then convert to a list.\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            pass\n",
    "    input_dataset.append(list(set(sent_indices)))\n",
    "# 'input_dataset' is now a list of lists:\n",
    "# each inner list is the unique word indices for one review.\n",
    "\n",
    "# 7) CONVERT LABELS INTO BINARY TARGETS (1 or 0)\n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == 'positive\\n':\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)\n",
    "# Each review's label is 1 if 'positive', else 0 for 'negative'.\n",
    "\n",
    "# 'input_dataset' and 'target_dataset' are now ready for use in a RNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Surprising Power of Averaged Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights_0_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 1) Compute vector norms for each row in weights_0_1\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#    'weights_0_1' is assumed to be a (vocab_size, embedding_dim) array\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#    norms will contain the sum of squares of each word vector\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m norms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mweights_0_1\u001b[49m \u001b[38;5;241m*\u001b[39m weights_0_1, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# norms is now shape: (vocab_size,)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 2) Resize 'norms' to be a column vector (vocab_size, 1)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m norms\u001b[38;5;241m.\u001b[39mresize(norms\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights_0_1' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Compute vector norms for each row in weights_0_1\n",
    "#    'weights_0_1' is assumed to be a (vocab_size, embedding_dim) array\n",
    "#    norms will contain the sum of squares of each word vector\n",
    "norms = np.sum(weights_0_1 * weights_0_1, axis=1)\n",
    "# norms is now shape: (vocab_size,)\n",
    "\n",
    "# 2) Resize 'norms' to be a column vector (vocab_size, 1)\n",
    "norms.resize(norms.shape[0], 1)\n",
    "\n",
    "# 3) Multiply each row in 'weights_0_1' by its norm.\n",
    "#    This effectively \"normalizes\" or scales each word vector\n",
    "#    according to the magnitude we computed above.\n",
    "normed_weights = weights_0_1 * norms\n",
    "\n",
    "def make_sent_vect(words):\n",
    "    \"\"\"\n",
    "    Convert a list of words into a single averaged vector using 'normed_weights'.\n",
    "    1) Filter out any words not in 'word2index'.\n",
    "    2) Convert each valid word to its index using 'word2index'.\n",
    "    3) Average the rows from 'normed_weights' corresponding to these indices.\n",
    "    \"\"\"\n",
    "    # Gather indices for all words present in 'word2index'\n",
    "    indices = list(\n",
    "        map(lambda x: word2index[x], \n",
    "            filter(lambda x: x in word2index, words)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Return the mean (1D vector of embedding_dim) over those word vectors\n",
    "    return np.mean(normed_weights[indices], axis=0)\n",
    "\n",
    "# 4) Convert each tokenized review into a single vector:\n",
    "#    by averaging the normalized word vectors of the words it contains.\n",
    "reviews2vectors = list()\n",
    "for review in tokens:  # 'tokens' should be a list of tokenized reviews\n",
    "    reviews2vectors.append(make_sent_vect(review))\n",
    "\n",
    "# Convert 'reviews2vectors' from a list of arrays to a 2D numpy array\n",
    "reviews2vectors = np.array(reviews2vectors)\n",
    "# shape is now (num_reviews, embedding_dim)\n",
    "\n",
    "def most_similar_reviews(review):\n",
    "    \"\"\"\n",
    "    Given a list of words (e.g. ['boring','awful']),\n",
    "    1) Construct a sentence vector 'v' by averaging the 'normed_weights' of these words.\n",
    "    2) Compute dot product of 'v' with each row in 'reviews2vectors'.\n",
    "    3) Sort reviews by similarity score (largest dot product).\n",
    "    4) Return the top 3 most similar reviews.\n",
    "    \"\"\"\n",
    "    # Build a single vector from the query words\n",
    "    v = make_sent_vect(review)\n",
    "    \n",
    "    from collections import Counter\n",
    "    scores = Counter()\n",
    "    \n",
    "    # Dot product of 'reviews2vectors' (shape (num_reviews, embedding_dim))\n",
    "    # with the query vector 'v' (shape (embedding_dim,))\n",
    "    # yields a similarity score for each review\n",
    "    for i, val in enumerate(reviews2vectors.dot(v)):\n",
    "        scores[i] = val\n",
    "    \n",
    "    most_similar = list()\n",
    "    \n",
    "    # 'scores.most_common(3)' returns the 3 highest dot products\n",
    "    for idx, score in scores.most_common(3):\n",
    "        # Retrieve the first 40 characters of the raw review text\n",
    "        most_similar.append(raw_reviews[idx][0:40])\n",
    "    \n",
    "    return most_similar\n",
    "\n",
    "# Example usage:\n",
    "most_similar_reviews(['boring','awful'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices that Change Absolutely Nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([0.1,0.2,0.3])\n",
    "c = np.array([-1,-0.5,0])\n",
    "d = np.array([0,0,0])\n",
    "\n",
    "identity = np.eye(3)\n",
    "print(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "[0.1 0.2 0.3]\n",
      "[-1.  -0.5  0. ]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(a.dot(identity))\n",
    "print(b.dot(identity))\n",
    "print(c.dot(identity))\n",
    "print(d.dot(identity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 15 17]\n",
      "[13. 15. 17.]\n"
     ]
    }
   ],
   "source": [
    "this = np.array([2,4,6])\n",
    "movie = np.array([10,10,10])\n",
    "rocks = np.array([1,1,1])\n",
    "\n",
    "print(this + movie + rocks)\n",
    "print((this.dot(identity) + movie).dot(identity) + rocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x_):\n",
    "    # Convert 'x_' to at least a 2D array so operations work in batch mode\n",
    "    x = np.atleast_2d(x_)\n",
    "    # Exponentiate each element\n",
    "    temp = np.exp(x)\n",
    "    # Normalize each row so the sum of exponentials = 1\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "# 1) Initialize word vectors for a small vocabulary\n",
    "#    Each word is represented by a 3-dimensional vector (all zeros here for demonstration).\n",
    "word_vects = {}\n",
    "word_vects['yankees'] = np.array([[0.,0.,0.]])\n",
    "word_vects['bears']   = np.array([[0.,0.,0.]])\n",
    "word_vects['braves']  = np.array([[0.,0.,0.]])\n",
    "word_vects['red']     = np.array([[0.,0.,0.]])\n",
    "word_vects['socks']   = np.array([[0.,0.,0.]])\n",
    "word_vects['lose']    = np.array([[0.,0.,0.]])\n",
    "word_vects['defeat']  = np.array([[0.,0.,0.]])\n",
    "word_vects['beat']    = np.array([[0.,0.,0.]])\n",
    "word_vects['tie']     = np.array([[0.,0.,0.]])\n",
    "\n",
    "# 2) Create a random matrix 'sent2output' of shape (3, vocabulary_size)\n",
    "#    We'll use this to map a 3D hidden state to a probability distribution\n",
    "#    over the 9 words (via softmax).\n",
    "sent2output = np.random.rand(3, len(word_vects))\n",
    "\n",
    "# 3) Define a 3x3 identity matrix\n",
    "#    This matrix acts like a simple \"transition\" for the hidden state,\n",
    "#    effectively copying its current values without transformation.\n",
    "identity = np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "  0.11111111 0.11111111 0.11111111]]\n"
     ]
    }
   ],
   "source": [
    "# 4) Forward pass:\n",
    "#    a) 'layer_0' is the embedding for the first word 'red'\n",
    "layer_0 = word_vects['red']\n",
    "\n",
    "#    b) 'layer_1' = layer_0 * identity + the embedding for 'socks'\n",
    "#       In a typical RNN, you'd have something like:\n",
    "#       h_t = f(W * h_(t-1) + U * x_t), but here we're using the identity\n",
    "#       and direct addition of the next word vector for simplicity.\n",
    "layer_1 = layer_0.dot(identity) + word_vects['socks']\n",
    "\n",
    "#    c) 'layer_2' = layer_1 * identity + the embedding for 'defeat'\n",
    "#       We continue the same pattern, adding the next word's embedding.\n",
    "layer_2 = layer_1.dot(identity) + word_vects['defeat']\n",
    "\n",
    "# 5) Predict the distribution over vocabulary:\n",
    "#    - Multiply 'layer_2' (shape (1,3)) by 'sent2output' (shape (3,9)) => (1,9)\n",
    "#    - Apply softmax to get a probability distribution across the 9 words.\n",
    "pred = softmax(layer_2.dot(sent2output))\n",
    "\n",
    "print(pred)\n",
    "# 'pred' is a 1 x 9 array indicating the predicted probability for each of the 9 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we Backpropagate into this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have a target one-hot vector indicating the correct word is \"yankees\"\n",
    "y = np.array([1,0,0,0,0,0,0,0,0]) \n",
    "\n",
    "# 'pred' is the predicted probability distribution from the forward pass\n",
    "# 'pred_delta' is the gradient of the output w.r.t. the loss\n",
    "# If we consider a simple cross-entropy-like approach: gradient = (pred - y)\n",
    "pred_delta = pred - y\n",
    "\n",
    "# 'layer_2_delta': we backprop through 'sent2output', so we multiply 'pred_delta' by\n",
    "# the transpose of 'sent2output' to see how changes in the hidden state (layer_2) \n",
    "# would affect the final output.\n",
    "layer_2_delta = pred_delta.dot(sent2output.T)\n",
    "\n",
    "# 'defeat_delta' is the gradient w.r.t. the word vector 'defeat'.\n",
    "# We multiply by 1 because there's no additional transformation after adding the word vector\n",
    "# (like an activation derivative).\n",
    "defeat_delta = layer_2_delta * 1 \n",
    "\n",
    "# 'layer_1_delta' is how the gradient flows back from layer_2 to layer_1\n",
    "# through the identity matrix. Multiplying by identity.T just returns the same vector.\n",
    "layer_1_delta = layer_2_delta.dot(identity.T)\n",
    "\n",
    "# Similarly for the 'socks' vector gradient.\n",
    "socks_delta = layer_1_delta * 1\n",
    "\n",
    "# 'layer_0_delta' flows further back from layer_1 to layer_0\n",
    "# once again dot with identity.T.\n",
    "layer_0_delta = layer_1_delta.dot(identity.T)\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.01\n",
    "\n",
    "# 1) Update the word vectors:\n",
    "#    Subtract the respective deltas scaled by 'alpha' from the word embeddings\n",
    "word_vects['red'] -= layer_0_delta * alpha\n",
    "word_vects['socks'] -= socks_delta * alpha\n",
    "word_vects['defeat'] -= defeat_delta * alpha\n",
    "\n",
    "# 2) Update the identity matrix:\n",
    "#    Because we used layer_0 and layer_1 in the forward pass with a dot product,\n",
    "#    we do an outer product of (input, gradient) for each step:\n",
    "identity -= np.outer(layer_0, layer_1_delta) * alpha\n",
    "identity -= np.outer(layer_1, layer_2_delta) * alpha\n",
    "\n",
    "# 3) Update the 'sent2output' matrix:\n",
    "#    The forward pass used layer_2.dot(sent2output), so the gradient w.r.t. sent2output\n",
    "#    is the outer product of (layer_2, pred_delta).\n",
    "sent2output -= np.outer(layer_2, pred_delta) * alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', 'moved', 'to', 'the', 'bathroom.'], ['john', 'went', 'to', 'the', 'hallway.'], ['where', 'is', 'mary?', '\\tbathroom\\t1']]\n"
     ]
    }
   ],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# 1) We load a subset of the bAbI tasks dataset\n",
    "#    'qa1_single-supporting-fact_train.txt' is a set of simple question-answer pairs.\n",
    "\n",
    "f = open('tasksv11/en/qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    # We take only the first 1000 lines\n",
    "    # Convert each line to lowercase, strip newline, split on spaces.\n",
    "    # Also skip the first token which is usually a numeric id in bAbI.\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
    "\n",
    "print(tokens[0:3])\n",
    "# A quick look at the first 3 tokenized lines (for sanity checking).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)\n",
    "# Convert the set to a list so we can index the words consistently.\n",
    "\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "# A mapping from each word to a unique integer (row index in embeddings).\n",
    "\n",
    "def words2indices(sentence):\n",
    "    \"\"\"\n",
    "    Convert a list of words into a list of integer indices \n",
    "    using 'word2index'.\n",
    "    \"\"\"\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Standard softmax function: exponentiate and normalize by sum of exps.\n",
    "    - x can be 1D or 2D, but typically we handle 1D input here.\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))  # subtract max for numerical stability\n",
    "    return e_x / e_x.sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "embed_size = 10\n",
    "\n",
    "# word embeddings: shape (vocab_size, embed_size)\n",
    "# each row is an embedding vector for one word\n",
    "embed = (np.random.rand(len(vocab), embed_size) - 0.5) * 0.1\n",
    "\n",
    "# recurrent: shape (embed_size, embed_size), \n",
    "# transforms one hidden state into the next\n",
    "# (initially the identity is used in some examples, but we do random or identity)\n",
    "recurrent = np.eye(embed_size)\n",
    "\n",
    "# 'start' is the sentence embedding for an empty sentence\n",
    "start = np.zeros(embed_size)\n",
    "\n",
    "# decoder: shape (embed_size, vocab_size),\n",
    "# maps from hidden state to a distribution over words\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1\n",
    "\n",
    "# one_hot: shape (vocab_size, vocab_size)\n",
    "# identity matrix used for converting word indices into one-hot vectors\n",
    "one_hot = np.eye(len(vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation with Arbitrary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sent):\n",
    "    \"\"\"\n",
    "    Predict function:\n",
    "    1. Start from 'start' hidden state.\n",
    "    2. For each word in 'sent', predict the *next* word distribution \n",
    "       using the current hidden state, then compute loss wrt. the true next word.\n",
    "    3. Update hidden state by applying 'recurrent' transform plus the embedding \n",
    "       for the actual word (like a simplified RNN).\n",
    "    4. Return a list of 'layers' (one per step) and the total cross-entropy loss.\n",
    "    \"\"\"\n",
    "\n",
    "    layers = list()\n",
    "    layer = {}\n",
    "    layer['hidden'] = start\n",
    "    layers.append(layer)\n",
    "\n",
    "    loss = 0\n",
    "    preds = list()\n",
    "\n",
    "    # forward propagate over each word in 'sent'\n",
    "    for target_i in range(len(sent)):\n",
    "\n",
    "        layer = {}\n",
    "\n",
    "        # 1) Predict next word distribution: \n",
    "        #    hidden_state.dot(decoder) => unnormalized scores, \n",
    "        #    then softmax => probabilities\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))\n",
    "\n",
    "        # 2) Compute cross-entropy loss = -log(prob of true next word)\n",
    "        #    'sent[target_i]' is the \"actual\" word index at this step.\n",
    "        loss += -np.log(layer['pred'][sent[target_i]])\n",
    "\n",
    "        # 3) Generate the next hidden state: \n",
    "        #    old_hidden.dot(recurrent) + embed for the current word\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[sent[target_i]]\n",
    "\n",
    "        layers.append(layer)\n",
    "\n",
    "    return layers, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation with Arbitrary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "for iter in range(30000):\n",
    "    alpha = 0.001\n",
    "    # We pick a sentence from 'tokens' in a cyclical manner\n",
    "    sent = words2indices(tokens[iter % len(tokens)][1:])\n",
    "    # skip the first word in that line (like the bAbI ID or something)\n",
    "\n",
    "    # 1) Forward pass\n",
    "    layers, loss = predict(sent)\n",
    "\n",
    "    # 2) Backpropagate\n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        # current layer\n",
    "        layer = layers[layer_idx]\n",
    "        # the 'true' word for this time step\n",
    "        target = sent[layer_idx - 1]  \n",
    "        # (layer_idx-1) because the first layer's \"next word\" is the first item in 'sent'\n",
    "\n",
    "        if(layer_idx > 0):\n",
    "            # The output delta = predicted distribution - one_hot_vector_of_true_word\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
    "            # 'new_hidden_delta': how changes in hidden affect the output\n",
    "            # multiply by decoder transpose\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
    "\n",
    "            # If this is the last layer, no next layer delta\n",
    "            # otherwise add the next layer's hidden delta \n",
    "            # multiplied by the transpose of recurrent.\n",
    "            if(layer_idx == len(layers) - 1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + \\\n",
    "                    layers[layer_idx + 1]['hidden_delta'].dot(recurrent.transpose())\n",
    "        else:\n",
    "            # For the first layer, we don't have an 'output_delta' (no pred for \"previous\" word)\n",
    "            # so we just gather hidden delta from the next layer\n",
    "            layer['hidden_delta'] = layers[layer_idx + 1]['hidden_delta'].dot(recurrent.transpose())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Update with Arbitrary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:82.07190057702232\n",
      "Perplexity:81.99201065315157\n",
      "Perplexity:81.90350186145962\n",
      "Perplexity:81.77235882739159\n",
      "Perplexity:81.54058043945564\n",
      "Perplexity:81.09214329745042\n",
      "Perplexity:80.16498978212181\n",
      "Perplexity:78.05384313423644\n",
      "Perplexity:72.10587993172675\n",
      "Perplexity:45.039919968348315\n",
      "Perplexity:23.808985116122862\n",
      "Perplexity:19.983231768605688\n",
      "Perplexity:18.701956753189645\n",
      "Perplexity:17.485693470844865\n",
      "Perplexity:15.850678254758108\n",
      "Perplexity:13.419442755410344\n",
      "Perplexity:10.29560845031423\n",
      "Perplexity:7.887729550068891\n",
      "Perplexity:6.646312421017693\n",
      "Perplexity:5.851879733939698\n",
      "Perplexity:5.286195672699819\n",
      "Perplexity:4.946446677849613\n",
      "Perplexity:4.734344492498793\n",
      "Perplexity:4.5860018979698705\n",
      "Perplexity:4.486785221893155\n",
      "Perplexity:4.425031891935898\n",
      "Perplexity:4.378844800385847\n",
      "Perplexity:4.326100599939156\n",
      "Perplexity:4.2550389484900535\n",
      "Perplexity:4.162303465910049\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "for iter in range(30000):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iter % len(tokens)][1:])\n",
    "    \n",
    "    layers, loss = predict(sent)\n",
    "\n",
    "    # ... backprop code ...\n",
    "\n",
    "    # update weights\n",
    "    start -= layers[0]['hidden_delta'] * alpha / float(len(sent))\n",
    "    # 'start' is the initial hidden state. \n",
    "    # We subtract the gradient scaled by alpha and the length of the sentence.\n",
    "\n",
    "    for layer_idx, layer in enumerate(layers[1:]):\n",
    "        # 1) Update 'decoder':\n",
    "        #    outer product of (the previous layer's hidden state) and (output_delta).\n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) \\\n",
    "                   * alpha / float(len(sent))\n",
    "\n",
    "        # 2) Update word embedding for the word 'embed_idx' at this step\n",
    "        embed_idx = sent[layer_idx]\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] \\\n",
    "                            * alpha / float(len(sent))\n",
    "\n",
    "        # 3) Update recurrent:\n",
    "        #    outer product of (previous layer's hidden) and (this layer's hidden_delta).\n",
    "        #    again scaled by alpha and sentence length.\n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) \\\n",
    "                     * alpha / float(len(sent))\n",
    "\n",
    "    if (iter % 1000) == 0:\n",
    "        print(\"Perplexity:\" + str(np.exp(loss / len(sent))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution and Output Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sandra', 'moved', 'to', 'the', 'garden.']\n",
      "Prev Input:sandra      True:moved          Pred:is\n",
      "Prev Input:moved       True:to             Pred:to\n",
      "Prev Input:to          True:the            Pred:the\n",
      "Prev Input:the         True:garden.        Pred:bedroom.\n"
     ]
    }
   ],
   "source": [
    "sent_index = 4\n",
    "\n",
    "# 'predict' returns (layers, loss).\n",
    "l, _ = predict(words2indices(tokens[sent_index]))\n",
    "\n",
    "print(tokens[sent_index])\n",
    "\n",
    "# Print each predicted word vs. the true word for each step in the sentence\n",
    "for i, each_layer in enumerate(l[1:-1]):\n",
    "    input_word = tokens[sent_index][i]\n",
    "    true_word = tokens[sent_index][i+1]\n",
    "    \n",
    "    # pick the predicted word index as argmax of layer['pred']\n",
    "    pred_word = vocab[each_layer['pred'].argmax()]\n",
    "    \n",
    "    print(\"Prev Input:\" + input_word + (' ' * (12 - len(input_word))) +\\\n",
    "          \"True:\" + true_word + (\" \" * (15 - len(true_word))) + \"Pred:\" + pred_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
