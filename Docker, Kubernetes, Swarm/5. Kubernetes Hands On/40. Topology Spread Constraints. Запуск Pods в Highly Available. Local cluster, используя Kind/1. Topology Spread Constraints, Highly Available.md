# Topology Spread Constraints. Запуск Pods в Highly Available. Local cluster, используя Kind

Topology Spread Constraints помогают нам распределить поды таким образом, что если они вышли из строя в одной
availability зоне то в других зонах они оставались доступными.

Кластер представляет собой 3 ondemand ноды и 4 спотовые.

![img.png](images/img.png)

Распределение по availability зонам

![img_1.png](images/img_1.png)

Теперь рассмотрим первый пример k8s/example-1 который представляет собой самый простой деплоймент без дополнительных
настроек.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: az-spread
  labels:
    app: kuber
spec:
  replicas: 45
  selector:
    matchLabels:
      app: http-server
  template:
    metadata:
      labels:
        app: http-server
    spec:
      containers:
        - name: kuber-app
          image: bakavets/kuber
          ports:
            - containerPort: 8000
```

    kubectl apply -f k8s/example-1/az-spread.yaml

Затем при помощи команды ниже посмотрим на распределение по availability зонам и хостам.

```for node in $(kubectl get po -o wide | grep -v NODE | awk '{print $7}'); do kubectl get node $node -L topology.kubernetes.io/zone,node.k8s/role | tail -n +2 | awk '{ print $1, "\033[32m" $6 "\033[0m","\033[35m" $7 "\033[0m"}'; done | sort | uniq -c```

![img_2.png](images/img_2.png)

Как мы видим распределение далеко от равномерного. Так что перейдем ко второму примеру, в нем мы уже будем использовать
nodeAffinity, а так же podAntiAffinity для распределения подов по разным availability зонам.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: az-spread-pod-anti-affinity
  labels:
    app: kuber
spec:
  replicas: 5
  selector:
    matchLabels:
      app: http-server
  template:
    metadata:
      labels:
        app: http-server
    spec:
      containers:
        - name: kuber-app
          image: bakavets/kuber
          ports:
            - containerPort: 8000
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node.k8s/role
                    operator: In
                    values:
                      - management
                      - app-worker
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - http-server
              topologyKey: topology.kubernetes.io/zone
```

Это так же не решает нашу проблему так как мы хотим поднять 5 подов, а availability зон у нас всего 3 то и поднимется
всего 3 пода из-за использования hardLimit.

В 3 примере мы будем использовать softLimit для podAntiAffinity и постараемся развернуть 21 реплику.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: az-spread-pod-anti-affinity-soft
  labels:
    app: kuber
spec:
  replicas: 21
  selector:
    matchLabels:
      app: http-server
  template:
    metadata:
      labels:
        app: http-server
        version: v1
    spec:
      containers:
        - name: kuber-app
          image: bakavets/kuber
          ports:
            - containerPort: 8000
          resources:
            limits:
              cpu: 400m
              memory: 256Mi
            requests:
              cpu: 400m
              memory: 256Mi
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node.k8s/role
                    operator: In
                    values:
                      - management
                      - app-worker
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - http-server
                topologyKey: topology.kubernetes.io/zone
```

Как видим результаты по availability зонам стал намного лучше, в каждой по 7 реплик.

![img_3.png](images/img_3.png)

Но это тоже не совсем то что нам надо поскольку при попытке обновления такого приложения поды опять могут распределиться
неравномерно из-за того что обновление происходит неравномерно и где-то где старые поды умерли быстрее новых будет
больше.

Теперь воспользуемся topologySpreadConstraints

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: az-spread-topology
  labels:
    app: kuber
spec:
  replicas: 21
  selector:
    matchLabels:
      app: http-server
  template:
    metadata:
      labels:
        app: http-server
        version: v1
    spec:
      containers:
        - name: kuber-app
          image: bakavets/kuber
          ports:
            - containerPort: 8000
          resources:
            limits:
              cpu: 400m
              memory: 256Mi
            requests:
              cpu: 400m
              memory: 256Mi
      topologySpreadConstraints:
        - maxSkew: 1 # Максимальное отклонение между зонами которые определяет topologyKey
          topologyKey: topology.kubernetes.io/zone # зоны
          whenUnsatisfiable: DoNotSchedule # ScheduleAnyway. Что делать если не получилось разместить
          # в данном случае поды просто будут висеть в Pending
          labelSelector: # Селектор Нод
            matchLabels:
              app: http-server
```

Теперь видим что распределение стало более равномерным. Но стоит не забывать про некоторые ограничения
topologySpreadConstraints которые указанны в документации
(https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/), а так же issue которые
перечисленны в файле README.

![img_4.png](images/img_4.png)

Еще одной полезной фичей для topologySpreadConstraints является matchLabelKeys — это список ключей меток модулей для
выбора модулей, по которым будет рассчитываться распространение. Ключи используются для поиска значений в метках
модулей, эти метки значений ключа объединяются с помощью оператора AND с помощью labelSelector, чтобы выбрать группу
существующих модулей, по которым будет рассчитываться распространение для входящего модуля. 



