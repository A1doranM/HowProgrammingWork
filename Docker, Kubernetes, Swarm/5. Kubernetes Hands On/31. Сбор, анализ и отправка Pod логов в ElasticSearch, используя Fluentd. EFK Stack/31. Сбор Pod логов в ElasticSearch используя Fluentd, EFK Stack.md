# Сбор, анализ и отправка Pod логов в ElasticSearch, используя Fluentd. EFK Stack

Fluentd - это сборщик данных для унифицированного сбора и ведения логов.

Вначале идет часть про Терраформ и разворачивание кластера с установкой Lens

(https://www.youtube.com/watch?v=d7IATODGxUI&list=PL3SzV1_k2H1VDePbSWUqERqlBXIk02wCQ&index=35)

После этого нам надо установить Elastic Search и Kibana - интерфес для эластика.

    helm repo add elastic https://helm.elastic.co

    helm upgrade --install elasticsearch --version 7.17.3 elastic/elasticsearch --set replicas=1 -n elastic --create-namespace --debug

    helm upgrade --install kibana --version 7.17.3 elastic/kibana -n elastic --debug

Затем выполнив порт форвард для кибаны

![img.png](img.png)

Зайдя на адрес кибаны увидим что ElasticSearch установился

![img_1.png](img_1.png)

Теперь задеплоим в кластер наше первое приложение которое просто будет спамить логи

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: counter-1
  labels:
    app: counter-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: counter-1
  template:
    metadata:
      labels:
        app: counter-1
    spec:
      containers:
        - name: count
          image: busybox
          # Спам логов
          args: [ /bin/sh, -c, 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done' ]
```

Деплой

    kubectl apply -f apps/deploy-1.yaml

Теперь установим Fluentd используя DaemonSet так как мы хотим чтобы Fluentd располагались на каждой ноде кластера.

Для начала создадим Namespace

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: fluentd
```

    kubectl apply -f fluentd/namespace.yaml

После чего создадим ServiceAccount и ClusterRole c ClusterRoleBinding (подробнее рассказано в уроке 34)

```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: fluentd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - namespaces
    verbs:
      - get
      - list
      - watch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
subjects:
  - kind: ServiceAccount
    name: fluentd
    namespace: fluentd
```

     kubectl apply -f fluentd/rbac.yaml

Затем создадим DaemonSet ниже по ссылке с 6 по 10 минуту разбор параметров контейнера.

(https://youtu.be/d7IATODGxUI?list=PL3SzV1_k2H1VDePbSWUqERqlBXIk02wCQ&t=545)

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: fluentd # Устанавливаем ранее созданный namespace 
spec:
  selector: # Определяем селектор который будет выбирать поды с fluentd
    matchLabels:
      k8s-app: fluentd-logging
      version: v1
  template: # Определяем шаблон для fluentd подов
    metadata:
      labels:
        k8s-app: fluentd-logging
        version: v1
    spec:
      # Указываем ServiceAccount для нашего DaemonSet он будет добавлен к поду 
      serviceAccount: fluentd
      serviceAccountName: fluentd
      containers: # И описываем контейнер c fluentd который будет запущен в подах 
        - name: fluentd
          image: fluent/fluentd-kubernetes-daemonset:v1.14-debian-elasticsearch7-1
          env:
            - name: FLUENT_ELASTICSEARCH_HOST
              value: "elasticsearch-master.elastic.svc.cluster.local"
            - name: FLUENT_ELASTICSEARCH_PORT
              value: "9200"
            - name: FLUENT_ELASTICSEARCH_SCHEME
              value: "http"
            # If the name of the Kubernetes node the plugin is running on is set as an environment variable with the name K8S_NODE_NAME, it will reduce cache misses and needless calls to the Kubernetes API.
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # - name: FLUENT_CONTAINER_TAIL_EXCLUDE_PATH
            #   value: >
            #     [
            #     "/var/log/containers/fluentd-*",
            #     "/var/log/containers/kube-proxy-*"
            #     ]
          resources:
            limits:
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 200Mi
          # Указываем папки которые хотим вмонтировать внутрь каждого fluentd агента.
          volumeMounts:
            - name: fluentd-config
              mountPath: /fluentd/etc
            - name: varlog
              mountPath: /var/log
            - name: dockercontainerlogdirectory
              mountPath: /var/lib/docker/containers
              readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
        - name: fluentd-config
          configMap:
            name: fluentd-config
        - name: varlog
          hostPath:
            path: /var/log
        - name: dockercontainerlogdirectory
          hostPath:
            path: /var/lib/docker/containers
```

Перед деплоем этого фала стоит так же задеплоить configmap.yaml

    kubectl apply -f fluentd/configmap.yaml

    kubectl apply -f fluentd/daemonset-elasticsearch.yaml

